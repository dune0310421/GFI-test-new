[
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1,
    "reporter": "wesm",
    "created_at": "2010-09-29T00:45:31+00:00",
    "closed_at": "2011-02-19T23:13:48+00:00",
    "resolver": "adamklein",
    "resolved_in": "6a6f3b7b9a2567ad04520291a5f9cd657598e3bf",
    "resolver_commit_num": 146,
    "title": "Enable element-wise comparison operations in DataMatrix objects",
    "body": "re: pystatsmodels e-mail\n\nhi everyone,\n\njust getting started with pandas and i was wondering if someone could\nhelp me out.  do pandas.DataMatrix objects support per item comparison\noperations?\n\ni have a two data matrices, and i want to do something like this:\n\ndiv[div > 0.5 \\* price] = 0\n\nthis would work if div and price were numpy.ndarray objects.  any idea\nhow i would do something like this with pandas.DataMatrix objects?\n\nthanks,\nandy\n",
    "labels": [],
    "comments": [
      "implemented in git HEAD\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 1,
    "additions": 1,
    "deletions": 1,
    "changed_files_list": [
      "pandas/tests/test_format.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10,
    "reporter": "wesm",
    "created_at": "2010-09-30T22:33:14+00:00",
    "closed_at": "2011-06-23T19:38:51+00:00",
    "resolver": "jreback",
    "resolved_in": "9c9956621d36bb881fa0372402b01bdd65df46f4",
    "resolver_commit_num": 3123,
    "title": "Improvements to pandas.io.pytables / unit testing",
    "body": "Need to incorporate the selection of ranges of data and write unit tests\n",
    "labels": [],
    "comments": [
      "Done in latest git HEAD\n"
    ],
    "events": [],
    "changed_files": 3,
    "additions": 11,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.16.1.txt",
      "pandas/core/nanops.py",
      "pandas/tseries/tests/test_timedeltas.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 134,
    "reporter": "wesm",
    "created_at": "2011-09-13T01:27:21+00:00",
    "closed_at": "2012-03-13T22:15:52+00:00",
    "resolver": "wesm",
    "resolved_in": "041d2a3a67344ebcdb7a8ce3c44a8a6fd95506dd",
    "resolver_commit_num": 1589,
    "title": "Partial multiple selection in advanced indexing",
    "body": "\n\nWant to be able to do\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 7,
    "additions": 87,
    "deletions": 18,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/core/index.py",
      "pandas/core/indexing.py",
      "pandas/core/panel.py",
      "pandas/core/series.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 152,
    "reporter": "wesm",
    "created_at": "2011-09-17T21:36:13+00:00",
    "closed_at": "2012-04-06T03:31:01+00:00",
    "resolver": "wesm",
    "resolved_in": "63207b83fc35c2ade37861ca581949d95f63ad61",
    "resolver_commit_num": 1707,
    "title": "Write fast OHLC aggregator",
    "body": "To work with GroupBy\n",
    "labels": [],
    "comments": [
      "speed of current OHLC function can be improved:\n\n```\ndef translate_grouping(how):\n    if set(how) == set('ohlc'):\n        return {'open'  : lambda arr: arr[0],\n                'low'   : lambda arr: arr.min(),\n                'high'  : lambda arr: arr.max(),\n                'close' : lambda arr: arr[-1]}\n\n    if how in 'last':\n        def picker(arr):\n            return arr[-1] if arr is not None and len(arr) else np.nan\n        return picker\n\n    raise ValueError(\"Unrecognized method: %s\" % how)\n```\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "assigned",
      "commented",
      "referenced"
    ],
    "changed_files": 82,
    "additions": 16580,
    "deletions": 2757,
    "changed_files_list": [
      ".gitignore",
      "NP_LICENSE.txt",
      "bench/bench_dense_to_sparse.py",
      "bench/bench_tseries.py",
      "pandas/core/api.py",
      "pandas/core/common.py",
      "pandas/core/daterange.py",
      "pandas/core/datetools.py",
      "pandas/core/format.py",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/indexing.py",
      "pandas/core/internals.py",
      "pandas/core/nanops.py",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/info.py",
      "pandas/io/parsers.py",
      "pandas/io/pytables.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/io/tests/test_pytables.py",
      "pandas/sparse/frame.py",
      "pandas/sparse/panel.py",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_sparse.py",
      "pandas/src/data_algos.pyx",
      "pandas/src/datetime.pxd",
      "pandas/src/datetime.pyx",
      "pandas/src/engines.pyx",
      "pandas/src/generate_code.py",
      "pandas/src/generated.pyx",
      "pandas/src/groupby.pyx",
      "pandas/src/hashtable.pxd",
      "pandas/src/hashtable.pyx",
      "pandas/src/inference.pyx",
      "pandas/src/khash.pxd",
      "pandas/src/np_datetime.c",
      "pandas/src/np_datetime.h",
      "pandas/src/np_datetime_strings.c",
      "pandas/src/np_datetime_strings.h",
      "pandas/src/numpy_helper.h",
      "pandas/src/reduce.pyx",
      "pandas/src/reindex.pyx",
      "pandas/src/sandbox.pyx",
      "pandas/src/skts.c",
      "pandas/src/skts.h",
      "pandas/src/tseries.pyx",
      "pandas/src/util.pxd",
      "pandas/stats/tests/common.py",
      "pandas/stats/tests/test_math.py",
      "pandas/stats/tests/test_moments.py",
      "pandas/stats/tests/test_ols.py",
      "pandas/tests/data/frame.pickle",
      "pandas/tests/data/series.pickle",
      "pandas/tests/test_daterange.py",
      "pandas/tests/test_datetools.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_graphics.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_interval.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_series.py",
      "pandas/tests/test_timeseries.py",
      "pandas/tests/test_tseries.py",
      "pandas/tools/merge.py",
      "pandas/tools/plotting.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tools/tests/test_pivot.py",
      "pandas/tools/tsplotting.py",
      "pandas/tseries/__init__.py",
      "pandas/tseries/tests/__init__.py",
      "pandas/tseries/tests/test_tools.py",
      "pandas/tseries/tools.py",
      "pandas/util/testing.py",
      "setup.py",
      "test.sh",
      "ts_todo.txt",
      "vb_suite/sparse.py",
      "vb_suite/timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 171,
    "reporter": "wesm",
    "created_at": "2011-09-25T15:04:15+00:00",
    "closed_at": "2012-07-12T20:59:01+00:00",
    "resolver": "wesm",
    "resolved_in": "2c44f5772407479b30040f41aef4f1bdea3c3736",
    "resolver_commit_num": 2207,
    "title": "Add Panel.dropna method of some kind",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 68,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/panel.py",
      "pandas/tests/test_panel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 184,
    "reporter": "wesm",
    "created_at": "2011-09-29T01:29:57+00:00",
    "closed_at": "2012-05-15T00:15:43+00:00",
    "resolver": "wesm",
    "resolved_in": "de66b56d865fdf74d19d876696b4b80a1ad5267e",
    "resolver_commit_num": 1897,
    "title": "Consistently set group name on groupby groups",
    "body": "Currently not done consistently, a couple of functions do it but it should be done everywhere preferably...\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 26,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 208,
    "reporter": "wesm",
    "created_at": "2011-10-09T14:11:38+00:00",
    "closed_at": "2012-04-13T01:45:48+00:00",
    "resolver": "wesm",
    "resolved_in": "63207b83fc35c2ade37861ca581949d95f63ad61",
    "resolver_commit_num": 1707,
    "title": "Datetime string parser integration with Index class",
    "body": "`Index.to_datetime` or something like that to convert Index containing parsable strings to `DatetimeIndex`\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 82,
    "additions": 16580,
    "deletions": 2757,
    "changed_files_list": [
      ".gitignore",
      "NP_LICENSE.txt",
      "bench/bench_dense_to_sparse.py",
      "bench/bench_tseries.py",
      "pandas/core/api.py",
      "pandas/core/common.py",
      "pandas/core/daterange.py",
      "pandas/core/datetools.py",
      "pandas/core/format.py",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/indexing.py",
      "pandas/core/internals.py",
      "pandas/core/nanops.py",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/info.py",
      "pandas/io/parsers.py",
      "pandas/io/pytables.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/io/tests/test_pytables.py",
      "pandas/sparse/frame.py",
      "pandas/sparse/panel.py",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_sparse.py",
      "pandas/src/data_algos.pyx",
      "pandas/src/datetime.pxd",
      "pandas/src/datetime.pyx",
      "pandas/src/engines.pyx",
      "pandas/src/generate_code.py",
      "pandas/src/generated.pyx",
      "pandas/src/groupby.pyx",
      "pandas/src/hashtable.pxd",
      "pandas/src/hashtable.pyx",
      "pandas/src/inference.pyx",
      "pandas/src/khash.pxd",
      "pandas/src/np_datetime.c",
      "pandas/src/np_datetime.h",
      "pandas/src/np_datetime_strings.c",
      "pandas/src/np_datetime_strings.h",
      "pandas/src/numpy_helper.h",
      "pandas/src/reduce.pyx",
      "pandas/src/reindex.pyx",
      "pandas/src/sandbox.pyx",
      "pandas/src/skts.c",
      "pandas/src/skts.h",
      "pandas/src/tseries.pyx",
      "pandas/src/util.pxd",
      "pandas/stats/tests/common.py",
      "pandas/stats/tests/test_math.py",
      "pandas/stats/tests/test_moments.py",
      "pandas/stats/tests/test_ols.py",
      "pandas/tests/data/frame.pickle",
      "pandas/tests/data/series.pickle",
      "pandas/tests/test_daterange.py",
      "pandas/tests/test_datetools.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_graphics.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_interval.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_series.py",
      "pandas/tests/test_timeseries.py",
      "pandas/tests/test_tseries.py",
      "pandas/tools/merge.py",
      "pandas/tools/plotting.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tools/tests/test_pivot.py",
      "pandas/tools/tsplotting.py",
      "pandas/tseries/__init__.py",
      "pandas/tseries/tests/__init__.py",
      "pandas/tseries/tests/test_tools.py",
      "pandas/tseries/tools.py",
      "pandas/util/testing.py",
      "setup.py",
      "test.sh",
      "ts_todo.txt",
      "vb_suite/sparse.py",
      "vb_suite/timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 288,
    "reporter": "lbeltrame",
    "created_at": "2011-10-25T14:16:59+00:00",
    "closed_at": "2012-01-23T15:13:00+00:00",
    "resolver": "adamklein",
    "resolved_in": "3ce01cfd901770d9b3d76d2904138d26352a5c56",
    "resolver_commit_num": 78,
    "title": "R-like range() function",
    "body": "In R, if I supply range(dataframe), I get back a vector of the minimum value in all the dataframe and the maximum value in all the dataframe. \n\nDataFrame's min() and max() functions operate only on single columns. The way to do it currently is\n\n\n\nIt's not a critical limitation, but it is convenient to have at times.\n",
    "labels": [],
    "comments": [
      "What about `df.describe()`?\n",
      "I looked into that: it returns a new DataFrame with the various statistics separated for each column. I was more interested in \"global\" (df-wide) values. I understand however that with mixed-type colums this may be a problem.\n",
      "Ah, I got it. Should just start a tools module and put things like this there. I don't think it merits an instance method. Do you think it should return a Series?\n",
      "In my opinion yes, a Series would do perfectly.\n",
      "@wesm any other useful functions to put in here? no sense in you spending time merging for 4 lines of code.\n",
      "feel free to just accumulate things in a branch and I'll merge them whenever. i'm about to put the kibosh on anymore changes to 0.5.1, do a little more sanity checking, fix up docs, then cut the release\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "subscribed",
      "commented",
      "mentioned",
      "commented"
    ],
    "changed_files": 3,
    "additions": 38,
    "deletions": 0,
    "changed_files_list": [
      "pandas/__init__.py",
      "pandas/tests/test_tools.py",
      "pandas/tools/describe.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 292,
    "reporter": "wesm",
    "created_at": "2011-10-26T04:09:46+00:00",
    "closed_at": "2012-06-05T19:38:42+00:00",
    "resolver": "wesm",
    "resolved_in": "ea1186da6a892ad9b66939c605554f4511816bbd",
    "resolver_commit_num": 2049,
    "title": "Add support for ordered factors and groupby handling",
    "body": "",
    "labels": [],
    "comments": [
      "Right now, I want things like \"give a nice binning for True / False, rather than histogramming\" .  Is that a subset of this bug?\n",
      "This might be a good recipe for the the bar charts for ordered factors...\n\nhttp://www.scipy.org/Cookbook/Matplotlib/BarCharts\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 45,
    "deletions": 4,
    "changed_files_list": [
      "pandas/core/factor.py",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 350,
    "reporter": "lbeltrame",
    "created_at": "2011-11-08T09:56:11+00:00",
    "closed_at": "2012-05-14T15:05:24+00:00",
    "resolver": "lbeltrame",
    "resolved_in": "eecc018030602be726dab958dcf8c7cb96c4e4ab",
    "resolver_commit_num": 4,
    "title": "Conversion of DataFrame to R's data.frame",
    "body": "Although I have already produced code for this (see below) I'm posting this as an issue rather than a pull request to discuss the design, because there are some issues open in my code:\n- Series of dtype object need an explicit cast or rpy2's numpy conversion will treat them improperly\n- The performance has not been profiled\n- Probably some room for optimizations\n- Proper name for the function\n- The generation of an intermediate OrdDict object may cause problems in case of very large datasets\n\nThe code in the current form is posted below. If there is interest, I will work towards integrating it in pandas.rpy.common and add unit tests.\n\n\n",
    "labels": [],
    "comments": [
      "I've been experimenting with alternative solutions so far, but at the moment most of them, save for the dict intermediate, are horribly slow.\n",
      "I think I might get some code to merge in for 0.8.0, but I'd need to adapt my unit tests (which use unittest) to pandas. Is there any document on how unit tests are handled in pandas and any guidelines to follow?\n",
      "Progress: the current implementation (not integrated in my pandas clone yet as I have no idea on how to handle things like MultiIndex, which I don't use in my normal workflow). The main advantage is that now nans are translated to R's NA:\n\n``` python\nimport numpy as np\nimport rpy2.robjects as robjects\nfrom rpy2.robjects.packages import importr\nimport rpy2.rlike.container as rlc\n\ndef dataset_to_data_frame(dataset, strings_as_factors=True):\n\n    base = importr(\"base\")\n    columns = rlc.OrdDict()\n\n    # Type casting is more efficient than rpy2's own numpy2ri\n\n    vectors = {np.float64: robjects.FloatVector,\n               np.float32: robjects.FloatVector,\n               np.float: robjects.FloatVector,\n               np.int: robjects.IntVector,\n               np.object_: robjects.StrVector,\n               np.str: robjects.StrVector}\n\n    columns = rlc.OrdDict()\n\n    for column in dataset:\n        value = dataset[column]\n        value = vectors[value.dtype.type](value)\n\n        # These SHOULD be fast as they use vector operations\n\n        if isinstance(value, robjects.StrVector):\n            value.rx[value.ro == \"nan\"] = robjects.NA_Character\n        else:\n            value.rx[base.is_nan(value)] = robjects.NA_Logical\n\n        if not strings_as_factors:\n            value = base.I(value)\n\n        columns[column] = value\n\n    dataframe = robjects.DataFrame(columns)\n\n    del columns\n\n    dataframe.rownames = robjects.StrVector(dataset.index)\n\n    return dataframe\n```\n",
      "I'm an rpy2 user. This is what I'm using to go between pandas and rpy2:\n\n```\nimport numpy as np\nimport rpy2.robjects as robj\nimport rpy2.rlike.container as rlc\n\ndef pandas_data_frame_to_rpy2_data_frame(pDataframe):\n    orderedDict = rlc.OrdDict()\n\n    for columnName in pDataframe:\n        columnValues = pDataframe[columnName].values\n        filteredValues = [value if pandas.notnull(value) else robj.NA_Real \n                          for value in columnValues]\n\n        try:\n            orderedDict[columnName] = robj.FloatVector(filteredValues)\n        except ValueError:\n            orderedDict[columnName] = robj.StrVector(filteredValues)\n\n    rDataFrame = robj.DataFrame(orderedDict)\n    rDataFrame.rownames = robj.StrVector(pDataframe.index)\n\n    return rDataFrame\n```\n\nThis:\n- avoids using importr(\"base\") which is horrendously slow\n- uses the `pandas` definition of what is and isn't missing data; this may be slower but I'm not sure it will be (I have large but not enormous datasets)\n- coerces to a FloatVector unless it can't; if memory usage is an issue, it might be tried to convert to an IntVector first\n- the function name makes explicit the direction of the conversion; the current `pandas.rpy.common` module is pretty confusing as `convert_robj()` could be conversion to or from robj\n\nI'm not sure if the call to `robj.r.I()` is necessary sometimes; I've omitted it as I almost never use StrVectors in data frames.\n",
      "The call to \"Importr\" can be substituted with a much faster:\n\n``` python\n\nI = robjects.baseenv.get(\"I\")\nis_nan = robjects.baseenv.get(\"is.nan\")\n```\n\nbut yes, it is necessary if you deal with, e.g., \"omics\" data where you have primary identifiers (the index) and a series of non-float columns (annotation) alongside measurements (floats). If you handle strings as factors, later on if you convert them back to Python objects you will get (unless you're careful) a list of ints, rather than of strings.\n\n Also \"pandas.notnull\" doesn't work with strings (and R has a NA character type, again useful for annotations).\n\nOf course (hopefully!) this will become much simpler when numpy adapts a missing data type.\n",
      "Okay, fair enough on the need to use `robj.r.I()` (which I'd prefer over `robj.baseenv.get(\"I\")`).\n\nI think it's a mistake to use R's definition of missing data when converting from pandas, though -- since `pandas.notnull` doesn't recognize `\"nan\"` as null, you probably shouldn't be using that as a null value in pandas (or since you apparently are, you should convert it to something that pandas understands to be null, such as `None` or `numpy.nan`).\n\nFinally, I'd definitely move the precomputed values (`I` and `vectors`) out of the function, both for speed and readability.\n",
      "After reviewing my own code that uses this, I noticed that It's there mostly for \"historical\" reasons:  probably it can be  substituted by pandas' own  notnull.\n",
      "``` python\nimport numpy as np\nfrom pandas import notnull\nimport rpy2.robjects as robjects\nfrom rpy2.robjects.packages import importr\nimport rpy2.rlike.container as rlc\n\nI = robjects.baseenv.get(\"I\")\nVECTOR_TYPES = {np.float64: robjects.FloatVector,\n               np.float32: robjects.FloatVector,\n               np.float: robjects.FloatVector,\n               np.int: robjects.IntVector,\n               np.object_: robjects.StrVector,\n               np.str: robjects.StrVector}\n\ndef dataset_to_data_frame(dataset, strings_as_factors=True):\n\n    columns = rlc.OrdDict()\n\n    for column in dataset:\n        values = dataset[column]\n        value_type = values.dtype.type\n        values = [item if notnull(item) else robjects.NA_Logical for item in values]\n        values = VECTOR_TYPES[value_type](values)\n\n        if not strings_as_factors:\n            values = I(values)\n\n        columns[column] = value\n\n    dataframe = robjects.DataFrame(columns)\n\n    del columns\n\n    dataframe.rownames = robjects.StrVector(dataset.index)\n\n    return dataframe\n```\n\nHere's another version. I should try to port my own unit tests for this to pandas....\n",
      "Since I don't use stuff like MultiIndex etc.: How do those DataFrames get converted by convert_robj? \n"
    ],
    "events": [
      "subscribed",
      "commented",
      "commented",
      "commented",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 1,
    "additions": 108,
    "deletions": 1,
    "changed_files_list": [
      "pandas/rpy/common.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 372,
    "reporter": "ukch",
    "created_at": "2011-11-16T15:04:33+00:00",
    "closed_at": "2011-11-16T19:18:40+00:00",
    "resolver": "wesm",
    "resolved_in": "ba26b4ca94a2264bc6f6e01511c63f980ca7edf6",
    "resolver_commit_num": 1005,
    "title": "Unable to build from HEAD",
    "body": "I've just tried to build the latest version of pandas from HEAD, and I get the following error:\n\n\n\nDid you forget to add this file to the repository?\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 17,
    "deletions": 0,
    "changed_files_list": [
      "pandas/src/cppsandbox.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 383,
    "reporter": "wesm",
    "created_at": "2011-11-18T06:17:18+00:00",
    "closed_at": "2012-05-14T20:01:58+00:00",
    "resolver": "wesm",
    "resolved_in": "dc4016705f67f00e1baae08627b41826c12e3a75",
    "resolver_commit_num": 1892,
    "title": "Pass multiple columns to DataFrameGroupBy.__getitem__",
    "body": "(preferably without copying data)\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned",
      "subscribed",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 58,
    "deletions": 31,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 391,
    "reporter": "wesm",
    "created_at": "2011-11-20T02:18:03+00:00",
    "closed_at": "2012-04-26T18:46:25+00:00",
    "resolver": "wesm",
    "resolved_in": "1d3ed8463b8aa782f93f28a8ad9ce39312de1a3b",
    "resolver_commit_num": 1759,
    "title": "Add more robust frequency inference function",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 8,
    "additions": 329,
    "deletions": 177,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/algorithms.py",
      "pandas/tests/test_datetools.py",
      "pandas/tseries/frequencies.py",
      "pandas/tseries/index.py",
      "pandas/tseries/offsets.py",
      "pandas/tseries/tests/test_frequencies.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 394,
    "reporter": "wesm",
    "created_at": "2011-11-21T18:56:54+00:00",
    "closed_at": "2011-11-22T05:50:21+00:00",
    "resolver": "wesm",
    "resolved_in": "b7c3d2c110b40c27516b067853e86628b60c385c",
    "resolver_commit_num": 1023,
    "title": ".ix[val] discards index names",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 38,
    "deletions": 4,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/index.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 402,
    "reporter": "wesm",
    "created_at": "2011-11-22T16:24:19+00:00",
    "closed_at": "2012-05-07T19:43:01+00:00",
    "resolver": "wesm",
    "resolved_in": "553bacc7abe341991e4d043a5c17521902c7f127",
    "resolver_commit_num": 1833,
    "title": "Support things other than column names in DataFrame.set_index",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 95,
    "deletions": 73,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/src/tseries.pyx",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 409,
    "reporter": "wesm",
    "created_at": "2011-11-24T01:52:48+00:00",
    "closed_at": "2012-04-11T03:21:30+00:00",
    "resolver": "wesm",
    "resolved_in": "8c8d06bb85b44ba876e6905449edbbc5b6d7f74b",
    "resolver_commit_num": 1683,
    "title": "A lot of partial setting stuff doesn't work with .ix, needs testing/work",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 41,
    "deletions": 3,
    "changed_files_list": [
      "pandas/core/indexing.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 415,
    "reporter": "wesm",
    "created_at": "2011-11-25T05:30:16+00:00",
    "closed_at": "2012-06-01T20:23:19+00:00",
    "resolver": "wesm",
    "resolved_in": "c14c0949f8893d93691ecd8937662b937cbcbffa",
    "resolver_commit_num": 2002,
    "title": "Implement equivalent of cut function from base R",
    "body": "",
    "labels": [],
    "comments": [
      "@jseabold suggested a solution like this on the ML\n\n```\n# make some age variables 15-70\nage = np.random.randint(15, 71, size=100)\n\n# bin them into groups 15-24, 25-34, 35-70.\nbin_idx = np.digitize(age, [25, 35, 71])\n\n# make dummy variables\nimport scikits.statsmodels.api as sm\nage_dummy, labels = sm.categorical(bin_idx, drop=True, dictnames=True)\n```\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 222,
    "deletions": 21,
    "changed_files_list": [
      "RELEASE.rst",
      "doc/source/whatsnew/v0.8.0.txt",
      "pandas/__init__.py",
      "pandas/tools/tests/test_tile.py",
      "pandas/tools/tile.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 437,
    "reporter": "wesm",
    "created_at": "2011-12-02T21:29:39+00:00",
    "closed_at": "2011-12-06T00:26:55+00:00",
    "resolver": "wesm",
    "resolved_in": "a94710a19bab7d2e2015b4a42d665439bd14f5c9",
    "resolver_commit_num": 1066,
    "title": "Fast as possible get_value/put_value methods for Series/DataFrame/Panel",
    "body": "Should feature minimal overhead. Some work already done here\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "referenced",
      "referenced",
      "closed",
      "referenced",
      "reopened"
    ],
    "changed_files": 2,
    "additions": 15,
    "deletions": 3,
    "changed_files_list": [
      "pandas/core/sparse.py",
      "pandas/tests/test_panel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 465,
    "reporter": "lodagro",
    "created_at": "2011-12-09T11:13:43+00:00",
    "closed_at": "2011-12-12T02:58:34+00:00",
    "resolver": "wesm",
    "resolved_in": "6aa80f9a9d7260cbd4927db23c5a6ad4c973f3f6",
    "resolver_commit_num": 1092,
    "title": "DataFrame.applymap(func) issue when func returns tuple",
    "body": "\n\nThis works fine\n\n\n\nLet`s try with a list:\n\n\n\nHad a look at the pandas.DataFrame.applymap() code and this behavior is related to numpy.\nI`m using numpy 1.5.1, i know that for other reason numpy 1.6.1 is preferred. Would more recent numpy version fix this?\nI tried to install more recent version of numpy, but so far unsuccessful (various compilation errors).\n\n\n",
    "labels": [],
    "comments": [
      "This looks buggy enough to me...will see if I can sort out a fix here\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "closed"
    ],
    "changed_files": 3,
    "additions": 7,
    "deletions": 11,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 471,
    "reporter": "lodagro",
    "created_at": "2011-12-12T07:46:39+00:00",
    "closed_at": "2011-12-12T15:30:09+00:00",
    "resolver": "wesm",
    "resolved_in": "1eb1e71e05566d8def65b218848e9e0359c8759a",
    "resolver_commit_num": 1102,
    "title": "ImportError: No module named sparse.api",
    "body": "Pulled from github master and did install (removed previous version and build dir), but get ImportError.\n\n\n\nThere is no sparce directory in <...>/lib/python2.7/site-packages/pandas.\nIf i build in place and import from the src directory there is no ImportError.\n\nUPDATE:\nRunning git bisect, gives the following commits as first _bad_ commit;\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 2,
    "deletions": 0,
    "changed_files_list": [
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 475,
    "reporter": "wesm",
    "created_at": "2011-12-12T20:45:34+00:00",
    "closed_at": "2011-12-12T20:59:22+00:00",
    "resolver": "wesm",
    "resolved_in": "d472cd7ee2485af0905408adcb19f0abc091f4ae",
    "resolver_commit_num": 1107,
    "title": "DataFrame constructor bug from single Series with different Index",
    "body": "reported by a user\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 23,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 495,
    "reporter": "adamklein",
    "created_at": "2011-12-15T15:46:25+00:00",
    "closed_at": "2011-12-15T17:04:54+00:00",
    "resolver": "wesm",
    "resolved_in": "2a4d40972d437ef134e0f7c40260ed4b1314ac30",
    "resolver_commit_num": 1138,
    "title": "invalid dtype access crashes interpreter (segfault)",
    "body": "In [1]: from pandas import *\n\nIn [2]: df = DataFrame([1,2,3,4], index=['a','b','c','d'])\n\nIn [3]: df\nOut[3]: \n   0\na  1\nb  2\nc  3\nd  4\n\nIn [4]: df.dtypes\nOut[4]: 0    int64\n\nIn [5]: df.dtypes[0]\nOut[5]: dtype('int64')\n\nIn [6]: df.dtypes[1]\n**\\* glibc detected **\\* /home/adam/.virtualenvs/py27/bin/python: free(): corrupted unsorted chunks: 0x00000000025db8e0 ***\n======= Backtrace: =========\n/lib/x86_64-linux-gnu/libc.so.6(+0x78a96)[0x7f8751957a96]\n/lib/x86_64-linux-gnu/libc.so.6(cfree+0x6c)[0x7f875195bd7c]\n/home/adam/.virtualenvs/py27/bin/python[0x4354d8]\n/home/adam/.virtualenvs/py27/bin/python[0x447a2c]\n/home/adam/.virtualenvs/py27/bin/python[0x4e4fe3]\n/home/adam/.virtualenvs/py27/bin/python[0x4e500d]\n======= Memory map: ========\n00400000-00633000 r-xp 00000000 08:05 5373988                            /home/adam/.virtualenvs/py27/bin/python\n00832000-00833000 r--p 00232000 08:05 5373988                            /home/adam/.virtualenvs/py27/bin/python\n00833000-0089c000 rw-p 00233000 08:05 5373988                            /home/adam/.virtualenvs/py27/bin/python\n0089c000-008ae000 rw-p 00000000 00:00 0 \n016f0000-0278e000 rw-p 00000000 00:00 0                                  [heap]\n",
    "labels": [],
    "comments": [
      "Sigh, play with fire and you get burned. \n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 29,
    "deletions": 35,
    "changed_files_list": [
      "pandas/core/series.py",
      "pandas/src/engines.pyx",
      "pandas/src/util.pxd",
      "pandas/tests/test_series.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 502,
    "reporter": "wesm",
    "created_at": "2011-12-19T04:26:17+00:00",
    "closed_at": "2012-05-12T18:30:31+00:00",
    "resolver": "wesm",
    "resolved_in": "59f0ee735638c08b71f3d216d687e1b96185848a",
    "resolver_commit_num": 1869,
    "title": "Add R-like match function to API",
    "body": "",
    "labels": [],
    "comments": [
      "oops, not done yet\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "closed",
      "reopened",
      "commented"
    ],
    "changed_files": 5,
    "additions": 63,
    "deletions": 7,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/algorithms.py",
      "pandas/core/api.py",
      "pandas/tests/test_algos.py",
      "vb_suite/miscellaneous.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 513,
    "reporter": "bshanks",
    "created_at": "2011-12-20T19:24:06+00:00",
    "closed_at": "2012-02-28T17:35:07+00:00",
    "resolver": "wesm",
    "resolved_in": "71244f213ea5503092a949e9ba8f7d158bb9e6d4",
    "resolver_commit_num": 1562,
    "title": "HDFStore loses data because it silently loses microseconds in datetime index conversion",
    "body": "\n\nThis happens because HDFStore uses .timetuple() to serialize, but two datetimes can be unique yet have the same .timetuple(), because .timetuple discards microseconds (slightly related discussion:  ; i suppose that whatever they decided to do to convert datetime to datetime64 might work?).\n",
    "labels": [],
    "comments": [
      "i think this is what datetime64 does (from https://github.com/numpy/numpy/blob/master/numpy/core/tests/test_datetime.py):\n\n``` python\n        # Construction from datetime.datetime\n        assert_equal(np.datetime64('1980-01-25T14:36:22.5Z'),\n                     np.datetime64(datetime.datetime(1980,1,25,\n                                                14,36,22,500000)))\n```\n"
    ],
    "events": [
      "subscribed",
      "commented"
    ],
    "changed_files": 2,
    "additions": 15,
    "deletions": 7,
    "changed_files_list": [
      "pandas/io/pytables.py",
      "pandas/io/tests/test_pytables.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 522,
    "reporter": "wesm",
    "created_at": "2011-12-21T22:33:55+00:00",
    "closed_at": "2012-05-07T16:42:33+00:00",
    "resolver": "wesm",
    "resolved_in": "20f69e7955ec7f98e9f9dda9e6f28af9e7743aed",
    "resolver_commit_num": 1826,
    "title": "Misbehaved reindexing with NaN labels",
    "body": "from user report\n\n\n",
    "labels": [],
    "comments": [
      "Looks like this has been fixed on master already.\n\nIn [8]: df = pandas.DataFrame([[1,2], [3,4], [numpy.nan,numpy.nan], [7,8], [9,10]], columns=['a', 'b'], index=[100.0, 101.0, numpy.nan, 102.0, 103.0])\n\nIn [9]: df\nOut[9]: \n       a   b\n 100   1   2\n 101   3   4\nNaN  NaN NaN\n 102   7   8\n 103   9  10\n\nIn [10]: \n\nIn [10]: df.reindex(index=[101.0, 102.0, 103.0])\nOut[10]: \n     a   b\n101  3   4\n102  7   8\n103  9  10\n\nIn [11]: df.reindex(index=[103.0])\nOut[11]: \n     a   b\n103  9  10\n\nIn [12]: df.reindex(index=[101.0])\nOut[12]: \n     a  b\n101  3  4\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 18,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 535,
    "reporter": "wesm",
    "created_at": "2011-12-23T17:37:29+00:00",
    "closed_at": "2012-12-05T01:41:25+00:00",
    "resolver": "wesm",
    "resolved_in": "9e95ce2ab4a9214972d93c23779676b32eeb7e6e",
    "resolver_commit_num": 2682,
    "title": "Improve performance of GroupBy apply on DataFrames by low-level trickery",
    "body": "Need a \"sliding iterator\" similar to recent Series groupby optimizations to cut down object creation overhead. Should be relatively straightforward to modify the existing Slider Cython type. Main issue will be taking care with passed functions that modify the size of the DataFrame (like adding a column), but this can obviously all be tested for.\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed",
      "assigned"
    ],
    "changed_files": 7,
    "additions": 219,
    "deletions": 34,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/src/numpy_helper.h",
      "pandas/src/reduce.pyx",
      "pandas/src/util.pxd",
      "pandas/tests/test_groupby.py",
      "vb_suite/groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 560,
    "reporter": "wesm",
    "created_at": "2011-12-30T03:37:29+00:00",
    "closed_at": "2012-05-19T20:09:35+00:00",
    "resolver": "wesm",
    "resolved_in": "0bc7fec8f1547ac345c0148eba14e694fb923f2e",
    "resolver_commit_num": 1918,
    "title": "Code blocks don't work right in ipython_directive",
    "body": "",
    "labels": [],
    "comments": [
      "I think this is working now (unless there was a more complex case you were thinking of?). \n\nIf I do:\n\n.. ipython::\n\n```\nIn [5]: long_series = Series(randn(1000))\n\nIn [6]: long_series.head()\nOut[6]:\n0   -0.663260\n1   -2.169555\n2    0.088584\n3    2.220296\n4   -0.149226\n\nIn [7]: long_series.tail(3)\nOut[7]:\n997   -1.818651\n998   -1.341884\n999   -0.033343\n```\n\nin basics.rst, the html output looks right (the output is replaced with newly sampled random numbers)\n",
      "No. I mean stuff like:\n\n```\n.. ipython:: python\n\n   for i in range(5):\n       print 'foo'\n       print 'bar'\n```\n\nThe block parser is not \"greedy\" enough, it uses `ast.parse` to determine if lines read are valid Python code which they are after the first print statement. Can probably come up with a reasonable enough algorithm that looks at the indentation level to determine whether the block is done or not\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented"
    ],
    "changed_files": 3,
    "additions": 31,
    "deletions": 12,
    "changed_files_list": [
      "doc/source/io.rst",
      "doc/source/merging.rst",
      "doc/sphinxext/ipython_directive.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 583,
    "reporter": "wesm",
    "created_at": "2012-01-06T20:13:10+00:00",
    "closed_at": "2012-01-06T20:22:30+00:00",
    "resolver": "wesm",
    "resolved_in": "cd4636bb6b69dde447b2529b8d940f459a9c598e",
    "resolver_commit_num": 1294,
    "title": "Converter returning string passed to read_csv causes exception",
    "body": "for example:\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 27,
    "deletions": 1,
    "changed_files_list": [
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 609,
    "reporter": "wesm",
    "created_at": "2012-01-11T20:09:10+00:00",
    "closed_at": "2012-05-13T04:32:12+00:00",
    "resolver": "wesm",
    "resolved_in": "e7af2b99634f5514554d10731f8b99dc070139cb",
    "resolver_commit_num": 1879,
    "title": "Reimplement groupby_indices-related functionality using enhanced tools",
    "body": "Majorly slow, especially in the multi-key case. Need to identify realistic use cases and vbench then fix\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 123,
    "deletions": 185,
    "changed_files_list": [
      "pandas/core/algorithms.py",
      "pandas/core/groupby.py",
      "pandas/src/groupby.pyx",
      "pandas/src/sandbox.pyx",
      "vb_suite/groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 623,
    "reporter": "wesm",
    "created_at": "2012-01-13T16:13:34+00:00",
    "closed_at": "2012-01-13T22:44:35+00:00",
    "resolver": "wesm",
    "resolved_in": "feba1be541c53d75aa73f97f90efe62d43134896",
    "resolver_commit_num": 1351,
    "title": "More float formatting bugs",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 127,
    "deletions": 74,
    "changed_files_list": [
      "pandas/core/format.py",
      "pandas/core/series.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 642,
    "reporter": "wesm",
    "created_at": "2012-01-17T21:37:40+00:00",
    "closed_at": "2012-05-14T21:10:27+00:00",
    "resolver": "wesm",
    "resolved_in": "851afe51d66859f2989c3fc08dacaa70048067dc",
    "resolver_commit_num": 1894,
    "title": "More flexible multiple function application with DataFrameGroupBy",
    "body": "cc @arthurgerigk\n\nSomething of this nature would be nice:\n\n\n\nWould also be nice to add some options to agg for what to do with the input dict (by default targets columns)\n",
    "labels": [],
    "comments": [
      "Here's the outcome:\n\n```\nIn [5]: df\nOut[5]: \n     A      B         C         D\n0  foo    one  0.727964  0.406402\n1  bar    one  1.653679  0.266521\n2  foo    two -1.677686 -1.143690\n3  bar  three -0.591334 -0.594644\n4  foo    two -0.839865  0.153575\n5  bar    two -0.857455 -0.054324\n6  foo    one -0.148167  1.080535\n7  foo  three  0.118469  0.715423\n\nIn [6]: grouped = df.groupby('A')\n\nIn [7]: result = grouped.aggregate({'C' : [np.mean, np.std],\n   ...:                                     'D' : [np.mean, np.std]})\n\nIn [8]: result\nOut[8]: \n            C                   D          \n         mean       std      mean       std\nA                                          \nbar  0.068297  1.379414 -0.127482  0.435219\nfoo -0.363857  0.925198  0.242449  0.848860\n\nIn [9]: grouped.aggregate({'C' : np.mean, 'D' : [np.mean, np.std]})\nOut[9]: \n            C         D          \n         mean      mean       std\nA                                \nbar  0.068297 -0.127482  0.435219\nfoo -0.363857  0.242449  0.848860\n\nIn [10]: paste\n        result = grouped.aggregate({'C' : np.mean,\n                                    'D' : {'foo': np.mean,\n                                           'bar': np.std}})\n## -- End pasted text --\n\nIn [11]: result\nOut[11]: \n            C         D          \n         mean       foo       bar\nA                                \nbar  0.068297 -0.127482  0.435219\nfoo -0.363857  0.242449  0.848860\n```\n"
    ],
    "events": [
      "mentioned",
      "subscribed",
      "subscribed",
      "commented"
    ],
    "changed_files": 3,
    "additions": 59,
    "deletions": 9,
    "changed_files_list": [
      "pandas/core/groupby.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 643,
    "reporter": "ara818",
    "created_at": "2012-01-17T23:28:02+00:00",
    "closed_at": "2012-04-05T00:23:03+00:00",
    "resolver": "wesm",
    "resolved_in": "651237705f9fd2d819cf3a0c596e5cf2cb363da8",
    "resolver_commit_num": 1645,
    "title": "Allow Series to merge multiple values w/ the same datetime.",
    "body": "We are having a problem when loading daily historical dividend data into a pandas Series().  Because companies sometimes declare two different dividends on the same day, we have an issue when trying to index this series\n\nThe following\n\n\n\nReturns\n\n\n\nWe're hoping for a function that will go through a pandas a series and produce a new series w/o duplicate dates via some sort of combination function. In this case, it makes the most sense to combine the two dividends by adding them together.\n",
    "labels": [],
    "comments": [
      "We're looking currently at building in support for non-unique datetime indexes. Probably a month or two off yet, though. cc @adamklein \n"
    ],
    "events": [
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 14,
    "deletions": 6,
    "changed_files_list": [
      "pandas/core/groupby.py",
      "pandas/tests/test_datetime64.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 647,
    "reporter": "fabriziop",
    "created_at": "2012-01-18T16:39:50+00:00",
    "closed_at": "2012-01-23T16:03:03+00:00",
    "resolver": "adamklein",
    "resolved_in": "5472443b4f2fe075018a295ebdacecb84aa5aeea",
    "resolver_commit_num": 79,
    "title": "enh request: new functions cummax, cummin",
    "body": "I have found useful cummax and cummin functions in R, so I propose to add them to pandas.\n\nAt fabriziop/pandas there is a forked branch with these changes, commit  660d5ddf05c49c55fabce072a41e5d5b5096e529 .\n\nThe new functions are dumbly derived from the cumsum function.\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 67,
    "deletions": 1,
    "changed_files_list": [
      "pandas/tests/test_frame.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 661,
    "reporter": "wesm",
    "created_at": "2012-01-21T20:03:32+00:00",
    "closed_at": "2012-03-13T23:31:28+00:00",
    "resolver": "wesm",
    "resolved_in": "79decd74b2b604b691ca11c4c57334576a4e0604",
    "resolver_commit_num": 1590,
    "title": "Enable pass dict to DataFrame.fillna",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 54,
    "deletions": 9,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/core/internals.py",
      "pandas/tests/test_frame.py",
      "vb_suite/frame_methods.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 682,
    "reporter": "wesm",
    "created_at": "2012-01-25T15:08:41+00:00",
    "closed_at": "2012-05-14T15:22:56+00:00",
    "resolver": "wesm",
    "resolved_in": "a7499124be00864cc7dcc249f2f306dff528960b",
    "resolver_commit_num": 1887,
    "title": "Optimize joins for primitive join key types, like int64",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 28,
    "deletions": 25,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/src/hashtable.pyx",
      "pandas/tools/merge.py",
      "vb_suite/join_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 691,
    "reporter": "jseabold",
    "created_at": "2012-01-26T18:34:16+00:00",
    "closed_at": "2012-01-26T20:14:37+00:00",
    "resolver": "adamklein",
    "resolved_in": "b848894da2e613096e00c00e7ff5bf738f309239",
    "resolver_commit_num": 97,
    "title": "assignment with ix and mixed dtypes",
    "body": "data = {\"title\" : ['foobar','bar','foobar'] + ['foobar'] \\* 17 , \"cruft\" : np.random.random(20)}\n\ndf = pandas.DataFrame(data)\n\nix = df[df['title'] == 'bar'].index\n# this doesn't work with a list of variables\n\ndf.ix[ix, ['title']] = 'foobar'\ndf.ix[ix, ['cruft']] = 0\n\nprint df\n# but this does\n\ndf.ix[ix, 'title'] = 'foobar'\ndf.ix[ix, 'cruft'] = 0\n\nprint df\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 4,
    "deletions": 4,
    "changed_files_list": [
      "pandas/core/frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 694,
    "reporter": "wesm",
    "created_at": "2012-01-26T20:33:43+00:00",
    "closed_at": "2012-01-27T17:48:50+00:00",
    "resolver": "wesm",
    "resolved_in": "88fcac5995d8cbb88a581c0bf02fdf71f5a09622",
    "resolver_commit_num": 1452,
    "title": "Make sep=',' the default in read_csv",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 47,
    "deletions": 25,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_graphics.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 697,
    "reporter": "wesm",
    "created_at": "2012-01-26T23:13:17+00:00",
    "closed_at": "2012-01-27T14:56:50+00:00",
    "resolver": "adamklein",
    "resolved_in": "b0ee06b83ff79532b909503dcce25e9b330b5e99",
    "resolver_commit_num": 102,
    "title": "GroupBy by level bug",
    "body": "reported by user, almost certainly a corner case introduced by the list changes recently\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed",
      "assigned",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 10,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/groupby.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 702,
    "reporter": "elpres",
    "created_at": "2012-01-27T16:01:25+00:00",
    "closed_at": "2012-01-27T16:31:08+00:00",
    "resolver": "adamklein",
    "resolved_in": "782f39291a35d5cd610a4fd04983e4f74e6422e2",
    "resolver_commit_num": 103,
    "title": "Bar plot fails if an axis parameter is supplied",
    "body": "Example code:\n\n\n\nIf the 'ax' parameter is removed, no problem. Line plots with a custom 'ax' work too.\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned",
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 1,
    "deletions": 0,
    "changed_files_list": [
      "pandas/core/frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 703,
    "reporter": "adamklein",
    "created_at": "2012-01-27T19:12:10+00:00",
    "closed_at": "2012-01-27T21:22:26+00:00",
    "resolver": "adamklein",
    "resolved_in": "a6c067860287ad776bef886575b7ae9d4e5e30e6",
    "resolver_commit_num": 104,
    "title": "EWMA uses adjustment, make note in docs",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "assigned",
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 6,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/computation.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 705,
    "reporter": "jtbates",
    "created_at": "2012-01-27T20:20:59+00:00",
    "closed_at": "2012-01-27T23:54:09+00:00",
    "resolver": "adamklein",
    "resolved_in": "c0fc368c82555da7f93b5cf674941f4e19e6e5b4",
    "resolver_commit_num": 105,
    "title": "Cannot save DataFrame with unicode to CSV",
    "body": "\n\nI think this should be separate from [#680]().  The CSV issue is also mentioned in this [comment on bug #300](#issuecomment-2735592).\n",
    "labels": [],
    "comments": [
      "I presume you're using python version < 3? The csv module does not handle unicode unfortunately. I'll see if there is a workaround, but as you can tell by the recurring issues, pandas isn't exactly unicode-friendly on <= python 2.7, but neither is python 2.7 ...\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 60,
    "deletions": 6,
    "changed_files_list": [
      "pandas/core/common.py",
      "pandas/core/frame.py",
      "pandas/io/parsers.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 709,
    "reporter": "wesm",
    "created_at": "2012-01-28T20:54:38+00:00",
    "closed_at": "2012-01-30T18:14:20+00:00",
    "resolver": "adamklein",
    "resolved_in": "618ff03db3188c13aedc164f7068fdeb4c010da0",
    "resolver_commit_num": 107,
    "title": "Unhelpful exception / bug in ix + MultiIndex use case",
    "body": "This should work IMHO\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 13,
    "deletions": 0,
    "changed_files_list": [
      "pandas/core/indexing.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 714,
    "reporter": "kieranholland",
    "created_at": "2012-01-30T11:08:22+00:00",
    "closed_at": "2012-02-04T22:40:26+00:00",
    "resolver": "wesm",
    "resolved_in": "b50af2014474f5a75e1cf115420a8aaa6c426eee",
    "resolver_commit_num": 1457,
    "title": "Series.unique() dies with many NaNs",
    "body": "Series.unique() dies with many NaNs:\n\n\n",
    "labels": [],
    "comments": [
      "Interesting. Guessing because the hash back-end doesn't realize nan != nan\n",
      "went a different route, added a float64 hash table with NA handling. getting this result now:\n\n```\n\nIn [1]: paste\nimport time\n\ndef test_unique(obj):\n    for n in range(6):\n        objs = Series([obj] * 10 ** n)\n        start = time.time()\n        objs.unique()\n        stop = time.time()\n        print('%6.0f %s' % (len(objs), stop - start))\n\ntest_unique('a')\n## -- End pasted text --\n     1 7.48634338379e-05\n    10 5.3882598877e-05\n   100 5.31673431396e-05\n  1000 7.41481781006e-05\n 10000 0.000144004821777\n100000 0.00133991241455\n\nIn [2]: test_unique(float('nan'))\n     1 5.19752502441e-05\n    10 3.40938568115e-05\n   100 3.19480895996e-05\n  1000 3.91006469727e-05\n 10000 9.29832458496e-05\n100000 0.000288009643555\n```\n",
      "Thanks for quick response.\nI encountered an issue with the new version.\nIt only happens with multiple nan instances _and_ other objects mixed.\n\n```\nimport time\n\ndef test_unique_single_nan_instance_and_non_nans():\n    for n in range(6):\n        s = []\n        nan = float('nan')\n        for _ in range(10 ** n):\n            s.append(nan)\n            s.append('a')\n        objs = Series(s)\n        start = time.time()\n        objs.unique()\n        stop = time.time()\n        print('%6.0f %s' % (len(objs), stop - start))\n\ntest_unique_single_nan_instance_and_non_nans() # fine\n\ndef test_unique_multiple_nan_instances():\n    for n in range(6):\n        s = []\n        for _ in range(10 ** n):\n            s.append(float('nan'))\n            s.append(float('nan'))\n        objs = Series(s)\n        start = time.time()\n        objs.unique()\n        stop = time.time()\n        print('%6.0f %s' % (len(objs), stop - start))\n\ntest_unique_multiple_nan_instances() # fine\n\ndef test_unique_multiple_nan_instances_and_non_nans():\n    for n in range(6):\n        s = []\n        for _ in range(10 ** n):\n            s.append(float('nan'))\n            s.append('a')\n        objs = Series(s)\n        start = time.time()\n        objs.unique()\n        stop = time.time()\n        print('%6.0f %s' % (len(objs), stop - start))\n\ntest_unique_multiple_nan_instances_and_non_nans() # not so fine\n```\n",
      "reopened the issue and will take a look\n",
      "fixed this in master, let me know if you have any more issues\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "closed",
      "subscribed",
      "commented",
      "commented",
      "reopened",
      "commented",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 90,
    "deletions": 6,
    "changed_files_list": [
      "pandas/core/series.py",
      "pandas/src/hashtable.pyx",
      "pandas/src/khash.h",
      "pandas/src/khash.pxd"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 718,
    "reporter": "wesm",
    "created_at": "2012-01-30T20:21:04+00:00",
    "closed_at": "2012-02-06T02:25:14+00:00",
    "resolver": "wesm",
    "resolved_in": "a7402c680f8dcc69deb850f61fc6a2c7f7834120",
    "resolver_commit_num": 1481,
    "title": "Split vbench results into a separate sphinx build under pandas/vb_suite",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 11,
    "additions": 977,
    "deletions": 13,
    "changed_files_list": [
      "doc/source/index.rst",
      "vb_suite/.gitignore",
      "vb_suite/attrs_caching.py",
      "vb_suite/make.py",
      "vb_suite/source/conf.py",
      "vb_suite/source/themes/agogo/layout.html",
      "vb_suite/source/themes/agogo/static/agogo.css_t",
      "vb_suite/source/themes/agogo/static/bgfooter.png",
      "vb_suite/source/themes/agogo/static/bgtop.png",
      "vb_suite/source/themes/agogo/theme.conf",
      "vb_suite/suite.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 719,
    "reporter": "wesm",
    "created_at": "2012-01-31T00:35:11+00:00",
    "closed_at": "2012-02-01T18:55:23+00:00",
    "resolver": "wesm",
    "resolved_in": "3fd516aea2c9e95314e4a305419bed355e2d75c6",
    "resolver_commit_num": 1459,
    "title": "DataFrame.to_panel needs to check the sortedness of the MultiIndex",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed",
      "cross-referenced"
    ],
    "changed_files": 8,
    "additions": 128,
    "deletions": 28,
    "changed_files_list": [
      "doc/source/indexing.rst",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_panel.py",
      "vb_suite/indexing.py",
      "vb_suite/suite.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 720,
    "reporter": "wesm",
    "created_at": "2012-01-31T00:39:18+00:00",
    "closed_at": "2012-02-01T18:55:23+00:00",
    "resolver": "wesm",
    "resolved_in": "3fd516aea2c9e95314e4a305419bed355e2d75c6",
    "resolver_commit_num": 1459,
    "title": "Speed up MultiIndex.sortlevel using enhanced tools",
    "body": "See code for DataFrame.sort_index. Should parcel some of this out into a standalone method\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed",
      "cross-referenced"
    ],
    "changed_files": 8,
    "additions": 128,
    "deletions": 28,
    "changed_files_list": [
      "doc/source/indexing.rst",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_panel.py",
      "vb_suite/indexing.py",
      "vb_suite/suite.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 728,
    "reporter": "wesm",
    "created_at": "2012-02-01T16:39:19+00:00",
    "closed_at": "2012-02-01T18:51:11+00:00",
    "resolver": "wesm",
    "resolved_in": "a90ab287d6955d8409fdc7cd22f99624fdaafaf4",
    "resolver_commit_num": 1458,
    "title": "GCC compiler warnings",
    "body": "should sort these out prior to release. all look very straightforward\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed"
    ],
    "changed_files": 9,
    "additions": 41,
    "deletions": 37,
    "changed_files_list": [
      "pandas/src/engines.pyx",
      "pandas/src/hashtable.pyx",
      "pandas/src/khash.h",
      "pandas/src/moments.pyx",
      "pandas/src/skiplist.h",
      "pandas/src/sparse.pyx",
      "pandas/src/tseries.pyx",
      "vb_suite/benchmarks.py",
      "vb_suite/test.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 736,
    "reporter": "wesm",
    "created_at": "2012-02-02T17:29:40+00:00",
    "closed_at": "2016-07-20T21:11:50+00:00",
    "resolver": "sinhrks",
    "resolved_in": "b25a2a1259f33ce8123b7f239f109ae42155d02c",
    "resolver_commit_num": 342,
    "title": "Create a generic version of convert_to_annual function in scikits.timeseries",
    "body": "The name is a misnomer. It's really a pivot operation\n",
    "labels": [],
    "comments": [
      "Link to scikits.timeseries meta issue:\nhttps://github.com/wesm/pandas/issues/630\n",
      "This kind of stuff would be nice to be able to do too: http://stackoverflow.com/questions/10458493/pandas-how-to-plot-yearly-data-on-top-of-each-other\n",
      "Is it planned to integrate this into 0.8 final?\n",
      "No, we pushed this off to 0.9.0 for now\n",
      "@timmie: there is a version of it already in pandas/tseries/util.py, but I need people like you to give me some more guidance on what features of it are actually required (we're talking about a 15 line function here). I suspect that pandas's groupby capability makes `convert_to_annual` less useful than it was in scikits.timeseries\n",
      "OK, very nice.\nI will give it a try until the end of the week.\n\nIs it also possible to do such operations on dataframes?\nCase:\nI have a data frame with datetime index with temperature, precipitation and wind speed for 10 years. Now I would like to get the annual average for all 3 parameters at once.\n",
      "Wouldn't `df.resample('A')` be what you want?\n",
      "Or otherwise `df.groupby(lambda x: x.year).mean()`\n",
      "Please help, I am getting the following errors:\n\n```\n\n>>> pivot_annual(ser, freq='A')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/lib/pymodules/python2.7/pandas/tseries/util.py\", line 58, in pivot_annual\n    raise NotImplementedError(freq)\nNotImplementedError: A\n\n>>> pivot_annual(ser)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/lib/pymodules/python2.7/pandas/tseries/util.py\", line 58, in pivot_annual\n    raise NotImplementedError(freq)\nNotImplementedError: None\n>>> \n\n```\n",
      "Please addd util to namespace\nI had to do\n\n```\nfrom pandas.tseries.util import pivot_annual\n```\n",
      "I suspect that \n\n```\ndf.groupby(lambda x: x.year).mean()\n\n```\n\nDoes not achieve a equal alignement over years for all parameters as commented in https://github.com/pydata/pandas/issues/736#issuecomment-5983210\n",
      "Why wouldn't it?\n",
      "Hmm. I feel I need to set up a Ipython notebook somewhere to have a test playground.\n",
      "And what would the NotImplementedError(freq) mean?\n",
      "If you look at the source for `pivot_annual`, only `D` and `M/BM` frequencies are implemented. \n",
      "You should also try `df.resample('A', how='mean', kind='period')`. It seems unlikely that `convert_to_annual / pivot_annual` is strictly necessary for what you're doing if the goal is to aggregate data.\n",
      "OK, I see. Then there must be a misunderstanding here.\n\ndid you try ts.convert_to_annual with a timeseries of hourly frequency covering at > 1 year?\n",
      "You're computing means over years or over hours? If I understand correctly now you want the mean value for each hour of the year across the data set? I agree that you can do that with `convert_to_annual` but not with groupby/resample. It would be very helpful to be looking at some real (or fake) data =P \n",
      "> If I understand correctly now you want the mean value for each hour of the year across the data set?\n\nYes. Could alos apply for daily, etc.\n\n> I agree that you can do that with convert_to_annual but not with groupby/resample. It would be very helpful to be looking at some real (or fake) data\n\nAgree. I will come up with an example data soon.\n",
      "> It would be very helpful to be looking at some real (or fake) data =P \n\nHere's the code:\n\n```\n\nimport numpy as np\nimport scikits.timeseries as ts\n\n# generate a time series with 10 years of random temperature data\n\ndata = np.random.uniform(low=-10.0, high=35.0, size=87671)\nstart_date = ts.Date(freq='H',year=2000,month=1,day=1, hour=1)\n\npytseries = ts.time_series(data, dtype=np.int, freq='H', \n                           start_date=start_date)\n\n\n\n# alignment of years in respect of the leap days\n\npytseries_annual = ts.extras.convert_to_annual(pytseries)\n\n# 10 years average of hourly temperatures\nlt_ave = pytseries_annual.mean(0).size\n\n```\n\nI also have this as htmlnotebook file and print but I donno how to attach here.\n",
      "Here is the pandas equivalent for daily data (one of the implemented frequencies) using `pivot_annual`:\n\n```\nIn [21]: rng = date_range('1/1/2000', '1/1/2010', freq='D')\n\nIn [22]: ts = Series(np.random.randn(len(rng)), rng)\n\nIn [23]: util.pivot_annual(ts).mean(0)\nOut[23]: \n1    -0.424473\n2    -0.221603\n3     0.388556\n4     0.383484\n5    -0.196986\n6     0.152771\n7    -0.430865\n8    -0.278414\n9     0.272135\n10   -0.216587\n11   -0.468601\n12    0.010926\n13    0.101746\n14    0.112212\n15   -0.462306\n...\n352   -0.417838\n353    0.459750\n354   -0.041899\n355   -0.011174\n356    0.220537\n357    0.242754\n358    0.313742\n359    0.300249\n360    0.040765\n361    0.046944\n362    0.040023\n363   -0.217559\n364    0.367547\n365    0.119563\n366    0.149244\nLength: 366\n```\n\nI'll see if I can find the time in the next 10 days to finish the other frequencies--- the hardest part is writing test cases, honestly.\n",
      "The htmlnotebook is now at:\nhttp://www.sendspace.com/file/5ukamo\n\n> I'll see if I can find the time in the next 10 days to finish the other frequencies--- the hardest part is writing test cases, honestly.\n\nHappy to read that we talked about teh same idea and purpose of the fuction.\n\nFor 0.9 I could see soem more improvements: although not being associated with a certain year, ths data is still datetime (months, days, hours, etc.). This is linked with my comment on the plots:\nhttp://permalink.gmane.org/gmane.comp.python.pystatsmodels/8439\n\nBut one after the other.\n",
      "Could you give some guidance for adding higher frequencies?\nI woulf appreciate to have at least hourly & minutely.\n",
      "While I am trying to get moving on the hourly freq., I still would like to get an opinion from @wesm \n\nWhy did we not copy the method at:\nhttps://github.com/pierregm/scikits.timeseries/blob/master/scikits/timeseries/extras.py#L135\n\nThanks in advance & sorry if I am too pushy on this.\n",
      "That method needs quite a bit of work to be adapted to work with pandas--  I already started doing it but ran out of time. It's just a matter of time and resources--note that you are _the only_ person to ever bring this up with us. \n",
      "> It's just a matter of time and resources--\n\nNoted that. And started to expand it by myself.\n\n> note that you are the only person to ever bring this up with us. \n\nAllow one more question:\n\nSo would you suppose: Others do not need this functionality or they just have their own function based n what already exists within pandas?\nIn the case of the latter, I will ask for help on the ML or Stack-X-change.\n",
      "@wesm:\n\nI finally got a working solution for the hourly frequency. But it diverts a but from the existing function.\n\nI found that using years as rows and hours (of the year) as columns is difficult in e.g. excel.\n\nSo I transposed the thing.\n\nHow would be best proceed.\n\nShall I share a working example via email?\n\nIf you are fine, I would then try to make a test and prepare a qualified PR.\n\nBTW: let's rename this issue to \"Create a generic version of convert_to_annual function in scikits.timeseriesfinalise in pivot_annual\". This way it would get found found better under the right topic.\n",
      "I have a code up for the hourly frequency.\n\nplease look at #2153\n",
      "Sorry about digging something old as this, but is there a way to do year over year aggregation directly?\n",
      "maybe show an example of what you are looking \n",
      "http://stackoverflow.com/questions/30379789/plot-pandas-data-frame-with-year-over-year-data\n",
      "@ispmarin Answered in so. Can you check?\n",
      "Sure. Gonna test and will report back. \n",
      "Works. Thanks!\n\nThis should be in the manual somewhere. It seems to be a very common question on stack overflow and very useful on financial circles.\n",
      "@sinhrks let's add this recipe to ththe cookbook and close this issue then\n",
      "I can help writing the docs, if needed.\n",
      "@ispmarin that would be gr8!. \n",
      "BTW, we do have a `pivot_annual` (https://github.com/pydata/pandas/blob/master/pandas/tseries/util.py), although I don't think we should put that in the picture ..\n",
      "Thanks for help, @ispmarin \n\n`pivot_annual` looks to be replaced by dt property/accessor and `TimeGrouper`. Enhance time grouping docs and deprecate it?\n",
      "So where do this issue stands? Is the answer on stack overflow the right way to do this, or there is another way?\n",
      "yes, @sinhrks 's answer at stackoverflow is the way to go\n",
      "Ok, got it. How is the best way to create the doc? Create a ipython notebook, annotated? Is there any guidelines for it?\n",
      "Can this issue be closed?\n",
      "idea was to add a recipe in the cookbook\nbut not sure it's that big of a deal\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "cross-referenced",
      "referenced",
      "referenced",
      "commented",
      "commented",
      "commented",
      "subscribed",
      "commented",
      "mentioned",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 4,
    "additions": 28,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/cookbook.rst",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/tseries/tests/test_util.py",
      "pandas/tseries/util.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 741,
    "reporter": "lodagro",
    "created_at": "2012-02-03T15:18:06+00:00",
    "closed_at": "2012-03-01T19:40:10+00:00",
    "resolver": "lodagro",
    "resolved_in": "a2e86c20f893075ed8efe6c199507582c4880df6",
    "resolver_commit_num": 24,
    "title": "plot/hist/boxplot with non numeric/date Index and MultiIndex",
    "body": "Plotting functions work fine if the index holds numeric or date data.\nBut error is generated when Index holds e.g strings, or when MultiIndex is used.\n",
    "labels": [],
    "comments": [
      "My guess is you know about this, but just in case: you can pass use_index=False to the plot method to avoid errors in this case.  Were you hoping it would use the strings as the tick marks?\n",
      "Idea is indeed to use a string representation of the index keys for tick marks.\n\n``` python\nIn [14]: df\nOut[14]: \n      consumption\ncar1  5.5        \ncar2  7.0        \ncar3  4.0        \n\nIn [15]: df.plot()\n\ngives: ValueError: could not convert string to float: car3\n```\n\n``` python\n\nIn [24]: df\nOut[24]: \n               consumption\nbrand  model              \nbrand1 model1  5.5        \n       model2  7.0        \nbrand2 model1  4.0\n\ngives: ValueError: setting an array element with a sequence.\n```\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "subscribed",
      "assigned"
    ],
    "changed_files": 4,
    "additions": 66,
    "deletions": 7,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/core/index.py",
      "pandas/core/series.py",
      "pandas/tests/test_graphics.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 742,
    "reporter": "james18",
    "created_at": "2012-02-03T22:42:02+00:00",
    "closed_at": "2012-02-05T23:44:13+00:00",
    "resolver": "adamklein",
    "resolved_in": "70c75093003bd4f0d6ece6a888175c98c92c7504",
    "resolver_commit_num": 136,
    "title": "DataFrame.rank() does not work for dtype object",
    "body": "from pandas import DataFrame\n\ndf = DataFrame([['b','c','a'],['a','c','b']])\n\nranked = df..rank(1)\n\nCurrently:\nranked = Empty DataFrame\n\nDesired\nranked  = a ranked DataFrame (by row) as in the case of DataFrame.rank(1) of dtype = float/int/etc\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned",
      "subscribed",
      "cross-referenced",
      "referenced",
      "subscribed",
      "referenced",
      "referenced",
      "closed",
      "referenced",
      "referenced",
      "reopened"
    ],
    "changed_files": 4,
    "additions": 68,
    "deletions": 66,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/src/stats.pyx",
      "pandas/src/tseries.pyx",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 743,
    "reporter": "ruidc",
    "created_at": "2012-02-04T09:25:29+00:00",
    "closed_at": "2012-02-04T21:37:43+00:00",
    "resolver": "adamklein",
    "resolved_in": "56e4ceeab6ad73538e26b8ada8d9a805c20fe35d",
    "resolver_commit_num": 131,
    "title": "Empty DataFrame returned when grouping on datetime column",
    "body": "import datetime\nimport numpy\nimport pandas\n\ndef simple_test(use_date=True):\n    data = [\n            [1, '2012-01-01', 1.0],\n            [2, '2012-01-02', 2.0],\n            [3, None, 3.0]\n          ]\n    if use_date:\n        data = [[row[0], datetime.datetime.strptime(row[1], '%Y-%m-\n%d').date() if row[1] else None, row[2]] for row in data]\n    df = pandas.DataFrame({'key': [x[0] for x in data], 'date': [x[1]\nfor x in data], 'value': [x[2] for x in data]})\n    df['weights'] = df['value']/df['value'].sum()\n    gb = df.groupby('date').aggregate(numpy.sum)\n\n\n\nstrings'\n    print df\n    print ''\n    print gb\n\nif **name** == '**main**':\n    simple_test(use_date=False)\n    print '*' \\* 40\n    simple_test(use_date=True)\n",
    "labels": [],
    "comments": [
      "Trying to get formatting right to reproduce \n\n```\nimport datetime\nimport numpy\nimport pandas\n\ndef simple_test(use_date=True):\n    data = [[1, '2012-01-01', 1.0],\n            [2, '2012-01-02', 2.0],\n            [3, None, 3.0]]\n    if use_date:\n        data = [[row[0], datetime.datetime.strptime(row[1], '%Y-%m-%d').date() if row[1] else None, row[2]] for row in data]\n    df = pandas.DataFrame({'key': [x[0] for x in data], 'date': [x[1] for x in data], 'value': [x[2] for x in data]})\n    df['weights'] = df['value']/df['value'].sum()\n    gb = df.groupby('date').aggregate(numpy.sum)\n\n    print 'II) Using date objects' if use_date else 'I) Using datestrings'\n    print df\n    print ''\n    print gb\n\nsimple_test(use_date=False)\nprint '*' * 40\nsimple_test(use_date=True)\n```\n",
      "So, the problem is the None.  Somewhere in the code it is throwing a TypeError trying to compare NoneType with datetime.datetime, getting caught and returning empty instead of passing the error up to the user. I'll see if there's a simple workaround.\n",
      "Simple workaround: call df.groupby('date', sort=False)\n\nYou can't compare datetime object and none object, don't see any other workaround.\n",
      "Actually, reopening because probably want error and not empty dataframe?\n",
      "My personal preference might be to see a warning here, e.g. in groupby.py line 1087\n\n```\n    for item in obj:\n        try:\n            colg = SeriesGroupBy(obj[item], column=item,\n                                 groupings=self.groupings)\n            result[item] = colg.agg(func, *args, **kwargs)\n        except (ValueError, TypeError), errmsg:\n            print >>sys.stderr, errmsg\n            cannot_agg.append(item)\n            continue\n```\n\nThoughts?\n",
      "`None` should be treated as NA and it is not. Marking as a bug\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "commented",
      "closed",
      "reopened",
      "commented",
      "commented",
      "commented",
      "subscribed",
      "assigned"
    ],
    "changed_files": 1,
    "additions": 1,
    "deletions": 1,
    "changed_files_list": [
      "pandas/src/hashtable.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 754,
    "reporter": "wesm",
    "created_at": "2012-02-05T22:05:54+00:00",
    "closed_at": "2012-04-10T15:57:33+00:00",
    "resolver": "wesm",
    "resolved_in": "d196363afcd88fce504254b6a18a45b956d7d9b9",
    "resolver_commit_num": 1675,
    "title": "Allow different na_sentinels via dict in read_csv, etc.",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 51,
    "deletions": 20,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 755,
    "reporter": "lbeltrame",
    "created_at": "2012-02-06T15:58:17+00:00",
    "closed_at": "2012-02-06T16:00:26+00:00",
    "resolver": "wesm",
    "resolved_in": "5602b98a913269a4aca3a51ca06d9ede04fc9d96",
    "resolver_commit_num": 1484,
    "title": "Current git master does not import cleanly if bottleneck is not installed",
    "body": "The culprit is in nanops.py, line 17 (some extra lines shown for context):\n\n\n\n`bn` is only defined if bottleneck is imported and the `try` statement doesn't catch the NameError that ensues (only AttributeError).\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 11,
    "additions": 767,
    "deletions": 724,
    "changed_files_list": [
      "pandas/__init__.py",
      "pandas/core/api.py",
      "pandas/core/common.py",
      "pandas/core/format.py",
      "pandas/core/frame.py",
      "pandas/core/index.py",
      "pandas/core/nanops.py",
      "pandas/core/series.py",
      "pandas/tests/test_common.py",
      "pandas/tests/test_format.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 758,
    "reporter": "soramame0518",
    "created_at": "2012-02-07T09:10:33+00:00",
    "closed_at": "2012-02-07T18:41:45+00:00",
    "resolver": "wesm",
    "resolved_in": "4f793bec8c0373caf0be0d7886697772d7d3682a",
    "resolver_commit_num": 1488,
    "title": "Strange behavior of Series/TimeSeries with datetime index",
    "body": "I had a trouble with Series and TimeSeries using datetime index. Here is an example:\n\nIn [1]: from pandas import *\n\nIn [2]: t = [datetime(2012, 2, 7, 0, 0, 0), datetime(2012, 2, 7, 23, 0, 0)]\n\nIn [3]: s = Series([0, 1], index=t)\n\nIn [4]: print s\n2012-02-07             0\n2012-02-07 00:00:00    1\n\nThe first index value should be '2012-02-07 00:00:00', and the second should be '2012-02-07 23:00:00' Am I doing wrong?\n",
    "labels": [],
    "comments": [
      "Confirmed,  it looks like an output or formatting error. It also looks like the data should be ok.\n\n```\nIn [5]: t\nOut[5]: [datetime.datetime(2012, 2, 7, 0, 0), datetime.datetime(2012, 2, 7, 23, 0)]\n\nIn [6]: s = Series([0,1], index=t)\n\nIn [7]: s\nOut[7]: \n2012-02-07             0\n2012-02-07 00:00:00    1\n\nIn [9]: s.index\nOut[9]: Index([2012-02-07 00:00:00, 2012-02-07 23:00:00], dtype=object)\n\nIn [10]: s.index[0]\nOut[10]: datetime.datetime(2012, 2, 7, 0, 0)\n\nIn [11]: s.index[1]\nOut[11]: datetime.datetime(2012, 2, 7, 23, 0)\n```\n",
      "Thanks for your comment!\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented"
    ],
    "changed_files": 4,
    "additions": 213,
    "deletions": 170,
    "changed_files_list": [
      "pandas/core/format.py",
      "pandas/core/index.py",
      "pandas/tests/test_format.py",
      "pandas/tests/test_index.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 764,
    "reporter": "adamklein",
    "created_at": "2012-02-08T15:56:21+00:00",
    "closed_at": "2012-02-08T16:46:19+00:00",
    "resolver": "adamklein",
    "resolved_in": "f86c3befcc2108e5be7fe634b49372ac4b6ee25e",
    "resolver_commit_num": 148,
    "title": "set_index with multindex on columns destroys column-multiindex",
    "body": "Per Chris Billington on pystatsmodels\n\n\n",
    "labels": [],
    "comments": [
      "Code to construct df:\n\n```\narrays = [[  'a',   'b',   'c',    'top',    'top', 'routine1', 'routine1', 'routine2'],\n        [   '',    '',    '',     'OD',     'OD', 'result1',   'result2', 'result1'],\n        [   '',    '',    '',     'wx',     'wy',        '',          '',        '']]\n\ntuples = zip(*arrays)\ntuples.sort()\nindex = MultiIndex.from_tuples(tuples)\n\ndf = DataFrame(randn(3,8), columns=index)\n```\n"
    ],
    "events": [
      "subscribed",
      "commented"
    ],
    "changed_files": 3,
    "additions": 20,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/index.py",
      "pandas/core/internals.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 766,
    "reporter": "wesm",
    "created_at": "2012-02-08T21:02:07+00:00",
    "closed_at": "2012-02-08T22:38:37+00:00",
    "resolver": "wesm",
    "resolved_in": "c0fb29c78f7cc592a186a24d92a551f0994e51ad",
    "resolver_commit_num": 1503,
    "title": "Improper handling of None key inserted as DataFrame column",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 16,
    "deletions": 8,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/src/inference.pyx",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_index.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 770,
    "reporter": "wesm",
    "created_at": "2012-02-09T17:29:53+00:00",
    "closed_at": "2012-02-09T17:42:11+00:00",
    "resolver": "wesm",
    "resolved_in": "d207a256fd5ab530c7f0d2124d0b549a693ce2ca",
    "resolver_commit_num": 1516,
    "title": "DateRange logic error with DateOffset(months=3)",
    "body": "This yields incorrect results:\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 19,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "doc/source/whatsnew/v0.7.0.txt",
      "pandas/core/datetools.py",
      "pandas/tests/test_daterange.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 775,
    "reporter": "wesm",
    "created_at": "2012-02-12T22:16:26+00:00",
    "closed_at": "2012-02-18T20:13:02+00:00",
    "resolver": "wesm",
    "resolved_in": "7cae5fd2bdd97c8092bc61f84e6a7cc377f782c6",
    "resolver_commit_num": 1545,
    "title": "Eliminate redundant groupby computation on applying different functions to each column",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 145,
    "deletions": 111,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py",
      "vb_suite/groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 777,
    "reporter": "davidbrai",
    "created_at": "2012-02-13T00:26:37+00:00",
    "closed_at": "2012-02-13T14:54:05+00:00",
    "resolver": "adamklein",
    "resolved_in": "27672501ed097005230e4e373196c368b1578d1a",
    "resolver_commit_num": 156,
    "title": "DataFrame with high ascii cannot be displayed in IPython (regression from 0.6.1)",
    "body": "edit: This happens on Windows 7 running IPython from cmd.\n\nIn pandas 0.6.1:\nIn [13]: pandas.version.version\nOut[13]: '0.6.1'\n\nIn [14]: pandas.DataFrame(['\\xc2'])\nOut[14]:\n   0\n0  \u252c\n\nIn pandas 0.7.0:\nIn [2]: pandas.version.version\nOut[2]: '0.7.0'\n\nIn [3]: pandas.DataFrame(['\\xc2'])\n.....\nC:\\Python26\\lib\\site-packages\\pandas\\core\\common.pyc in _stringify(col)\n    503 def _stringify(col):\n    504     # unicode workaround\n\n--> 505     return unicode(col)\n    506\n    507 def _maybe_make_list(obj):\n\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 0: ordinal not in range(128)\n",
    "labels": [],
    "comments": [
      "Confirmed in head revision on linux/ipython as well.\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 9,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/common.py",
      "pandas/tests/test_format.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 780,
    "reporter": "xdong",
    "created_at": "2012-02-14T07:23:50+00:00",
    "closed_at": "2012-02-18T00:50:47+00:00",
    "resolver": "adamklein",
    "resolved_in": "35fcc17e7052ba42fac7b3c6b41311ef470266fb",
    "resolver_commit_num": 160,
    "title": ".ix strange bug for float index",
    "body": "In [1]: import pandas\n\nIn [2]: index = [52195.504153, 52196.303147, 52198.369883]\n\nIn [3]: a = pandas.DataFrame(randn(3, 2), index)\n\nIn [4]: a\nOut[4]: \n                     0         1\n52195.504153  1.367681  0.243237\n52196.303147 -0.745796 -1.054106\n52198.369883 -1.462461 -0.683286\n\nIn [5]: a.ix[52195.:52196.]\nOut[5]: \nEmpty DataFrame\nColumns: array([0, 1])\nIndex: array([], dtype=object)\n\nIn [6]: a.ix[52195.1:52196.5]\nOut[6]: \nEmpty DataFrame\nColumns: array([0, 1])\nIndex: array([], dtype=object)\n\nIn [7]: a.ix[52195.1:52196.6]\nOut[7]: \n                     0         1\n52195.504153  1.367681  0.243237\n52196.303147 -0.745796 -1.054106\n",
    "labels": [],
    "comments": [
      "Thanks for the quick fix. I was going to comment on another issue on float based slicing, but I saw that you had it fixed in commit bc1932f.\n\nNow the float based slicing works as expected when the floats are whole numbers. For example, df.ix[2.0:5.0] is considered label-based as promised in the documentation. However, if I mix integer and float then:\n\ndf.ix[2:5.0] is interpretated as interger-based;\ndf.ix[2:5.1] is interpretated as label-based.\n\nI am worried that it may introduce subtle bugs (admittedly, it's bad practice to mix integer and float.)\n",
      "There is definitely still some weirdness in slicing. It's been a game of whack-a-mole. \n\nPart of the complexity is that slicing is context-dependent on what's in the index.\n\nI believe the slicing you point out will be consistent as long as the index type doesn't change ... what I mean is:\n\n```\nIn [76]: x = Index([1.5, 2, 3, 4, 5])\n\nIn [77]: df = DataFrame(rand(5,5), index=x)\n\nIn [78]: df.ix[1.5:4]\nOut[78]: \n     0        1        2         3       4     \n1.5  0.06102  0.25070  0.009453  0.6829  0.6631\n2    0.81916  0.95604  0.397659  0.7903  0.3951\n3    0.30179  0.64651  0.701975  0.3746  0.6955\n4    0.13221  0.04839  0.788082  0.3093  0.1095\n\nIn [79]: df.ix[4:5]\nOut[79]: \n   0       1       2        3       4      \n5  0.5301  0.8182  0.05318  0.8247  0.01699\n\nIn [80]: df.ix[1.5:4].index\nOut[80]: Index([1.5, 2, 3, 4], dtype=object)\n\nIn [81]: df.ix[4:5].index\nOut[81]: Int64Index([5])\n```\n\nThis is a bit surprising, that depending on where in the index you slice, you get integer or label based. \n\nI think that maybe the index shouldn't change types when it is subsetted (ie, if it's not an Int64Index, should never become one when sliced).\n\nFurthermore, from the docs: \"Therefore, advanced indexing with .ix will always attempt label-based indexing, before falling back on integer-based indexing.\"\n\nThis doesn't seem to be true per the last output, may need fixing here.\n"
    ],
    "events": [
      "subscribed",
      "closed",
      "subscribed",
      "commented",
      "commented",
      "reopened",
      "referenced",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 17,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/indexing.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 791,
    "reporter": "wesm",
    "created_at": "2012-02-16T00:09:40+00:00",
    "closed_at": "2012-04-05T00:14:24+00:00",
    "resolver": "wesm",
    "resolved_in": "4f4c3db67aa755ec7945ed4c247af1245a49c5c3",
    "resolver_commit_num": 1644,
    "title": "Write function to ensure boolean indexing of DatetimeIndex preserves frequency, where relevant",
    "body": "If the mask is a contiguous range (check for this), should preserve the frequency metadata and set any relevant flags\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 69,
    "deletions": 25,
    "changed_files_list": [
      "pandas/core/index.py",
      "pandas/src/tseries.pyx",
      "pandas/tests/test_datetime64.py",
      "pandas/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 792,
    "reporter": "wesm",
    "created_at": "2012-02-16T01:28:19+00:00",
    "closed_at": "2012-02-16T01:29:32+00:00",
    "resolver": "wesm",
    "resolved_in": "ab9898ce26589d697c6b73ae614f799c2893e808",
    "resolver_commit_num": 1528,
    "title": "List of Series passed to DataFrame ctor does not behave like list of dicts",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 72,
    "deletions": 13,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/core/panel.py",
      "pandas/tests/test_frame.py",
      "vb_suite/groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 795,
    "reporter": "wesm",
    "created_at": "2012-02-17T04:48:12+00:00",
    "closed_at": "2012-03-15T19:30:54+00:00",
    "resolver": "wesm",
    "resolved_in": "381ef5d48f7695705c8a19abcbb6aa75537ca73d",
    "resolver_commit_num": 1587,
    "title": "Unicode repr failure in DataFrame",
    "body": "\n\nhere are lines\n\n\n",
    "labels": [],
    "comments": [
      "I'm hitting this issue right now with the MovieLens 100k dataset, which uses the iso8859_2 encoding (as inferred by chardet).\n\nMy issue: when I call df.to_string, I can pass \"force_unicode=True\". However, I am not sure how to set force_unicode=True for all calls to to_string, e.g. any time repr() is called on df, which occurs when printing the df to the shell.\n\nGnarly issue. Character encodings in Python are never fun.\n",
      "@hammer : Is this with the current development version of pandas? Can you post the traceback somewhere?\n",
      "@hammer I'm able to reproduce the issue on the movielens data. If you pass `encoding='iso8859_2'` when you use `read_csv` everything works fine. \n\n```\nIn [8]: df = read_csv('/Users/wesm/code/pandas/pandas/tests/unicode_series.csv', header=None, encoding='iso8859_2')\n\nIn [9]: df\nOut[9]: \n     X.1                                                 X.2\n0   1582                Invitation, The (Zaproszenie) (1986)\n1   1583                      Symphonie pastorale, La (1946)\n2   1584                               American Dream (1990)\n3   1585                               Lashou shentan (1992)\n4   1586                       Terror in a Texas Town (1958)\n5   1587                                Salut cousin! (1996)\n6   1588                                  Schizopolis (1996)\n7   1589                              To Have, or Not (1995)\n8   1590                               Duoluo tianshi (1995)\n9   1591                              Magic Hour, The (1998)\n10  1592                           Death in Brunswick (1991)\n11  1593                                      Everest (1998)\n12  1594                                     Shopping (1994)\n13  1595                            Nemesis 2: Nebula (1995)\n14  1596                               Romper Stomper (1992)\n15  1597                             City of Industry (1997)\n16  1598                       Someone Else's America (1995)\n17  1599                                 Guantanamera (1994)\n18  1600                                Office Killer (1997)\n19  1601                        Price Above Rubies, A (1998)\n20  1602                                       Angela (1995)\n21  1603                           He Walked by Night (1948)\n22  1604                                Love Serenade (1996)\n23  1605                                     Deceiver (1997)\n24  1606                            Hurricane Streets (1998)\n25  1607                                        Buddy (1997)\n26  1608                                      B*A*P*S (1997)\n27  1609                  Truth or Consequences, N.M. (1997)\n28  1610                           Intimate Relations (1996)\n29  1611                             Leading Man, The (1996)\n30  1612                                   Tokyo Fist (1995)\n31  1613                     Reluctant Debutante, The (1958)\n32  1614                           Warriors of Virtue (1997)\n33  1615                                 Desert Winds (1995)\n34  1616                                    Hugo Pool (1997)\n35  1617                             King of New York (1990)\n36  1618                              All Things Fair (1996)\n37  1619                               Sixth Man, The (1997)\n38  1620                               Butterfly Kiss (1995)\n39  1621                                Paris, France (1993)\n40  1622                                C\u00e9r\u00e9monie, La (1995)\n41  1623                                         Hush (1998)\n42  1624                                   Nightwatch (1997)\n43  1625          Nobody Loves Me (Keiner liebt mich) (1994)\n44  1626                                    Wife, The (1995)\n45  1627                                     Lamerica (1994)\n46  1628                                    Nico Icon (1995)\n47  1629  Silence of the Palace, The (Saimt el Qusur) (1994)\n48  1630                               Slingshot, The (1993)\n49  1631         Land and Freedom (Tierra y libertad) (1995)\n50  1632                  \u00c1 k\u00f6ldum klaka (Cold Fever) (1994)\n51  1633     Etz Hadomim Tafus (Under the Domin Tree) (1994)\n52  1634                                 Two Friends (1986) \n53  1635                          Brothers in Trouble (1995)\n54  1636                                   Girls Town (1996)\n55  1637                                  Normal Life (1996)\n56  1638                 Bitter Sugar (Azucar Amargo) (1996)\n57  1639                              Eighth Day, The (1996)\n58  1640                                     Dadetown (1995)\n59  1641                            Some Mother's Son (1996)\n60  1642                                   Angel Baby (1995)\n61  1643                             Sudden Manhattan (1996)\n62  1644                             Butcher Boy, The (1998)\n63  1645                                Men With Guns (1997)\n64  1646                                      Hana-bi (1997)\n65  1647                             Niagara, Niagara (1997)\n66  1648                                 Big One, The (1997)\n67  1649                             Butcher Boy, The (1998)\n68  1650                        Spanish Prisoner, The (1997)\n69  1651                    Temptress Moon (Feng Yue) (1996)\n70  1652   Entertaining Angels: The Dorothy Day Story (1996)\n71  1653                        Chairman of the Board (1998)\n72  1654                                   Favor, The (1994)\n73  1655                                  Little City (1998)\n74  1656                                       Target (1995)\n75  1657                       Substance of Fire, The (1996)\n76  1658                     Getting Away With Murder (1996)\n77  1659                                  Small Faces (1995)\n78  1660                                 New Age, The (1994)\n79  1661                                  Rough Magic (1995)\n80  1662                             Nothing Personal (1995)\n81  1663                      8 Heads in a Duffel Bag (1997)\n82  1664                            Brother's Kiss, A (1997)\n83  1665                                         Ripe (1996)\n84  1666                               Next Step, The (1995)\n85  1667                           Wedding Bell Blues (1996)\n86  1668                            MURDER and murder (1996)\n87  1669                                      Tainted (1998)\n88  1670                           Further Gesture, A (1996)\n89  1671                                         Kika (1993)\n90  1672                                       Mirage (1995)\n91  1673                                   Mamma Roma (1962)\n92  1674                               Sunchaser, The (1996)\n93  1675                             War at Home, The (1996)\n94  1676                                Sweet Nothing (1995)\n95  1677                                   Mat' i syn (1997)\n96  1678                                    B. Monkey (1998)\n97  1679                                Sliding Doors (1998)\n98  1680                                 You So Crazy (1994)\n99  1681           Scream of Stone (Schrei aus Stein) (1991)\n```\n\nI'll see about doing this automatically with chardet or some way to modify the repr code to not blow up with a UnicodeError\n",
      "ok @hammer I think I have this sorted out. If you don't specify the encoding it will not blow up anymore: \n\n```\nIn [3]: df = read_csv('pandas/tests/unicode_series.csv', header=None)\nIn [4]: df\nOut[4]: \n     X.1                                                 X.2\n0   1617                             King of New York (1990)\n1   1618                              All Things Fair (1996)\n2   1619                               Sixth Man, The (1997)\n3   1620                               Butterfly Kiss (1995)\n4   1621                                Paris, France (1993)\n5   1622                                C?r?monie, La (1995)\n6   1623                                         Hush (1998)\n7   1624                                   Nightwatch (1997)\n8   1625          Nobody Loves Me (Keiner liebt mich) (1994)\n9   1626                                    Wife, The (1995)\n10  1627                                     Lamerica (1994)\n11  1628                                    Nico Icon (1995)\n12  1629  Silence of the Palace, The (Saimt el Qusur) (1994)\n13  1630                               Slingshot, The (1993)\n14  1631         Land and Freedom (Tierra y libertad) (1995)\n15  1632                  ? k?ldum klaka (Cold Fever) (1994)\n16  1633     Etz Hadomim Tafus (Under the Domin Tree) (1994)\n17  1634                                  Two Friends (1986)\n```\n\nbut if you do, it will render the Unicode correctly in the console. \n\n```\nIn [5]: df = read_csv('pandas/tests/unicode_series.csv', header=None, encoding='iso-8859-2')\nIn [6]: df\nOut[6]: \n     X.1                                                 X.2\n0   1617                             King of New York (1990)\n1   1618                              All Things Fair (1996)\n2   1619                               Sixth Man, The (1997)\n3   1620                               Butterfly Kiss (1995)\n4   1621                                Paris, France (1993)\n5   1622                                C\u00e9r\u00e9monie, La (1995)\n6   1623                                         Hush (1998)\n7   1624                                   Nightwatch (1997)\n8   1625          Nobody Loves Me (Keiner liebt mich) (1994)\n9   1626                                    Wife, The (1995)\n10  1627                                     Lamerica (1994)\n11  1628                                    Nico Icon (1995)\n12  1629  Silence of the Palace, The (Saimt el Qusur) (1994)\n13  1630                               Slingshot, The (1993)\n14  1631         Land and Freedom (Tierra y libertad) (1995)\n15  1632                  \u00c1 k\u00f6ldum klaka (Cold Fever) (1994)\n16  1633     Etz Hadomim Tafus (Under the Domin Tree) (1994)\n17  1634                                  Two Friends (1986)\n```\n\nShort of shipping chardet I don't know if there's a way to automatically infer the encoding\n",
      "However this broke Python 3 tests. leaving issue open\n",
      "I'll look into it with Python 3.\n",
      "I get one failure, with reading the newly added CSV file. `pandas.core.common._get_handle` (https://github.com/pydata/pandas/blob/master/pandas/core/common.py#L651) returns a text-mode file handle in Python 3, and the default behaviour is to throw errors if the file can't be decoded with the (platform dependent) default encoding. Specifying `errors=\"replace\"` gets the same behaviour as in Python 2 here (unknown characters replaced with \ufffd), but I've not looked at where else `_get_handle` is used.\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "referenced",
      "closed",
      "commented",
      "subscribed",
      "commented",
      "mentioned",
      "reopened",
      "commented",
      "mentioned",
      "closed",
      "commented",
      "mentioned",
      "reopened",
      "commented",
      "commented",
      "commented",
      "referenced",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 56,
    "deletions": 5,
    "changed_files_list": [
      "pandas/core/common.py",
      "pandas/core/format.py",
      "pandas/core/index.py",
      "pandas/tests/test_format.py",
      "pandas/tests/unicode_series.csv"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 812,
    "reporter": "wesm",
    "created_at": "2012-02-22T22:05:00+00:00",
    "closed_at": "2012-02-24T18:56:22+00:00",
    "resolver": "adamklein",
    "resolved_in": "869cf0fb84c5f0cbaabb8577b3cf9e62d4830d89",
    "resolver_commit_num": 208,
    "title": "Reindex with Series with name field drops name",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned",
      "subscribed",
      "closed",
      "reopened"
    ],
    "changed_files": 14,
    "additions": 306,
    "deletions": 59,
    "changed_files_list": [
      "RELEASE.rst",
      "doc/source/basics.rst",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/series.py",
      "pandas/src/inference.pyx",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_series.py",
      "pandas/tools/plotting.py",
      "scripts/git_code_churn.py",
      "scripts/groupby_speed.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 813,
    "reporter": "wesm",
    "created_at": "2012-02-22T22:43:28+00:00",
    "closed_at": "2012-05-15T19:18:20+00:00",
    "resolver": "wesm",
    "resolved_in": "3fdc6d986a155953a7abb1548ff4d6389f9aadcb",
    "resolver_commit_num": 1902,
    "title": "Implement ordered merge function",
    "body": "Something like this:\n\n\n\nbut should be able to propagate certain fields and allow others to be NA\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 155,
    "deletions": 34,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/src/join.pyx",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py",
      "vb_suite/join_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 815,
    "reporter": "wesm",
    "created_at": "2012-02-23T04:56:54+00:00",
    "closed_at": "2012-05-19T19:18:37+00:00",
    "resolver": "wesm",
    "resolved_in": "9020b9e2ecf6b551fe3d2a4615d83d719e5e216d",
    "resolver_commit_num": 1916,
    "title": "Add more careful check preventing malformed BlockManager",
    "body": "see #814\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 20,
    "deletions": 6,
    "changed_files_list": [
      "pandas/core/internals.py",
      "pandas/io/pytables.py",
      "pandas/tools/merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 816,
    "reporter": "wesm",
    "created_at": "2012-02-23T05:05:39+00:00",
    "closed_at": "2013-08-16T19:27:44+00:00",
    "resolver": "adamklein",
    "resolved_in": "869cf0fb84c5f0cbaabb8577b3cf9e62d4830d89",
    "resolver_commit_num": 208,
    "title": "np.diff fails when called on a Series",
    "body": "\n",
    "labels": [],
    "comments": [
      "On Dec 3, 2012, at 1:31 PM, Dan Allan notifications@github.com wrote:\n\n> There is still trouble here.\n> \n> In [163]: s = Series(np.arange(10))\n> \n> In [164]: np.diff(s)\n> Out[164]: \n> 0   NaN\n> 1     0\n> 2     0\n> 3     0\n> 4     0\n> 5     0\n> 6     0\n> 7     0\n> 8     0\n> 9   NaN\n> \u2014\n> Reply to this email directly or view it on GitHub.\n\nHi Dan, you should be using s.diff() instead.\nnp.diff treats the input argument as index unaware so it does not do the right thing for a Series. \nIf you want to duplicate np.diff results, you can do np.diff(s.values)\n",
      "I'm reopening and moving this to at minimum 0.11. The solution here is to make Series _not_ an ndarray, once and for all. The issue is the `numpy.diff` uses `asanyarray` instead of `asarray`, and Series has different array behavior than `diff`\n",
      "I just ask that downstream packages have plenty of time to look at this before it becomes released / merged. I don't know the implications yet, but I'm almost positive that I abuse the Series is an array sub-class more than I think. \n",
      "Do I understand it right that you want to make Series not a subclass of ndarray? Does that mean that you have to re-implement all the statistical helper functions inside? Or would this imply a lot of API changes for Series?\n",
      "current behavior (0.12):\n\n```\nIn [6]: np.diff(Series(np.arange(10)))\nOut[6]: \n0   NaN\n1     0\n2     0\n3     0\n4     0\n5     0\n6     0\n7     0\n8     0\n9   NaN\ndtype: float64\n\n```\n\nafter #3482 \n\n```\nIn [1]: s = Series(np.arange(10))\n\nIn [2]: np.diff(s)\nOut[2]: array([1, 1, 1, 1, 1, 1, 1, 1, 1])\n\nIn [3]: np.diff(s.values)\nOut[3]: array([1, 1, 1, 1, 1, 1, 1, 1, 1])\nIn [4]: s.diff()\n\nOut[4]: \n0   NaN\n1     1\n2     1\n3     1\n4     1\n5     1\n6     1\n7     1\n8     1\n9     1\ndtype: float64\n\n```\n",
      "closed by #3482\n"
    ],
    "events": [
      "subscribed",
      "assigned",
      "subscribed",
      "closed",
      "reopened",
      "closed",
      "commented",
      "commented",
      "reopened",
      "commented",
      "commented",
      "cross-referenced",
      "commented"
    ],
    "changed_files": 14,
    "additions": 306,
    "deletions": 59,
    "changed_files_list": [
      "RELEASE.rst",
      "doc/source/basics.rst",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/series.py",
      "pandas/src/inference.pyx",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_series.py",
      "pandas/tools/plotting.py",
      "scripts/git_code_churn.py",
      "scripts/groupby_speed.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 817,
    "reporter": "adamklein",
    "created_at": "2012-02-23T15:54:34+00:00",
    "closed_at": "2012-02-24T20:50:26+00:00",
    "resolver": "adamklein",
    "resolved_in": "869cf0fb84c5f0cbaabb8577b3cf9e62d4830d89",
    "resolver_commit_num": 208,
    "title": "Groupby performance discrepency in timestamped data",
    "body": "From mailing list\n\nserie = pandas.io.parsers.read_csv(f, parse_dates=True,\ndate_parser=dateParser, index_col=0)\ndateRange = pandas.DateRange(start, end, offset=5 *\npandas.datetools.Minute())\ngrouped = serie.groupby(dateRange.asof)\n# version 1\n\nt = time()\nfor date in serie.index:\n   k = grouped.grouper(date) # returns the key of the group where\ndate belongs\n   g = grouped.get_group(k)\nprint time()-t\n# version 2\n\nt = time()\nfor date in serie.index:\n   k = grouped.grouper(date)\n   g = serie.ix[grouped.groups[k]]\nprint time()-t\n\nserie is something looking like this (financial data indexed with\ndatetime)\n<class 'pandas.core.frame.DataFrame'>\nIndex: 476640 entries, 2011-01-03 00:00:00 to 2011-11-29 23:59:00\nData columns:\nOpen      476640  non-null values\nHigh      476640  non-null values\nLow       476640  non-null values\nClose     476640  non-null values\nVolume    476640  non-null values\ndtypes: float64(5)\n\nFor 100.000 elts, version 1 performs in 480 secs, while version 2\ntakes only 25 secs\nFor the full 460.000 elts, we then get about 40 mins and 135 secs\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned",
      "subscribed",
      "closed",
      "reopened",
      "referenced"
    ],
    "changed_files": 14,
    "additions": 306,
    "deletions": 59,
    "changed_files_list": [
      "RELEASE.rst",
      "doc/source/basics.rst",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/series.py",
      "pandas/src/inference.pyx",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_series.py",
      "pandas/tools/plotting.py",
      "scripts/git_code_churn.py",
      "scripts/groupby_speed.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 818,
    "reporter": "adamklein",
    "created_at": "2012-02-23T16:03:18+00:00",
    "closed_at": "2012-02-24T17:59:57+00:00",
    "resolver": "adamklein",
    "resolved_in": "869cf0fb84c5f0cbaabb8577b3cf9e62d4830d89",
    "resolver_commit_num": 208,
    "title": "izip iterator method for DataFrame",
    "body": "From mailing list:\n\nOut of curiosity, is there a reason returning an izip iterator of the\nindex + cols isn't DataFrame method? I've got the following in my\ntoolbox now:\n\ndef iziprows(df):\n   series = [df[col] for col in df.columns]\n   series.insert(0,\ndf.index)\n   return izip(*series)\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned",
      "closed",
      "reopened"
    ],
    "changed_files": 14,
    "additions": 306,
    "deletions": 59,
    "changed_files_list": [
      "RELEASE.rst",
      "doc/source/basics.rst",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/series.py",
      "pandas/src/inference.pyx",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_series.py",
      "pandas/tools/plotting.py",
      "scripts/git_code_churn.py",
      "scripts/groupby_speed.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 819,
    "reporter": "adamklein",
    "created_at": "2012-02-23T16:28:07+00:00",
    "closed_at": "2012-02-24T19:01:40+00:00",
    "resolver": "adamklein",
    "resolved_in": "869cf0fb84c5f0cbaabb8577b3cf9e62d4830d89",
    "resolver_commit_num": 208,
    "title": "Bug in groupby when as_index=False",
    "body": "Recreation:\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned",
      "closed",
      "referenced",
      "subscribed",
      "reopened"
    ],
    "changed_files": 14,
    "additions": 306,
    "deletions": 59,
    "changed_files_list": [
      "RELEASE.rst",
      "doc/source/basics.rst",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/series.py",
      "pandas/src/inference.pyx",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_series.py",
      "pandas/tools/plotting.py",
      "scripts/git_code_churn.py",
      "scripts/groupby_speed.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 822,
    "reporter": "wesm",
    "created_at": "2012-02-23T21:59:08+00:00",
    "closed_at": "2012-02-24T20:58:23+00:00",
    "resolver": "adamklein",
    "resolved_in": "869cf0fb84c5f0cbaabb8577b3cf9e62d4830d89",
    "resolver_commit_num": 208,
    "title": "Non-string columns should get casted to string in DataFrame.to_records",
    "body": "data like this\n\n\n\nand\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned",
      "subscribed"
    ],
    "changed_files": 14,
    "additions": 306,
    "deletions": 59,
    "changed_files_list": [
      "RELEASE.rst",
      "doc/source/basics.rst",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/series.py",
      "pandas/src/inference.pyx",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_series.py",
      "pandas/tools/plotting.py",
      "scripts/git_code_churn.py",
      "scripts/groupby_speed.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 823,
    "reporter": "wesm",
    "created_at": "2012-02-24T00:03:42+00:00",
    "closed_at": "2012-05-12T16:53:13+00:00",
    "resolver": "wesm",
    "resolved_in": "b3a6107c5cc4a45c9e9001c50195778c86eada12",
    "resolver_commit_num": 1866,
    "title": "Handling of dict return values in groupby.apply",
    "body": "Should be treated the same as returning Series\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 29,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py",
      "vb_suite/groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 826,
    "reporter": "wesm",
    "created_at": "2012-02-24T20:18:45+00:00",
    "closed_at": "2012-06-11T17:53:35+00:00",
    "resolver": "paddymul",
    "resolved_in": "0cbee1253ee406792f0f7cf2529a59701b498377",
    "resolver_commit_num": 0,
    "title": "Try to workaround Yahoo! finance 404 errors, better error messages",
    "body": "randomly get this crap\n\n\n",
    "labels": [],
    "comments": [
      "What is the desired behavior for calling code when yahoo returns a 404?  Should this call fail?  Retry?  \n",
      "It should probably retry once or twice and then raise an exception if all 2-3 attempts fail\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 2,
    "additions": 46,
    "deletions": 11,
    "changed_files_list": [
      "pandas/io/data.py",
      "pandas/io/tests/test_yahoo.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 827,
    "reporter": "adamklein",
    "created_at": "2012-02-24T21:25:41+00:00",
    "closed_at": "2012-02-24T21:43:37+00:00",
    "resolver": "adamklein",
    "resolved_in": "869cf0fb84c5f0cbaabb8577b3cf9e62d4830d89",
    "resolver_commit_num": 208,
    "title": "rename fill_method to method in dataframe/series align",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "assigned",
      "subscribed"
    ],
    "changed_files": 14,
    "additions": 306,
    "deletions": 59,
    "changed_files_list": [
      "RELEASE.rst",
      "doc/source/basics.rst",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/series.py",
      "pandas/src/inference.pyx",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_series.py",
      "pandas/tools/plotting.py",
      "scripts/git_code_churn.py",
      "scripts/groupby_speed.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 831,
    "reporter": "wesm",
    "created_at": "2012-02-26T16:32:24+00:00",
    "closed_at": "2012-05-07T19:31:31+00:00",
    "resolver": "wesm",
    "resolved_in": "4c31c8339955d8c1cfc53eaae0166cf3a70af2a2",
    "resolver_commit_num": 1832,
    "title": "Pass list of arrays to create MultiIndex",
    "body": "Make it easier to create a hierarchical index from a list of arrays in the Series and DataFrame constructors\n",
    "labels": [],
    "comments": [
      "OK, I just made this work minimally without as_multi, used pared down versions of your test cases:\n\n```\nIn [2]: s = Series([1, 2, 3, 4], index=[[1, 1, 2, 2], [0, 0, 1, 1]])\n\nIn [3]: s\nOut[3]: \n1  0    1\n   0    2\n2  1    3\n   1    4\n```\n",
      "Need to add something to the docs for this guy\n"
    ],
    "events": [
      "subscribed",
      "assigned",
      "subscribed",
      "referenced",
      "commented",
      "commented"
    ],
    "changed_files": 5,
    "additions": 36,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/core/index.py",
      "pandas/core/series.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 835,
    "reporter": "mcobzarenco",
    "created_at": "2012-02-27T13:20:57+00:00",
    "closed_at": "2012-02-28T18:50:18+00:00",
    "resolver": "adamklein",
    "resolved_in": "c2da1291d9ab6c6c5fa0612b16703150cfbe4318",
    "resolver_commit_num": 221,
    "title": "When parsing a CSV file without an index, if the list with columns names is too short or too long, one gets a \"Index contains duplicate entries\" exception.",
    "body": "Hi Wes,\n\nJust a minor bug submisson:\nWhen parsing a CSV file without an index, if the list with columns names is too short or too long, one gets a \"Index contains duplicate entries\" exception.\n\nE.g.:\n\nThe file test.csv contains:\nabc,1,a\nabc,2,b\ndef,3,c\n\nRunning:\n`table = pandas.read_csv('reports/9/test.csv', header=None, names=['a', 'b'],index_col=None)`\n## Results in:\n\n\n\nMany thanks,\nMarius\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed",
      "assigned",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 9,
    "deletions": 5,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 836,
    "reporter": "wesm",
    "created_at": "2012-02-27T22:36:48+00:00",
    "closed_at": "2012-05-08T23:12:32+00:00",
    "resolver": "wesm",
    "resolved_in": "a76ba84b627c1e0ad944770c0bf7225cd65fa045",
    "resolver_commit_num": 1842,
    "title": "Improve DataFrame multi-axis reindexing performance on homogeneous data",
    "body": "Could reindex a float array in a single swipe instead of producing an intermediate copy. Currently very inefficient if the result object is small relative to the source object\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned",
      "subscribed",
      "referenced",
      "assigned",
      "referenced"
    ],
    "changed_files": 1,
    "additions": 45,
    "deletions": 0,
    "changed_files_list": [
      "pandas/core/indexing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 837,
    "reporter": "lbeltrame",
    "created_at": "2012-02-28T09:20:30+00:00",
    "closed_at": "2012-02-28T18:22:09+00:00",
    "resolver": "adamklein",
    "resolved_in": "95cfc7c415c559cfb116a06d6fbd0955ee973b41",
    "resolver_commit_num": 220,
    "title": "ExcelFile throws an exception if parsing a file with only two lines",
    "body": "Example (paste this in an Excel file):\n\nTest    Test1\naaaa   bbbbb\n\n\n\nWorks correctly if the number of lines is at least 3.  Pandas is from yesterday's git (path names have been slightly trimmed to improve readability).\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 11,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/parsers.py",
      "pandas/io/tests/test2.xls",
      "pandas/io/tests/test_parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 839,
    "reporter": "adamklein",
    "created_at": "2012-02-29T00:19:35+00:00",
    "closed_at": "2012-02-29T18:04:21+00:00",
    "resolver": "adamklein",
    "resolved_in": "a55ded2929c5216b36539e267f4bd588d1025000",
    "resolver_commit_num": 233,
    "title": "int64 Series and NA assignment",
    "body": "I think a bug:\n\nIn [1]: s = Series(arange(10))\n\nIn [2]: s\nOut[2]: \n0    0\n1    1\n2    2\n3    3\n4    4\n5    5\n6    6\n7    7\n8    8\n9    9\n\nIn [3]: s[::2] = np.nan\n\nIn [4]: s\nOut[4]: \n0   -9223372036854775808\n1                      1\n2   -9223372036854775808\n3                      3\n4   -9223372036854775808\n5                      5\n6   -9223372036854775808\n7                      7\n8   -9223372036854775808\n9                      9\n\nIn [5]: s.dtype\nOut[5]: dtype('int64')\n\nDataframe seems fine:\n\nIn [12]: df = DataFrame(np.random.random_integers(5, size=(5,5)))\n\nIn [13]: df\nOut[13]: \n   0  1  2  3  4\n0  5  5  3  1  2\n1  1  3  3  3  2\n2  1  5  2  2  3\n3  5  3  1  1  4\n4  2  5  2  3  2\n\nIn [14]: df[2] = np.nan \n\nIn [15]: df\nOut[15]: \n   0  1   2  3  4\n0  5  5 NaN  1  2\n1  1  3 NaN  3  2\n2  1  5 NaN  2  3\n3  5  3 NaN  1  4\n4  2  5 NaN  3  2\n",
    "labels": [],
    "comments": [
      "This should raise an exception\n",
      "cool. just finished test coverage.\n",
      "observed in git master\n\n```\nIn [1]: s = Series(np.arange(10))\n\nIn [2]: s[:5] = np.nan\n\nIn [3]: s\nOut[3]: \n0   -9223372036854775808\n1   -9223372036854775808\n2   -9223372036854775808\n3   -9223372036854775808\n4   -9223372036854775808\n5                      5\n6                      6\n7                      7\n8                      8\n9                      9\n```\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "closed",
      "referenced",
      "reopened",
      "commented"
    ],
    "changed_files": 2,
    "additions": 9,
    "deletions": 0,
    "changed_files_list": [
      "pandas/core/series.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 844,
    "reporter": "scouredimage",
    "created_at": "2012-03-01T23:11:21+00:00",
    "closed_at": "2012-03-15T20:35:43+00:00",
    "resolver": "wesm",
    "resolved_in": "0fb5725e15592dd6fbfc27c0f4daa71c26f1dfaa",
    "resolver_commit_num": 1604,
    "title": "Applying np.sum to empty series results in NaN",
    "body": "\n",
    "labels": [],
    "comments": [
      "This is a deliberate design choice. I'm not actually sure what's the right behavior. Probably 0 is right but need to think a bit\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 25,
    "deletions": 2,
    "changed_files_list": [
      "pandas/core/nanops.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 846,
    "reporter": "wesm",
    "created_at": "2012-03-01T23:48:54+00:00",
    "closed_at": "2012-04-09T14:58:31+00:00",
    "resolver": "wesm",
    "resolved_in": "9ea685e3e9795b3c432d9e6b28a7026e5ecb498a",
    "resolver_commit_num": 1662,
    "title": "Buggy creation of int DataFrame from data containing NAs",
    "body": "\n",
    "labels": [],
    "comments": [
      "Changing dtype at runtime still causes the wrong cast, however:\n\n``` python\n\nIn [7]: test = pandas.DataFrame(data={\"Values\":[1.0, 2.0, 3.0, np.nan]})\n\nIn [8]: test[\"Values\"].astype(\"int64\")\nOut[8]: \n0                      1\n1                      2\n2                      3\n3   -9223372036854775808\nName: Values\n```\n"
    ],
    "events": [
      "subscribed",
      "referenced",
      "closed",
      "commented",
      "subscribed",
      "reopened"
    ],
    "changed_files": 6,
    "additions": 39,
    "deletions": 9,
    "changed_files_list": [
      "pandas/core/common.py",
      "pandas/core/internals.py",
      "pandas/core/series.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 847,
    "reporter": "wesm",
    "created_at": "2012-03-02T15:48:22+00:00",
    "closed_at": "2012-03-15T16:54:08+00:00",
    "resolver": "wesm",
    "resolved_in": "19bbb4a1cd2eba1aeea41467ff93e58f8373fb9f",
    "resolver_commit_num": 1602,
    "title": "Don't raise exception if HDFStore file is read only",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 8,
    "deletions": 1,
    "changed_files_list": [
      "pandas/io/pytables.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 853,
    "reporter": "adamklein",
    "created_at": "2012-03-04T16:55:17+00:00",
    "closed_at": "2012-04-15T01:36:26+00:00",
    "resolver": "wesm",
    "resolved_in": "fc56b64c1985d481dd36ce93b768b6778c7a8313",
    "resolver_commit_num": 1715,
    "title": "possibly allow parse_dates to take column list",
    "body": "From mailing list:\n\nThe argument \"parse_dates\" will try to parse every column in the index.\n\nMost of the times I have a dataset which looks like below and the first n columns will be used to set the multi-index.\nNotice that my first column \"date\" is a date while the column \"unitcode\" is a integer.\nIf you use \"parse_dates=True\" than everything will be parsed.\nShouldn't it be me more logical that for parsing dates one has the option to explicitely parse certain columns like the index_col argument?\nFor the moment I uses read_csv without parsing, use the converters argument and then I use the set_index method. This works fine.\n\nimport pandas as pd\nfrom StringIO import StringIO\ndata = '''date;destination;ventilationcode;unitcode;units\n01/01/2010;P;P;50;1\n01/01/2010;P;R;50;1\n15/01/2010;P;P;50;1\n01/05/2010;P;P;50;1'''\ndf = pd.read_csv(StringIO(data),sep=\";\",index_col = range(4),parse_dates=True)\nprint df.to_string()\n# patch\n\ndf = pd.read_csv(StringIO(data),sep=\";\",converters={'date':pd.datetools.dateutil.parser.parse})\ndf = df.set_index(col_or_cols=['date','destination','ventilationcode','unitcode'])\nprint df.to_string()\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 8,
    "additions": 111,
    "deletions": 31,
    "changed_files_list": [
      "pandas/core/datetools.py",
      "pandas/core/index.py",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/datetime.pyx",
      "pandas/src/inference.pyx",
      "pandas/tests/test_tseries.py",
      "scripts/count_code.sh"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 854,
    "reporter": "adamklein",
    "created_at": "2012-03-04T16:57:02+00:00",
    "closed_at": "2012-04-15T01:36:26+00:00",
    "resolver": "wesm",
    "resolved_in": "fc56b64c1985d481dd36ce93b768b6778c7a8313",
    "resolver_commit_num": 1715,
    "title": "make easier to parse european-style dates in to_csv",
    "body": "Again from mailing list:\n\nExample:\ndata1 = '''date;value\n01/05/2010;1\n15/05/2010;2\n31/05/2010;3\n'''\n\ndf = pd.read_csv(StringIO(data1),sep=\";\",converters={'date':pd.datetools.dateutil.parser.parse})\nprint df.to_string()\n\nReturns the first row wrong:\n                  date  value\n0  2010-01-05 00:00:00      1\n1  2010-05-15 00:00:00      2\n2  2010-05-31 00:00:00      3\n\ndf = pd.read_csv(StringIO(data1),sep=\";\",converters={'date':lambda x: pd.datetools.dateutil.parser.parse(x, dayfirst=True)})\nprint df.to_string()\n\nReturns the correct output\n                  date  value\n0  2010-05-01 00:00:00      1\n1  2010-05-15 00:00:00      2\n2  2010-05-31 00:00:00      3\n\nWhen using index_col and parse_dates, you can get also some of the dates that are wrong, some of them correct.\n\ndf = pd.read_csv(StringIO(data1),sep=\";\",index_col=[0],parse_dates=True)\nprint df.to_string()\n\nReturns the first line also wrong:\n            value\ndate  \n2010-01-05      1\n2010-05-15      2\n2010-05-31      3\n",
    "labels": [],
    "comments": [
      "Sometimes, there are also separators such as \".\", \"-\" but I assume this doesn't play a role in here.\n",
      "Do you want us to supply more date formats as examples to enhance the parser?\n",
      "@timmie, absolutely, any examples you want supported would be great\n",
      "Here are some commonly encountered formats:\n\n```\n1/1/2012,0:00:00,180.8\n1/1/2012,0:04:00,180.6\n1/1/2012,0:08:00,180.8\n1/1/2012,0:12:00,180.8\n1/1/2012,0:16:00,180.6\n1/1/2012,0:20:00,180.5\n1/1/2012,0:24:00,180.2\n1/1/2012,0:28:00,180.5\n1/1/2012,0:32:00,180.2\n1/1/2012,0:36:00,180.1\n\n\n\n#begindata\ndate \"YYYY-MM-DD\" time \"hh:mm\"\n2012-01-01 00:00     0.0     0.0     0.0     0.0     0.0     0.0 \n2012-01-01 01:00     0.0     0.0     0.0     0.0     0.0     0.0 \n2012-01-01 02:00     0.0     0.0     0.0     0.0     0.0     0.0 \n2012-01-01 03:00     0.0     0.0     0.0     0.0     0.0     0.0 \n2012-01-01 04:00     0.0     0.0     0.0     0.0     0.0     0.0 \n2012-01-01 05:00     0.0     0.0     0.0     0.0     0.0     0.0 \n2012-01-01 06:00     0.0     0.0     0.0     0.0     0.0     0.0 \n2012-01-01 07:00     0.0     0.0     0.0     0.0     0.0     0.0 \n2012-01-01 08:00     0.0     0.0     0.0     0.0     0.0     0.0 \n2012-01-01 09:00     0.0     0.0     0.0     0.0     0.0     0.0\n\n\n\n\ndd.mm.yy;hh:mm:ss;\n********;**************;*******;\n13.1.2012;18:00:00;0;\n13.1.2012;18:10:00;0;\n13.1.2012;18:20:00;0;\n13.1.2012;18:30:00;0;\n13.1.2012;18:40:00;0;\n13.1.2012;18:50:00;0;\n13.1.2012;19:00:00;0;\n13.1.2012;19:10:00;0;\n13.1.2012;19:20:00;0;\n13.1.2012;19:30:00;0;\n13.1.2012;19:40:00;0;\n13.1.2012;19:50:00;0;\n\n\n\nYear;Month;Day;Hour;Value;\n2012;3;1;01.0000;0;\n2012;3;1;03.0000;0;\n2012;3;1;03.0000;0;\n2012;3;1;04.0000;0;\n2012;3;1;05.0000;0;\n2012;3;1;06.0000;0;\n2012;3;1;07.0000;1;\n2012;3;1;08.0000;1;\n2012;3;1;09.0000;2;\n2012;3;1;10.0000;3;\n2012;3;1;11.0000;3;\n```\n",
      "Often I also see that instead of using hour numbers from 0-23 (like Python datetime) some use 1-24 for storage:\n\n```\n07.03.12 23:20:00      \n07.03.12 23:30:00      \n07.03.12 23:40:00      \n07.03.12 23:50:00      \n07.03.12 24:00:00      \n08.03.12 00:03:00      \n08.03.12 00:20:00      \n08.03.12 00:30:00      \n```\n\nThe reason is that people what to show with this that the value belongs to the end of the described data interval and not the beginning or middle.\nThis is important in some natural science observations or engineering when the data represents information integrated over the inverval (e.g. 10min.) period.\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "commented",
      "commented"
    ],
    "changed_files": 8,
    "additions": 111,
    "deletions": 31,
    "changed_files_list": [
      "pandas/core/datetools.py",
      "pandas/core/index.py",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/datetime.pyx",
      "pandas/src/inference.pyx",
      "pandas/tests/test_tseries.py",
      "scripts/count_code.sh"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 856,
    "reporter": "wesm",
    "created_at": "2012-03-04T17:14:30+00:00",
    "closed_at": "2012-03-04T20:15:21+00:00",
    "resolver": "adamklein",
    "resolved_in": "3029f4b623e2d090f8cff13a00f8bcc5c7593d11",
    "resolver_commit_num": 242,
    "title": "Fix unhelpful error message in parsers (\"tried columns 1-X as index\")",
    "body": "Users are confused by this:\n\n\n\nthis was added as I recall to give a somewhat better error message when INFERENCE (fewer column labels than columns of data) but this error message is also getting raised when there are duplicates in an explicitly specified `index_col`\n\nsee: -dataframe-desired-index-has-duplicate-values\n",
    "labels": [],
    "comments": [
      "Yes, I had made it raise separate error messages but then we switched to this unified message. The two-message case is in a commit somewhere, I'll look.\n",
      "I think i suggested this message hoping that you would extrapolate to at least replace `X` with the inferred number of index levels =P\n",
      "Ah, yes, it was because I had to add an \"infer_index\" parameter to differentiate the two cases. I suppose what is needed is to track that inference is occuring and report that up to user.\n",
      "Ha, yes, sometimes I take things too literally (re: 1-X)\n",
      "That would be consistent with POLS\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "assigned"
    ],
    "changed_files": 2,
    "additions": 20,
    "deletions": 2,
    "changed_files_list": [
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 858,
    "reporter": "wesm",
    "created_at": "2012-03-04T17:20:09+00:00",
    "closed_at": "2012-04-05T04:32:03+00:00",
    "resolver": "wesm",
    "resolved_in": "63207b83fc35c2ade37861ca581949d95f63ad61",
    "resolver_commit_num": 1707,
    "title": "Add vectorized, NA friendly Series of strings -> Series of datetime.datetime or datetime64 function",
    "body": "see for example: -dataframe-desired-index-has-duplicate-values\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 82,
    "additions": 16580,
    "deletions": 2757,
    "changed_files_list": [
      ".gitignore",
      "NP_LICENSE.txt",
      "bench/bench_dense_to_sparse.py",
      "bench/bench_tseries.py",
      "pandas/core/api.py",
      "pandas/core/common.py",
      "pandas/core/daterange.py",
      "pandas/core/datetools.py",
      "pandas/core/format.py",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/indexing.py",
      "pandas/core/internals.py",
      "pandas/core/nanops.py",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/info.py",
      "pandas/io/parsers.py",
      "pandas/io/pytables.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/io/tests/test_pytables.py",
      "pandas/sparse/frame.py",
      "pandas/sparse/panel.py",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_sparse.py",
      "pandas/src/data_algos.pyx",
      "pandas/src/datetime.pxd",
      "pandas/src/datetime.pyx",
      "pandas/src/engines.pyx",
      "pandas/src/generate_code.py",
      "pandas/src/generated.pyx",
      "pandas/src/groupby.pyx",
      "pandas/src/hashtable.pxd",
      "pandas/src/hashtable.pyx",
      "pandas/src/inference.pyx",
      "pandas/src/khash.pxd",
      "pandas/src/np_datetime.c",
      "pandas/src/np_datetime.h",
      "pandas/src/np_datetime_strings.c",
      "pandas/src/np_datetime_strings.h",
      "pandas/src/numpy_helper.h",
      "pandas/src/reduce.pyx",
      "pandas/src/reindex.pyx",
      "pandas/src/sandbox.pyx",
      "pandas/src/skts.c",
      "pandas/src/skts.h",
      "pandas/src/tseries.pyx",
      "pandas/src/util.pxd",
      "pandas/stats/tests/common.py",
      "pandas/stats/tests/test_math.py",
      "pandas/stats/tests/test_moments.py",
      "pandas/stats/tests/test_ols.py",
      "pandas/tests/data/frame.pickle",
      "pandas/tests/data/series.pickle",
      "pandas/tests/test_daterange.py",
      "pandas/tests/test_datetools.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_graphics.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_interval.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_series.py",
      "pandas/tests/test_timeseries.py",
      "pandas/tests/test_tseries.py",
      "pandas/tools/merge.py",
      "pandas/tools/plotting.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tools/tests/test_pivot.py",
      "pandas/tools/tsplotting.py",
      "pandas/tseries/__init__.py",
      "pandas/tseries/tests/__init__.py",
      "pandas/tseries/tests/test_tools.py",
      "pandas/tseries/tools.py",
      "pandas/util/testing.py",
      "setup.py",
      "test.sh",
      "ts_todo.txt",
      "vb_suite/sparse.py",
      "vb_suite/timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 859,
    "reporter": "adamklein",
    "created_at": "2012-03-04T17:35:54+00:00",
    "closed_at": "2012-03-06T22:33:56+00:00",
    "resolver": "adamklein",
    "resolved_in": "5834ccd0bfb495a8044e62e39e0bfcd9cc6a255a",
    "resolver_commit_num": 256,
    "title": "better err msg for failed boolean slicing of dataframe",
    "body": "In [57]: df = DataFrame(np.random.rand(5,5))\n\nIn [58]: df[df > 0.5]\nERROR: An unexpected error occurred while tokenizing input\nThe following traceback may be corrupted or invalid\n## The error message is: ('EOF in multi-line statement', (63, 0))\n\nKeyError                                  Traceback (most recent call last)\n/Volumes/HD2/adam/Documents/Code/python/rapidquant/<ipython-input-58-8ec7cde9a766> in <module>()\n----> 1 df[df > 0.5]\n\n/Volumes/HD2/adam/Documents/Code/python/pandas/pandas/core/frame.pyc in **getitem**(self, key)\n   1419             return self._getitem_multilevel(key)\n   1420         else:\n-> 1421             return self._get_item_cache(key)\n   1422 \n   1423     def _getitem_array(self, key):\n\n/Volumes/HD2/adam/Documents/Code/python/pandas/pandas/core/generic.pyc in _get_item_cache(self, item)\n    280             return cache[item]\n    281         except Exception:\n--> 282             values = self._data.get(item)\n    283             res = self._box_item_values(item, values)\n    284             cache[item] = res\n\n/Volumes/HD2/adam/Documents/Code/python/pandas/pandas/core/internals.pyc in get(self, item)\n    612 \n    613     def get(self, item):\n--> 614         _, block = self._find_block(item)\n    615         return block.get(item)\n    616 \n\n/Volumes/HD2/adam/Documents/Code/python/pandas/pandas/core/internals.pyc in _find_block(self, item)\n    702 \n    703     def _find_block(self, item):\n--> 704         self._check_have(item)\n    705         for i, block in enumerate(self.blocks):\n    706             if item in block:\n\n/Volumes/HD2/adam/Documents/Code/python/pandas/pandas/core/internals.pyc in _check_have(self, item)\n    709     def _check_have(self, item):\n    710         if item not in self.items:\n--> 711             raise KeyError('no item named %s' % str(item))\n    712 \n    713     def reindex_axis(self, new_axis, method=None, axis=0, copy=True):\n\nKeyError: 'no item named        0     1      2      3      4\\n0  False  True  False   True  False\\n1   True  True  False   True   True\\n2   True  True   True   True  False\\n3  False  True  False  False   True\\n4  False  True  False  False   True'\n\nAnd then, this actually silently fails:\n\ndf.ix[df > 0.5]\n\nIe, it returns the wrong data\n",
    "labels": [],
    "comments": [
      "The first case needs a better error message. The second is a bug-- I don't want to support either of those cases (unless you were to return a DataFrame with all NA in the False locations. I don't think I want that)\n",
      "Ah right, only works on assignment as it should. Ok, yeah, better error msgs then.\n",
      "One other thought: it would be nice to do this:\n\ndf[ df_std > 3 ] = df.std() \\* 3\n\nie, insert values into the masked dataframe using broadcasting.  right now, i get\n\n**\\* ValueError: array is not broadcastable to correct shape\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 15,
    "deletions": 4,
    "changed_files_list": [
      "pandas/core/generic.py",
      "pandas/core/indexing.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 864,
    "reporter": "adamklein",
    "created_at": "2012-03-05T01:56:06+00:00",
    "closed_at": "2012-04-27T01:11:55+00:00",
    "resolver": "wesm",
    "resolved_in": "4cf02506d62d7ddfe5a9f81c4d29871c5f227524",
    "resolver_commit_num": 1770,
    "title": "handle indices with different time zones",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 7,
    "additions": 291,
    "deletions": 91,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/series.py",
      "pandas/src/datetime.pyx",
      "pandas/src/sandbox.pyx",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tools.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 865,
    "reporter": "adamklein",
    "created_at": "2012-03-05T17:16:08+00:00",
    "closed_at": "2012-04-23T17:14:51+00:00",
    "resolver": "wesm",
    "resolved_in": "428c177695894af8a706d3eff3d9969bf0c7bdef",
    "resolver_commit_num": 1743,
    "title": "add first, last time subsetting functions",
    "body": "Like xts has.\n\neg, something like\n\nts.first(\"5W\")   # first five weeks of data\nts.first(\"7D\")   # first seven days\nts.last(\"3W\").first(\"3B\") # first 3 biz days of last three weeks\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 128,
    "deletions": 19,
    "changed_files_list": [
      "pandas/core/generic.py",
      "pandas/tseries/frequencies.py",
      "pandas/tseries/offsets.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 867,
    "reporter": "wesm",
    "created_at": "2012-03-05T18:10:59+00:00",
    "closed_at": "2012-04-22T03:56:52+00:00",
    "resolver": "wesm",
    "resolved_in": "45ba190e695a6d3921bd5efca878d3fccdfff813",
    "resolver_commit_num": 1733,
    "title": "Promote datetime.date in reindexing operations (if it's easy)",
    "body": "",
    "labels": [],
    "comments": [
      "Does this means we can do frame.asfreq when index was a series of datetime.date objects since asfreq using reindex method.\n",
      "@yinhm, yes, this should hopefully be possible\n",
      "That would be great. Thanks.\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented"
    ],
    "changed_files": 5,
    "additions": 74,
    "deletions": 14,
    "changed_files_list": [
      "pandas/core/index.py",
      "pandas/src/datetime.pxd",
      "pandas/src/datetime.pyx",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 869,
    "reporter": "wesm",
    "created_at": "2012-03-06T01:08:39+00:00",
    "closed_at": "2012-03-06T22:01:21+00:00",
    "resolver": "adamklein",
    "resolved_in": "665af950e88c06a639f1ac758313a714c244a619",
    "resolver_commit_num": 255,
    "title": "Series.count cannot accept a string (level name) in the level argument",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 39,
    "deletions": 0,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/core/series.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 870,
    "reporter": "welch",
    "created_at": "2012-03-06T01:22:25+00:00",
    "closed_at": "2012-03-06T16:56:02+00:00",
    "resolver": "adamklein",
    "resolved_in": "1723257b0a5ee911f3f9be3b32902b198c00ad85",
    "resolver_commit_num": 253,
    "title": "nosetest failures in reshape",
    "body": "I've just done a git clone/install of pandas on Mac OSX 10.6.8 after a fresh install of EPD_free-7.2 to get the dependencies. The build was successful, but in running nostests I get 27 failures, each bottoming out at:\n\n  File \"/Users/welch/src/pandas/pandas/core/reshape.py\", line 95, in _make_selectors\n    group_mask.put(group_index, True)\nTypeError: array cannot be safely cast to required type\n\nhere are the failed tests:\n\ntest_crosstab_margins (pandas.tools.tests.test_pivot.TestCrosstab)\ntest_crosstab_multiple (pandas.tools.tests.test_pivot.TestCrosstab)\ntest_crosstab_ndarray (pandas.tools.tests.test_pivot.TestCrosstab)\ntest_crosstab_pass_values (pandas.tools.tests.test_pivot.TestCrosstab)\ntest_crosstab_single (pandas.tools.tests.test_pivot.TestCrosstab)\ntest_margins (pandas.tools.tests.test_pivot.TestPivotTable)\ntest_pivot_integer_columns (pandas.tools.tests.test_pivot.TestPivotTable)\ntest_pivot_multi_functions (pandas.tools.tests.test_pivot.TestPivotTable)\ntest_pivot_multi_values (pandas.tools.tests.test_pivot.TestPivotTable)\ntest_pivot_table (pandas.tools.tests.test_pivot.TestPivotTable)\ntest_pivot_table_multiple (pandas.tools.tests.test_pivot.TestPivotTable)\ntest_pivot (pandas.tests.test_frame.TestDataFrame)\ntest_pivot_empty (pandas.tests.test_frame.TestDataFrame)\ntest_stack_unstack (pandas.tests.test_frame.TestDataFrame)\ntest_unstack_to_series (pandas.tests.test_frame.TestDataFrame)\ntest_frame_describe_multikey (pandas.tests.test_groupby.TestGroupBy)\ntest_groupby_multiple_columns (pandas.tests.test_groupby.TestGroupBy)\ntest_stack (pandas.tests.test_multilevel.TestMultiLevel)\ntest_stack_level_name (pandas.tests.test_multilevel.TestMultiLevel)\ntest_stack_unstack_multiple (pandas.tests.test_multilevel.TestMultiLevel)\ntest_stack_unstack_preserve_names (pandas.tests.test_multilevel.TestMultiLevel)\ntest_unstack (pandas.tests.test_multilevel.TestMultiLevel)\ntest_unstack_bug (pandas.tests.test_multilevel.TestMultiLevel)\ntest_unstack_level_name (pandas.tests.test_multilevel.TestMultiLevel)\ntest_unstack_preserve_types (pandas.tests.test_multilevel.TestMultiLevel)\ntest_pivot (pandas.tests.test_panel.TestLongPanel)\ntest_unstack (pandas.tests.test_series.TestSeries)\n\nAny advice for me? \n(And is this the proper place for such a question?)\n",
    "labels": [],
    "comments": [
      "This is currently an open problem following a commit meant to address issues with 32-bit overflows in groupby.  EPD-free / MacOS is 32bit; we are seeing the same problem with our 32-bit windows builds.  Will try to get resolved asap.\n\nLast passing commit (though subject to possible int32 overflow in corner cases): 89bdb1eed407048d901f2eaefe31ec3f85f9f692\n\n@wes I'll email you jenkins log output, but I'll start digging.\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 3,
    "deletions": 2,
    "changed_files_list": [
      "pandas/core/reshape.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 871,
    "reporter": "lbeltrame",
    "created_at": "2012-03-06T09:33:27+00:00",
    "closed_at": "2012-03-06T21:34:14+00:00",
    "resolver": "adamklein",
    "resolved_in": "a182a80ebcb32111756b4faece5517e911dcef7f",
    "resolver_commit_num": 254,
    "title": "concat on axis=1 and ignore_index=True raises TypeError",
    "body": "Test case:\n\n\n\nFrom today's git.\n",
    "labels": [],
    "comments": [
      "Thx for reporting\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 39,
    "deletions": 9,
    "changed_files_list": [
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 881,
    "reporter": "adamklein",
    "created_at": "2012-03-08T17:05:02+00:00",
    "closed_at": "2012-03-08T17:41:16+00:00",
    "resolver": "adamklein",
    "resolved_in": "2ef6ee99183f9127bdc308b749e15795cd08f325",
    "resolver_commit_num": 264,
    "title": "dataframe slice assignment failure",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 2,
    "additions": 16,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 883,
    "reporter": "wesm",
    "created_at": "2012-03-08T19:18:03+00:00",
    "closed_at": "2012-03-13T03:42:49+00:00",
    "resolver": "wesm",
    "resolved_in": "7e4b0861bcc4cc33071d677678f7ab5b47e4093b",
    "resolver_commit_num": 1585,
    "title": "Enable setting existing columns (only) via attributes on DataFrame/Panel",
    "body": "Purely a convenience (especially in IPython/notebook)\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 23,
    "deletions": 0,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 885,
    "reporter": "wesm",
    "created_at": "2012-03-08T19:51:26+00:00",
    "closed_at": "2012-03-13T01:39:01+00:00",
    "resolver": "wesm",
    "resolved_in": "a87432d9fc2853050bf1e4c07b57455977027c4f",
    "resolver_commit_num": 1583,
    "title": "Intercept __builtin__.sum in groupby",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 25,
    "deletions": 0,
    "changed_files_list": [
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 887,
    "reporter": "craustin",
    "created_at": "2012-03-08T20:52:15+00:00",
    "closed_at": "2012-03-11T17:00:57+00:00",
    "resolver": "wesm",
    "resolved_in": "a262f3037b9802722a9de194d69575fb3f78caa9",
    "resolver_commit_num": 1578,
    "title": "combineAdd NotImplementedError for SparseDataFrame",
    "body": "[also, should probably be combine_add]\n\nfrom pandas import DataFrame\ndf = DataFrame()\nsdf = df.to_sparse()\nsdf.combineAdd(DataFrame())\n\npandas\\core\\frame.pyc in combineAdd(self, other)\n   3861         DataFrame\n   3862         \"\"\"\n-> 3863         return self.add(other, fill_value=0.)\n   3864\n   3865     def combineMult(self, other):\n\npandas\\core\\frame.pyc in f(self, other, axis, level, fill_value)\n    173     def f(self, other, axis=default_axis, level=None, fill_value=None):\n    174         if isinstance(other, DataFrame):    # Another DataFrame\n--> 175             return self._combine_frame(other, func, fill_value, level)\n    176         elif isinstance(other, Series):\n    177             return self._combine_series(other, func, fill_value, axis, level)\n\npandas\\sparse\\frame.pyc in _combine_frame(self, other, func, fill_value, level)\n    414\n    415         if fill_value is not None or level is not None:\n--> 416             raise NotImplementedError\n    417\n    418         if not self and not other:\n\nNotImplementedError:\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 25,
    "deletions": 4,
    "changed_files_list": [
      "pandas/sparse/frame.py",
      "pandas/sparse/tests/test_sparse.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 890,
    "reporter": "lodagro",
    "created_at": "2012-03-09T14:07:51+00:00",
    "closed_at": "2012-03-11T16:26:59+00:00",
    "resolver": "wesm",
    "resolved_in": "edc90b3d8f8d270d62ed303d211d197b5430dc00",
    "resolver_commit_num": 1576,
    "title": "DataFrame.to_html columns broken",
    "body": "Reported on [mailing list](?fromgroups#!topic/pystatsmodels/lK1odlK5zjU), columns argument in `DataFrame.to_html()` broken.\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 12,
    "additions": 68,
    "deletions": 122,
    "changed_files_list": [
      "pandas/core/format.py",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/series.py",
      "pandas/stats/ols.py",
      "pandas/stats/tests/common.py",
      "pandas/stats/tests/test_ols.py",
      "pandas/stats/tests/test_var.py",
      "pandas/stats/var.py",
      "pandas/tests/test_format.py",
      "scripts/file_sizes.py",
      "scripts/git_code_churn.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 891,
    "reporter": "lodagro",
    "created_at": "2012-03-09T14:15:05+00:00",
    "closed_at": "2012-03-15T21:09:53+00:00",
    "resolver": "wesm",
    "resolved_in": "3b8a1924e38a1c1bf1694dd5bd6015edc65f235a",
    "resolver_commit_num": 1605,
    "title": "DataFrame.to_html encoding",
    "body": "see also [mailing list](?fromgroups#!topic/pystatsmodels/e5J1yvCGmmI)\n\n\n\nfurther more from the thread:\n\n\n",
    "labels": [],
    "comments": [
      "@wesm Had a quick look at this.\n\nCurrently `DataFrame.to_string()` and `DataFrame.to_html()` have the same docstring (through Appender decorator), except for _bold_rows_. However `to_html()` misses _justify_ and _force_unicode_ arguments. It probably makes sense for html not to add the _force_unicode_ argument and always use unicode.\n\nBy the way _bold_rows=True_ also makes the column labels bold.\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 48,
    "deletions": 31,
    "changed_files_list": [
      "pandas/core/format.py",
      "pandas/core/frame.py",
      "pandas/tests/test_format.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 893,
    "reporter": "gerigk",
    "created_at": "2012-03-09T18:19:30+00:00",
    "closed_at": "2012-03-15T16:49:22+00:00",
    "resolver": "wesm",
    "resolved_in": "39efc7b5382963b9e0662e4bc0655978c5f17a36",
    "resolver_commit_num": 1601,
    "title": "DataFrame.from_records() should optionally convert None to NaN",
    "body": "I still run frequently into problems with \"NaN\" and \"None\". \nFor example:  \nI retrieve Data from an SQL DB (postgres in my case) and then I have a list of tuples with \"None\"  (in SQL NULL) values.\nI then convert to a DF by using DataFrame.from_records()\nNow, to avoid object Series it would be nice to have None automatically mapped to \"NaN\"  in the same way as \"from_csv\" does.\nRight now I would have to do something like DF.fillna(np.nan) and then convert the numeric columns again to their original type which is kind of ugly.\n\n\n\n> > 0:00:07.477906 0:00:01.971754\n\nThe fillna takes much longer than the construction of the DF itself. Example with 750k rows.\n",
    "labels": [],
    "comments": [
      "Hi, could you give a self-contained example showing how the original tuples and how they end up not being converted to numeric dtype by `from_records`? I just added a test case (see linked commit) that shows that everything checks out in a simple case. from_records is intended to convert a \"SQL column\" containing only `None` and other numeric values into a `float64` column. Let me know so I can get this sorted out and close the issue.\n",
      "So this is apparently my fault but still for my specific case it would be nice to have this option:\n\npsycopg2 returns type decimal.Decimal  for columns of type \"numeric\".\n\nfrom decimal import Decimal\nx = [(Decimal(10),5),(None,None)]\ny = DataFrame.from_records(x)\n\n> > > y\n> > >       0   1\n> > > 0    10   5\n> > > 1  None NaN\n\nBut with the current setup there is no bijective mapping from NULL (SQL) to NaN (pandas)  if I have object and numeric columns. If it was possible to say \"convert None to NaN for object columns\" which would then use some speedy conversion when constructing the DataFrame it would be helpful in my opinion. \n"
    ],
    "events": [
      "subscribed",
      "referenced",
      "subscribed",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 37,
    "deletions": 16,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/io/sql.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 895,
    "reporter": "timmie",
    "created_at": "2012-03-10T00:53:13+00:00",
    "closed_at": "2012-03-11T16:41:21+00:00",
    "resolver": "wesm",
    "resolved_in": "dc86dba9b2a991711bda3f25a07ac4670884e878",
    "resolver_commit_num": 1577,
    "title": "DateRange in docs not working",
    "body": "\n\ntriggers:\nNameError: name 'BDay' is not defined\n\nshould be\n\n\n\n-docs/stable/timeseries.html#generating-date-ranges-daterange\n",
    "labels": [],
    "comments": [
      "This is a good point, BDay is not in the global pandas namespace. We should probably change the docs.\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 1,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/timeseries.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 910,
    "reporter": "aman-thakral",
    "created_at": "2012-03-13T21:49:05+00:00",
    "closed_at": "2012-03-15T01:09:59+00:00",
    "resolver": "wesm",
    "resolved_in": "f982225dcc86b259ec40b5c2f04ec44a19d719c8",
    "resolver_commit_num": 1596,
    "title": "Error when subtracting mixed type DataFrame",
    "body": "Sample code to reproduce the error:\n\nimport numpy as np\nimport pandas\n\nX = np.random.rand(10,10)\nY = np.ones((10,1),dtype=int)\ndf1 = pandas.DataFrame(X)\ndf1 = df1.join(pandas.DataFrame(Y),lsuffix='.X')\n\nprint df1-df1.mean()\n\n---\n\nTraceback (most recent call last):\n  File \"pls_model.py\", line 383, in <module>\n    run(df)\n  File \"pls_model.py\", line 305, in run\n    report(df.copy(),n_comp,priors=priors,plot=False)\n  File \"pls_model.py\", line 83, in report\n    plsgnb.fit(X,Y,priors=priors)\n  File \"C:\\src\\multivariate\\src\\multivariate\\PLS.py\", line 477, in fit\n    self.model.fit(X,Y)\n  File \"C:\\src\\multivariate\\src\\multivariate\\PLS.py\", line 145, in fit\n    print x - self.Xmean\n  File \"C:\\src\\pandas\\pandas\\core\\frame.py\", line 177, in f\n    return self._combine_series(other, func, fill_value, axis, level)\n  File \"C:\\src\\pandas\\pandas\\core\\frame.py\", line 2526, in _combine_series\n    return self._combine_series_infer(other, func, fill_value)\n  File \"C:\\src\\pandas\\pandas\\core\\frame.py\", line 2541, in _combine_series_infer\n    return self._combine_match_columns(other, func, fill_value)\n  File \"C:\\src\\pandas\\pandas\\core\\frame.py\", line 2552, in _combine_match_columns\n    left, right = self.align(other, join='outer', axis=1, copy=False)\n  File \"C:\\src\\pandas\\pandas\\core\\frame.py\", line 1735, in align\n    method=method)\n  File \"C:\\src\\pandas\\pandas\\core\\frame.py\", line 1804, in _align_series\n    return (left_result.fillna(fill_value, method=method),\n  File \"C:\\src\\pandas\\pandas\\core\\frame.py\", line 2426, in fillna\n    new_data = self._data.fillna(value)\n  File \"C:\\src\\pandas\\pandas\\core\\internals.py\", line 938, in fillna\n    new_blocks = [b.fillna(value) for b in self.blocks]\n  File \"C:\\src\\pandas\\pandas\\core\\internals.py\", line 211, in fillna\n    new_values.flat[mask] = value\nValueError: cannot convert float NaN to integer\n",
    "labels": [],
    "comments": [
      "I just checked this against 0.7.0 and the problem is not present.  This is related to 0.7.1. Would this problem also be a good candidate to include in the tests?\n",
      "Absolutely, needs to get in the test suite and fixed! Good catch\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "commented",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 27,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/core/internals.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 911,
    "reporter": "lodagro",
    "created_at": "2012-03-14T08:15:11+00:00",
    "closed_at": "2012-03-14T10:08:53+00:00",
    "resolver": "lodagro",
    "resolved_in": "bc5b23006f3cd6135baca4cb64507c9922f72fa3",
    "resolver_commit_num": 27,
    "title": "DataFrame.set_value() issue",
    "body": "Accedentically swapping the index and column label in using `DataFrame.set_value()` gave an unexpected result.\nAll values, except the one to be set, in the _DataFrame_ became _NaN_.\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 7,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/generic.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 917,
    "reporter": "wesm",
    "created_at": "2012-03-15T02:41:35+00:00",
    "closed_at": "2012-03-15T02:49:31+00:00",
    "resolver": "wesm",
    "resolved_in": "be66c2735611a56649ea7289755782a98f03e4bb",
    "resolver_commit_num": 1599,
    "title": "Potential out-of-bounds array access in Series",
    "body": "Can cause segfault if dtype=object\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 8,
    "deletions": 3,
    "changed_files_list": [
      "pandas/src/util.pxd",
      "pandas/tests/test_series.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 924,
    "reporter": "jreback",
    "created_at": "2012-03-16T01:17:54+00:00",
    "closed_at": "2012-09-18T16:34:30+00:00",
    "resolver": "wesm",
    "resolved_in": "fa5f315a4c67186bd8e1c643ec7d6eeb74ab8406",
    "resolver_commit_num": 2399,
    "title": "dropna able to accept a list/tuple for axis argument",
    "body": "rationale:\n\nI sometimes work with square matricies that potentially could have a column (and row) of all na's\n\nI find myself doing:\n\n`df.dropna(how='all',axis=0).dropna(how='all',axis=1)`\n\nso\n\n`df.dropna(how='all',axis=[0,1])`\n\ne.g.\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 24,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 927,
    "reporter": "mattharrison",
    "created_at": "2012-03-16T17:47:10+00:00",
    "closed_at": "2012-03-16T17:55:27+00:00",
    "resolver": "wesm",
    "resolved_in": "f53f235bb46a260b0b7964f93dd6c141665a020f",
    "resolver_commit_num": 1607,
    "title": "pip install borked...",
    "body": "Tried the following on Ubuntu machine\n\n\n\nIt would be really cool if this worked :)\n",
    "labels": [],
    "comments": [
      "Looks like you're running NumPy 1.3 which is no longer supported\n",
      "You are correct, but the install should fail or note that or try to install it.\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented"
    ],
    "changed_files": 1,
    "additions": 4,
    "deletions": 2,
    "changed_files_list": [
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 928,
    "reporter": "mattharrison",
    "created_at": "2012-03-16T21:29:14+00:00",
    "closed_at": "2012-11-01T15:17:40+00:00",
    "resolver": "wesm",
    "resolved_in": "e30167146a100b9eb104c7739cada915d3f68ddd",
    "resolver_commit_num": 2504,
    "title": "Not able to change direction of sort on per column basis for DataFrame",
    "body": "The `sort` method on DataFrame supports sorting by multiple columns, yet only supports sorting in one direction (ascending or descending). It would be great to specify this. Perhaps something like this: `df.sort(columns=['Name|descending', 'Age']` which would sort name first in descending order and then where names match use ascending order.\n\nAlso I see no mention of whether the sort is stable. If it is, this can be replicated by multiple sort calls.\n",
    "labels": [],
    "comments": [
      "I wish to have this feature too, which can be done in MS Excel.\n",
      "I was thinking you would do\n\n`df.sort_index(by=['a', 'b'], ascending=[False, True])`\n\nany opinions?\n",
      "Thanks for the quick response.\nThat looks quite intuitive and consistent with the current API if the list is short.  However, I cannot think of any good idea for a longer list like\n    df.sort_index(by=['a', 'b', 'c', 'd', 'e'], ascending=[False, True, False, True, True])\n",
      "About as short as you can make it I guess. `ascending=[0, 1, 0, 1, 1]` would work too\n",
      "That's clever; it didn't occur to me.  Thanks.  Then, I think your proposal serves my need.\n",
      "Got around to this finally:\n\n```\nIn [2]: df\nOut[2]: \n    A  B         C\n0   2  0 -0.696943\n1   2  2 -0.304504\n2   0  1 -0.686636\n3   1  4  0.298355\n4   0  3 -1.167454\n5   0  0  0.478933\n6   2  3 -1.859343\n7   2  4  0.040016\n8   1  1  0.484016\n9   0  4  0.355799\n10  0  2 -0.127496\n11  1  2  1.078274\n12  2  1  1.920544\n13  1  3  0.678504\n14  1  0  0.210838\n\nIn [3]: df.sort\ndf.sort        df.sort_index  df.sortlevel   \n\nIn [3]: df.sort(['A', 'B'])\nOut[3]: \n    A  B         C\n5   0  0  0.478933\n2   0  1 -0.686636\n10  0  2 -0.127496\n4   0  3 -1.167454\n9   0  4  0.355799\n14  1  0  0.210838\n8   1  1  0.484016\n11  1  2  1.078274\n13  1  3  0.678504\n3   1  4  0.298355\n0   2  0 -0.696943\n12  2  1  1.920544\n1   2  2 -0.304504\n6   2  3 -1.859343\n7   2  4  0.040016\n\nIn [4]: df.sort(['A', 'B'], ascending=[1, 0])\nOut[4]: \n    A  B         C\n9   0  4  0.355799\n4   0  3 -1.167454\n10  0  2 -0.127496\n2   0  1 -0.686636\n5   0  0  0.478933\n3   1  4  0.298355\n13  1  3  0.678504\n11  1  2  1.078274\n8   1  1  0.484016\n14  1  0  0.210838\n7   2  4  0.040016\n6   2  3 -1.859343\n1   2  2 -0.304504\n12  2  1  1.920544\n0   2  0 -0.696943\n\nIn [5]: df.sort(['A', 'B'], ascending=[1, 1])\nOut[5]: \n    A  B         C\n5   0  0  0.478933\n2   0  1 -0.686636\n10  0  2 -0.127496\n4   0  3 -1.167454\n9   0  4  0.355799\n14  1  0  0.210838\n8   1  1  0.484016\n11  1  2  1.078274\n13  1  3  0.678504\n3   1  4  0.298355\n0   2  0 -0.696943\n12  2  1  1.920544\n1   2  2 -0.304504\n6   2  3 -1.859343\n7   2  4  0.040016\n\nIn [6]: df.sort(['A', 'B'], ascending=[0, 0])\nOut[6]: \n    A  B         C\n7   2  4  0.040016\n6   2  3 -1.859343\n1   2  2 -0.304504\n12  2  1  1.920544\n0   2  0 -0.696943\n3   1  4  0.298355\n13  1  3  0.678504\n11  1  2  1.078274\n8   1  1  0.484016\n14  1  0  0.210838\n9   0  4  0.355799\n4   0  3 -1.167454\n10  0  2 -0.127496\n2   0  1 -0.686636\n5   0  0  0.478933\n```\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 5,
    "additions": 90,
    "deletions": 16,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/series.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 934,
    "reporter": "dalejung",
    "created_at": "2012-03-18T02:01:11+00:00",
    "closed_at": "2012-04-02T19:15:29+00:00",
    "resolver": "wesm",
    "resolved_in": "0d81f0aada2447975a9083303847aa635bec22ba",
    "resolver_commit_num": 1630,
    "title": "ddof no longer passed for Series.std and Series.var",
    "body": "As part of the following commit:\n\n#L5L799\n\nSeries.std and Series.var no longer pass along ddof to the nanops.nanvar function. \n\nAlso since ddof defaults to 1 in pandas and 0 in numpy.\n\n\n\nWhich I don't know if it's a bug but it's non-intuitive.\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "cross-referenced"
    ],
    "changed_files": 6,
    "additions": 65,
    "deletions": 22,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/series.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 936,
    "reporter": "wesm",
    "created_at": "2012-03-18T21:38:45+00:00",
    "closed_at": "2016-07-20T17:23:59+00:00",
    "resolver": "jreback",
    "resolved_in": "786edc7ed737d9f912613f29a62997017e729a37",
    "resolver_commit_num": 4057,
    "title": "Add time-length windowing capability to moving statistics",
    "body": "",
    "labels": [],
    "comments": [
      "If I understand this right, it's similar to what I'm asking here: http://stackoverflow.com/questions/14300768/pandas-rolling-computation-with-window-based-on-values-instead-of-counts .  I think the facility should not be time-specific.  You should be able to use windows of any value range on any sort of values, not just time values.\n",
      "Thanks for sharing!\n",
      "@jreback - cookbook.\n",
      "in cookbook...closing\n",
      " When rolling_mean was found in stats.moments, it is naturally assumed time is weighted, otherwise it should appear in stats instead. However I found it is not weighted.\n\n```\n>>> ts = Series(randn(15), index=date_range('1/1/2000', periods=15))\n>>> ts\n2000-01-01   -0.195255\n2000-01-02    0.920142\n2000-01-03    1.498506\n2000-01-04   -0.923250\n2000-01-05   -0.775110\n2000-01-06    1.533274\n2000-01-07    1.455366\n2000-01-08   -1.738300\n2000-01-09    0.102575\n2000-01-10   -1.767898\n2000-01-11    1.890013\n2000-01-12   -1.106158\n2000-01-13    0.457826\n2000-01-14   -0.951881\n2000-01-15   -1.738844\nFreq: D, dtype: float64\n>>> ts2 = Series(ts, index=date_range('1/1/2000', periods=10)+date_range('20/1/2000', periods=5))\n>>> ts2\n2000-01-01   -0.195255\n2000-01-02    0.920142\n2000-01-03    1.498506\n2000-01-04   -0.923250\n2000-01-05   -0.775110\n2000-01-06    1.533274\n2000-01-07    1.455366\n2000-01-08   -1.738300\n2000-01-09    0.102575\n2000-01-10   -1.767898\n2000-01-20         NaN\n2000-01-21         NaN\n2000-01-22         NaN\n2000-01-23         NaN\n2000-01-24         NaN\ndtype: float64\n>>> ts2[-5:] = [1.890013,-1.106158,0.457826,-0.951881,-1.738844]\n>>> # now that ts1 and ts2 are idential in value but different in index\n>>> ts2\n2000-01-01   -0.195255\n2000-01-02    0.920142\n2000-01-03    1.498506\n2000-01-04   -0.923250\n2000-01-05   -0.775110\n2000-01-06    1.533274\n2000-01-07    1.455366\n2000-01-08   -1.738300\n2000-01-09    0.102575\n2000-01-10   -1.767898\n2000-01-20    1.890013\n2000-01-21   -1.106158\n2000-01-22    0.457826\n2000-01-23   -0.951881\n2000-01-24   -1.738844\ndtype: float64\n>>> TS = ts.cumsum()\n>>> TS2 = ts2.cumsum()\n>>> # you will about to find rolling_mean(TS) and rolling_mean(TS2) produce same result!\n>>> rolling_mean(TS, 1)\n2000-01-01   -0.195255\n2000-01-02    0.724887\n2000-01-03    2.223392\n2000-01-04    1.300143\n2000-01-05    0.525033\n2000-01-06    2.058307\n2000-01-07    3.513673\n2000-01-08    1.775373\n2000-01-09    1.877948\n2000-01-10    0.110050\n2000-01-11    2.000063\n2000-01-12    0.893905\n2000-01-13    1.351731\n2000-01-14    0.399849\n2000-01-15   -1.338994\nFreq: D, dtype: float64\n>>> rolling_mean(TS2, 1)\n2000-01-01   -0.195255\n2000-01-02    0.724887\n2000-01-03    2.223392\n2000-01-04    1.300143\n2000-01-05    0.525033\n2000-01-06    2.058307\n2000-01-07    3.513673\n2000-01-08    1.775373\n2000-01-09    1.877948\n2000-01-10    0.110050\n2000-01-20    2.000063\n2000-01-21    0.893905\n2000-01-22    1.351731\n2000-01-23    0.399850\n2000-01-24   -1.338994\ndtype: float64\n```\n\nThe above experiment demonstrated that rolling_mean disregard the 10 missing days in TS2, and calculate as if the data is evenly sampled, that is, as if it is not Time Series. I am afraid users are not expecting this behaviour.\n\nI believe this problem is integral to the feature asked here. If time is properly weighted in the calculation, there is no reason why window canot be specified with a time-frame. Solving the asked feature also solves this unwanted behaviour.\n",
      "in cookbook...closing  --> IN which chapter of the cookbook? Looked, not found. (it is not searchable and Google search with rolling_mean as keyword only yeild results outside of the cookbook: if what you meant of cookbook is this one: http://pandas.pydata.org/pandas-docs/stable/cookbook.html\n",
      "http://pandas.pydata.org/pandas-docs/stable/cookbook.html#expanding-data\n",
      "Oh I see, what is added to the cookbook is a link from cookbook to stackoverflow, that's why \"Google search with rolling_mean as keyword only yeild results outside of the cookbook\"\n",
      "you can search from the docs, the API box\n",
      "the cookbook is really just a collection of interesting links\n",
      "@jreback, the cookbook entry is related but does it truely close this issue? can't say.\n",
      "I also think the cookbook entry is not the real solution to this issue, although you can in principle solve this issue with it (but not that trivially for users I think). \nWhat I understand from the original issue, is that you could do something like this:\n\n```\npd.rolling_mean(ts, window='30min')\n```\n\nWhen you have eg regular timeseries of 5 min frequency, this would be the same as `pd.rolling_mean(ts, 6)` but 1) more convenient and 2) also applicable for irregular time series. \nI think this would be a very valuable addition.\n\n@zhangweiwu for the example you give, you can also use the `freq` keyword (will resampla the data to the give frequency) or manually resample the data first.\n",
      "Is there any update on this? Very interested in this - very useful for irregular time series that are large data sets\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "closed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "reopened",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 9,
    "additions": 2349,
    "deletions": 687,
    "changed_files_list": [
      "ci/lint.sh",
      "doc/source/computation.rst",
      "doc/source/timeseries.rst",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/generic.py",
      "pandas/core/window.py",
      "pandas/tests/test_window.py",
      "pandas/window.pyx",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 938,
    "reporter": "wesm",
    "created_at": "2012-03-18T23:07:20+00:00",
    "closed_at": "2012-04-09T21:38:49+00:00",
    "resolver": "wesm",
    "resolved_in": "108e181b55c933328888d29e16d05a458469d6c0",
    "resolver_commit_num": 1666,
    "title": "Add groupby option to skip MultiIndex creation in apply",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 68,
    "deletions": 28,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 942,
    "reporter": "adamklein",
    "created_at": "2012-03-19T19:29:53+00:00",
    "closed_at": "2012-04-10T03:28:50+00:00",
    "resolver": "wesm",
    "resolved_in": "68a2791fa91ab9afd65734ca4127d28c6d299592",
    "resolver_commit_num": 1667,
    "title": "add panel.to_frame documentation",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 15,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "doc/source/dsintro.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 943,
    "reporter": "hammer",
    "created_at": "2012-03-20T01:36:35+00:00",
    "closed_at": "2012-03-20T02:27:28+00:00",
    "resolver": "wesm",
    "resolved_in": "330c088e2272b505e5448685b30c5cda10dd0c5e",
    "resolver_commit_num": 1613,
    "title": "DataFrame gives uninformative error message when comparing to constants",
    "body": "For any DataFrame with numeric values, saying something like `df == 'cmp'`results in `PandasError: DataFrame constructor not properly called!`, which is pretty unintuitive. I'm not sure what behavior would be correct, but I do know that this error message is not helpful.\n",
    "labels": [],
    "comments": [
      "Surprised this never came up until now:\n\n```\nIn [9]: np.random.randn(10) == 'cmp'\nOut[9]: False\n```\n\nEasy to check for and will raise a more helpful exception\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 14,
    "deletions": 3,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 946,
    "reporter": "wesm",
    "created_at": "2012-03-20T20:58:22+00:00",
    "closed_at": "2012-04-09T15:31:27+00:00",
    "resolver": "wesm",
    "resolved_in": "374ba068f3e46fd07318625fcacf1569a58be748",
    "resolver_commit_num": 1664,
    "title": "Push label slicing into DataFrame.__getitem__",
    "body": "E.g. `df[date1:date2]` vs. `df.ix[date1:date2]`\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 28,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 947,
    "reporter": "leonbaum",
    "created_at": "2012-03-21T18:00:09+00:00",
    "closed_at": "2012-03-21T18:12:10+00:00",
    "resolver": "adamklein",
    "resolved_in": "c1a5e9a8d3e3e2903154b1f1de765a8e14b1c673",
    "resolver_commit_num": 298,
    "title": "Timeseries branch won't build with Python 3.",
    "body": "I got the following error when trying to build the timeseries branch with python-3.2:\n\ncythoning pandas/src/tseries.pyx to pandas/src/tseries.c\n## Error compiling Cython file:\n\n...\n    to use start ('S') or end ('E') of interval.\n    \"\"\"\n    cdef:\n        long retval\n\n\n\n---\n\npandas/src/datetime.pyx:961:42: undeclared name not builtin: basestring\nbuilding 'pandas._tseries' extension\ngcc -pthread -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/usr/local/lib/python3.2/site-packages/numpy/core/include -I/usr/local/include/python3.2m -c pandas/src/tseries.c -o build/temp.linux-x86_64-3.2/pandas/src/tseries.o\npandas/src/tseries.c:1:2: error: #error Do not use this file, it is the result of a failed Cython compilation.\n\nReplacing all uses of \"basestring\" in tseries.pyx with \"str\" allows it to build. (I'm new to python, so I'm not sure whether this solution is appropriate.)\n",
    "labels": [],
    "comments": [
      "Thanks for bringing to attention. Will get this fixed up.\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 3,
    "deletions": 3,
    "changed_files_list": [
      "pandas/src/datetime.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 948,
    "reporter": "adamklein",
    "created_at": "2012-03-21T20:45:38+00:00",
    "closed_at": "2012-03-21T21:45:02+00:00",
    "resolver": "adamklein",
    "resolved_in": "f58126fabbd85f30485fd428cfa9a99c557dc02e",
    "resolver_commit_num": 302,
    "title": "inconsistent comparison operation results",
    "body": "In [4]: i = Index([1,2,3,4])\n\nIn [5]: i\nOut[5]: Int64Index([1, 2, 3, 4])\n\nIn [6]: i == i\nOut[6]: Int64Index([   1,    1,    1,    1])\n\nIn [7]: i == 0\nOut[7]: array([False, False, False, False], dtype=bool)\n\nIn [8]: i == 1\nOut[8]: array([ True, False, False, False], dtype=bool)\n\nIn [9]: i == np.array([1,2,3,4])\nOut[9]: array([ True,  True,  True,  True], dtype=bool)\n\nIn [10]: i == Index([1,2,3,4])\nOut[10]: Int64Index([   1,    1,    1,    1])\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 3,
    "deletions": 3,
    "changed_files_list": [
      "pandas/tests/test_index.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 949,
    "reporter": "leonbaum",
    "created_at": "2012-03-21T21:52:54+00:00",
    "closed_at": "2012-03-22T19:50:36+00:00",
    "resolver": "adamklein",
    "resolved_in": "c9a0bccc58c1114e4ddde6d2284dc4195ac2d9c7",
    "resolver_commit_num": 304,
    "title": "Import fails with Python 3 and timeseries branch",
    "body": "Thanks to Adam's quick fix, I am now able to build the timeseries branch with python-3.2, but when I try to load it, I get\n\nImportError: /usr/local/lib/python3.2/site-packages/pandas-0.7.2.dev-py3.2-linux-x86_64.egg/pandas/_tseries.cpython-32m.so: undefined symbol: PyString_FromString\n\nI couldn't figure out why it's trying to use that symbol on Python 3.\n",
    "labels": [],
    "comments": [
      "For better or worse, pandas is written in python 2.7... just checking, did you use Py2to3 to convert the code base first?\n",
      "Ah ok, I did not realize that. The master branch had just worked with Python 3. I will give Py2to3 a try.\n",
      "@adamklein the scikits.timeseries C code you brought over probably uses deprecated / removed Python 2k APIs, especially any ones having to do with strings\n",
      "[Yup](https://github.com/pydata/pandas/blob/timeseries/pandas/src/skts.c#L1253), though it looks like it's only used in a couple of places. It will need to use PyUnicode or PyBytes as appropriate. PyBytes is #defined to PyString in 2.6 and 2.7. Or if the relevant parts can be written in Cython, that handles the version differences.\n",
      "Cool, thanks for taking note. Easy to push parts into cython to handle this. Cython FTW\n",
      "Pushed fix (commit 26071d19658a531568b66b89f27816e490be4594)\n",
      "Thanks for the fix!  But it looks like there is another deprecated one:\n\nImportError: /usr/local/lib/python3.2/site-packages/pandas-0.7.2.dev-py3.2-linux-x86_64.egg/pandas/_tseries.cpython-32m.so: undefined symbol: PyString_FromFormat\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "closed",
      "commented",
      "reopened"
    ],
    "changed_files": 1,
    "additions": 10,
    "deletions": 5,
    "changed_files_list": [
      "pandas/src/skts.c"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 954,
    "reporter": "lodagro",
    "created_at": "2012-03-22T18:24:34+00:00",
    "closed_at": "2012-03-27T09:10:36+00:00",
    "resolver": "lodagro",
    "resolved_in": "d4382cba521d0b0e34cfbcddc9857b85582d2083",
    "resolver_commit_num": 29,
    "title": "DataFrame.plot() does not treat float index as numeric",
    "body": "DataFrame.plot() does not treat float index as numeric.\n\nThis is becasue `DataFrame.index.is_numeric()` returns `False` when the index holds floats. \n`DataFrame.index.is_numeric()` checks on the dtype of the index to know if it holds numeric data. Apparently dtype of a float index is `object`.\n\nexample from the [mailing list](?fromgroups#!topic/pystatsmodels/M77RuKGvC6c)\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "cross-referenced",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 16,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/index.py",
      "pandas/tests/test_index.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 961,
    "reporter": "wesm",
    "created_at": "2012-03-23T21:40:45+00:00",
    "closed_at": "2012-05-07T18:57:51+00:00",
    "resolver": "wesm",
    "resolved_in": "41b3f9c73d7ce98f93a58f0bfc18b4df0e201c24",
    "resolver_commit_num": 1831,
    "title": "Add DataFrame.update method: in-place modification alternative to combine_first",
    "body": "-way-to-construct-a-pandas-dataframe-composed-of-different-chunks\n",
    "labels": [],
    "comments": [
      "Thanks for making this an issue.  I will try the combine_first option , but I think .update would indeed be a nice enhancement.\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 6,
    "additions": 89,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "doc/source/merging.rst",
      "pandas/core/frame.py",
      "pandas/core/series.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 977,
    "reporter": "wesm",
    "created_at": "2012-03-28T22:31:18+00:00",
    "closed_at": "2012-04-02T03:48:49+00:00",
    "resolver": "wesm",
    "resolved_in": "9388d96f1a53df2b50ae21e2b1aaee84c45a106f",
    "resolver_commit_num": 1623,
    "title": "Improve Series.apply docstring re: elementwise application",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 5,
    "deletions": 5,
    "changed_files_list": [
      "pandas/core/frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 983,
    "reporter": "lbeltrame",
    "created_at": "2012-03-30T14:40:52+00:00",
    "closed_at": "2012-04-10T17:28:45+00:00",
    "resolver": "wesm",
    "resolved_in": "cb730a7510d348db63a3177f03a7cbcbd381bffb",
    "resolver_commit_num": 1673,
    "title": "Series name information is lost when doing DataFrame.apply()",
    "body": "On both axes, the generated Series on which the function has to work lack the \"name\" attribute (either the row name or th column name).\n",
    "labels": [],
    "comments": [
      "I tested this on a non-mixed type DataFrame, I'm unsure in the case of mixed-types.\n",
      "see als mailing list [discussion](http://groups.google.com/group/pydata/msg/15f6935dbffd22b5)\n",
      "it's a done deal\n",
      "Unfortunately, there are still some issues. When doing axis=1, the name is preserved only in the first iteration (or not preserved at all in some cases): I checked by using a function that only prints out the Series out and returns the same series and after the first iteration, name information is lost.\n\nThe behavior is more consistent with axis=0.\n\n``` python\n\nIn [17]: df = pandas.DataFrame(index=[\"A\", \"B\", \"C\"],\n                               data={\"Foo\": [1,2,3],  \"Bar\": [5,6,7], \n                                     \"Baz\":[8, 9, 10]})\n\nIn [18]: def foo(x):\n    print x.name\n    return x\n   ....: \n\nIn [19]: df.apply(foo, axis=1)\nNone\nNone\nNone\nOut[19]: \n   Bar  Baz  Foo\nA    5    8    1\nB    6    9    2\nC    7   10    3\n\nIn [20]: df.apply(foo, axis=0)\nBar\nBaz\nFoo\nOut[20]: \n   Bar  Baz  Foo\nA    5    8    1\nB    6    9    2\nC    7   10    3\n```\n",
      "You're right-- I only fixed it in the reduction code. I'll take another look\n",
      "OK, fixed this and added tests. looks OK now\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "closed",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "reopened",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 31,
    "deletions": 6,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/src/reduce.pyx",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 989,
    "reporter": "jseabold",
    "created_at": "2012-04-03T13:08:29+00:00",
    "closed_at": "2012-04-09T15:11:55+00:00",
    "resolver": "wesm",
    "resolved_in": "99b5618c3ee9772cd65be7cccbdb65476f166e46",
    "resolver_commit_num": 1663,
    "title": "Slicing a Series returns an Array and fails",
    "body": "From pystatsmodels ML \"[pandas] slicing a series returns an array?\"\n\nGot it. It only happens if the series is longer than 200\n\n\n\nThis is part of what I was talking about that Series aren't quite\narray-like. Something like this\n\nnp.dot(x, np.random.random((1,10))\n\ndoesn't work, and I can't coerce x to be 2d. However, if X is less\nthan length 201, I sure can coerce it to be 2d, which is why I\ncouldn't reproduce\n\n\n\nBut I still can't dot\n\n\n\nI realize that trying to 'dot' might not make sense with a Series or\nthere's no intuitive interpretation to the output in terms of the\nSeries that I can think of offhand. Just an example of the 1d vs 2d\nissue. Mainly, I guess I'd expect a series to be able to pass through\nan atleast_2d check.\n",
    "labels": [],
    "comments": [
      "This works:\n\n`np.dot(x[:, None], randn(1, 10))`\n\nBut this does not:\n\n`np.dot(x.reshape((-1, 1)), randn(1, 10))`\n\nThis is definitely a bug; guess I need to override reshape and return an array in the case of > 1D output.\n\nFrankly, I'm partially considering making Series not a subclass of ndarray for reasons like these. I'm fearful of all the code people have written like `isinstance(obj1d, np.ndarray)` that this would break, but otherwise as long as Series obeys the array interface ufuncs and all that will continue to work fine. This would allow me to push Series completely into Cython if I wanted. \n",
      "I'm tentatively +1 on having isinstance(series, np.ndarray) return False. I'd be interested to hear what other people think though. IMO, it's not array-like enough. I've pretty much abandoned trying to use Series in any calculations that involve other data for this reason.\n\nOT: I've been playing with a factor class that subclasses Series and it also doesn't make sense as an array.\n",
      "Yeah, you sacrifice too much by preserving the array interface in array computations. I guess making Series minimally array-compatible (so ufuncs work, like DataFrame) would be the way to go. Need to assess how much havoc it would cause for users. \n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "commented"
    ],
    "changed_files": 2,
    "additions": 21,
    "deletions": 0,
    "changed_files_list": [
      "pandas/core/series.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 995,
    "reporter": "wesm",
    "created_at": "2012-04-05T00:24:46+00:00",
    "closed_at": "2012-04-10T04:35:19+00:00",
    "resolver": "wesm",
    "resolved_in": "e1583c172684c14316a6a262229f18f10fa5a25d",
    "resolver_commit_num": 1672,
    "title": "Don't add nonsense 'result' name in groupby results with no obvious name",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 12,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 998,
    "reporter": "wesm",
    "created_at": "2012-04-05T02:45:11+00:00",
    "closed_at": "2012-04-27T01:36:26+00:00",
    "resolver": "wesm",
    "resolved_in": "828a218f2ae2b182bf3fdb5f09a555fea072f7e6",
    "resolver_commit_num": 1773,
    "title": "Add frequency inference to DatetimeIndex set ops to preserve frequency where possible",
    "body": "",
    "labels": [],
    "comments": [
      "Do you agree that only a regular conforming datetimeindex (ie with no gaps) should have a frequency associated with it? An aside, it gets a bit muddy to think about \"frequency\" with an interval index, which uses the same term to refer to duration of time within each interval, and not to the regular recurrence of observations. I might prefer another term altogether (\"interval\" comes to mind) for intervalindex to disambiguate.\n",
      "I'm not sure. freq was what was used in scikits.timeseries, but given the dual distinctions... Will have to think about it and see what seems most reasonable\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented"
    ],
    "changed_files": 3,
    "additions": 63,
    "deletions": 24,
    "changed_files_list": [
      "pandas/tests/test_frame.py",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 999,
    "reporter": "wesm",
    "created_at": "2012-04-05T04:27:22+00:00",
    "closed_at": "2012-04-13T00:20:45+00:00",
    "resolver": "wesm",
    "resolved_in": "63207b83fc35c2ade37861ca581949d95f63ad61",
    "resolver_commit_num": 1707,
    "title": "Function to convert array of datetime w/ NaN to datetime64 w/ NaT",
    "body": "May have to wait for NumPy 1.7?\n",
    "labels": [],
    "comments": [
      "NaT handling seems non-existant in 1.6. In 1.7,\n\n#define NPY_DATETIME_NAT NPY_MIN_INT64\n\nThe pandas files np_datetime.c/h and np_datetime_strings.c/h files are a bit of a mashup from the corresponding numpy files and 1.6/1.7 branches, with edits to get at non-exported numpy datetime functionality via cython.\n\nI'm not sure how much needs to change to make it NaT friendly, but there are clearly some NaT additions in the 1.7 branch that are likely to be important to add into the np_datetime\\* files.\n"
    ],
    "events": [
      "subscribed",
      "cross-referenced",
      "commented",
      "subscribed"
    ],
    "changed_files": 82,
    "additions": 16580,
    "deletions": 2757,
    "changed_files_list": [
      ".gitignore",
      "NP_LICENSE.txt",
      "bench/bench_dense_to_sparse.py",
      "bench/bench_tseries.py",
      "pandas/core/api.py",
      "pandas/core/common.py",
      "pandas/core/daterange.py",
      "pandas/core/datetools.py",
      "pandas/core/format.py",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/indexing.py",
      "pandas/core/internals.py",
      "pandas/core/nanops.py",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/info.py",
      "pandas/io/parsers.py",
      "pandas/io/pytables.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/io/tests/test_pytables.py",
      "pandas/sparse/frame.py",
      "pandas/sparse/panel.py",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_sparse.py",
      "pandas/src/data_algos.pyx",
      "pandas/src/datetime.pxd",
      "pandas/src/datetime.pyx",
      "pandas/src/engines.pyx",
      "pandas/src/generate_code.py",
      "pandas/src/generated.pyx",
      "pandas/src/groupby.pyx",
      "pandas/src/hashtable.pxd",
      "pandas/src/hashtable.pyx",
      "pandas/src/inference.pyx",
      "pandas/src/khash.pxd",
      "pandas/src/np_datetime.c",
      "pandas/src/np_datetime.h",
      "pandas/src/np_datetime_strings.c",
      "pandas/src/np_datetime_strings.h",
      "pandas/src/numpy_helper.h",
      "pandas/src/reduce.pyx",
      "pandas/src/reindex.pyx",
      "pandas/src/sandbox.pyx",
      "pandas/src/skts.c",
      "pandas/src/skts.h",
      "pandas/src/tseries.pyx",
      "pandas/src/util.pxd",
      "pandas/stats/tests/common.py",
      "pandas/stats/tests/test_math.py",
      "pandas/stats/tests/test_moments.py",
      "pandas/stats/tests/test_ols.py",
      "pandas/tests/data/frame.pickle",
      "pandas/tests/data/series.pickle",
      "pandas/tests/test_daterange.py",
      "pandas/tests/test_datetools.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_graphics.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_interval.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_series.py",
      "pandas/tests/test_timeseries.py",
      "pandas/tests/test_tseries.py",
      "pandas/tools/merge.py",
      "pandas/tools/plotting.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tools/tests/test_pivot.py",
      "pandas/tools/tsplotting.py",
      "pandas/tseries/__init__.py",
      "pandas/tseries/tests/__init__.py",
      "pandas/tseries/tests/test_tools.py",
      "pandas/tseries/tools.py",
      "pandas/util/testing.py",
      "setup.py",
      "test.sh",
      "ts_todo.txt",
      "vb_suite/sparse.py",
      "vb_suite/timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1001,
    "reporter": "ijmcf",
    "created_at": "2012-04-05T18:54:21+00:00",
    "closed_at": "2012-04-09T14:25:01+00:00",
    "resolver": "adamklein",
    "resolved_in": "8caedef55ab0ed25cba2ce4b6032df940e6163c2",
    "resolver_commit_num": 320,
    "title": "Problem with DataFrame.lookup()",
    "body": "If the row and/or column labels are not present in the DataFrame, the lookup() method returns ... whatever it wants, apparently. I would expect either a NaN or an exception, personally - but whatever the result, it shouldn't be the wrong piece of data.\n\n> > > df = pandas.DataFrame({'A': pandas.Series(['A', 'B', 'C', 'D', 'E'], index=range(5))})\n> > > \n> > > df.lookup([2], ['A'])              # Correct\n> > > array([C], dtype=object)\n> > > \n> > > df.lookup([10], ['A'])            # 10 is not present in the index, so we get .. the last value in A?\n> > > array([E], dtype=object)\n> > > \n> > > df.lookup([3], ['B'])               # There is no column 'B', so we get ... the value from the last column (A) in that row?\n> > > array([C], dtype=object)\n> > > \n> > > df.lookup([10], ['B'])             # Neither row or column label is valid, so we get ... I can't rationalize this one.\n> > > array([D], dtype=object)\n",
    "labels": [],
    "comments": [
      "Indexing lookup returns -1 for non-existing label, which is then used to \"wrap around\" per python array indexing semantics. Probably should have check in lookup function:\n\n```\n        if (ridx == -1).any():\n            raise ValueError(\"Bad row label(s)\")\n        if (cidx == -1).any():\n            raise ValueError(\"Bad col label(s)\")\n```\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 10,
    "deletions": 0,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1003,
    "reporter": "wesm",
    "created_at": "2012-04-06T01:18:37+00:00",
    "closed_at": "2012-04-06T04:39:46+00:00",
    "resolver": "wesm",
    "resolved_in": "63207b83fc35c2ade37861ca581949d95f63ad61",
    "resolver_commit_num": 1707,
    "title": "autofmt_xdate issues with datetime64",
    "body": "The way that matplotlib is being told about the x ticks causes autofmt_xdate to now work. Figure out how to have sane, attractive default tick labeling in time series plots\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 82,
    "additions": 16580,
    "deletions": 2757,
    "changed_files_list": [
      ".gitignore",
      "NP_LICENSE.txt",
      "bench/bench_dense_to_sparse.py",
      "bench/bench_tseries.py",
      "pandas/core/api.py",
      "pandas/core/common.py",
      "pandas/core/daterange.py",
      "pandas/core/datetools.py",
      "pandas/core/format.py",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/indexing.py",
      "pandas/core/internals.py",
      "pandas/core/nanops.py",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/info.py",
      "pandas/io/parsers.py",
      "pandas/io/pytables.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/io/tests/test_pytables.py",
      "pandas/sparse/frame.py",
      "pandas/sparse/panel.py",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_sparse.py",
      "pandas/src/data_algos.pyx",
      "pandas/src/datetime.pxd",
      "pandas/src/datetime.pyx",
      "pandas/src/engines.pyx",
      "pandas/src/generate_code.py",
      "pandas/src/generated.pyx",
      "pandas/src/groupby.pyx",
      "pandas/src/hashtable.pxd",
      "pandas/src/hashtable.pyx",
      "pandas/src/inference.pyx",
      "pandas/src/khash.pxd",
      "pandas/src/np_datetime.c",
      "pandas/src/np_datetime.h",
      "pandas/src/np_datetime_strings.c",
      "pandas/src/np_datetime_strings.h",
      "pandas/src/numpy_helper.h",
      "pandas/src/reduce.pyx",
      "pandas/src/reindex.pyx",
      "pandas/src/sandbox.pyx",
      "pandas/src/skts.c",
      "pandas/src/skts.h",
      "pandas/src/tseries.pyx",
      "pandas/src/util.pxd",
      "pandas/stats/tests/common.py",
      "pandas/stats/tests/test_math.py",
      "pandas/stats/tests/test_moments.py",
      "pandas/stats/tests/test_ols.py",
      "pandas/tests/data/frame.pickle",
      "pandas/tests/data/series.pickle",
      "pandas/tests/test_daterange.py",
      "pandas/tests/test_datetools.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_graphics.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_interval.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_series.py",
      "pandas/tests/test_timeseries.py",
      "pandas/tests/test_tseries.py",
      "pandas/tools/merge.py",
      "pandas/tools/plotting.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tools/tests/test_pivot.py",
      "pandas/tools/tsplotting.py",
      "pandas/tseries/__init__.py",
      "pandas/tseries/tests/__init__.py",
      "pandas/tseries/tests/test_tools.py",
      "pandas/tseries/tools.py",
      "pandas/util/testing.py",
      "setup.py",
      "test.sh",
      "ts_todo.txt",
      "vb_suite/sparse.py",
      "vb_suite/timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1008,
    "reporter": "changhiskhan",
    "created_at": "2012-04-07T20:43:40+00:00",
    "closed_at": "2012-04-12T00:46:33+00:00",
    "resolver": "wesm",
    "resolved_in": "928da9d9b407e2b12c2d212a253472fb21ff5dc2",
    "resolver_commit_num": 1689,
    "title": "Function in OLS results to get predicted values",
    "body": "new function should take in new values for exogenous variables and produced predicted y values.\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned",
      "cross-referenced"
    ],
    "changed_files": 2,
    "additions": 37,
    "deletions": 3,
    "changed_files_list": [
      "pandas/stats/ols.py",
      "pandas/stats/tests/test_ols.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1009,
    "reporter": "wesm",
    "created_at": "2012-04-07T21:02:47+00:00",
    "closed_at": "2012-04-12T01:06:15+00:00",
    "resolver": "wesm",
    "resolved_in": "26f66944a2b8b01eff27abb51884e8a3c7d6b59c",
    "resolver_commit_num": 1690,
    "title": "Improve docs about Panel.from_dict with orient='minor'",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 22,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "doc/source/dsintro.rst",
      "pandas/io/parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1010,
    "reporter": "wesm",
    "created_at": "2012-04-07T21:30:55+00:00",
    "closed_at": "2012-04-10T03:41:58+00:00",
    "resolver": "wesm",
    "resolved_in": "f643da915f5d4e8a7812d5a0eb3bfecd7d8f6a0a",
    "resolver_commit_num": 1668,
    "title": "Unicode repr issues with MultiIndex",
    "body": "from mailing list\n\n\"\"\"\nHello All,\n\nWorking on 0.7.2.\n\nI have a DataFrame with a MultiIndex (2 levels).\nBecause my second level contains non ascii character I can't use the\nmethods ;\n- DataFrame.index.levels\n- DataFrame.get_level_values(1)\n\nIs it a known issue?\n\nThanks!\n\"\"\"\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 19,
    "deletions": 0,
    "changed_files_list": [
      "pandas/core/index.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1011,
    "reporter": "changhiskhan",
    "created_at": "2012-04-07T21:34:28+00:00",
    "closed_at": "2012-04-10T03:51:04+00:00",
    "resolver": "wesm",
    "resolved_in": "7375dd2efc4d0224f350062608893e2bb1c53484",
    "resolver_commit_num": 1669,
    "title": "DataFrame.plot(logy=True) seems to have no effect",
    "body": "df = DataFrame(np.random.randn(100, 5)*1000000)\n\ndf.plot() looks the same as df.plot(logy=True)\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 1,
    "additions": 5,
    "deletions": 7,
    "changed_files_list": [
      "pandas/tools/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1012,
    "reporter": "changhiskhan",
    "created_at": "2012-04-08T16:25:36+00:00",
    "closed_at": "2012-04-11T22:09:58+00:00",
    "resolver": "wesm",
    "resolved_in": "6bf2ef9bd9027a192bcf08e4b7aab92a0605784b",
    "resolver_commit_num": 1685,
    "title": "Allow option to set histogram fontsize",
    "body": "Right now DataFrame.hist plots histogram matrix and axis labels all overlap.\n",
    "labels": [],
    "comments": [
      "And set rotation\n"
    ],
    "events": [
      "assigned",
      "subscribed",
      "commented",
      "subscribed",
      "cross-referenced"
    ],
    "changed_files": 2,
    "additions": 98,
    "deletions": 16,
    "changed_files_list": [
      "pandas/tests/test_graphics.py",
      "pandas/tools/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1013,
    "reporter": "changhiskhan",
    "created_at": "2012-04-08T16:46:29+00:00",
    "closed_at": "2012-04-10T18:29:12+00:00",
    "resolver": "wesm",
    "resolved_in": "590136499047bfcd4902512a6579247bff94f40d",
    "resolver_commit_num": 1680,
    "title": "DataFrame.ix[tup, list] throws exception",
    "body": "per pydata email:\n\n---\n\nI created a dataframe with multi index like this:\n                      a   b   c\ny        m d  \n2000 1 1       1   2  3\n             2       7   8  9\n             3       4   5  6\n\n...\n\nit doesn't work to do this:  df.ix[(2000,1,1), ['a','b']]\n## it gives KeyError = 1\n\nin _NDFrameIndexer._getitem_tuple, the return value of the first ix._getitem_axis(key, axis=i) call returns a Series so the second call with axis=1 fails with KeyError=1\n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 13,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/indexing.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1015,
    "reporter": "wesm",
    "created_at": "2012-04-09T13:20:12+00:00",
    "closed_at": "2012-04-09T13:44:54+00:00",
    "resolver": "wesm",
    "resolved_in": "db5548293bb63d34b96c9eb8abaf2f5ea6ac9571",
    "resolver_commit_num": 1659,
    "title": "SparsePanel-Panel buggy interactions",
    "body": "SparsePanel.multiply(Panel)\nSparsePanel.subtract(Panel)\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 21,
    "deletions": 7,
    "changed_files_list": [
      "pandas/sparse/panel.py",
      "pandas/sparse/tests/test_sparse.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1016,
    "reporter": "changhiskhan",
    "created_at": "2012-04-10T12:46:41+00:00",
    "closed_at": "2012-04-10T16:04:07+00:00",
    "resolver": "wesm",
    "resolved_in": "d8e082d81ddc7186a84141267b6516e859b2600c",
    "resolver_commit_num": 1676,
    "title": "Series comparison causes crash",
    "body": "In [68]: a = Series(['a', 'b', 'c'])\n\nIn [69]: b = Series(['b', 'a'])\n\nIn [70]: a > b\n\nWarning: Program '/home/chang/epd/bin/ipython' crashed.                                                  \n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 14,
    "deletions": 0,
    "changed_files_list": [
      "pandas/src/tseries.pyx",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1018,
    "reporter": "wesm",
    "created_at": "2012-04-10T16:50:10+00:00",
    "closed_at": "2012-04-14T21:36:15+00:00",
    "resolver": "wesm",
    "resolved_in": "2fa4ba87bb18eac32868688114b171261b668e81",
    "resolver_commit_num": 1712,
    "title": "Implement product Cython bin aggregation function",
    "body": "For doing group cumulative returns etc\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 6,
    "additions": 146,
    "deletions": 21,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/src/groupby.pyx",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_timeseries.py",
      "pandas/tests/test_tseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1019,
    "reporter": "wesm",
    "created_at": "2012-04-10T16:50:35+00:00",
    "closed_at": "2012-04-14T21:57:15+00:00",
    "resolver": "wesm",
    "resolved_in": "d022e04e7a061abe8a8a347b59485243c010719f",
    "resolver_commit_num": 1713,
    "title": "Implement fast group min/max and bin versions",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 297,
    "deletions": 3,
    "changed_files_list": [
      "pandas/core/groupby.py",
      "pandas/src/groupby.pyx",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_tseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1026,
    "reporter": "echlebek",
    "created_at": "2012-04-11T16:42:23+00:00",
    "closed_at": "2012-05-07T18:02:34+00:00",
    "resolver": "kisielk",
    "resolved_in": "0b5a0078c8774a3563f78a0ede22ff6a2a2d5c07",
    "resolver_commit_num": 1,
    "title": "Indexing with namedtuple is broken",
    "body": "Although it is possible to index MultiIndexed DataFrames with multiple index columns, one or more of which have a compound type, it is not possible to index an Indexed DataFrame with a compound type for its column, nor is it possible to index a MultiIndexed Dataframe with a single column that has a compound type.\n\ntl;dr - I can't index a DataFrame with a namedtuple, even though I can create one.\n\nIn the first example, I try to index a dataframe with a namedtuple with a regular Index, which fails.\n\nIn the second example, I index a dataframe with a tuple of namedtuples (MultiIndex), which succeeds.\n\nIn the third example, I try to index a dataframe with a length-1 tuple of namedtuples, again with a MultiIndex, which fails.\n\n\n",
    "labels": [],
    "comments": [
      "The issue looks a bit more complicated now...\n\nFirst of all, we realized the above test is reporting false positive because of #1069\n\nSecondly, an additional problem lies [here](https://github.com/pydata/pandas/blob/master/pandas/core/indexing.py#L168). In particular, `_is_list_like` prevents using any iterable object as an `Index` key.\n\nAt this point, it's a question of where you want to go with the indexing interface. I think might be reasonable to limit the types (aside from `Index` itself) used for supplying index sequences to, say, `tuple`, `list` and `numpy.array`. The upside is not having to think about adding more exceptions (currently there's `basestring`, plus, in our case, a tuple subclass); the downside is not supporting arbitrary iterables such as generators. I would personally be in favour of the former because it is the simplest of the two (internal logic and behaviour-wise) in the long run.\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "subscribed",
      "closed",
      "subscribed",
      "commented",
      "subscribed",
      "referenced",
      "reopened"
    ],
    "changed_files": 2,
    "additions": 10,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/indexing.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1028,
    "reporter": "wesm",
    "created_at": "2012-04-11T18:16:13+00:00",
    "closed_at": "2012-04-22T03:12:59+00:00",
    "resolver": "wesm",
    "resolved_in": "e5113c21f773ffceafc44fe6d14f3e77892f4097",
    "resolver_commit_num": 1732,
    "title": "Add order (sorting) method to Index classes",
    "body": "",
    "labels": [],
    "comments": [
      "Radix sort on int64!\n\nOn Apr 11, 2012, at 2:16 PM, Wes McKinney\nreply@reply.github.com\nwrote:\n\n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/pydata/pandas/issues/1028\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 47,
    "deletions": 6,
    "changed_files_list": [
      "pandas/core/index.py",
      "pandas/core/series.py",
      "pandas/tseries/index.py",
      "vb_suite/timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1031,
    "reporter": "wesm",
    "created_at": "2012-04-12T01:39:48+00:00",
    "closed_at": "2012-04-14T21:23:20+00:00",
    "resolver": "wesm",
    "resolved_in": "992b1ff5f17a0111a15963cf8387180e368a814a",
    "resolver_commit_num": 1711,
    "title": "Add normalize options to date_range, bdate_range",
    "body": "Right now there is a hackjob floating around with a _normalizeFirst parameter or something. This should be done away with and deal with in a more structured way. Related to #506\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 156,
    "deletions": 138,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/daterange.py",
      "pandas/core/datetools.py",
      "pandas/core/index.py",
      "pandas/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1035,
    "reporter": "wesm",
    "created_at": "2012-04-12T18:59:08+00:00",
    "closed_at": "2012-04-12T23:27:26+00:00",
    "resolver": "wesm",
    "resolved_in": "63207b83fc35c2ade37861ca581949d95f63ad61",
    "resolver_commit_num": 1707,
    "title": "DatetimeBlock -> FloatBlock in reindex op",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 82,
    "additions": 16580,
    "deletions": 2757,
    "changed_files_list": [
      ".gitignore",
      "NP_LICENSE.txt",
      "bench/bench_dense_to_sparse.py",
      "bench/bench_tseries.py",
      "pandas/core/api.py",
      "pandas/core/common.py",
      "pandas/core/daterange.py",
      "pandas/core/datetools.py",
      "pandas/core/format.py",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/indexing.py",
      "pandas/core/internals.py",
      "pandas/core/nanops.py",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/info.py",
      "pandas/io/parsers.py",
      "pandas/io/pytables.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/io/tests/test_pytables.py",
      "pandas/sparse/frame.py",
      "pandas/sparse/panel.py",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_sparse.py",
      "pandas/src/data_algos.pyx",
      "pandas/src/datetime.pxd",
      "pandas/src/datetime.pyx",
      "pandas/src/engines.pyx",
      "pandas/src/generate_code.py",
      "pandas/src/generated.pyx",
      "pandas/src/groupby.pyx",
      "pandas/src/hashtable.pxd",
      "pandas/src/hashtable.pyx",
      "pandas/src/inference.pyx",
      "pandas/src/khash.pxd",
      "pandas/src/np_datetime.c",
      "pandas/src/np_datetime.h",
      "pandas/src/np_datetime_strings.c",
      "pandas/src/np_datetime_strings.h",
      "pandas/src/numpy_helper.h",
      "pandas/src/reduce.pyx",
      "pandas/src/reindex.pyx",
      "pandas/src/sandbox.pyx",
      "pandas/src/skts.c",
      "pandas/src/skts.h",
      "pandas/src/tseries.pyx",
      "pandas/src/util.pxd",
      "pandas/stats/tests/common.py",
      "pandas/stats/tests/test_math.py",
      "pandas/stats/tests/test_moments.py",
      "pandas/stats/tests/test_ols.py",
      "pandas/tests/data/frame.pickle",
      "pandas/tests/data/series.pickle",
      "pandas/tests/test_daterange.py",
      "pandas/tests/test_datetools.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_graphics.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_interval.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_series.py",
      "pandas/tests/test_timeseries.py",
      "pandas/tests/test_tseries.py",
      "pandas/tools/merge.py",
      "pandas/tools/plotting.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tools/tests/test_pivot.py",
      "pandas/tools/tsplotting.py",
      "pandas/tseries/__init__.py",
      "pandas/tseries/tests/__init__.py",
      "pandas/tseries/tests/test_tools.py",
      "pandas/tseries/tools.py",
      "pandas/util/testing.py",
      "setup.py",
      "test.sh",
      "ts_todo.txt",
      "vb_suite/sparse.py",
      "vb_suite/timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1036,
    "reporter": "wesm",
    "created_at": "2012-04-12T19:02:52+00:00",
    "closed_at": "2012-04-12T21:42:17+00:00",
    "resolver": "wesm",
    "resolved_in": "63207b83fc35c2ade37861ca581949d95f63ad61",
    "resolver_commit_num": 1707,
    "title": "time_rule/timeRule broken in DateRange",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 82,
    "additions": 16580,
    "deletions": 2757,
    "changed_files_list": [
      ".gitignore",
      "NP_LICENSE.txt",
      "bench/bench_dense_to_sparse.py",
      "bench/bench_tseries.py",
      "pandas/core/api.py",
      "pandas/core/common.py",
      "pandas/core/daterange.py",
      "pandas/core/datetools.py",
      "pandas/core/format.py",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/indexing.py",
      "pandas/core/internals.py",
      "pandas/core/nanops.py",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/info.py",
      "pandas/io/parsers.py",
      "pandas/io/pytables.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/io/tests/test_pytables.py",
      "pandas/sparse/frame.py",
      "pandas/sparse/panel.py",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_sparse.py",
      "pandas/src/data_algos.pyx",
      "pandas/src/datetime.pxd",
      "pandas/src/datetime.pyx",
      "pandas/src/engines.pyx",
      "pandas/src/generate_code.py",
      "pandas/src/generated.pyx",
      "pandas/src/groupby.pyx",
      "pandas/src/hashtable.pxd",
      "pandas/src/hashtable.pyx",
      "pandas/src/inference.pyx",
      "pandas/src/khash.pxd",
      "pandas/src/np_datetime.c",
      "pandas/src/np_datetime.h",
      "pandas/src/np_datetime_strings.c",
      "pandas/src/np_datetime_strings.h",
      "pandas/src/numpy_helper.h",
      "pandas/src/reduce.pyx",
      "pandas/src/reindex.pyx",
      "pandas/src/sandbox.pyx",
      "pandas/src/skts.c",
      "pandas/src/skts.h",
      "pandas/src/tseries.pyx",
      "pandas/src/util.pxd",
      "pandas/stats/tests/common.py",
      "pandas/stats/tests/test_math.py",
      "pandas/stats/tests/test_moments.py",
      "pandas/stats/tests/test_ols.py",
      "pandas/tests/data/frame.pickle",
      "pandas/tests/data/series.pickle",
      "pandas/tests/test_daterange.py",
      "pandas/tests/test_datetools.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_graphics.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_interval.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_series.py",
      "pandas/tests/test_timeseries.py",
      "pandas/tests/test_tseries.py",
      "pandas/tools/merge.py",
      "pandas/tools/plotting.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tools/tests/test_pivot.py",
      "pandas/tools/tsplotting.py",
      "pandas/tseries/__init__.py",
      "pandas/tseries/tests/__init__.py",
      "pandas/tseries/tests/test_tools.py",
      "pandas/tseries/tools.py",
      "pandas/util/testing.py",
      "setup.py",
      "test.sh",
      "ts_todo.txt",
      "vb_suite/sparse.py",
      "vb_suite/timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1037,
    "reporter": "wesm",
    "created_at": "2012-04-12T19:04:25+00:00",
    "closed_at": "2012-04-12T21:52:34+00:00",
    "resolver": "wesm",
    "resolved_in": "63207b83fc35c2ade37861ca581949d95f63ad61",
    "resolver_commit_num": 1707,
    "title": "Arrays of datetime64 not handled in DataFrame constructor with passed dict",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 82,
    "additions": 16580,
    "deletions": 2757,
    "changed_files_list": [
      ".gitignore",
      "NP_LICENSE.txt",
      "bench/bench_dense_to_sparse.py",
      "bench/bench_tseries.py",
      "pandas/core/api.py",
      "pandas/core/common.py",
      "pandas/core/daterange.py",
      "pandas/core/datetools.py",
      "pandas/core/format.py",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/indexing.py",
      "pandas/core/internals.py",
      "pandas/core/nanops.py",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/info.py",
      "pandas/io/parsers.py",
      "pandas/io/pytables.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/io/tests/test_pytables.py",
      "pandas/sparse/frame.py",
      "pandas/sparse/panel.py",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_sparse.py",
      "pandas/src/data_algos.pyx",
      "pandas/src/datetime.pxd",
      "pandas/src/datetime.pyx",
      "pandas/src/engines.pyx",
      "pandas/src/generate_code.py",
      "pandas/src/generated.pyx",
      "pandas/src/groupby.pyx",
      "pandas/src/hashtable.pxd",
      "pandas/src/hashtable.pyx",
      "pandas/src/inference.pyx",
      "pandas/src/khash.pxd",
      "pandas/src/np_datetime.c",
      "pandas/src/np_datetime.h",
      "pandas/src/np_datetime_strings.c",
      "pandas/src/np_datetime_strings.h",
      "pandas/src/numpy_helper.h",
      "pandas/src/reduce.pyx",
      "pandas/src/reindex.pyx",
      "pandas/src/sandbox.pyx",
      "pandas/src/skts.c",
      "pandas/src/skts.h",
      "pandas/src/tseries.pyx",
      "pandas/src/util.pxd",
      "pandas/stats/tests/common.py",
      "pandas/stats/tests/test_math.py",
      "pandas/stats/tests/test_moments.py",
      "pandas/stats/tests/test_ols.py",
      "pandas/tests/data/frame.pickle",
      "pandas/tests/data/series.pickle",
      "pandas/tests/test_daterange.py",
      "pandas/tests/test_datetools.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_graphics.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_interval.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_series.py",
      "pandas/tests/test_timeseries.py",
      "pandas/tests/test_tseries.py",
      "pandas/tools/merge.py",
      "pandas/tools/plotting.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tools/tests/test_pivot.py",
      "pandas/tools/tsplotting.py",
      "pandas/tseries/__init__.py",
      "pandas/tseries/tests/__init__.py",
      "pandas/tseries/tests/test_tools.py",
      "pandas/tseries/tools.py",
      "pandas/util/testing.py",
      "setup.py",
      "test.sh",
      "ts_todo.txt",
      "vb_suite/sparse.py",
      "vb_suite/timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1041,
    "reporter": "wesm",
    "created_at": "2012-04-13T00:25:01+00:00",
    "closed_at": "2012-04-14T20:21:26+00:00",
    "resolver": "wesm",
    "resolved_in": "9385b62d07c12ea361112b167ae811b3da62d3f2",
    "resolver_commit_num": 1709,
    "title": "Legacy time rule support",
    "body": "E.g. A@DEC -> BA-DEC\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 37,
    "deletions": 17,
    "changed_files_list": [
      "pandas/core/datetools.py",
      "pandas/tests/test_datetools.py",
      "pandas/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1042,
    "reporter": "wesm",
    "created_at": "2012-04-13T00:56:48+00:00",
    "closed_at": "2012-05-03T23:25:18+00:00",
    "resolver": "wesm",
    "resolved_in": "cf3167ea84c7aa7cbc29383272ab813dd29fa027",
    "resolver_commit_num": 1804,
    "title": "API deprecations (time_rule -> freq) with moving window functions",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 2,
    "additions": 77,
    "deletions": 42,
    "changed_files_list": [
      "pandas/stats/moments.py",
      "pandas/stats/tests/test_moments.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1043,
    "reporter": "wesm",
    "created_at": "2012-04-13T01:39:02+00:00",
    "closed_at": "2012-05-12T19:23:14+00:00",
    "resolver": "wesm",
    "resolved_in": "a98035c9f642345ad1e3b90a9a819ac3f65ad7b0",
    "resolver_commit_num": 1870,
    "title": "Add first / last optimized groupby functions",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "cross-referenced"
    ],
    "changed_files": 5,
    "additions": 238,
    "deletions": 18,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/src/groupby.pyx",
      "pandas/tests/test_groupby.py",
      "vb_suite/groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1044,
    "reporter": "wesm",
    "created_at": "2012-04-13T02:15:36+00:00",
    "closed_at": "2012-04-14T20:42:13+00:00",
    "resolver": "wesm",
    "resolved_in": "d988d11a2bcd281f587a557f79aa5000e0b38a9c",
    "resolver_commit_num": 1710,
    "title": "Make partial slicing work with DatetimeIndex",
    "body": "e.g. `ts['2000':'2004']`\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 47,
    "deletions": 11,
    "changed_files_list": [
      "pandas/core/index.py",
      "pandas/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1045,
    "reporter": "wesm",
    "created_at": "2012-04-13T04:37:55+00:00",
    "closed_at": "2012-04-14T20:09:47+00:00",
    "resolver": "wesm",
    "resolved_in": "ad9cffdc2c263cfd423abccde264d54b270f307b",
    "resolver_commit_num": 1708,
    "title": "NumPy reductions don't work with convert / TimeGrouper",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 124,
    "deletions": 59,
    "changed_files_list": [
      "pandas/core/groupby.py",
      "pandas/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1048,
    "reporter": "twiecki",
    "created_at": "2012-04-13T12:50:06+00:00",
    "closed_at": "2012-05-07T16:36:05+00:00",
    "resolver": "wesm",
    "resolved_in": "f03dd5cf4045838f3021086dc891837a397a5719",
    "resolver_commit_num": 1825,
    "title": "Segfault in .groupby() with empty groups",
    "body": "I get a segfault when running:\n\npandas.DataFrame([1,2,3]).groupby([]).groups\n\nAs an aside, in my scenario (which I don't know how general it is) the user can supply the grouping. If no grouping should take place (i.e. groupby([])) I would like .groupby() to return the whole data, instead there is a:\n\n/usr/local/lib/python2.7/dist-packages/pandas/core/groupby.pyc in _get_compressed_labels(self)\n    547                 group_index = get_group_index(all_labels, self.shape)\n    548             else:\n--> 549                 group_index = all_labels[0]\n    550             comp_ids, obs_group_ids = _compress_group_index(group_index)\n    551             return comp_ids, obs_group_ids\n\nIndexError: list index out of range\n\nI'm not sure whether it would be sensible to change the behavior so that the whole DataFrame is returned, but to me it would be more useful than raising an error.\n",
    "labels": [],
    "comments": [
      "I just updated to 9385b62d07c12ea361112b167ae811b3da62d3f2 and now also see a segfault when running the tests:\n\n```\ntest_aggregate_item_by_item (pandas.tests.test_groupby.TestGroupBy) ... Segmentation fault: 11\n```\n\nas well as when running this line of code\n\n```\npandas.DataFrame([1,2,3]).groupby([]).groups\n```\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "referenced",
      "subscribed",
      "closed",
      "reopened",
      "subscribed",
      "assigned"
    ],
    "changed_files": 5,
    "additions": 10,
    "deletions": 26,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/src/generate_code.py",
      "pandas/src/generated.pyx",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1053,
    "reporter": "tkf",
    "created_at": "2012-04-14T13:38:21+00:00",
    "closed_at": "2012-04-16T19:33:22+00:00",
    "resolver": "lodagro",
    "resolved_in": "219bdcb472a8fbcc7cd218f49e3ae5e4df757f78",
    "resolver_commit_num": 30,
    "title": "Bug in DataFrame.to_html with MultiIndex",
    "body": "\n\ngenerates\n\n<table border=\"1\">\n  <thead>\n    <tr>\n      <th><table><tbody><tr><td>a</td></tr><tr><td>x</td></tr></tbody></table></th>\n      <th><table><tbody><tr><td>a</td></tr><tr><td>y</td></tr></tbody></table></th>\n      <th><table><tbody><tr><td>b</td></tr><tr><td>x</td></tr></tbody></table></th>\n      <th><table><tbody><tr><td>b</td></tr><tr><td>y</td></tr></tbody></table></th>\n    </tr>\n    </thead>\n    <tbody>\n    <tr>\n      <td><strong>0</strong></td>\n      <td> 0.280664</td>\n      <td> 0.465508</td>\n      <td> 0.616751</td>\n      <td> 0.246239</td>\n    </tr>\n    <tr>\n      <td><strong>1</strong></td>\n      <td> 0.124957</td>\n      <td> 0.867052</td>\n      <td> 0.504789</td>\n      <td> 0.178313</td>\n    </tr>\n  </tbody>\n</table>\n",
    "labels": [],
    "comments": [
      "see also #909, issue only pops up when columns use MultiIndex without level names. \n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "assigned",
      "commented"
    ],
    "changed_files": 2,
    "additions": 81,
    "deletions": 0,
    "changed_files_list": [
      "pandas/core/format.py",
      "pandas/tests/test_format.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1057,
    "reporter": "wesm",
    "created_at": "2012-04-14T21:56:52+00:00",
    "closed_at": "2012-04-14T22:20:02+00:00",
    "resolver": "wesm",
    "resolved_in": "141df576f1a634097fa2ad6f2008f7902325f8c3",
    "resolver_commit_num": 1714,
    "title": "Pure python apply broken with TimeGrouper",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 32,
    "deletions": 18,
    "changed_files_list": [
      "pandas/core/groupby.py",
      "pandas/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1058,
    "reporter": "wesm",
    "created_at": "2012-04-15T01:32:14+00:00",
    "closed_at": "2012-05-14T15:39:42+00:00",
    "resolver": "wesm",
    "resolved_in": "55ea789c2824f873e561044b456d03977b0ad9cb",
    "resolver_commit_num": 1889,
    "title": "What should Series scalars be when datetime dtype?",
    "body": "Maybe should return timestamps\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 11,
    "deletions": 5,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/src/engines.pyx",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1061,
    "reporter": "wesm",
    "created_at": "2012-04-16T02:51:14+00:00",
    "closed_at": "2012-05-07T14:46:18+00:00",
    "resolver": "wesm",
    "resolved_in": "ebb076ef71fe5f5aa09a6f18580b24a54b745764",
    "resolver_commit_num": 1820,
    "title": "Better error messages during build/import for NumPy < 1.6",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 4,
    "deletions": 1,
    "changed_files_list": [
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1062,
    "reporter": "wesm",
    "created_at": "2012-04-16T03:08:29+00:00",
    "closed_at": "2012-04-22T02:02:50+00:00",
    "resolver": "wesm",
    "resolved_in": "e4880d92422d209e47df96a51ea1959e26170166",
    "resolver_commit_num": 1728,
    "title": "KeyError raised out of DatetimeEngine formatted as int64",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 20,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1063,
    "reporter": "wesm",
    "created_at": "2012-04-16T03:32:17+00:00",
    "closed_at": "2012-04-22T00:14:11+00:00",
    "resolver": "wesm",
    "resolved_in": "ec290748b1a453605f32e3f690a674be4619cca6",
    "resolver_commit_num": 1723,
    "title": "Series.shift bugs, set default to periods=1",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "referenced"
    ],
    "changed_files": 9,
    "additions": 35,
    "deletions": 31,
    "changed_files_list": [
      "pandas/core/datetools.py",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/series.py",
      "pandas/src/datetime.pyx",
      "pandas/tests/test_datetools.py",
      "pandas/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1065,
    "reporter": "gerigk",
    "created_at": "2012-04-16T10:58:16+00:00",
    "closed_at": "2012-04-23T17:46:20+00:00",
    "resolver": "wesm",
    "resolved_in": "4dd1ce086770c2bbea58650408d360916223664b",
    "resolver_commit_num": 1745,
    "title": "Group By datatype screwed when group consists of one value",
    "body": "x = DataFrame(np.arange(9).reshape(3,3))\nx['test']=0\nx['fl']= [1.3,1.5,1.6]\nx.groupby('test').agg({'fl':'sum',2:'size'}).dtypes\nOut[]:\n2      int64\nfl    object\n(version 0.8)\n\nOutput in 0.7.3\nOut[]:\n2       int64\nfl    float64\n\nThe output for x['test'] = [1,2,3] works fine\n",
    "labels": [],
    "comments": [
      "Apparently fl is not only an object, but also an array!\n\nfrom pandas import *\nimport numpy as np\nx = DataFrame(np.arange(9).reshape(3,3))\nx['test']=0\nx['fl']= [1.3,1.5,1.6]\nx.groupby('test').agg({'fl':'sum',2:'size'}).dtypes\ngrouped = x.groupby('test').agg({'fl':'sum',2:'size'})\ntype(grouped['fl'][0])\n\noutput:  type 'numpy.ndarray'\n\nprint type(grouped[2][0])\n\noutput: type 'numpy.int64'\n\nThis issue is bugging me a lot because I am doing aggregations on data which may or may not have more than one category per group.\n\nprint type(grouped['fl'][0].astype(float))\n\nstill returns type 'numpy.ndarray'\n\nwhereas it works for the int column:\n\nprint type(grouped[2][0].astype(float))\n\noutput: type 'numpy.float64'\n",
      "if grouped.shape[0]==1:\n    grouped = DataFrame([tuple([grouped[y].iget(0).item() if isinstance(grouped[y].iget(0),np.ndarray) else grouped[y].iget(0) for y in grouped.columns])], index=grouped.index, columns=grouped.columns)\nis an ugly workaround in case somebody else walks into this\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "commented",
      "assigned",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 19,
    "deletions": 10,
    "changed_files_list": [
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1068,
    "reporter": "wesm",
    "created_at": "2012-04-16T20:35:37+00:00",
    "closed_at": "2012-04-16T20:45:45+00:00",
    "resolver": "wesm",
    "resolved_in": "e1552a607be435b71e16da02940118b30a25b939",
    "resolver_commit_num": 1717,
    "title": "Partial date indexing off by one error",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 12,
    "deletions": 13,
    "changed_files_list": [
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1074,
    "reporter": "elpres",
    "created_at": "2012-04-18T15:06:23+00:00",
    "closed_at": "2012-05-07T14:17:05+00:00",
    "resolver": "wesm",
    "resolved_in": "dd89d214c026476c9a9e1bed068ef3c772f45bfe",
    "resolver_commit_num": 1816,
    "title": "Crash when combining columns containing NaNs",
    "body": "Here's the code:\n\n\n\nThis seems to happen when performing any kind of boolean operation on a column containing a NaN. Calling .fillna() on it before the operation doesn't help (i.e. \"d['a'].fillna(False) | d['b']\" crashes as well). Also, \"d['a'] & True\" will fail too, but without the messages about \"tokenizing input\".\n",
    "labels": [],
    "comments": [
      "There are multiple bugs here actually. Looking into this\n"
    ],
    "events": [
      "subscribed",
      "assigned",
      "subscribed",
      "commented"
    ],
    "changed_files": 3,
    "additions": 37,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/series.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1077,
    "reporter": "wesm",
    "created_at": "2012-04-18T17:23:39+00:00",
    "closed_at": "2012-04-23T18:04:02+00:00",
    "resolver": "wesm",
    "resolved_in": "918266ec1dfa77983f5869b0257604eb8ac0d6c9",
    "resolver_commit_num": 1747,
    "title": "What should happen to a non-unique time series in frequency conversion?",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 16,
    "deletions": 4,
    "changed_files_list": [
      "pandas/core/index.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1079,
    "reporter": "nmichaud",
    "created_at": "2012-04-18T20:16:09+00:00",
    "closed_at": "2012-05-08T19:13:42+00:00",
    "resolver": "wesm",
    "resolved_in": "0a36dd58eb9873b9f240f95eb5f14ff8efd0c0f9",
    "resolver_commit_num": 1839,
    "title": "Can't compare Dataframe to None",
    "body": "> import pandas\n> import numpy\n> df = pandas.DataFrame(numpy.random.randn(8, 3), index=range(8),columns=['A', 'B', 'C'])\n> None == df\n\nraises: pandas.core.common.PandasError: DataFrame constructor not properly called!\n",
    "labels": [],
    "comments": [
      "It seems that the comparison operators in https://github.com/pydata/pandas/blob/master/pandas/core/frame.py#L564 should be using numpy's comparison operators (so the None comparison is broadcast properly)\n",
      "I think this ought to raise a TypeError now (I think you must be using an old version of pandas, I didn't have to make any changes for this):\n\n```\nIn [3]: df\nOut[3]: \n          A         B         C\n0  0.545436 -0.658971 -0.537160\n1 -0.919375  1.978925 -0.255735\n2 -1.641768  0.852692  0.402625\n3 -0.377459  0.856477  0.603272\n4 -1.931178 -1.177506 -2.266807\n5  0.013705  1.124144  0.304718\n6 -1.269860 -0.827188  0.749840\n7  0.626440  1.225717  3.689088\n\nIn [4]: df == None\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/home/wesm/code/pandas/<ipython-input-4-c0373cc02cc9> in <module>()\n----> 1 df == None\n\n/home/wesm/code/pandas/pandas/core/frame.pyc in f(self, other)\n    233             return self._combine_series_infer(other, func)\n    234         else:\n--> 235             return self._combine_const(other, func)\n    236 \n    237     f.__name__ = name\n\n/home/wesm/code/pandas/pandas/core/frame.pyc in _combine_const(self, other, func)\n   2720         if not isinstance(result_values, np.ndarray):\n   2721             raise TypeError('Could not compare %s with DataFrame values'\n-> 2722                             % repr(other))\n   2723 \n   2724         return self._constructor(result_values, index=self.index,\n\nTypeError: Could not compare None with DataFrame values\n```\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 15,
    "deletions": 0,
    "changed_files_list": [
      "pandas/core/series.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1080,
    "reporter": "wesm",
    "created_at": "2012-04-19T00:11:36+00:00",
    "closed_at": "2012-04-22T15:28:01+00:00",
    "resolver": "wesm",
    "resolved_in": "491e636cde8ed08bb0c26d80cd608ca8679407b3",
    "resolver_commit_num": 1737,
    "title": "Check/test partial time series indexing in non-monotonic time series",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 15,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1085,
    "reporter": "wesm",
    "created_at": "2012-04-19T22:34:34+00:00",
    "closed_at": "2012-04-20T18:02:56+00:00",
    "resolver": "wesm",
    "resolved_in": "e3ef17af82a602598c29a3635fd0a890d95541d3",
    "resolver_commit_num": 1720,
    "title": "Empty string -> NaT in to_datetime",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 15,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/datetools.py",
      "pandas/src/datetime.pyx",
      "pandas/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1088,
    "reporter": "wesm",
    "created_at": "2012-04-20T02:05:23+00:00",
    "closed_at": "2012-04-20T20:33:43+00:00",
    "resolver": "wesm",
    "resolved_in": "d840edafd017664d53f40bd951a05cba0cddebe1",
    "resolver_commit_num": 1721,
    "title": "Week of month alias cleanup + legacy naming",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 10,
    "deletions": 9,
    "changed_files_list": [
      "pandas/core/datetools.py",
      "pandas/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1089,
    "reporter": "wesm",
    "created_at": "2012-04-20T02:11:21+00:00",
    "closed_at": "2012-04-20T17:42:49+00:00",
    "resolver": "wesm",
    "resolved_in": "cc44795fe98325d0800383a24eaf85e82bacd1a2",
    "resolver_commit_num": 1719,
    "title": "Enable passing string + validation to tz field in DatetimeIndex",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 16,
    "deletions": 2,
    "changed_files_list": [
      "pandas/core/datetools.py",
      "pandas/core/index.py",
      "pandas/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1092,
    "reporter": "wesm",
    "created_at": "2012-04-20T19:30:07+00:00",
    "closed_at": "2012-04-22T01:26:59+00:00",
    "resolver": "wesm",
    "resolved_in": "77cbbc6bbedcc33bc3c539acdb787686cd30599b",
    "resolver_commit_num": 1726,
    "title": "Trim scipy.stats dependencies",
    "body": "Need to ship scoreatpercentile and a couple other things\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 9,
    "additions": 270,
    "deletions": 33,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/compat/scipy.py",
      "pandas/core/frame.py",
      "pandas/core/series.py",
      "pandas/stats/misc.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_series.py",
      "pandas/tests/test_tseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1095,
    "reporter": "wesm",
    "created_at": "2012-04-20T20:38:32+00:00",
    "closed_at": "2012-04-22T01:49:53+00:00",
    "resolver": "wesm",
    "resolved_in": "a88afc79f1696470471fbc731d881b6d513f7a0f",
    "resolver_commit_num": 1727,
    "title": "Better error message when date_range improperly specified",
    "body": "noticed in test failure:\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 45,
    "deletions": 18,
    "changed_files_list": [
      "pandas/core/common.py",
      "pandas/tseries/index.py",
      "pandas/tseries/offsets.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1096,
    "reporter": "wesm",
    "created_at": "2012-04-20T20:42:23+00:00",
    "closed_at": "2012-04-23T17:20:59+00:00",
    "resolver": "wesm",
    "resolved_in": "1a98cbc1dd26c8b4994f11fc39082d26f9a3a87a",
    "resolver_commit_num": 1744,
    "title": "Enable +/- ops to work between DatetimeIndex and DateOffset objects",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 8,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1099,
    "reporter": "wesm",
    "created_at": "2012-04-21T17:39:34+00:00",
    "closed_at": "2012-04-22T02:54:41+00:00",
    "resolver": "wesm",
    "resolved_in": "3602da391f4ae7bcafa943e35391a4e857a35940",
    "resolver_commit_num": 1731,
    "title": "Reindexing DatetimeIndex with list/array of datetime.datetime",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 26,
    "deletions": 6,
    "changed_files_list": [
      "pandas/tools/tests/test_tools.py",
      "pandas/tseries/index.py",
      "pandas/tseries/offsets.py",
      "pandas/tseries/tests/test_daterange.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1100,
    "reporter": "wesm",
    "created_at": "2012-04-22T02:07:48+00:00",
    "closed_at": "2012-04-23T18:08:57+00:00",
    "resolver": "wesm",
    "resolved_in": "978b396775fffc3e919a7d4e748dfc3a91e04397",
    "resolver_commit_num": 1748,
    "title": "Datetime formatting for year < 1900",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 8,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1101,
    "reporter": "wesm",
    "created_at": "2012-04-22T02:23:21+00:00",
    "closed_at": "2012-04-22T04:49:15+00:00",
    "resolver": "wesm",
    "resolved_in": "be5cd06b669367596b4272398a3125ff9c03d600",
    "resolver_commit_num": 1734,
    "title": "TimeSeries.take loses tz",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 35,
    "deletions": 3,
    "changed_files_list": [
      "pandas/core/common.py",
      "pandas/src/tseries.pyx",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1102,
    "reporter": "wesm",
    "created_at": "2012-04-22T02:30:08+00:00",
    "closed_at": "2012-04-25T19:17:22+00:00",
    "resolver": "wesm",
    "resolved_in": "f750485c264a6fdf1a69c9374673fa95c8373860",
    "resolver_commit_num": 1753,
    "title": "Add frequency tastefully to TimeSeries/DataFrame repr",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 86,
    "deletions": 30,
    "changed_files_list": [
      "pandas/core/format.py",
      "pandas/core/series.py",
      "pandas/tests/test_datetools.py",
      "pandas/tseries/index.py",
      "pandas/tseries/offsets.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1104,
    "reporter": "gerigk",
    "created_at": "2012-04-22T11:56:12+00:00",
    "closed_at": "2012-04-22T14:48:56+00:00",
    "resolver": "wesm",
    "resolved_in": "04e3c6bd09564be594496f5ddc5d73b893155d93",
    "resolver_commit_num": 1735,
    "title": "Import error after update",
    "body": "> > > from pandas import *\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File \"/usr/local/lib/python2.7/dist-packages/pandas-0.8.0.dev_be5cd06-py2.7-linux-x86_64.egg/pandas/**init**.py\", line 22, in <module>\n> > >     from pandas.core.api import *\n> > >   File \"/usr/local/lib/python2.7/dist-packages/pandas-0.8.0.dev_be5cd06-py2.7-linux-x86_64.egg/pandas/core/api.py\", line 6, in <module>\n> > >     from pandas.core.datetools import DateOffset, to_datetime\n> > >   File \"/usr/local/lib/python2.7/dist-packages/pandas-0.8.0.dev_be5cd06-py2.7-linux-x86_64.egg/pandas/core/datetools.py\", line 3, in <module>\n> > >     from pandas.tseries.tools import *\n> > > ImportError: No module named tseries.tools\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 11,
    "additions": 27,
    "deletions": 16,
    "changed_files_list": [
      "pandas/core/api.py",
      "pandas/core/daterange.py",
      "pandas/core/factor.py",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/series.py",
      "pandas/sparse/tests/test_sparse.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tseries/tests/test_daterange.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1107,
    "reporter": "lodagro",
    "created_at": "2012-04-23T07:29:07+00:00",
    "closed_at": "2012-04-23T15:54:32+00:00",
    "resolver": "wesm",
    "resolved_in": "17b0357bea7610bcb4abc320a7b971f506a32c45",
    "resolver_commit_num": 1741,
    "title": "nosetests  pandas.tseries.tests.test_resample 'double free or corruption'",
    "body": "\n\n\n\n\n",
    "labels": [],
    "comments": [
      "What version of NumPy are you using? Can you paste the output of `nosetests -v` called on that file? I was debugging some code potentially accessing out of bounds memory but I thought I got everything (not having any such bugs on my mac...). You rebuilt the C extensions right?\n",
      "I'm able to reproduce the error so no worries; I'll get it fixed shortly\n",
      "numpy version 1.6.1\n\nC extensions are rebuild, i even un-installed pandas, removed build dir and installed again. Same issue.\n\n```\n(pandas)[1853][n] nosetests -v pandas.tseries.tests.test_resample\n\ntest_custom_grouper (pandas.tseries.tests.test_resample.TestResample) ... *** glibc detected *** ~/.virtualenvs/pandas/bin/python: double free or corruption (!prev): 0x000000001c447dc0 ***\n======= Backtrace: =========\n/lib64/libc.so.6[0x3d7047245f]\n/lib64/libc.so.6(cfree+0x4b)[0x3d704728bb]\n~/.virtualenvs/pandas/lib/python2.7/site-packages/numpy/core/multiarray.so[0x2ab383f8726f]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2ab37c956083]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x5ef9)[0x2ab37c9c60b9]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x5ed3)[0x2ab37c9c6093]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x8d2)[0x2ab37c9c7722]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x54f6)[0x2ab37c9c56b6]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x8d2)[0x2ab37c9c7722]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2ab37c957522]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2ab37c930d18]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0xd29)[0x2ab37c9c0ee9]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x8d2)[0x2ab37c9c7722]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x54f6)[0x2ab37c9c56b6]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x5ed3)[0x2ab37c9c6093]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x8d2)[0x2ab37c9c7722]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2ab37c957522]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2ab37c930d18]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0xd29)[0x2ab37c9c0ee9]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x8d2)[0x2ab37c9c7722]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2ab37c957522]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2ab37c930d18]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2ab37c93f18d]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2ab37c930d18]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2ab37c98715a]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2ab37c930d18]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x1268)[0x2ab37c9c1428]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x5ed3)[0x2ab37c9c6093]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x8d2)[0x2ab37c9c7722]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2ab37c957522]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2ab37c930d18]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0xd29)[0x2ab37c9c0ee9]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x8d2)[0x2ab37c9c7722]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2ab37c957522]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2ab37c930d18]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2ab37c93f18d]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2ab37c930d18]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2ab37c98715a]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2ab37c930d18]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x1268)[0x2ab37c9c1428]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x8d2)[0x2ab37c9c7722]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2ab37c957522]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2ab37c930d18]\n~/.virtualenvs/pandas/lib/libpy/home/wovermei/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x1268)[0x2ab37c9c1428]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x8d2)[0x2ab37c9c7722]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2ab37c957522]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2ab37c930d18]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0xd29)[0x2ab37c9c0ee9]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0x8d2)[0x2ab37c9c7722]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2ab37c957522]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2ab37c930d18]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2ab37c93f18d]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0(PyObject_Call+0x68)[0x2ab37c930d18]\n~/.virtualenvs/pandas/lib/libpython2.7.so.1.0[0x2ab37c98715a]\n~/.virtualenzsh: abort      nosetests -v pandas.tseries.tests.test_resample\n```\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "commented"
    ],
    "changed_files": 6,
    "additions": 32,
    "deletions": 30,
    "changed_files_list": [
      "pandas/core/groupby.py",
      "pandas/src/groupby.pyx",
      "pandas/src/sandbox.pyx",
      "pandas/tests/test_tseries.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1109,
    "reporter": "wesm",
    "created_at": "2012-04-23T17:51:22+00:00",
    "closed_at": "2012-04-28T20:09:31+00:00",
    "resolver": "wesm",
    "resolved_in": "876df6fb67a9ac3cfe238045745a608aad3b3de4",
    "resolver_commit_num": 1787,
    "title": "Implement resampling of objects with PeriodIndex",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 56,
    "deletions": 24,
    "changed_files_list": [
      "pandas/tseries/frequencies.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1110,
    "reporter": "wesm",
    "created_at": "2012-04-23T17:54:08+00:00",
    "closed_at": "2012-04-25T20:32:03+00:00",
    "resolver": "wesm",
    "resolved_in": "288f0cc98f5088146c9a83a6c6bd679c02f17aee",
    "resolver_commit_num": 1755,
    "title": "DatetimeIndex.repeat needs to be overridden",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 15,
    "deletions": 1,
    "changed_files_list": [
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1114,
    "reporter": "wesm",
    "created_at": "2012-04-23T19:46:12+00:00",
    "closed_at": "2012-05-12T17:01:55+00:00",
    "resolver": "wesm",
    "resolved_in": "9995694ef5b324c4b7bbcb8dde78e17c41ea7a18",
    "resolver_commit_num": 1867,
    "title": "Handling of \"non-full\" PeriodIndex",
    "body": "scikits.timeseries notions of fill_missing_dates, duplicates possible, etc. Little to no testing in this regard\n",
    "labels": [],
    "comments": [
      "Maybe makes sense to add an API function here that fills a time series relative to a passed frequency (timestamps) or fills a PeriodIndex'd series\n"
    ],
    "events": [
      "subscribed",
      "assigned",
      "commented",
      "referenced"
    ],
    "changed_files": 2,
    "additions": 30,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1116,
    "reporter": "wesm",
    "created_at": "2012-04-23T19:52:57+00:00",
    "closed_at": "2012-04-26T22:58:14+00:00",
    "resolver": "wesm",
    "resolved_in": "1a357468cce4ae266de14cb6a464ec45931d79ab",
    "resolver_commit_num": 1768,
    "title": "Test/fix Period.to_timestamp issues",
    "body": "Should conform with new PeriodIndex.to_timestamp, add lots of test for annual, quarterly, other frequencies\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 56,
    "deletions": 26,
    "changed_files_list": [
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1117,
    "reporter": "wesm",
    "created_at": "2012-04-23T20:12:05+00:00",
    "closed_at": "2012-04-26T19:34:51+00:00",
    "resolver": "wesm",
    "resolved_in": "12b97975961d4eb730a31d86f3e264c69d75f959",
    "resolver_commit_num": 1760,
    "title": "Implement take on PeriodIndex",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 81,
    "deletions": 46,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_offsets.py",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1118,
    "reporter": "wesm",
    "created_at": "2012-04-23T20:19:39+00:00",
    "closed_at": "2012-04-27T17:48:37+00:00",
    "resolver": "wesm",
    "resolved_in": "da5f7105e9e61247627b13af6b41c6551f5b1ee5",
    "resolver_commit_num": 1777,
    "title": "PeriodIndex constructor should take frequency from start period if has one",
    "body": "EDIT: actually just need a better error message\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 10,
    "deletions": 2,
    "changed_files_list": [
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1119,
    "reporter": "wesm",
    "created_at": "2012-04-23T20:21:17+00:00",
    "closed_at": "2012-05-15T01:01:05+00:00",
    "resolver": "wesm",
    "resolved_in": "af8c36a7297c016a46fd5e1641f7e9c587b8e750",
    "resolver_commit_num": 1899,
    "title": "Support for different \"bases\" with multiples of deltas",
    "body": "Want a sane way to be able to do like \n\n`ts.resample('5min', base=2)`\n\nbase=0 should be the default (as it is now)\n",
    "labels": [],
    "comments": [
      "After #1165, this should be pretty straightforward to needle in. Need to decide about API, though.\n",
      "outcome: \n\n```\nIn [6]: ts\nOut[6]: \n2000-01-01 00:00:00    0.775840\n2000-01-01 00:00:01    0.227596\n2000-01-01 00:00:02   -1.457081\n2000-01-01 00:00:03    0.734133\n2000-01-01 00:00:04    0.356800\n2000-01-01 00:00:05    0.981898\n2000-01-01 00:00:06   -0.160146\n2000-01-01 00:00:07   -1.960776\n2000-01-01 00:00:08   -1.457221\n2000-01-01 00:00:09   -0.829302\n2000-01-01 00:00:10    0.413782\n2000-01-01 00:00:11    1.550657\n2000-01-01 00:00:12    0.324632\n2000-01-01 00:00:13   -0.951606\n2000-01-01 00:00:14    0.173150\n...\n2000-01-01 00:39:45   -0.989591\n2000-01-01 00:39:46    0.659910\n2000-01-01 00:39:47    0.961344\n2000-01-01 00:39:48    0.030223\n2000-01-01 00:39:49    1.165185\n2000-01-01 00:39:50   -0.011536\n2000-01-01 00:39:51    0.476151\n2000-01-01 00:39:52    0.290827\n2000-01-01 00:39:53    0.592913\n2000-01-01 00:39:54   -0.597697\n2000-01-01 00:39:55    0.622408\n2000-01-01 00:39:56   -1.543871\n2000-01-01 00:39:57    1.602200\n2000-01-01 00:39:58   -0.382568\n2000-01-01 00:39:59    0.327734\nFreq: S, Length: 2400\n\nIn [7]: ts.resample('5min')\nOut[7]: \n2000-01-01 00:00:00    0.775840\n2000-01-01 00:05:00   -0.005698\n2000-01-01 00:10:00    0.078389\n2000-01-01 00:15:00   -0.018959\n2000-01-01 00:20:00    0.013095\n2000-01-01 00:25:00   -0.015351\n2000-01-01 00:30:00    0.037673\n2000-01-01 00:35:00    0.083128\n2000-01-01 00:40:00    0.099568\nFreq: 5T\n\nIn [8]: ts.resample('5min', base=3)\nOut[8]: \n2000-01-01 00:03:00    0.070912\n2000-01-01 00:08:00   -0.018604\n2000-01-01 00:13:00    0.091425\n2000-01-01 00:18:00   -0.073576\n2000-01-01 00:23:00   -0.022931\n2000-01-01 00:28:00    0.115416\n2000-01-01 00:33:00   -0.025325\n2000-01-01 00:38:00    0.098755\n2000-01-01 00:43:00    0.166773\nFreq: 5T\n```\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 54,
    "deletions": 40,
    "changed_files_list": [
      "pandas/core/generic.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1120,
    "reporter": "wesm",
    "created_at": "2012-04-23T20:22:32+00:00",
    "closed_at": "2012-04-25T20:21:05+00:00",
    "resolver": "wesm",
    "resolved_in": "7d6fc61ce737ac1f54435b14f55693ba514eebc5",
    "resolver_commit_num": 1754,
    "title": "Parsing mixed time strings?",
    "body": "Like `2hr30min`. Can use regular expressions\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 45,
    "deletions": 6,
    "changed_files_list": [
      "pandas/tseries/frequencies.py",
      "pandas/tseries/offsets.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1127,
    "reporter": "wesm",
    "created_at": "2012-04-24T15:20:57+00:00",
    "closed_at": "2012-04-27T15:47:51+00:00",
    "resolver": "wesm",
    "resolved_in": "87bffb3631f1a0f856f087a68b5861b6aa95021f",
    "resolver_commit_num": 1775,
    "title": "Add option for label shift in resample",
    "body": "Suppose we're doing 5 minute data and want to get 9:34:59 instead of 9:35:00 for the resampled labels. want to do like:\n\n\n\nStraightforward to convert the timedelta into microseconds and adjust the datetime64 values\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 36,
    "deletions": 8,
    "changed_files_list": [
      "pandas/core/generic.py",
      "pandas/tests/test_index.py",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_resample.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1129,
    "reporter": "wesm",
    "created_at": "2012-04-24T17:57:36+00:00",
    "closed_at": "2012-04-27T15:38:42+00:00",
    "resolver": "wesm",
    "resolved_in": "3418cc89616fde783ac5729fa257ad159421a2b2",
    "resolver_commit_num": 1774,
    "title": "Series/DataFrame.asfreq not implemented for PeriodIndex",
    "body": "This is not a surprise but needs to be fixed\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 6,
    "additions": 73,
    "deletions": 68,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/series.py",
      "pandas/tseries/period.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1130,
    "reporter": "wesm",
    "created_at": "2012-04-25T03:05:34+00:00",
    "closed_at": "2012-04-27T20:14:54+00:00",
    "resolver": "wesm",
    "resolved_in": "ba17598da1e8c5578398d76fcf941c16aa75669d",
    "resolver_commit_num": 1778,
    "title": "Method for selecting values at or as of time of day for each date",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 6,
    "additions": 137,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/core/series.py",
      "pandas/src/datetime.pyx",
      "pandas/tseries/index.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1132,
    "reporter": "wesm",
    "created_at": "2012-04-25T21:45:42+00:00",
    "closed_at": "2012-04-27T22:24:25+00:00",
    "resolver": "wesm",
    "resolved_in": "839d9917d56414ff163d57913dab74398bfe9733",
    "resolver_commit_num": 1781,
    "title": "Add normalization check / normalize method?",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 81,
    "deletions": 9,
    "changed_files_list": [
      "pandas/src/datetime.pyx",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1138,
    "reporter": "wesm",
    "created_at": "2012-04-26T19:43:25+00:00",
    "closed_at": "2012-04-27T21:52:56+00:00",
    "resolver": "wesm",
    "resolved_in": "0555ab8cc65e0c4299f6c905da9b55b2726d7a3a",
    "resolver_commit_num": 1780,
    "title": "Test joins and set ops with PeriodIndex-ed data",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 125,
    "deletions": 3,
    "changed_files_list": [
      "pandas/core/format.py",
      "pandas/core/index.py",
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1142,
    "reporter": "vincentarelbundock",
    "created_at": "2012-04-27T01:49:47+00:00",
    "closed_at": "2012-05-07T15:36:15+00:00",
    "resolver": "wesm",
    "resolved_in": "6bf62743a3db5db25ccd0b677717ea9e67be33ab",
    "resolver_commit_num": 1822,
    "title": ".ix column assignment",
    "body": "Hi, \n\nI was wondering if there was a good reason why this works: \n\n\n\nbut not this: \n\n\n",
    "labels": [],
    "comments": [
      "What does dat look like? For example, I can do the following:\n\nIn [16]: dat = DataFrame(np.random.randn(5, 3))\n\nIn [17]: dat.ix[:, 2] + 2\nOut[17]: \n0    1.681506\n1    1.930261\n2    1.274177\n3    3.159334\n4    3.843346\nName: 2\n\nIn [18]: dat.ix[:, 2] = dat.ix[:, 2] + 2\n\nIn [19]: dat.ix[:, 2]\nOut[19]: \n0    1.681506\n1    1.930261\n2    1.274177\n3    3.159334\n4    3.843346\nName: 2\n",
      "``` python\nIn [10]: dat.ix[:,20] = dat.ix[:,20]\n---------------------------------------------------------------------------\nIndexingError                             Traceback (most recent call last)\n/home/myname/<ipython-input-10-18d0ebbb8418> in <module>()\n----> 1 dat.ix[:,20] = dat.ix[:,20]\n\n/usr/lib/python2.7/site-packages/pandas/core/indexing.py in __setitem__(self, key, value)\n     63             indexer = self._convert_to_indexer(key)\n     64 \n---> 65         self._setitem_with_indexer(indexer, value)\n     66 \n     67     def _convert_tuple(self, key):\n\n/usr/lib/python2.7/site-packages/pandas/core/indexing.py in _setitem_with_indexer(self, indexer, value)\n     85 \n     86             if not np.isscalar(value):\n---> 87                 raise IndexingError('setting on mixed-type frames only '\n     88                                     'allowed with scalar values')\n     89 \n\nIndexingError: setting on mixed-type frames only allowed with scalar values\n\nIn [11]: dat.ix[:,20]\nOut[11]: \nCountry Code\nAFG                530\nALB               9800\nDZA              41800\nASM                 12\nADO                NaN\nAGO                NaN\nATG                228\nARB             186160\nARG             180000\nARM                NaN\nAUS                NaN\nAUT             294000\nAZE                NaN\nBHS                 71\nBHR                NaN\n...\nAFG            NaN\nBDI            NaN\nCAF            NaN\nZAR            NaN\nCIV            NaN\nGNB            NaN\nHTI            NaN\nIRQ            NaN\nKSV            NaN\nLBR            NaN\nNPL            NaN\nSLB            NaN\nSOM            NaN\nSDN            NaN\nTMP            NaN\nName: 1976, Length: 215786\n```\n",
      "Ah you have heterogeneous data.\ndat.ix[:, 20] retrieves a series so + 2 works as expected.\nAssignment for heterogeneous DataFrames via .ix is different because internally the data is split between multiple homogeneous blocks.\nYou can always do dat[col_name] = dat[col_name] + 2.\n(\nWe can maybe think about being even more clever with assignment with .ix when it's just a single column (or maybe check that they're in the same block?). \n",
      "Ah, I see. Thanks for looking into this. Coming from R, it just feels much more natural to use the .ix indexing, since I never remember which axis it'll slice on if I only pass one list of labels. I also usually refer to columns by position, not labels, since it's convenient to loop over a range() call. I guess I can specify columns by indexing dat.columns[i], but that feels like a workaround for something that should be straightforward. \n\nUnfortunately, I just started playing around with python/pandas a few days ago, so I'm not in a position to contribute code, but thanks for considering!\n",
      "fyi, you can loop over _dat.columns_ directly, this will give you the column labels.\n\n``` python\nfor label in dat.columns:\n    print label\n    print dat[label]\n```\n\nOther option is to use `df.iteritems()`\nYou can also use `dat.icols()` and `dat.irow()` methods, if you want positional indexing.\n\ngood luck with pandas\n",
      "Of course I can! Thanks for the tip lodagro!\n",
      "I'll look into this, should be fairly easy to fix\n"
    ],
    "events": [
      "subscribed",
      "closed",
      "reopened",
      "commented",
      "subscribed",
      "commented",
      "commented",
      "subscribed",
      "closed",
      "reopened",
      "commented",
      "commented",
      "commented",
      "commented",
      "subscribed",
      "assigned"
    ],
    "changed_files": 2,
    "additions": 7,
    "deletions": 5,
    "changed_files_list": [
      "pandas/core/indexing.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1149,
    "reporter": "wesm",
    "created_at": "2012-04-28T15:09:13+00:00",
    "closed_at": "2012-05-08T02:23:19+00:00",
    "resolver": "wesm",
    "resolved_in": "2cda234dc18059502be7be24264b0ffb3d3d07bb",
    "resolver_commit_num": 1835,
    "title": "Resample on other axes, tests for Panel aggregation",
    "body": "Want to be able to resample Panel data easily also (either on axis=1 or 2)\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 93,
    "deletions": 54,
    "changed_files_list": [
      "pandas/core/groupby.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1152,
    "reporter": "wesm",
    "created_at": "2012-04-28T23:00:09+00:00",
    "closed_at": "2012-05-07T13:33:42+00:00",
    "resolver": "wesm",
    "resolved_in": "12e72716c931f573642daadbefaf07e7a76f6a34",
    "resolver_commit_num": 1815,
    "title": "Improve performance of resample on data with PeriodIndex",
    "body": "Underperforms timestamped data significantly right now\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 9,
    "deletions": 4,
    "changed_files_list": [
      "pandas/core/groupby.py",
      "pandas/tseries/period.py",
      "vb_suite/timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1154,
    "reporter": "wesm",
    "created_at": "2012-04-28T23:32:44+00:00",
    "closed_at": "2012-05-07T14:36:57+00:00",
    "resolver": "wesm",
    "resolved_in": "2c1b5c7f9eb1f4cdfe7fa608555c49a7782c74ac",
    "resolver_commit_num": 1818,
    "title": "Indexing PeriodIndex with duplicate periods",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed",
      "closed",
      "reopened"
    ],
    "changed_files": 1,
    "additions": 19,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1156,
    "reporter": "wesm",
    "created_at": "2012-04-29T00:17:09+00:00",
    "closed_at": "2012-05-05T19:26:42+00:00",
    "resolver": "wesm",
    "resolved_in": "99a7444de6a4c7f10af6b9aa4cf4cb6de843d8f3",
    "resolver_commit_num": 1808,
    "title": "Timestamps don't always get the right tzinfo attached",
    "body": "Each timezone in pytz has a `_tzinfos` dict:\n\nIn [3]: type(pytz.timezone('US/Eastern'))\nOut[3]: pytz.tzfile.US/Eastern\n\nIn [4]: pytz.timezone('US/Eastern')._tzinfos\nOut[4]: \n{(datetime.timedelta(-1, 68400),\n  datetime.timedelta(0),\n  'EST'): <DstTzInfo 'US/Eastern' EST-1 day, 19:00:00 STD>,\n (datetime.timedelta(-1, 72000),\n  datetime.timedelta(0, 3600),\n  'EDT'): <DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>,\n (datetime.timedelta(-1, 72000),\n  datetime.timedelta(0, 3600),\n  'EPT'): <DstTzInfo 'US/Eastern' EPT-1 day, 20:00:00 DST>,\n (datetime.timedelta(-1, 72000),\n  datetime.timedelta(0, 3600),\n  'EWT'): <DstTzInfo 'US/Eastern' EWT-1 day, 20:00:00 DST>}\n\nNeed some kind of sane (read: performant) way to retrieve the right one of these to attach when boxing a timestamp. Is not as trivial as I thought\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned",
      "referenced",
      "referenced"
    ],
    "changed_files": 1,
    "additions": 5,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tseries/tests/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1158,
    "reporter": "wesm",
    "created_at": "2012-04-29T14:54:24+00:00",
    "closed_at": "2012-05-05T20:42:26+00:00",
    "resolver": "wesm",
    "resolved_in": "6f61bd5b96f1cf84cb28dc81ee913c519a2533a8",
    "resolver_commit_num": 1811,
    "title": "What should DatetimeIndex + 1 do?",
    "body": "Perhaps should shift like PeriodIndex if frequency is set\n",
    "labels": [],
    "comments": [
      "how would the adding timedelta API look like then?\nmight be confusing if DatetimeIndex + int shifts and DatetimeIndex + timedelta tshifts\n",
      "Well I think `index + 1` would be the same as `index + index.freq`. \n",
      "ah ok that i agree with. the \"shift\" wording threw me. all tshifts here. \n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "commented"
    ],
    "changed_files": 2,
    "additions": 23,
    "deletions": 2,
    "changed_files_list": [
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1160,
    "reporter": "wesm",
    "created_at": "2012-04-29T15:23:12+00:00",
    "closed_at": "2012-05-15T00:48:13+00:00",
    "resolver": "wesm",
    "resolved_in": "720007450bdef64829f6b1ae69fed908a1759de5",
    "resolver_commit_num": 1898,
    "title": "Should avoid hash table creation in monotonic DatetimeIndex?",
    "body": "In very large time series, a first call to\n\n\n\nwill cause a hash table population to look up the location of `stamp`. If the index has, say, 10mm timestamps, the allocated hash table will take up at minimum 160 megabytes ((timestamp, location) pairs, each 8 bytes) probably rounded up to 256 megabytes. I would say either always use binary search or don't create the hash table when the index exceeds a certain length. \n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 62,
    "deletions": 22,
    "changed_files_list": [
      "pandas/__init__.py",
      "pandas/src/engines.pyx",
      "vb_suite/timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1165,
    "reporter": "wesm",
    "created_at": "2012-04-29T17:53:16+00:00",
    "closed_at": "2012-05-12T21:28:11+00:00",
    "resolver": "wesm",
    "resolved_in": "31ca168faba43d761f5b53326b18250804ccd6ef",
    "resolver_commit_num": 1873,
    "title": "Minutely resampling needs to be independent of start timestamp",
    "body": "Date ranges are very naively generated:\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 8,
    "additions": 88,
    "deletions": 13,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/core/internals.py",
      "pandas/tseries/index.py",
      "pandas/tseries/offsets.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py",
      "pandas/tseries/tests/test_util.py",
      "pandas/tseries/util.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1167,
    "reporter": "wesm",
    "created_at": "2012-04-29T21:12:14+00:00",
    "closed_at": "2012-05-07T16:06:56+00:00",
    "resolver": "wesm",
    "resolved_in": "05bd9d6bd070eea0e49b4a9e0f0ab7e9ecc4e98f",
    "resolver_commit_num": 1823,
    "title": "Floats stored in object Index may not slice properly when passed float64 scalars",
    "body": "Should include this fix in 0.7.x maintenance branch\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 68,
    "deletions": 9,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/core/indexing.py",
      "pandas/core/series.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1168,
    "reporter": "changhiskhan",
    "created_at": "2012-04-29T21:12:22+00:00",
    "closed_at": "2012-05-04T14:04:52+00:00",
    "resolver": "wesm",
    "resolved_in": "5fafeba7290488bffa42a0b4a414d41f1d0522a2",
    "resolver_commit_num": 1805,
    "title": "Faster implementation of Series.asof for sequence inputs",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned",
      "subscribed"
    ],
    "changed_files": 11,
    "additions": 124,
    "deletions": 137,
    "changed_files_list": [
      "pandas/core/algorithms.py",
      "pandas/core/daterange.py",
      "pandas/core/index.py",
      "pandas/core/series.py",
      "pandas/sparse/array.py",
      "pandas/src/engines.pyx",
      "pandas/src/tseries.pyx",
      "pandas/tests/test_series.py",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1169,
    "reporter": "wesm",
    "created_at": "2012-04-29T22:06:25+00:00",
    "closed_at": "2012-05-27T19:12:31+00:00",
    "resolver": "wesm",
    "resolved_in": "8436757a3a036b8d1b2f00c0e4ae971f3870f83a",
    "resolver_commit_num": 1978,
    "title": "Decide what time series stuff goes in the pandas.* namespace",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 9,
    "additions": 44,
    "deletions": 36,
    "changed_files_list": [
      "pandas/__init__.py",
      "pandas/core/api.py",
      "pandas/core/datetools.py",
      "pandas/tests/test_index.py",
      "pandas/tseries/api.py",
      "pandas/tseries/offsets.py",
      "pandas/tseries/tests/test_daterange.py",
      "pandas/tseries/tests/test_plotting.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1173,
    "reporter": "wesm",
    "created_at": "2012-04-30T18:55:22+00:00",
    "closed_at": "2012-05-07T14:33:27+00:00",
    "resolver": "wesm",
    "resolved_in": "6f65882c9f8606b1fff7b051ff3af79644cbebfc",
    "resolver_commit_num": 1817,
    "title": "Allow duplicate index values from read_* functions",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 2,
    "additions": 12,
    "deletions": 10,
    "changed_files_list": [
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1181,
    "reporter": "wesm",
    "created_at": "2012-05-02T17:39:01+00:00",
    "closed_at": "2012-05-08T18:27:05+00:00",
    "resolver": "wesm",
    "resolved_in": "773d86177092c5cd6bb2248875b2e011f06aa16d",
    "resolver_commit_num": 1837,
    "title": "Don't create (or add option to not create) empty columns when unstacking",
    "body": "from @rkern on the pydata mailing list:\n\n\n\nto my response\n\n\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "mentioned",
      "subscribed",
      "subscribed",
      "assigned"
    ],
    "changed_files": 6,
    "additions": 144,
    "deletions": 6,
    "changed_files_list": [
      "pandas/core/reshape.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tools/pivot.py",
      "pandas/tools/tests/test_pivot.py",
      "vb_suite/groupby.py",
      "vb_suite/reshape.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1183,
    "reporter": "wesm",
    "created_at": "2012-05-02T18:38:53+00:00",
    "closed_at": "2012-05-07T18:23:41+00:00",
    "resolver": "wesm",
    "resolved_in": "e8fc1c0c7089979ccbf2b749da1987444cad9171",
    "resolver_commit_num": 1830,
    "title": "Series.apply bug when result same length as series",
    "body": "\n\nBut:\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 16,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/series.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1187,
    "reporter": "changhiskhan",
    "created_at": "2012-05-03T12:10:01+00:00",
    "closed_at": "2012-05-08T19:29:13+00:00",
    "resolver": "wesm",
    "resolved_in": "4bbf5830f2280c6af7e1fb276217f2d932cf1f64",
    "resolver_commit_num": 1840,
    "title": "reindexing / \"'Index' object has no attribute 'freq'\"",
    "body": "from pystatsmodels mailing list:\n\nI've created a DataFrame using read_clipboard() (Pandas built from Git trunk) and am having problems reindexing unless I select rows using the [:] notation.  IIRC the index was defined using 'set_index(\"Timestamp\")'.  \n\nIn [47]:\n\ndf\nOut[47]:\n<class 'pandas.core.frame.DataFrame'>\nIndex: 61507 entries, 2010-01-31 15:30:00 to 2011-11-03 09:15:00\nData columns:\nQ      61507  non-null values\nNTU    61507  non-null values\ndtypes: float64(2)\n\nIn [50]:\n\ndf[:50000000]['Q'] < 14\nOut[50]:\ntime\n2010-01-31 15:30:00     True\n2010-01-31 15:45:00     True\n2010-01-31 16:00:00     True\n2010-01-31 16:15:00    False\n2010-01-31 16:30:00    False\n<<snip; all fine here>>\n\nIn [51]:\n## df['Q'] < 14\n\nAttributeError                            Traceback (most recent call last)\n/home/will/Dropbox/PhD/AI_general/Semblance/<ipython-input-51-2f53055c9488> in <module>()\n----> 1 q_and_turb_df['Q'] < 14\n\n/usr/lib/python2.7/dist-packages/IPython/core/displayhook.pyc in **call**(self, result)\n    236             self.start_displayhook()\n    237             self.write_output_prompt()\n--> 238             format_dict = self.compute_format_data(result)\n    239             self.write_format_data(format_dict)\n    240             self.update_user_ns(result)\n\n/usr/lib/python2.7/dist-packages/IPython/core/displayhook.pyc in compute_format_data(self, result)\n    148             MIME type representation of the object.\n    149         \"\"\"\n--> 150         return self.shell.display_formatter.format(result)\n    151 \n    152     def write_format_data(self, format_dict):\n\n/usr/lib/python2.7/dist-packages/IPython/core/formatters.pyc in format(self, obj, include, exclude)\n    124                     continue\n    125             try:\n--> 126                 data = formatter(obj)\n    127             except:\n    128                 # FIXME: log the exception\n\n/usr/lib/python2.7/dist-packages/IPython/core/formatters.pyc in **call**(self, obj)\n    445                 type_pprinters=self.type_printers,\n    446                 deferred_pprinters=self.deferred_printers)\n--> 447             printer.pretty(obj)\n    448             printer.flush()\n    449             return stream.getvalue()\n\n/usr/lib/python2.7/dist-packages/IPython/lib/pretty.pyc in pretty(self, obj)\n    352                 if callable(obj_class._repr_pretty_):\n    353                     return obj_class._repr_pretty_(obj, self, cycle)\n--> 354             return _default_pprint(obj, self, cycle)\n    355         finally:\n    356             self.end_group()\n\n/usr/lib/python2.7/dist-packages/IPython/lib/pretty.pyc in _default_pprint(obj, p, cycle)\n    472     if getattr(klass, '__repr__', None) not in _baseclass_reprs:\n    473         # A user-provided repr.\n\n--> 474         p.text(repr(obj))\n    475         return\n    476     p.begin_group(1, '<')\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.8.0.dev_1ea766c-py2.7-linux-x86_64.egg/pandas/core/series.pyc in **repr**(self)\n    710                     else fmt.print_config.max_rows)\n    711         if len(self.index) > max_rows:\n--> 712             result = self._tidy_repr(min(30, max_rows - 4))\n    713         elif len(self.index) > 0:\n    714             result = self._get_repr(print_header=True,\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.8.0.dev_1ea766c-py2.7-linux-x86_64.egg/pandas/core/series.pyc in _tidy_repr(self, max_vals)\n    728                                                   name=False)\n    729         result = head + '\\n...\\n' + tail\n--> 730         return '%s\\n%s' % (result, self._repr_footer())\n    731 \n    732     def _repr_footer(self):\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.8.0.dev_1ea766c-py2.7-linux-x86_64.egg/pandas/core/series.pyc in _repr_footer(self)\n   2551 \n   2552     def _repr_footer(self):\n-> 2553         if self.index.freq is not None:\n   2554             freqstr = 'Freq: %s, ' % self.index.freqstr\n   2555         else:\n\nAttributeError: 'Index' object has no attribute 'freq'\n",
    "labels": [],
    "comments": [
      "see commit: 4c4c7b38bb\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "cross-referenced",
      "commented"
    ],
    "changed_files": 2,
    "additions": 15,
    "deletions": 1,
    "changed_files_list": [
      "pandas/tests/test_frame.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1193,
    "reporter": "wesm",
    "created_at": "2012-05-03T19:29:50+00:00",
    "closed_at": "2012-05-03T19:52:28+00:00",
    "resolver": "wesm",
    "resolved_in": "b69fbabbda6dbedbc8f7a8c7ac54135dd710914d",
    "resolver_commit_num": 1803,
    "title": "logy argument not respected in pandas.tseries.plotting",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 43,
    "deletions": 25,
    "changed_files_list": [
      "pandas/src/sandbox.pyx",
      "pandas/tools/plotting.py",
      "pandas/tseries/plotting.py",
      "pandas/tseries/tests/test_plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1199,
    "reporter": "wesm",
    "created_at": "2012-05-07T02:37:39+00:00",
    "closed_at": "2012-05-12T16:03:01+00:00",
    "resolver": "wesm",
    "resolved_in": "56051cf6a0ce2d0cc468eccf22dc2316819aeb92",
    "resolver_commit_num": 1863,
    "title": "Remove period multipliers",
    "body": "After some thought I don't think I want period multipliers. They're rigid and inflexible as they are, probably likely to lead to more trouble than they're worth right now\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 63,
    "deletions": 64,
    "changed_files_list": [
      "pandas/src/datetime.pyx",
      "pandas/tseries/frequencies.py",
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1201,
    "reporter": "wesm",
    "created_at": "2012-05-07T14:21:18+00:00",
    "closed_at": "2012-07-12T21:36:26+00:00",
    "resolver": "wesm",
    "resolved_in": "4406d37b71cc3303b40847010299534c03723651",
    "resolver_commit_num": 2208,
    "title": ".ix indexing can't handle duplicate index values",
    "body": "\n",
    "labels": [],
    "comments": [
      "I am still encountering this issue under 0.8.0, but now when trying to index with a list: (using the running example)\n\n```\nresult.ix[['qux']]\nException: Reindexing only valid with uniquely valued Index objects\n```\n"
    ],
    "events": [
      "subscribed",
      "closed",
      "commented",
      "subscribed",
      "reopened"
    ],
    "changed_files": 4,
    "additions": 30,
    "deletions": 5,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/indexing.py",
      "pandas/io/tests/test_yahoo.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1204,
    "reporter": "changhiskhan",
    "created_at": "2012-05-07T16:53:04+00:00",
    "closed_at": "2012-11-28T01:11:25+00:00",
    "resolver": "wesm",
    "resolved_in": "238f52276a3ee22acf7bd24e19f7c92fad18dc3d",
    "resolver_commit_num": 2632,
    "title": "Improve parser performance when handling special characters",
    "body": "thousands separator and comments (#962) processing is slow right now.\n\nat least comments should be preprocessed\n",
    "labels": [],
    "comments": [
      "Thousands handling is handled at a low level in the new parser. Have to think about the comments argument and how to do it without slowing down the tokenizer\n",
      "Thousands handling is way faster, but the C parser doesn't do comments yet. Should be simple (ish) to add to the tokenizer.\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "commented"
    ],
    "changed_files": 5,
    "additions": 58,
    "deletions": 11,
    "changed_files_list": [
      "pandas/io/parsers.py",
      "pandas/src/parser.pyx",
      "pandas/src/parser/tokenizer.c",
      "pandas/src/parser/tokenizer.h",
      "vb_suite/parser.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1206,
    "reporter": "GlenHertz",
    "created_at": "2012-05-07T19:24:40+00:00",
    "closed_at": "2012-07-21T16:51:10+00:00",
    "resolver": "wesm",
    "resolved_in": "8526264255791e4eb22f5dc03e60d1e4d019f4ae",
    "resolver_commit_num": 1990,
    "title": "Series.interpolate with index of type float gives wrong result",
    "body": "Hi,\n\nThe logic for Series.interpolate assumes the indexes are equally spaced.  With a floating point index this is not the desired interpolation.  For example:\n\n\n\nI expect sig1 and sig2 to have more points than df1 and df2 but with the values interpolated. There are a few points that are not overlapping because it is assumed they are equally spaced.  In my opinion if the index is a floating point the user wants to interpolate by the index's value and don't assume they are equally spaced.  It should do something like this:\n\n\n\nThanks \n",
    "labels": [],
    "comments": [
      "This is implemented in git master now and will be part of the 0.8.0 release\n",
      "This seems to still be a problem as of 0.8.1.dev-e2633d4.\n\n``` python\nimport pandas\nimport numpy as np\nimport pylab as pl\nfrom scipy.interpolate import interp1d\n\ntime_fast = np.arange(50000.,50010.,.4) +.1\ntime_slow = np.arange(50000.,50010.,1.)\n\nx_fast = np.sin(time_fast)\nx_slow = np.sin(time_slow)\n\ndf_fast = pandas.DataFrame(x_fast, index=time_fast, columns=['fast'])\ndf_slow = pandas.DataFrame(x_slow, index=time_slow, columns=['slow'])\n\ndf_joined = df_fast.join(df_slow, how='outer')\n\ndf_joined['pandas interpolate'] = df_joined['slow'].interpolate()\n\nf = interp1d(df_slow.index, df_slow['slow'], bounds_error=False)\ndf_joined['scipy interp1d'] = f(df_joined.index)\n\ndf_joined['pandas interpolate'].plot(style='o')\ndf_joined['scipy interp1d'].plot(style='o')\ndf_slow['slow'].plot(style='r.:')\n\npl.title('Linearly interpolated points are expected to lie on the dotted red lines.')\n\npl.legend()\npl.show()\n```\n",
      "I also observed this with indices that are Datetime objects.  The title of this issue may be too narrow.\n",
      "@nlsn you have to do:\n\n`df_joined['slow'].interpolate(method='values')`\n\nThe default of interpolate assumes that each value is evenly spaced, while `method='values'` uses the index values\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "cross-referenced",
      "unsubscribed",
      "closed",
      "subscribed",
      "commented",
      "referenced",
      "subscribed",
      "commented",
      "subscribed",
      "reopened",
      "commented"
    ],
    "changed_files": 5,
    "additions": 24,
    "deletions": 5,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/series.py",
      "pandas/tests/test_series.py",
      "pandas/tseries/tests/test_offsets.py",
      "pandas/tseries/tests/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1208,
    "reporter": "changhiskhan",
    "created_at": "2012-05-07T19:58:28+00:00",
    "closed_at": "2012-05-19T19:24:19+00:00",
    "resolver": "wesm",
    "resolved_in": "9df5d7513503675670d36ffcf86ac8d13473f92d",
    "resolver_commit_num": 1917,
    "title": "Eliminate doc + strings requiring index values be unique",
    "body": "These are in lots of places. Need a round of docs clean up after full support for non-unique has been built in\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 5,
    "deletions": 5,
    "changed_files_list": [
      "pandas/core/series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1211,
    "reporter": "changhiskhan",
    "created_at": "2012-05-08T00:32:18+00:00",
    "closed_at": "2012-05-08T14:31:25+00:00",
    "resolver": "wesm",
    "resolved_in": "094e5e4fc4dcdcdec01a7df7f311849eb69faa9d",
    "resolver_commit_num": 1836,
    "title": "Can't use PeriodIndex as DataFrame columns",
    "body": "This raises an Exception:\n\nDataFrame(randn(10, 5), columns=period_range('1/1/2000', periods=5))\n\n/pandas/pandas/core/frame.pyc in _apply_standard(self, func, axis, ignore_failures)\n   3209                 try:\n   3210                     if hasattr(e, 'args'):\n-> 3211                         e.args = e.args + ('occurred at index %s' % str(k),)\n   3212                 except NameError:\n   3213                     # no k defined yet\n\nUnboundLocalError: local variable 'k' referenced before assignment\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 63,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/index.py",
      "pandas/core/internals.py",
      "pandas/core/series.py",
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1214,
    "reporter": "wesm",
    "created_at": "2012-05-08T13:58:52+00:00",
    "closed_at": "2012-05-12T22:00:07+00:00",
    "resolver": "wesm",
    "resolved_in": "b5bc1d1afe1713bd7a565c73331f13cda33ed7dc",
    "resolver_commit_num": 1875,
    "title": "lib.reduce can't handle non-object index types",
    "body": "Not sure how much work this would be but would be a big speed boost\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 36,
    "deletions": 8,
    "changed_files_list": [
      "pandas/src/reduce.pyx",
      "pandas/tests/test_tseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1215,
    "reporter": "wesm",
    "created_at": "2012-05-08T14:21:15+00:00",
    "closed_at": "2012-07-12T20:34:16+00:00",
    "resolver": "wesm",
    "resolved_in": "da5e2c1457d0114ceed8389aae9912378b49ee6b",
    "resolver_commit_num": 2206,
    "title": "Handling of array of Period objects -> PeriodIndex",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 69,
    "deletions": 28,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/index.py",
      "pandas/src/inference.pyx",
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1217,
    "reporter": "wesm",
    "created_at": "2012-05-08T15:13:49+00:00",
    "closed_at": "2012-05-08T23:33:05+00:00",
    "resolver": "wesm",
    "resolved_in": "1f481e65b167ef331db105bb564e9dc1e513a713",
    "resolver_commit_num": 1843,
    "title": "MultiIndex formatting failure when index name coincides with first value",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed",
      "unsubscribed",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 20,
    "deletions": 5,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/index.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1221,
    "reporter": "wesm",
    "created_at": "2012-05-09T01:53:46+00:00",
    "closed_at": "2012-05-12T16:23:03+00:00",
    "resolver": "wesm",
    "resolved_in": "3bc1f178c938fac784f0c7773be0a0dc20e70dd7",
    "resolver_commit_num": 1864,
    "title": "Optimize _ensure_* functions in Cython using NumPy C API",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 88,
    "deletions": 30,
    "changed_files_list": [
      "pandas/core/common.py",
      "pandas/src/generate_code.py",
      "pandas/src/generated.pyx",
      "pandas/src/tseries.pyx",
      "pandas/src/util.pxd"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1228,
    "reporter": "wesm",
    "created_at": "2012-05-12T16:33:05+00:00",
    "closed_at": "2012-05-12T19:41:52+00:00",
    "resolver": "wesm",
    "resolved_in": "441d6e87c20ce3b942cf3fbc3fd248d4acc9bd3d",
    "resolver_commit_num": 1871,
    "title": "Broken quarter -> Period parsing",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 162,
    "deletions": 147,
    "changed_files_list": [
      "pandas/tseries/frequencies.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tools.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1229,
    "reporter": "wesm",
    "created_at": "2012-05-12T18:15:41+00:00",
    "closed_at": "2012-05-12T19:46:37+00:00",
    "resolver": "wesm",
    "resolved_in": "73661c1fdef4c2b4d3ca30555e6035b3f2f06fd5",
    "resolver_commit_num": 1872,
    "title": "Series.repeat does not do the right thing",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 21,
    "deletions": 0,
    "changed_files_list": [
      "pandas/core/series.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1231,
    "reporter": "wesm",
    "created_at": "2012-05-12T20:29:16+00:00",
    "closed_at": "2012-05-12T21:40:51+00:00",
    "resolver": "wesm",
    "resolved_in": "413e5d854288405063da522a60ad83d53c5ae3ad",
    "resolver_commit_num": 1874,
    "title": "Resampling of minutely / secondly periods -> 5 minute, etc.",
    "body": "May not work properly, need tests\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 16,
    "deletions": 2,
    "changed_files_list": [
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1232,
    "reporter": "wesm",
    "created_at": "2012-05-13T04:36:58+00:00",
    "closed_at": "2012-05-14T03:55:26+00:00",
    "resolver": "wesm",
    "resolved_in": "2393ba98fb6ffaa5860969b3e2a8f6bd7fb54880",
    "resolver_commit_num": 1882,
    "title": "Storing time zones HDFStore",
    "body": "Need to be careful to not break backwards compat (don't REQUIRE that time zone be stored in the hdf5 file)\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 38,
    "deletions": 7,
    "changed_files_list": [
      "pandas/io/pytables.py",
      "pandas/io/tests/test_pytables.py",
      "pandas/tseries/index.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1239,
    "reporter": "wesm",
    "created_at": "2012-05-15T15:38:07+00:00",
    "closed_at": "2012-05-15T19:58:54+00:00",
    "resolver": "wesm",
    "resolved_in": "548cca31b045d0d1ce0f488846ae6ebd2607ccd6",
    "resolver_commit_num": 1904,
    "title": "Change default merge suffixes to use underscores instead of periods",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 2,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1240,
    "reporter": "iandanforth",
    "created_at": "2012-05-15T18:39:32+00:00",
    "closed_at": "2012-05-15T20:16:16+00:00",
    "resolver": "wesm",
    "resolved_in": "fc1c5130cdf6e8022dd9d49909e51b56befd9a54",
    "resolver_commit_num": 1906,
    "title": "Request: keys() method on dataFrame",
    "body": "Return a list of column names. \n",
    "labels": [],
    "comments": [
      "The potential for confusion here is that Series has a keys() method and it returns the index.\n\nWhat's the use case? Duck typing with dict?\n",
      "I think it's pretty harmless; DataFrame already has `.iteritems()`, but of course Series.values is a property, not a function call, so duck-typing isn't 100% possible. But good enough, I suppose.\n\nNote that list(df) is the same as what would be `df.keys()`\n",
      "While learning Pandas this kind of method is useful to move from the well-understood dict structure to the more powerful DataFrame. As a pandas novice this kind of mental mapping would be much appreciated.\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "subscribed",
      "subscribed",
      "commented"
    ],
    "changed_files": 2,
    "additions": 6,
    "deletions": 0,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1249,
    "reporter": "changhiskhan",
    "created_at": "2012-05-17T22:36:39+00:00",
    "closed_at": "2012-05-19T18:50:56+00:00",
    "resolver": "wesm",
    "resolved_in": "6ad265584a4ad0c5b7e2fb8bb55e414eca63ca11",
    "resolver_commit_num": 1915,
    "title": "resample.values_at_time error",
    "body": "In [66]: ts = tm.makeTimeSeries()\n## In [67]: ts.at_time(ts.index[2])\n\nIndexError                                Traceback (most recent call last)\n/home/chang/Dropbox/git/pandas/<ipython-input-67-324c3adda78b> in <module>()\n----> 1 ts.at_time(ts.index[2])\n\n/home/chang/Dropbox/git/pandas/pandas/core/series.py in at_time(self, time, tz, asof)\n   2676         \"\"\"\n   2677         from pandas.tseries.resample import values_at_time\n-> 2678         return values_at_time(self, time, tz=tz, asof=asof)\n   2679 \n   2680     def tz_convert(self, tz, copy=True):\n\n/home/chang/Dropbox/git/pandas/pandas/tseries/resample.py in values_at_time(obj, time, tz, asof)\n    363 \n    364     mus = _time_to_microsecond(time)\n--> 365     indexer = lib.values_at_time(obj.index.asi8, mus)\n    366     indexer = com._ensure_platform_int(indexer)\n    367     return obj.take(indexer)\n\n/home/chang/Dropbox/git/pandas/pandas/_tseries.so in pandas._tseries.values_at_time (pandas/src/tseries.c:38356)()\n\nIndexError: Out of bounds on buffer access (axis 0)\n\n> /home/chang/Dropbox/git/pandas/datetime.pyx(1155)pandas._tseries.values_at_time (pandas/src/tseries.c:38356)()\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 8,
    "deletions": 1,
    "changed_files_list": [
      "pandas/src/datetime.pyx",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1254,
    "reporter": "tkf",
    "created_at": "2012-05-18T17:31:16+00:00",
    "closed_at": "2012-07-12T21:43:53+00:00",
    "resolver": "wesm",
    "resolved_in": "5b277bd1c3ba81ab93c7d31f61f0f37a187f67a2",
    "resolver_commit_num": 2209,
    "title": "HDFStore.get(non_existing_key) raises NoSuchNodeError",
    "body": "Here is clipped traceback.  I guess you need to catch NoSuchNodeError also.  Or maybe it is better to ask pytables to make NoSuchNodeError a subclass of AttributeError?  I am using PyTables 2.3.1 and 0.7.3.\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 4,
    "deletions": 3,
    "changed_files_list": [
      "pandas/io/pytables.py",
      "pandas/io/tests/test_pytables.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1257,
    "reporter": "wesm",
    "created_at": "2012-05-18T22:00:20+00:00",
    "closed_at": "2012-05-19T20:12:40+00:00",
    "resolver": "wesm",
    "resolved_in": "345b7f6fb8f087835f72c79f45bd73b21b3b2910",
    "resolver_commit_num": 1919,
    "title": "GroupBy[Index] raises TypeError",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 4,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1259,
    "reporter": "wesm",
    "created_at": "2012-05-19T16:08:09+00:00",
    "closed_at": "2012-05-20T02:57:41+00:00",
    "resolver": "wesm",
    "resolved_in": "a67920d42e7d0672ebd7842922152ad0268d9a88",
    "resolver_commit_num": 1921,
    "title": "Period resampling buglet",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 40,
    "deletions": 40,
    "changed_files_list": [
      "pandas/src/groupby.pyx",
      "pandas/src/period.c",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1260,
    "reporter": "wesm",
    "created_at": "2012-05-20T16:27:48+00:00",
    "closed_at": "2012-05-25T22:33:33+00:00",
    "resolver": "wesm",
    "resolved_in": "9f71e0ff635af08c56d1264cc38dd9c3c185cd95",
    "resolver_commit_num": 1966,
    "title": "Test equality of timestamps with different time zones",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 20,
    "deletions": 19,
    "changed_files_list": [
      "TODO.rst",
      "pandas/src/datetime.pyx",
      "pandas/tseries/tests/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1261,
    "reporter": "wesm",
    "created_at": "2012-05-20T17:21:28+00:00",
    "closed_at": "2012-05-21T20:23:49+00:00",
    "resolver": "wesm",
    "resolved_in": "f826c0e44302fd69a007dfaac5786111ba2794a6",
    "resolver_commit_num": 1931,
    "title": "Add nanosecond field accessor functions",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 11,
    "deletions": 1,
    "changed_files_list": [
      "pandas/src/datetime.pyx",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1262,
    "reporter": "wesm",
    "created_at": "2012-05-20T17:41:17+00:00",
    "closed_at": "2012-05-20T18:37:35+00:00",
    "resolver": "wesm",
    "resolved_in": "819c44edd326e162d0021af6bc33982ceaa3f8d3",
    "resolver_commit_num": 1925,
    "title": "pandas should not import scipy automatically",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 4,
    "deletions": 3,
    "changed_files_list": [
      "pandas/tools/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1267,
    "reporter": "wesm",
    "created_at": "2012-05-21T17:36:25+00:00",
    "closed_at": "2012-05-21T18:47:29+00:00",
    "resolver": "wesm",
    "resolved_in": "fd90053a1c2699d7e17bf26c690207937250ca1d",
    "resolver_commit_num": 1930,
    "title": "Better error message if nothing passed to reindex(...)",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 6,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/series.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1268,
    "reporter": "wesm",
    "created_at": "2012-05-21T17:52:47+00:00",
    "closed_at": "2012-05-21T18:41:33+00:00",
    "resolver": "wesm",
    "resolved_in": "16b18c3d10f416684de2827cc6c44ac4fb4b42e6",
    "resolver_commit_num": 1929,
    "title": "Multiple aggregation inflexbility",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 27,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1271,
    "reporter": "wesm",
    "created_at": "2012-05-21T17:59:00+00:00",
    "closed_at": "2012-05-25T20:54:19+00:00",
    "resolver": "wesm",
    "resolved_in": "070943a9e9507229a462f0ac1e56427bae172d0f",
    "resolver_commit_num": 1962,
    "title": "Add percent change function to Series / DataFrame?",
    "body": "e.g. `s / s.shift(1) - 1`, potentially also shifting values over NAs\n",
    "labels": [],
    "comments": [
      "Sweet. I'm going to add \"shifting over NAs\" option before closing\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "referenced",
      "cross-referenced",
      "commented"
    ],
    "changed_files": 3,
    "additions": 27,
    "deletions": 8,
    "changed_files_list": [
      "pandas/core/generic.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1272,
    "reporter": "wesm",
    "created_at": "2012-05-21T18:03:08+00:00",
    "closed_at": "2012-06-03T20:39:23+00:00",
    "resolver": "wesm",
    "resolved_in": "47520f4c3f0693faa37fe9a0eeacc16ff6287297",
    "resolver_commit_num": 2026,
    "title": "0.7.3 -> 0.8.0 brief pandas migration guide",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 85,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/whatsnew/v0.8.0.txt"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1273,
    "reporter": "wesm",
    "created_at": "2012-05-21T18:03:25+00:00",
    "closed_at": "2012-06-04T19:31:34+00:00",
    "resolver": "wesm",
    "resolved_in": "b9aa410ea6c15b1ea6d1cc1db3a29e588f877f27",
    "resolver_commit_num": 2034,
    "title": "Brief scikits.timeseries -> pandas 0.8.0 migration guide",
    "body": "",
    "labels": [],
    "comments": [
      "I think this could more go into a FAQ user contrib doc sections.\n\nAgain, I migt be able to help once I know how pandas timeseries stuff works....\n",
      "if you give some 2nd level headlines, I could see what part of the text I fill in this week.\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "commented",
      "referenced",
      "commented"
    ],
    "changed_files": 3,
    "additions": 123,
    "deletions": 9,
    "changed_files_list": [
      "doc/source/faq.rst",
      "pandas/src/plib.pyx",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1274,
    "reporter": "wesm",
    "created_at": "2012-05-21T18:04:35+00:00",
    "closed_at": "2012-05-25T22:08:26+00:00",
    "resolver": "wesm",
    "resolved_in": "f48513a514cd88515e9e1f7179f0ae32fd829086",
    "resolver_commit_num": 1965,
    "title": "Test unpickling length 0 0.7.3 DateRange",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 19,
    "deletions": 2,
    "changed_files_list": [
      "pandas/core/index.py",
      "pandas/tseries/tests/data/series_daterange0.pickle",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1282,
    "reporter": "lbeltrame",
    "created_at": "2012-05-22T07:37:10+00:00",
    "closed_at": "2012-05-23T21:40:22+00:00",
    "resolver": "lbeltrame",
    "resolved_in": "bc407295170295af057791d7e946ccf6e18902df",
    "resolver_commit_num": 9,
    "title": "Conversion to R matrix wreaks havoc in case of mixed type DataFrames",
    "body": "When converting a mixed_type DataFrame to a R matrix (not a data.frame), everything is casted to string vectors, because a matrix can't be heterogeneously typed, to my knowledge.\n\nI have two possible solutions I can send as pull request, and I would like to ask which is better:\n1. raise ValueError for mixed type dataframes\n2. raise a warning\n\nGiven that most matrices needed in R are the numeric ones, I would go for option 1.\n",
    "labels": [],
    "comments": [
      "Fixing this with a warning is opening a can of worms. Setting with the exception for now. \n",
      "That seems like the right move. As long as heterogeneous data.frames are not an issue\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "commented",
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 55,
    "deletions": 5,
    "changed_files_list": [
      "pandas/rpy/common.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1291,
    "reporter": "wesm",
    "created_at": "2012-05-22T23:36:04+00:00",
    "closed_at": "2012-05-25T16:35:22+00:00",
    "resolver": "wesm",
    "resolved_in": "bca484a14166dba01758cf054e67476d09ad5698",
    "resolver_commit_num": 1956,
    "title": "Get rid of key_0, ... stuff in groupby",
    "body": "These don't make a ton of sense in the end, pretty arbitrary in my opinion\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 10,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1297,
    "reporter": "wesm",
    "created_at": "2012-05-23T16:48:59+00:00",
    "closed_at": "2012-05-29T00:31:16+00:00",
    "resolver": "wesm",
    "resolved_in": "3f9bff603ea00a7362c068bff487d8e8e09bc9f0",
    "resolver_commit_num": 1988,
    "title": "scatter_matrix doesn't do any NA handling",
    "body": "cc @orbitfold\n",
    "labels": [],
    "comments": [
      "I'll most likely fix this today (should not be difficult)\n"
    ],
    "events": [
      "subscribed",
      "mentioned",
      "subscribed",
      "commented"
    ],
    "changed_files": 1,
    "additions": 12,
    "deletions": 5,
    "changed_files_list": [
      "pandas/tools/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1298,
    "reporter": "adamklein",
    "created_at": "2012-05-23T19:40:22+00:00",
    "closed_at": "2012-06-11T17:42:33+00:00",
    "resolver": "wesm",
    "resolved_in": "659b9a64fed8b5448c4ee862fd95e3ae32ff4324",
    "resolver_commit_num": 2071,
    "title": "unfriendly error regarding duplicate column names",
    "body": "## In [6]: DataFrame.from_records( [(1,2,3)], columns=['a','b','a'])\n\nIndexError                                Traceback (most recent call last)\n/home/adam/bin/jtds/<ipython-input-6-879cfff3d748> in <module>()\n----> 1 DataFrame.from_records( [(1,2,3)], columns=['a','b','a'])\n\n/home/adam/code/pandas/pandas/core/frame.pyc in from_records(cls, data, index, exclude, columns, names, coerce_float)\n    760             result_index = np.arange(len(data))\n    761 \n--> 762         return cls(sdict, index=result_index, columns=columns)\n    763 \n    764     def to_records(self, index=True):\n\n/home/adam/code/pandas/pandas/core/frame.pyc in **init**(self, data, index, columns, dtype, copy)\n    301             mgr = self._init_mgr(data, index, columns, dtype=dtype, copy=copy)\n    302         elif isinstance(data, dict):\n--> 303             mgr = self._init_dict(data, index, columns, dtype=dtype)\n    304         elif isinstance(data, ma.MaskedArray):\n    305             mask = ma.getmaskarray(data)\n\n/home/adam/code/pandas/pandas/core/frame.pyc in _init_dict(self, data, index, columns, dtype)\n    391 \n    392         # segregates dtypes and forms blocks matching to columns\n\n--> 393         blocks = form_blocks(homogenized, axes)\n    394 \n    395         # consolidate for now\n\n/home/adam/code/pandas/pandas/core/internals.pyc in form_blocks(data, axes)\n   1125 \n   1126     if len(int_dict):\n-> 1127         int_block = _simple_blockify(int_dict, items, np.int64)\n   1128         blocks.append(int_block)\n   1129 \n\n/home/adam/code/pandas/pandas/core/internals.pyc in _simple_blockify(dct, ref_items, dtype)\n   1154 \n   1155 def _simple_blockify(dct, ref_items, dtype):\n-> 1156     block_items, values = _stack_dict(dct, ref_items, dtype)\n   1157     # CHECK DTYPE?\n\n   1158     if values.dtype != dtype: # pragma: no cover\n\n/home/adam/code/pandas/pandas/core/internals.pyc in _stack_dict(dct, ref_items, dtype)\n   1187     stacked = np.empty(shape, dtype=dtype)\n   1188     for i, item in enumerate(items):\n-> 1189         stacked[i] = _asarray_compat(dct[item])\n   1190 \n   1191     # stacked = np.vstack([_asarray_compat(dct[k]) for k in items])\n\nIndexError: index out of bounds\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 17,
    "deletions": 10,
    "changed_files_list": [
      "pandas/core/algorithms.py",
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1303,
    "reporter": "wesm",
    "created_at": "2012-05-23T23:58:23+00:00",
    "closed_at": "2012-05-25T21:53:26+00:00",
    "resolver": "wesm",
    "resolved_in": "737c5cef692bdb4dd13767e94e18aa33fa2a017d",
    "resolver_commit_num": 1964,
    "title": "DatetimeIndex should accept list of integers",
    "body": "brought up by #1263\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 27,
    "deletions": 15,
    "changed_files_list": [
      "pandas/io/pytables.py",
      "pandas/src/datetime.pyx",
      "pandas/src/tseries.pyx",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tools.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1304,
    "reporter": "wesm",
    "created_at": "2012-05-24T00:21:23+00:00",
    "closed_at": "2012-05-25T23:24:09+00:00",
    "resolver": "wesm",
    "resolved_in": "95e1b844cac1b2524a9a8667209f2755ca7cca22",
    "resolver_commit_num": 1967,
    "title": "Second vs. nanosecond resolution in encoding indexes vs datetime.datetime objects in JSON encoding",
    "body": "related to #1263\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 10,
    "additions": 64,
    "deletions": 49,
    "changed_files_list": [
      "pandas/src/datetime.pxd",
      "pandas/src/datetime/np_datetime.c",
      "pandas/src/datetime/np_datetime.h",
      "pandas/src/datetime/np_datetime_strings.c",
      "pandas/src/datetime/np_datetime_strings.h",
      "pandas/src/numpy_helper.h",
      "pandas/src/stdint.h",
      "pandas/src/ujson/python/objToJSON.c",
      "pandas/tests/test_ujson.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1306,
    "reporter": "leonbaum",
    "created_at": "2012-05-24T02:52:45+00:00",
    "closed_at": "2012-05-25T02:45:12+00:00",
    "resolver": "wesm",
    "resolved_in": "fc474303cd3c8491e254639c7a0e8dcf39a71c2b",
    "resolver_commit_num": 1950,
    "title": "Incorrect join of DataFrames with non-unique datetime indices",
    "body": "I'm not sure whether joining of DFs with non-unique indices is now supported, but it's not giving an error and this simple example don't make sense:\n\n\n\nShouldn't the 1st row of df1 join to both rows of df2?\n",
    "labels": [],
    "comments": [
      "I just noticed the timestamp is also screwed up, but I'm guessing that's a separate issue.\n\nI'm using the latest master branch, btw.\n",
      "It looks to me like an edge case, I'll look into it. I'll fix the timestamp issue too; unfortunately the NumPy datetime API is a disaster in NumPy 1.6.1 and I'm doing my best to work around it. Affairs will be much improved in NumPy 1.7 and later\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "commented",
      "subscribed",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 113,
    "deletions": 62,
    "changed_files_list": [
      "pandas/core/index.py",
      "pandas/core/internals.py",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1310,
    "reporter": "wesm",
    "created_at": "2012-05-24T14:37:57+00:00",
    "closed_at": "2012-05-28T16:11:13+00:00",
    "resolver": "wesm",
    "resolved_in": "88f86267bdc3601c2f0ff5e3387d3ac292bf79c2",
    "resolver_commit_num": 1980,
    "title": "NaT field access?",
    "body": "",
    "labels": [],
    "comments": [
      "return NaN for NaT fields? would have to upcast field vectors from int to float though, which feels messy \n",
      "I'm think I'm going to have them return -1. I don't think it's the end of the world\n",
      "Yeah i think fine for now. Maybe want to revisit later and decide on how much you want to keep consistency with numpy datetime64 which interprets neg numbers as amount of time prior to base.\n\nIn [1]: np.datetime64(-1)\nOut[1]: 1969-12-31 23:59:59.999999\n",
      "Well, these are fields and not timestamps, so I think it's OK. \n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 2,
    "additions": 62,
    "deletions": 2,
    "changed_files_list": [
      "pandas/src/datetime.pyx",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1313,
    "reporter": "wesm",
    "created_at": "2012-05-24T17:35:56+00:00",
    "closed_at": "2012-05-25T16:31:55+00:00",
    "resolver": "wesm",
    "resolved_in": "a8ece23a2c77253096d0ebb8cf8749e7e774f401",
    "resolver_commit_num": 1955,
    "title": "groupby on level=0 with normal Index discards name",
    "body": "example:\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 9,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1314,
    "reporter": "wesm",
    "created_at": "2012-05-24T22:12:09+00:00",
    "closed_at": "2012-05-25T20:42:13+00:00",
    "resolver": "wesm",
    "resolved_in": "805ab79c9a1e45ddce0994a441403b23c2c11fb3",
    "resolver_commit_num": 1952,
    "title": "Handle other unit types in datetime64 scalar objects",
    "body": "\n",
    "labels": [],
    "comments": [
      "cc @leonbaum this is all sorted out in git master\n",
      "Thanks Wes!\n",
      "@wesm \nI just tested it some more, and while it is now working for dates, it still has problems with datetimes:\n\n```\nIn [21]: pandas.DataFrame([1], index=[np.datetime64('2012-05-25')])\nOut[21]: \n            0\n2012-05-25  1\n\nIn [22]: pandas.DataFrame([1], index=[np.datetime64('2012-05-25 12:40')])\nOut[22]: \n                            0\n2175-09-08 17:41:45.161793  1\n```\n",
      "I'm not seeing that. You probably need to rebuild your C extensions:\n\n```\nIn [2]: pandas.DataFrame([1], index=[np.datetime64('2012-05-25')])\nOut[2]: \n            0\n2012-05-25  1\n\nIn [3]: pandas.DataFrame([1], index=[np.datetime64('2012-05-25 12:40')])\nOut[3]: \n                     0\n2012-05-25 12:40:00  1\n```\n",
      "Just to confirm, you're on NumPy 1.6?\n",
      "Ah, looks like you're on NumPy 1.7dev. I hadn't tested yet but see that the tests are failing. \n",
      "Sorry, yes, I'm using NumPy 1.7dev.\n",
      "@leonbaum I believe I've got this working (tests pass on 1.7dev and 1.6). Let me know if you encounter any further problems\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "closed",
      "commented",
      "mentioned",
      "commented",
      "commented",
      "mentioned",
      "commented",
      "commented",
      "commented",
      "reopened",
      "commented"
    ],
    "changed_files": 7,
    "additions": 41,
    "deletions": 6,
    "changed_files_list": [
      "pandas/src/datetime.pxd",
      "pandas/src/datetime.pyx",
      "pandas/src/engines.pyx",
      "pandas/src/np_datetime.c",
      "pandas/src/np_datetime.h",
      "pandas/src/numpy_helper.h",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1315,
    "reporter": "wesm",
    "created_at": "2012-05-25T00:27:22+00:00",
    "closed_at": "2012-05-27T17:13:46+00:00",
    "resolver": "wesm",
    "resolved_in": "5b428dcdac852acfb55e12c6b92aceb9783d3fae",
    "resolver_commit_num": 1976,
    "title": "New name for pandas._tseries extension",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 44,
    "additions": 67,
    "deletions": 75,
    "changed_files_list": [
      "pandas/__init__.py",
      "pandas/core/algorithms.py",
      "pandas/core/common.py",
      "pandas/core/factor.py",
      "pandas/core/format.py",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/internals.py",
      "pandas/core/nanops.py",
      "pandas/core/panel.py",
      "pandas/core/series.py",
      "pandas/io/date_converters.py",
      "pandas/io/parsers.py",
      "pandas/io/pytables.py",
      "pandas/io/tests/test_date_converters.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/sparse/array.py",
      "pandas/src/generate_code.py",
      "pandas/src/generated.pyx",
      "pandas/stats/moments.py",
      "pandas/tests/test_common.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_series.py",
      "pandas/tests/test_tseries.py",
      "pandas/tests/test_ujson.py",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tseries/frequencies.py",
      "pandas/tseries/index.py",
      "pandas/tseries/offsets.py",
      "pandas/tseries/period.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_frequencies.py",
      "pandas/tseries/tests/test_offsets.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tests/test_timezones.py",
      "pandas/tseries/tools.py",
      "pandas/util/decorators.py",
      "scripts/bench_join.py",
      "scripts/bench_join_multi.py",
      "scripts/groupby_test.py",
      "scripts/roll_median_leak.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1316,
    "reporter": "wesm",
    "created_at": "2012-05-25T13:28:18+00:00",
    "closed_at": "2012-05-25T21:42:26+00:00",
    "resolver": "wesm",
    "resolved_in": "7567fffa8d646434f56d59b6dd41287675110c09",
    "resolver_commit_num": 1963,
    "title": "No equals check when joining non-unique indexes?",
    "body": "Need to think about this one\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 18,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tests/test_index.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1318,
    "reporter": "wesm",
    "created_at": "2012-05-25T16:14:39+00:00",
    "closed_at": "2012-07-09T19:38:35+00:00",
    "resolver": "wesm",
    "resolved_in": "4f49001bc904df5897e80ddf4101c14d3e27c8e0",
    "resolver_commit_num": 2172,
    "title": "Handling TypeErrors in PyObject khash-table",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 28,
    "deletions": 7,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/src/khash.h"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1320,
    "reporter": "wesm",
    "created_at": "2012-05-25T21:50:50+00:00",
    "closed_at": "2012-06-12T02:21:26+00:00",
    "resolver": "wesm",
    "resolved_in": "07ae6e8e35fe08e0f77e141127e40d5398231c10",
    "resolver_commit_num": 2085,
    "title": "Review PyTables serialization formats for index types prior to release",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 6,
    "deletions": 3,
    "changed_files_list": [
      "pandas/io/pytables.py",
      "pandas/tseries/index.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1321,
    "reporter": "changhiskhan",
    "created_at": "2012-05-25T22:02:55+00:00",
    "closed_at": "2012-05-26T16:28:15+00:00",
    "resolver": "wesm",
    "resolved_in": "330c0673c63a483ca267586a1e2b5d2038c88315",
    "resolver_commit_num": 1969,
    "title": "DataFrame.ix with boolean arrays",
    "body": "Works right in one dimension but not both:\n\nIn [19]: df = DataFrame(np.random.randn(3, 2))\n\nIn [20]: df\nOut[20]: \n          0         1\n0  0.666325  0.812192\n1  0.023781  1.719397\n2  0.318738  0.227240\n\nIn [21]: df.ix[df.index==0, :]\nOut[21]: \n          0         1\n0  0.666325  0.812192\n\nIn [24]: df.ix[:, df.columns==1]\nOut[24]: \n          1\n0  0.812192\n1  1.719397\n2  0.227240\n\nBut not both:\n\nIn [23]: df.ix[df.index==0, df.columns==1]\nOut[23]: \n              0         1\nTrue   0.023781  1.719397\nFalse  0.666325  0.812192\nFalse  0.666325  0.812192\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 17,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/indexing.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1323,
    "reporter": "wesm",
    "created_at": "2012-05-26T21:05:25+00:00",
    "closed_at": "2012-05-28T16:33:23+00:00",
    "resolver": "wesm",
    "resolved_in": "e59b5821402fa9f2fa55de9b6176115ce2af42e4",
    "resolver_commit_num": 1981,
    "title": "Take care with series.astype('O') and DataFrame.value with datetime64",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 4,
    "additions": 50,
    "deletions": 13,
    "changed_files_list": [
      "pandas/core/common.py",
      "pandas/core/internals.py",
      "pandas/tests/test_internals.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1327,
    "reporter": "changhiskhan",
    "created_at": "2012-05-28T14:11:19+00:00",
    "closed_at": "2012-05-28T17:59:39+00:00",
    "resolver": "wesm",
    "resolved_in": "383b589a33708b74db77e1d373a5e7bab168bcd9",
    "resolver_commit_num": 1983,
    "title": "Resampling to unanchored week fails",
    "body": "\n",
    "labels": [],
    "comments": [
      "`W` I think should actually alias to `W-SUN`. I'll take care of this\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 27,
    "deletions": 9,
    "changed_files_list": [
      "pandas/tests/test_ujson.py",
      "pandas/tseries/frequencies.py",
      "pandas/tseries/tests/test_frequencies.py",
      "pandas/tseries/tests/test_resample.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1328,
    "reporter": "changhiskhan",
    "created_at": "2012-05-28T16:12:31+00:00",
    "closed_at": "2012-05-28T22:58:47+00:00",
    "resolver": "wesm",
    "resolved_in": "6478c87b5973179c0057f10e352ca5de534e5953",
    "resolver_commit_num": 1986,
    "title": "Better error msg for malformed freq alias",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 12,
    "deletions": 1,
    "changed_files_list": [
      "pandas/tseries/frequencies.py",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_frequencies.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1329,
    "reporter": "wesm",
    "created_at": "2012-05-28T16:20:36+00:00",
    "closed_at": "2012-05-28T16:46:33+00:00",
    "resolver": "wesm",
    "resolved_in": "3c1cc3504caa08d1105a67690be196002c5f8dae",
    "resolver_commit_num": 1982,
    "title": "Scalars don't propagate in DataFrame constructor",
    "body": "This only works if an index is specified; fails in the index inference process\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 13,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1331,
    "reporter": "wesm",
    "created_at": "2012-05-28T18:25:41+00:00",
    "closed_at": "2012-05-29T00:43:15+00:00",
    "resolver": "wesm",
    "resolved_in": "2c1de1af3669e7ce0ca3506b1aef815d3c88baa3",
    "resolver_commit_num": 1989,
    "title": "Enable DateOffset equality with string aliases",
    "body": "e.g. `Day() == 'M'` yields False. Currently raises Exception\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 32,
    "deletions": 17,
    "changed_files_list": [
      "pandas/tseries/tests/test_offsets.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1333,
    "reporter": "wesm",
    "created_at": "2012-05-28T19:47:18+00:00",
    "closed_at": "2012-05-28T22:47:25+00:00",
    "resolver": "wesm",
    "resolved_in": "3f9aad16d2b05d256cdebd434fc708589eb5b7b9",
    "resolver_commit_num": 1985,
    "title": "Unit test construction of Period, PeriodIndex from negative ordinals",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 2,
    "additions": 184,
    "deletions": 61,
    "changed_files_list": [
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1336,
    "reporter": "wesm",
    "created_at": "2012-05-29T00:15:16+00:00",
    "closed_at": "2012-05-29T00:18:46+00:00",
    "resolver": "wesm",
    "resolved_in": "a53cfa934a87e98f08adbfd4bd3cc4687e05f50c",
    "resolver_commit_num": 1987,
    "title": "Display local time instead of UTC in DatetimeIndex.__repr__",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 11,
    "deletions": 4,
    "changed_files_list": [
      "pandas/core/format.py",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1338,
    "reporter": "wesm",
    "created_at": "2012-05-29T15:25:25+00:00",
    "closed_at": "2012-06-02T21:20:55+00:00",
    "resolver": "wesm",
    "resolved_in": "3ad0f0a4c22dae764b4cbb833e2808072d05a357",
    "resolver_commit_num": 2012,
    "title": "tsplot doesn't respect style parameter",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 10,
    "deletions": 4,
    "changed_files_list": [
      "pandas/tools/plotting.py",
      "pandas/tseries/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1341,
    "reporter": "wesm",
    "created_at": "2012-05-29T20:58:23+00:00",
    "closed_at": "2012-05-29T21:30:42+00:00",
    "resolver": "wesm",
    "resolved_in": "7fffdb53f8e918ac2b0fa5aaba196f2a0e8c973c",
    "resolver_commit_num": 1994,
    "title": "logy argument ignored in KdePlot",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 17,
    "deletions": 20,
    "changed_files_list": [
      "pandas/tools/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1345,
    "reporter": "wesm",
    "created_at": "2012-05-29T23:41:04+00:00",
    "closed_at": "2012-06-03T17:21:17+00:00",
    "resolver": "wesm",
    "resolved_in": "3f6530c962050bc274ca3174651c873750861cae",
    "resolver_commit_num": 2021,
    "title": "Check idx.asobject / idx.astype('O') for adding tzinfo metadata",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "cross-referenced"
    ],
    "changed_files": 1,
    "additions": 16,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tseries/tests/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1349,
    "reporter": "wesm",
    "created_at": "2012-05-30T13:30:44+00:00",
    "closed_at": "2012-05-30T14:02:37+00:00",
    "resolver": "wesm",
    "resolved_in": "c52a95bf95afb77d3ceacbeda9a86ab3f0e82648",
    "resolver_commit_num": 2000,
    "title": "Optimize DataFrame.corr for case with no missing data",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 2,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1352,
    "reporter": "adamklein",
    "created_at": "2012-05-30T19:05:27+00:00",
    "closed_at": "2012-06-03T15:56:58+00:00",
    "resolver": "wesm",
    "resolved_in": "911fec2bb2081887ef4396a1993f2ea4ee9c9f0a",
    "resolver_commit_num": 2019,
    "title": "MultiIndex partial slicing error",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 21,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/indexing.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1353,
    "reporter": "wesm",
    "created_at": "2012-05-30T20:44:33+00:00",
    "closed_at": "2012-06-03T15:19:54+00:00",
    "resolver": "wesm",
    "resolved_in": "6ea9099eabb8347bd36a72c94ea3754a0e4a6d36",
    "resolver_commit_num": 2017,
    "title": "pandas.unique should return ndarray",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 5,
    "additions": 142,
    "deletions": 17,
    "changed_files_list": [
      "pandas/core/algorithms.py",
      "pandas/core/nanops.py",
      "pandas/src/hashtable.pyx",
      "pandas/src/kvec.h",
      "pandas/tests/test_algos.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1356,
    "reporter": "wesm",
    "created_at": "2012-05-30T21:23:17+00:00",
    "closed_at": "2012-06-03T17:36:55+00:00",
    "resolver": "wesm",
    "resolved_in": "f4655f38d791a529e0873ca3c501b55c7c8f8abb",
    "resolver_commit_num": 2023,
    "title": "Time zone handling with Timestamp scalar values",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 1,
    "additions": 24,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/timeseries.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1358,
    "reporter": "wesm",
    "created_at": "2012-05-31T01:02:42+00:00",
    "closed_at": "2012-07-12T17:45:45+00:00",
    "resolver": "wesm",
    "resolved_in": "20446016fef2c76f47b597b74c72d7bfbabfea06",
    "resolver_commit_num": 2205,
    "title": "Add accelerated Cython median groupby method",
    "body": "Use linear-time median finder\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 6,
    "additions": 147,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/src/groupby.pyx",
      "pandas/src/moments.pyx",
      "pandas/tests/test_groupby.py",
      "vb_suite/groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1359,
    "reporter": "wesm",
    "created_at": "2012-05-31T02:02:47+00:00",
    "closed_at": "2012-06-03T17:44:06+00:00",
    "resolver": "wesm",
    "resolved_in": "6b53334c9f0389a2bbb73fa8f3405cc0d4b014c9",
    "resolver_commit_num": 2024,
    "title": "List of tuples, functions not properly handled in groupby",
    "body": "e.g. `gb.agg([('foo', 'mean'), 'std'])\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 2,
    "additions": 18,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1362,
    "reporter": "wesm",
    "created_at": "2012-05-31T15:22:50+00:00",
    "closed_at": "2012-06-02T20:00:29+00:00",
    "resolver": "wesm",
    "resolved_in": "952b272c7d2c829390acda90fbca30f66c01e1a8",
    "resolver_commit_num": 2009,
    "title": "groupby string function-name shortcuts don't work with transform",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 20,
    "deletions": 8,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1364,
    "reporter": "wesm",
    "created_at": "2012-05-31T15:52:00+00:00",
    "closed_at": "2012-06-02T19:55:10+00:00",
    "resolver": "wesm",
    "resolved_in": "f62f571dca322c5477c90b066987c013a450ced4",
    "resolver_commit_num": 2008,
    "title": "Should GroupBy.transform exclude nuisance columns?",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 35,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1365,
    "reporter": "wesm",
    "created_at": "2012-05-31T15:53:09+00:00",
    "closed_at": "2012-06-02T19:44:02+00:00",
    "resolver": "wesm",
    "resolved_in": "0d3c3ec56dc8d9dfacf058f0d18f93ca2255f6e3",
    "resolver_commit_num": 2007,
    "title": "Column selection not honored in GroupBy.transform",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 14,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1366,
    "reporter": "JWCornV",
    "created_at": "2012-05-31T18:33:28+00:00",
    "closed_at": "2012-06-02T21:31:02+00:00",
    "resolver": "wesm",
    "resolved_in": "2e537fd87fe06b22f814107e6440a0eaee01ddf8",
    "resolver_commit_num": 2013,
    "title": "Cannot index a dataframe with a boolean dataframe",
    "body": "Sorry if there is a specific reason why this is not supported, but with boolean dataframe indexing, I would expect the following to work:\n\n-docs/dev/indexing.html#indexing-a-dataframe-with-a-boolean-dataframe\n\n\n",
    "labels": [],
    "comments": [
      "I have to see if this is possible (it should be theoretically); it wasn't a deliberate choice one way or the other.\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 20,
    "deletions": 10,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1367,
    "reporter": "wesm",
    "created_at": "2012-05-31T20:58:17+00:00",
    "closed_at": "2012-06-01T14:55:44+00:00",
    "resolver": "wesm",
    "resolved_in": "abac90dd3ac32dd3c34ce80c686304cb1be9d065",
    "resolver_commit_num": 2001,
    "title": "DataFrame.corrwith should accept TimeSeries",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 6,
    "additions": 329,
    "deletions": 267,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/stats/misc.py",
      "pandas/tests/test_frame.py",
      "pandas/tools/tests/test_tile.py",
      "pandas/tools/tile.py",
      "pandas/tseries/util.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1372,
    "reporter": "wesm",
    "created_at": "2012-06-01T13:11:19+00:00",
    "closed_at": "2012-06-03T18:09:47+00:00",
    "resolver": "wesm",
    "resolved_in": "c3910a39bf6bf3a754acc39cbfffe5c37a7f535e",
    "resolver_commit_num": 2025,
    "title": "Add method to cast Timestamp to datetime.datetime / DatetimeIndex -> datetime.datetime array",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 78,
    "deletions": 5,
    "changed_files_list": [
      "pandas/src/datetime.pyx",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1374,
    "reporter": "lodagro",
    "created_at": "2012-06-01T13:17:57+00:00",
    "closed_at": "2012-06-03T17:11:15+00:00",
    "resolver": "wesm",
    "resolved_in": "2e95a0fe5638d68c5a56c34d4237d60eb413e1a8",
    "resolver_commit_num": 2020,
    "title": "What to do with irow in case of duplicate index?",
    "body": "Since irow translates the integer index to a label one and then retrieves the row, in case of duplicate index multiple rows are returned.\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 7,
    "additions": 117,
    "deletions": 44,
    "changed_files_list": [
      "pandas/core/format.py",
      "pandas/core/frame.py",
      "pandas/core/indexing.py",
      "pandas/core/internals.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_internals.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1378,
    "reporter": "wesm",
    "created_at": "2012-06-01T20:39:44+00:00",
    "closed_at": "2012-06-05T02:27:42+00:00",
    "resolver": "wesm",
    "resolved_in": "3e904fddda17151cdbe52e7d6cee241da7949154",
    "resolver_commit_num": 2038,
    "title": "Add quantile cut method for decile, quartile analysis",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "referenced"
    ],
    "changed_files": 7,
    "additions": 151,
    "deletions": 38,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/algorithms.py",
      "pandas/core/series.py",
      "pandas/src/datetime.pyx",
      "pandas/tools/tests/test_tile.py",
      "pandas/tools/tile.py",
      "scripts/count_code.sh"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1379,
    "reporter": "leonbaum",
    "created_at": "2012-06-01T21:44:04+00:00",
    "closed_at": "2012-06-02T18:47:55+00:00",
    "resolver": "wesm",
    "resolved_in": "6afa61b14a7fa4cf9d3c61d5272641f025338ec7",
    "resolver_commit_num": 2004,
    "title": "Setting datetime64 array as DataFrame column does not convert unit to nanoseconds",
    "body": "Using pandas 0.8 and numpy 1.7,  I get errors when I add columns that are of type datetime64 for everything but very short columns:\n\n\n",
    "labels": [],
    "comments": [
      "This is a bug in `isnull` on the datetime64 dtype. I'll get it fixed over the weekend.\n",
      "I updated the issue description and will see about fixing. This is really a pain-- the NumPy datetime64 API has been nothing but trouble. \n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented"
    ],
    "changed_files": 4,
    "additions": 92,
    "deletions": 2,
    "changed_files_list": [
      "pandas/core/internals.py",
      "pandas/src/datetime.pyx",
      "pandas/tools/tile.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1384,
    "reporter": "wesm",
    "created_at": "2012-06-04T14:06:12+00:00",
    "closed_at": "2012-06-04T20:08:05+00:00",
    "resolver": "wesm",
    "resolved_in": "a5f773bdf2c1437a6f46a1782b089621ffac0870",
    "resolver_commit_num": 2035,
    "title": "Cast other datetime64 units to nanoseconds in Index constructor",
    "body": "Currently being interpreted as nanoseconds instead of being converted\n\ndtypes also seem to be ignored in Index constructor\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 101,
    "deletions": 31,
    "changed_files_list": [
      "pandas/core/index.py",
      "pandas/sparse/frame.py",
      "pandas/sparse/tests/test_sparse.py",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1389,
    "reporter": "wesm",
    "created_at": "2012-06-04T20:00:26+00:00",
    "closed_at": "2012-06-05T00:55:51+00:00",
    "resolver": "wesm",
    "resolved_in": "422869fe71986a602283d538762ecb450bb80ac3",
    "resolver_commit_num": 2036,
    "title": "Test Timestamp adding timedelta pushing over DST boundary",
    "body": "Almost 100% sure this won't do the right thing\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 7,
    "additions": 681,
    "deletions": 378,
    "changed_files_list": [
      "pandas/src/datetime.pyx",
      "pandas/src/offsets.pyx",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tests/test_timezones.py",
      "pandas/tseries/tools.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1391,
    "reporter": "wesm",
    "created_at": "2012-06-04T21:18:58+00:00",
    "closed_at": "2012-06-05T01:22:34+00:00",
    "resolver": "wesm",
    "resolved_in": "d1e3e1b3c42244b3cb42f84e2f421d8ca0e83bc8",
    "resolver_commit_num": 2037,
    "title": "Tighter Timestamp repr with time zone",
    "body": "This is a bit much:\n\n\n\nProbably ISO8601-ish would be preferred\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 6,
    "deletions": 0,
    "changed_files_list": [
      "pandas/src/datetime.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1392,
    "reporter": "wesm",
    "created_at": "2012-06-05T01:36:14+00:00",
    "closed_at": "2012-06-05T02:29:57+00:00",
    "resolver": "wesm",
    "resolved_in": "c6cee43ac0d0720958227ebddfb1f62652413436",
    "resolver_commit_num": 2039,
    "title": "Add value_counts function as top level method",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 10,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/algorithms.py",
      "pandas/core/api.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1394,
    "reporter": "lenolib",
    "created_at": "2012-06-05T06:47:28+00:00",
    "closed_at": "2012-06-05T15:41:50+00:00",
    "resolver": "wesm",
    "resolved_in": "effed956934f18424ab48b80b05a2ab9f244a2f1",
    "resolver_commit_num": 2041,
    "title": "Series.append() excepts with some funky overlapping dates",
    "body": "With pandas v0.8.0b1\n\nIn [90]: u\nOut[90]: \n2012-05-08 01:45:00     86\n2012-05-08 01:50:00    170\n2012-05-08 01:55:00    130\n2012-05-08 02:00:00    206\n2012-05-08 02:05:00     52\n2012-05-08 02:10:00      4\nFreq: 5T\n\nIn [91]: u.append(u)\n\nTraceback (most recent call last):\n  File \"<ipython console>\", line 1, in <module>\n  File \"C:\\Python27\\lib\\site-packages\\pandas\\core\\series.py\", line 1566, in append\n    return concat(to_concat, ignore_index=False, verify_integrity=True)\n  File \"C:\\Python27\\lib\\site-packages\\pandas\\tools\\merge.py\", line 836, in concat\n    verify_integrity=verify_integrity)\n  File \"C:\\Python27\\lib\\site-packages\\pandas\\tools\\merge.py\", line 895, in **init**\n    self.new_axes = self._get_new_axes()\n  File \"C:\\Python27\\lib\\site-packages\\pandas\\tools\\merge.py\", line 1066, in _get_new_axes\n    concat_axis = self._get_concat_axis()\n  File \"C:\\Python27\\lib\\site-packages\\pandas\\tools\\merge.py\", line 1097, in _get_concat_axis\n    self._maybe_check_integrity(concat_axis)\n  File \"C:\\Python27\\lib\\site-packages\\pandas\\tools\\merge.py\", line 1106, in _maybe_check_integrity\n    % str(overlap))\nException: Indexes have overlapping values: [1970-01-16 225:45:00, 1970-01-16 225:50:00, 1970-01-16 225:55:00, 1970-01-16 226:00:00, 1970-01-16 226:05:00, 1970-01-16 226:10:00]\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 9,
    "additions": 235,
    "deletions": 204,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/core/series.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_series.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_cursor.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1395,
    "reporter": "sadruddin",
    "created_at": "2012-06-05T11:52:39+00:00",
    "closed_at": "2012-09-18T15:04:48+00:00",
    "resolver": "wesm",
    "resolved_in": "cfa70db8919f2447d325c6eafe42d24ec20d9e0f",
    "resolver_commit_num": 2397,
    "title": "datetime.date support in 0.8 new timeseries framework",
    "body": "Is is the intention that datetime.date would be supported as a valid input type for some methods? I have observed that it is only partially supported. Example, when using 0.8.0b1:\n\n\n",
    "labels": [],
    "comments": [
      "There is only partial support in these methods for `datetime.date`. I won't have the time to add sufficient testing that they all work with datetime.date; would need additional development help\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 28,
    "deletions": 20,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/offsets.py",
      "pandas/tseries/tests/test_offsets.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1396,
    "reporter": "gerigk",
    "created_at": "2012-06-05T12:29:01+00:00",
    "closed_at": "2012-06-07T15:26:45+00:00",
    "resolver": "wesm",
    "resolved_in": "bd55aadba2cd10d9fad6ba5df3d7547613bdb116",
    "resolver_commit_num": 2062,
    "title": "New errors in nosetests",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "referenced",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 19,
    "deletions": 4,
    "changed_files_list": [
      "pandas/core/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1397,
    "reporter": "wesm",
    "created_at": "2012-06-05T12:56:30+00:00",
    "closed_at": "2012-06-05T16:38:04+00:00",
    "resolver": "wesm",
    "resolved_in": "b2b938878491e48719cfa416eb039ba836217a6f",
    "resolver_commit_num": 2044,
    "title": "UTC localization / conversion broken in Timestamp",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 27,
    "deletions": 3,
    "changed_files_list": [
      "pandas/src/datetime.pyx",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1398,
    "reporter": "Komnomnomnom",
    "created_at": "2012-06-05T14:10:46+00:00",
    "closed_at": "2012-06-05T15:47:53+00:00",
    "resolver": "wesm",
    "resolved_in": "600e0c93c12c7451f1a690c036f941eb762a4581",
    "resolver_commit_num": 2042,
    "title": "Build problem on OSX",
    "body": "Function declaration in `src/period.h`\n\n\n\ndoesn't match definition in `src/period.c`\n\n\n\nChanging `int64_t` to `npy_int64` in `period.h` fixes the build and all tests pass on OSX.\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 2,
    "deletions": 1,
    "changed_files_list": [
      "pandas/src/period.h",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1403,
    "reporter": "wesm",
    "created_at": "2012-06-05T17:40:32+00:00",
    "closed_at": "2012-06-05T18:25:08+00:00",
    "resolver": "wesm",
    "resolved_in": "6d01c3c4891e19530ebd3a96887fb9465fd58ffd",
    "resolver_commit_num": 2046,
    "title": "Split time zone localization out of tz_convert into tz_localize",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 7,
    "additions": 150,
    "deletions": 162,
    "changed_files_list": [
      "pandas/core/generic.py",
      "pandas/core/series.py",
      "pandas/src/datetime.pyx",
      "pandas/src/engines.pyx",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tests/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1404,
    "reporter": "wesm",
    "created_at": "2012-06-05T17:40:54+00:00",
    "closed_at": "2012-06-05T18:00:38+00:00",
    "resolver": "wesm",
    "resolved_in": "a6fd608b850d785ffc1544880bdaa5525fdb8f77",
    "resolver_commit_num": 2045,
    "title": "Raise exception in comparisons between tz-naive and tz-aware Timestamps",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 41,
    "deletions": 12,
    "changed_files_list": [
      "pandas/src/datetime.pyx",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tests/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1405,
    "reporter": "wesm",
    "created_at": "2012-06-05T19:33:00+00:00",
    "closed_at": "2012-06-08T00:39:41+00:00",
    "resolver": "wesm",
    "resolved_in": "10fcffccd999b2102763611a9214af65d0ab4719",
    "resolver_commit_num": 2063,
    "title": "Implement Factor comparison methods",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 131,
    "deletions": 32,
    "changed_files_list": [
      "pandas/core/factor.py",
      "pandas/tests/test_factor.py",
      "pandas/tests/test_index.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1406,
    "reporter": "wesm",
    "created_at": "2012-06-05T19:37:23+00:00",
    "closed_at": "2012-06-12T01:32:35+00:00",
    "resolver": "wesm",
    "resolved_in": "92f0234e660fda9c0d5cec05a91c9bce33f796e3",
    "resolver_commit_num": 2083,
    "title": "Raise exception if group keys and levels don't overlap in concat",
    "body": "If no keys are found in the levels\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 53,
    "deletions": 22,
    "changed_files_list": [
      "pandas/core/factor.py",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1407,
    "reporter": "wesm",
    "created_at": "2012-06-05T19:39:27+00:00",
    "closed_at": "2012-06-12T00:52:22+00:00",
    "resolver": "wesm",
    "resolved_in": "682ca2568f7060cf04eddc00c897041571ca6d5a",
    "resolver_commit_num": 2082,
    "title": "Document minimal Factor support in groupby, cut/qcut functions",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 71,
    "deletions": 7,
    "changed_files_list": [
      "doc/source/basics.rst",
      "doc/source/groupby.rst",
      "doc/source/timeseries.rst",
      "pandas/tools/tests/test_tile.py",
      "pandas/tools/tile.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1409,
    "reporter": "wesm",
    "created_at": "2012-06-06T01:34:37+00:00",
    "closed_at": "2012-06-12T00:28:18+00:00",
    "resolver": "wesm",
    "resolved_in": "8f94009830f9809d0ffe5330fb7d06d99398e831",
    "resolver_commit_num": 2081,
    "title": "Out of bounds checking / handling in cut, qcut",
    "body": "currently:\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 36,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/algorithms.py",
      "pandas/tests/test_factor.py",
      "pandas/tools/tests/test_tile.py",
      "pandas/tools/tile.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1411,
    "reporter": "wesm",
    "created_at": "2012-06-06T02:19:34+00:00",
    "closed_at": "2012-06-14T20:17:33+00:00",
    "resolver": "wesm",
    "resolved_in": "d5e8b8c2fea60b9c71b0d14492cb6ecc2bf5473f",
    "resolver_commit_num": 2106,
    "title": "include_lowest parameter in cut function",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 29,
    "deletions": 15,
    "changed_files_list": [
      "pandas/tools/tests/test_tile.py",
      "pandas/tools/tile.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1414,
    "reporter": "lbeltrame",
    "created_at": "2012-06-06T12:12:47+00:00",
    "closed_at": "2012-06-11T18:27:53+00:00",
    "resolver": "wesm",
    "resolved_in": "20b52181e0d65228786970ed3465f655132f8361",
    "resolver_commit_num": 2074,
    "title": "Series apply() and map() may produce a Series with different dtype than the original",
    "body": "I'm unsure if it's by design or not, so I'm leaving this here.\n\n\n",
    "labels": [],
    "comments": [
      "Would you prefer that once dtype is object it stays object?\nIt is by design that the ouput of apply func is evaluated to determine the dtype of return series.\n\n``` python\nIn [46]: s\nOut[46]:\n0    1\n1    2\n2    3\n3    4\n4    5\n\nIn [47]: def divby3(x):\n   ....:     return x/3.0\n   ....:\n\nIn [48]: s.dtype\nOut[48]: dtype('int64')\n\nIn [49]: s.apply(divby3).dtype\nOut[49]: dtype('float64')\n\nIn [50]: s.apply(divby3)\nOut[50]:\n0    0.333333\n1    0.666667\n2    1.000000\n3    1.333333\n4    1.666667\n\nIn [51]: s.apply(str).dtype\nOut[51]: dtype('object')\n```\n",
      "In data mercoled\u00ec 6 giugno 2012 05:33:20, hai scritto:\n\n> Would you prefer that once dtype is object it stays object?\n\nPersonally yes, because otherwise to achieve the same result as the function \nposted in the bug report:\n1. I have to create a temporary list\n2. I have to add all the items from the Series to the new list doing the work \n   the function does\n3. I have to create a Series with the desired dtype at the end, constructed \n   with the list and the index of the original one.\n\nOf course if there are technical reasons for this I'll just keep this hack \naround.\n",
      "Note that you can use astype:\n\n``` python\nIn [15]: original.apply(convert)\nOut[15]:\n0     1\n1     2\n2     3\n3   NaN\n\nIn [16]: original.apply(convert).astype('object')\nOut[16]:\n0      1\n1      2\n2      3\n3    NaN\n\nIn [17]: original.apply(convert).astype('object').dtype\nOut[17]: dtype('object')\n```\n",
      "It doesn't however keep the elements as ints, but instead as floats (so all \".0\" etc). So \"convert\" doesn't do actually anything.\n\n``` python\n\nIn [191]: type(original.apply(convert).astype(\"object\")[0])\nOut[191]: float\n\n```\n",
      "[int and na](http://pandas.pydata.org/pandas-docs/dev/gotchas.html#support-for-integer-na) don`t match and the proposed workaround to use object has its limits also, or is it astype that does not do a proper job?\n",
      "I don't think it's astype. I think this is a side effect of int being promoted to float.  For now I guess I'll keep a temporary list hack, even though this is pretty inefficient for large Series.\n",
      "This is by design. I will see about adding an option to leave the result as `dtype=object` (the type inference helps much more than it hurts)\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 20,
    "deletions": 5,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/series.py",
      "pandas/src/inference.pyx",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1415,
    "reporter": "wesm",
    "created_at": "2012-06-06T17:15:07+00:00",
    "closed_at": "2012-06-06T17:33:06+00:00",
    "resolver": "wesm",
    "resolved_in": "4dd13b930d28893d793941a63c0ec20f5d0852f2",
    "resolver_commit_num": 2056,
    "title": "Use index name as xlabel or ylabel in plot types",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 26,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tools/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1419,
    "reporter": "lodagro",
    "created_at": "2012-06-07T08:28:10+00:00",
    "closed_at": "2012-06-12T00:03:12+00:00",
    "resolver": "wesm",
    "resolved_in": "b1edf548ae1668ee76f01ca1d9c407a8d2d84781",
    "resolver_commit_num": 2080,
    "title": "Allow concat names arg to rename existing MultiIndex level.",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 20,
    "deletions": 2,
    "changed_files_list": [
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1421,
    "reporter": "manuteleco",
    "created_at": "2012-06-07T17:01:36+00:00",
    "closed_at": "2012-06-11T21:07:40+00:00",
    "resolver": "wesm",
    "resolved_in": "e11777e0b1a45af18ebc90584deea84459853fe5",
    "resolver_commit_num": 2077,
    "title": "Missing rows on DataFrame outer join with MultiIndex",
    "body": "Hi,\n\nI'm trying to compute an outer join on several columns applied to several DataFrame objects in one step. However, the result I get seems to force the uniqueness on the set of join columns and, as a consequence, some rows are missing.\n\nHere is some example code that shows the output from a join operation over 3 dataframes in one step and a merge operation (in 2 steps) over the same data. Comparing both, we see that the join operation doesn't include the row \"1  1  10  100  1000\".\n\n\n\nHowever, this problem doesn't seem to arise when we specify \"how={anything other that outer}\" in the join operation.\n\nSo, either this is a bug or I'm missing something here. In either case, I would appreciate any comment regarding this issue. And, BTW, it would be really could if \"merge\" could accept a list of DataFrames and join them efficiently in one step.\n\nThanks and regards.\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 51,
    "deletions": 12,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1422,
    "reporter": "wesm",
    "created_at": "2012-06-07T21:41:42+00:00",
    "closed_at": "2012-06-11T17:17:08+00:00",
    "resolver": "wesm",
    "resolved_in": "89e6eca1e52fcd05dbe72ae8f724b4f0b37f3b2b",
    "resolver_commit_num": 2070,
    "title": "Joins between equal, but not same time zone DatetimeIndex yields non-UTC DatetimeIndex",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 37,
    "deletions": 6,
    "changed_files_list": [
      "pandas/core/series.py",
      "pandas/tseries/tests/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1423,
    "reporter": "dalejung",
    "created_at": "2012-06-07T22:50:52+00:00",
    "closed_at": "2012-06-11T18:54:21+00:00",
    "resolver": "wesm",
    "resolved_in": "efea76b80ee5325fed7bb8181149caa444c811c5",
    "resolver_commit_num": 2075,
    "title": "select() within a function closure not working as agg function",
    "body": "I'm running into a weird issue with groupby and function closure. For some reason the function closure doesn't work unless I access the grouped series. You can see in agg_before I have a fix flag that will just access the data var. \n\n\n\n\n\nRunning an agg function that isn't a closure works fine. Any ideas on this?\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "cross-referenced"
    ],
    "changed_files": 2,
    "additions": 39,
    "deletions": 0,
    "changed_files_list": [
      "pandas/src/reduce.pyx",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1424,
    "reporter": "wesm",
    "created_at": "2012-06-07T23:18:57+00:00",
    "closed_at": "2012-06-08T01:00:40+00:00",
    "resolver": "wesm",
    "resolved_in": "758249d9ef3f226e390010c6634320e118286d76",
    "resolver_commit_num": 2064,
    "title": "Limit argument not respected in `resample`",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 19,
    "deletions": 1,
    "changed_files_list": [
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1426,
    "reporter": "wesm",
    "created_at": "2012-06-08T00:17:44+00:00",
    "closed_at": "2012-06-11T17:00:19+00:00",
    "resolver": "wesm",
    "resolved_in": "15a02d711dc313192e9162e1ace3a06cc1ff9afc",
    "resolver_commit_num": 2069,
    "title": "Raise exception on joins between tz-naive DatetimeIndex and tz-aware ones",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 15,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1430,
    "reporter": "changhiskhan",
    "created_at": "2012-06-08T13:57:30+00:00",
    "closed_at": "2012-06-08T21:19:26+00:00",
    "resolver": "wesm",
    "resolved_in": "72fb8c66c4c6e3ccc94bde63dc038de031e3b13e",
    "resolver_commit_num": 2065,
    "title": "timeseries related .groups bug",
    "body": "related to #1423\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 24,
    "deletions": 9,
    "changed_files_list": [
      "pandas/tests/test_groupby.py",
      "pandas/tseries/index.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1432,
    "reporter": "lodagro",
    "created_at": "2012-06-08T15:28:58+00:00",
    "closed_at": "2012-06-11T16:55:14+00:00",
    "resolver": "wesm",
    "resolved_in": "5a3f67c7c28fc38ebc7d711109872dc96ba0914c",
    "resolver_commit_num": 2068,
    "title": "DataFrame.ix assign fails for mixed dtypes",
    "body": "from [mailing list](#)\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 35,
    "deletions": 26,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/indexing.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1433,
    "reporter": "gerigk",
    "created_at": "2012-06-08T15:34:05+00:00",
    "closed_at": "2012-06-14T03:04:37+00:00",
    "resolver": "wesm",
    "resolved_in": "b8afb0f4be98fe42b349d1465571e54a6ff87bb3",
    "resolver_commit_num": 2096,
    "title": "Unexpected behaviour when converting datetime column to np.datetime64",
    "body": "To me this behaviour appeared to be weird.\nNumpy 1.7 and latest Pandas\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 9,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/internals.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1437,
    "reporter": "saroele",
    "created_at": "2012-06-08T22:34:54+00:00",
    "closed_at": "2012-06-11T16:30:50+00:00",
    "resolver": "wesm",
    "resolved_in": "2cee459a3f227c0ea723df29d36621737c45985b",
    "resolver_commit_num": 2067,
    "title": "datetimeindex returns wrong datetimes with tolist() and values()",
    "body": "In [82]: dr = pandas.date_range(start='2012-01-01', periods=10, freq='30Min')\n\nIn [83]: dr\nOut[83]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2012-01-01 00:00:00, ..., 2012-01-01 04:30:00]\nLength: 10, Freq: 30T, Timezone: None\n\nIn [84]: dr.values\nOut[84]: \narray([1970-01-16 224:00:00, 1970-01-16 224:30:00, 1970-01-16 225:00:00,\n       1970-01-16 225:30:00, 1970-01-16 226:00:00, 1970-01-16 226:30:00,\n       1970-01-16 227:00:00, 1970-01-16 227:30:00, 1970-01-16 228:00:00,\n       1970-01-16 228:30:00], dtype=datetime64[ns])\n",
    "labels": [],
    "comments": [
      "I'll fix `tolist` but `.values` can't be fixed-- it's a bug with how NumPy 1.6 displays nanosecond timestamps. See:\n\nhttp://pandas.pydata.org/pandas-docs/dev/whatsnew.html#potential-porting-issues-for-pandas-0-7-3-users\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 12,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1447,
    "reporter": "wesm",
    "created_at": "2012-06-12T00:42:29+00:00",
    "closed_at": "2012-06-12T01:40:28+00:00",
    "resolver": "wesm",
    "resolved_in": "92cddfbf4584d13c20e58fb64b13d4ed268fcc26",
    "resolver_commit_num": 2084,
    "title": "Update time zone handling documentation",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 27,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/timeseries.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1451,
    "reporter": "wesm",
    "created_at": "2012-06-12T21:49:29+00:00",
    "closed_at": "2012-06-14T16:47:57+00:00",
    "resolver": "wesm",
    "resolved_in": "b31610d5ef4a85d029b2c2def56b9e67992cc8e8",
    "resolver_commit_num": 2102,
    "title": "Monthly resampling bug encountered at PyGotham",
    "body": "To reproduce:\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 26,
    "deletions": 3,
    "changed_files_list": [
      "pandas/tseries/offsets.py",
      "pandas/tseries/tests/test_offsets.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1455,
    "reporter": "dalejung",
    "created_at": "2012-06-13T00:46:11+00:00",
    "closed_at": "2012-06-14T14:48:58+00:00",
    "resolver": "wesm",
    "resolved_in": "1a62f1a3386a66504a82e586fbe83c06dcf2a501",
    "resolver_commit_num": 2100,
    "title": "SeriesGroupBy.count() errors when original series is MultiIndex with Datetime as level 0",
    "body": "\n\nSeems to have happened within the last few days, maybe even just today. \n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 19,
    "deletions": 5,
    "changed_files_list": [
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1457,
    "reporter": "wesm",
    "created_at": "2012-06-13T19:26:07+00:00",
    "closed_at": "2012-06-14T03:26:21+00:00",
    "resolver": "wesm",
    "resolved_in": "126ca4ec0e3f9a133430f74a1171c8f4dec8408f",
    "resolver_commit_num": 2097,
    "title": "Factor with -1 in labels, testing",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 42,
    "deletions": 22,
    "changed_files_list": [
      "pandas/core/common.py",
      "pandas/core/factor.py",
      "pandas/tests/test_factor.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1460,
    "reporter": "wesm",
    "created_at": "2012-06-13T21:47:32+00:00",
    "closed_at": "2012-06-13T22:23:18+00:00",
    "resolver": "wesm",
    "resolved_in": "965b8649141a7ee0cdaadcf2d14bd84965456f93",
    "resolver_commit_num": 2093,
    "title": "labels argument to cut not exactly what's desired",
    "body": "If pass `len(bins) - 1` labels, should use those without forming intervals\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 4,
    "additions": 54,
    "deletions": 34,
    "changed_files_list": [
      "pandas/core/api.py",
      "pandas/core/factor.py",
      "pandas/tools/tests/test_tile.py",
      "pandas/tools/tile.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1463,
    "reporter": "wesm",
    "created_at": "2012-06-13T22:04:38+00:00",
    "closed_at": "2012-06-14T19:36:29+00:00",
    "resolver": "wesm",
    "resolved_in": "f5001a172597dd163e433b964f8dffa11da4444d",
    "resolver_commit_num": 2105,
    "title": "Return NA in Categorical for out of range values",
    "body": "See R's behavior\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 9,
    "deletions": 20,
    "changed_files_list": [
      "pandas/tools/tests/test_tile.py",
      "pandas/tools/tile.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1465,
    "reporter": "dalejung",
    "created_at": "2012-06-14T01:40:20+00:00",
    "closed_at": "2012-06-14T03:38:05+00:00",
    "resolver": "wesm",
    "resolved_in": "0fdd8817f905f9eb66791a94979fb9a32a9b9a04",
    "resolver_commit_num": 2098,
    "title": "resample error with closed='left' and label='right'.",
    "body": "\n\nHappens whenever the binlabels are trimmed and the label is set to 'right'. It ends up reducing the labels twice. \n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 26,
    "deletions": 6,
    "changed_files_list": [
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1467,
    "reporter": "wesm",
    "created_at": "2012-06-14T03:11:42+00:00",
    "closed_at": "2012-06-14T22:58:10+00:00",
    "resolver": "wesm",
    "resolved_in": "e79f4815e6a5ad555ebfe1ef82612f9da5deba83",
    "resolver_commit_num": 2107,
    "title": "Refactor MultiIndex to not always store tuples. Benchmark, ensure legacy pickle support",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed"
    ],
    "changed_files": 12,
    "additions": 127,
    "deletions": 52,
    "changed_files_list": [
      "pandas/core/algorithms.py",
      "pandas/core/common.py",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/internals.py",
      "pandas/core/series.py",
      "pandas/src/engines.pyx",
      "pandas/src/reduce.pyx",
      "pandas/tests/test_index.py",
      "pandas/tests/test_tseries.py",
      "vb_suite/reshape.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1468,
    "reporter": "wesm",
    "created_at": "2012-06-14T03:14:32+00:00",
    "closed_at": "2012-06-14T19:14:33+00:00",
    "resolver": "wesm",
    "resolved_in": "3850dd707e9282aa6a17c1225e3c545e02adb655",
    "resolver_commit_num": 2104,
    "title": "factor.py -> categorical.py (or something)",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 11,
    "additions": 19,
    "deletions": 19,
    "changed_files_list": [
      "pandas/core/api.py",
      "pandas/core/categorical.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/panel.py",
      "pandas/io/pytables.py",
      "pandas/tests/test_factor.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_index.py",
      "pandas/tools/merge.py",
      "pandas/tools/tile.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1470,
    "reporter": "gerigk",
    "created_at": "2012-06-14T14:55:06+00:00",
    "closed_at": "2012-06-14T15:01:15+00:00",
    "resolver": "wesm",
    "resolved_in": "2e0b9b1758d5fd2b91fe4e11e15b8df8bd5f4d17",
    "resolver_commit_num": 2101,
    "title": "makeQuantiles accesses remove_na which is not defined",
    "body": "",
    "labels": [],
    "comments": [
      "That's a deprecated function that's never been documented anywhere. I'm just going to get rid of it\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 10,
    "deletions": 58,
    "changed_files_list": [
      "pandas/stats/misc.py",
      "pandas/tools/tile.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1475,
    "reporter": "lesteve",
    "created_at": "2012-06-15T12:01:41+00:00",
    "closed_at": "2012-06-21T17:03:18+00:00",
    "resolver": "wesm",
    "resolved_in": "32d73012285d1228665b05f01059bd82aea04ceb",
    "resolver_commit_num": 2136,
    "title": "beta 0.8.0b2: creating timeseries with index in year 1400 yields wrong timestamp",
    "body": "There seems to be some kind of wrap-around phenomenon if your index is too far away from now. For example if I create a timeseries with an index in 1400, the timeseries I get back has an index which is some time in 1984:\n\n\n\nIt doesn't seem to be a display issue because I can do:\n\n\n\nI tested I could get the same behaviour both using 32-bit and 64-bit python on windows. It seems to work fine for 0.7.3:\n\n\n",
    "labels": [],
    "comments": [
      "Actually it seems to work fine when constructing the index from strings instead of datetimes:\n\n``` python\nIn [1]: import pandas\n\nIn [2]: ts = pandas.TimeSeries(index=['1400-02-01'], data = [1])\n\nIn [3]: ts\nOut[3]: 1400-02-01    1\n```\n\nand the numpy.datetime64 construction from datetime.datetime seems fine as well:\n\n``` python\nIn [5]: np.datetime64(datetime.datetime(1400,1,2))\nOut[5]: 1400-01-02 00:00:00\n```\n\nso the blame seems to be on pandas for this one.\n",
      "`DatetimeIndex`, which uses a nanosecond unit, doesn't support dates that far back in time. This was a deliberate design tradeoff in work revamping the time series processing infrastructure. You can, however, represent dates that far back using `PeriodIndex`. Could you tell me a bit more about your use case?\n",
      "The use case is the following: we are using the Boost.Date_time library in our C++ application. Timeseries created in our C++ application can be queried using python and we are looking into using pandas for the timeseries objects returned inside python instead of some bespoke python timeseries classes we are currently using.\n\nIn the Boost.Date_time library, 1400-Jan-01 00:00:00 is the minimum timestamp and 9999-Dec-31 23:59:59.999999 is the maximum timestamp. As such our timeseries can have any timestamps in between in principle, although our main use case of crazy (where crazy means very far away from our current times) timestamps is due to constant timeseries which have a single timestamp (1400-Jan-01). In most cases with crazy indices, timeseries have a single timestamp on 1400-Jan-01 and then a big gap until more reasonable indices come along.\n\nOut of interest, is the limitation in DatetimeIndex documented somewhere? In particular what are the minimum and maximum timestamp I can use with DatetimeIndex?\n",
      "I've just pushed some checks for out of bounds timestamps and documented the limitation here (with suggested workaround for representing pre-year 1677 and post-year 2262 dates):\n\nhttp://pandas.pydata.org/pandas-docs/dev/gotchas.html#timestamp-limitations\n",
      "Actually going to leave this open. Need to do more rigorous checking when converting a list or array of datetime.datetime to DatetimeIndex (your code example is still an issue)\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "commented",
      "commented",
      "subscribed",
      "commented",
      "commented",
      "closed",
      "reopened",
      "commented"
    ],
    "changed_files": 2,
    "additions": 14,
    "deletions": 1,
    "changed_files_list": [
      "pandas/src/datetime.pyx",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1480,
    "reporter": "wesm",
    "created_at": "2012-06-15T19:38:31+00:00",
    "closed_at": "2012-06-19T17:36:17+00:00",
    "resolver": "wesm",
    "resolved_in": "6a67e3bf2954a495f7b2c0c3aff4a23a34ec024d",
    "resolver_commit_num": 2122,
    "title": "Slicing with duplicate index values error",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 2,
    "additions": 18,
    "deletions": 0,
    "changed_files_list": [
      "pandas/core/indexing.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1481,
    "reporter": "wesm",
    "created_at": "2012-06-15T19:49:46+00:00",
    "closed_at": "2012-06-19T20:01:28+00:00",
    "resolver": "wesm",
    "resolved_in": "ae56d6f1d135e749c422ef7843ba4738d27e5ff9",
    "resolver_commit_num": 2123,
    "title": "Test at_time / between_time functions on tz-localized time series",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 151,
    "deletions": 210,
    "changed_files_list": [
      "pandas/core/series.py",
      "pandas/src/datetime.pyx",
      "pandas/tseries/index.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1484,
    "reporter": "saltantsolutions",
    "created_at": "2012-06-16T23:24:50+00:00",
    "closed_at": "2012-06-21T23:11:06+00:00",
    "resolver": "wesm",
    "resolved_in": "87d6da19b68c459573a15c911f1a827705767fe5",
    "resolver_commit_num": 2141,
    "title": "TestDataFramePlots.test_scatter fails with matplotlib v1.0.1",
    "body": "# Synopsis\n\nThe FreeBSD port for matplotlib has not yet been updated to v1.1.0. In the mean time, this test fails due to a default marker type ('.') that is unknown in matplotlib v1.0.1.\n# Details\n## Environment\n\n\n## How to reproduce\n\n\n## Code analysis\n\n[pandas/tests/test_graphics.py:252](#L252) does not specify a marker type.\n[pandas/tools/plotting.py:22](#L22) defines `'.'` as the default value for the `marker`\n[matplotlib/axes.py:5669-5682](#L5669-5682) list the marker types recognized in matplotlib v1.0.1.\n## Suggested resolution\n\nChoose a marker type available in matplotlib v1.0.1 (e.g. `'o'`).\n",
    "labels": [],
    "comments": [
      "I'll have a look. Would like `'.'` to be the default but should work around in earlier matplotlib.\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 8,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tools/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1485,
    "reporter": "wesm",
    "created_at": "2012-06-17T00:23:46+00:00",
    "closed_at": "2012-06-19T21:21:06+00:00",
    "resolver": "wesm",
    "resolved_in": "7b484e01b20f711a17703391db42a77856e8ef7d",
    "resolver_commit_num": 2126,
    "title": "DataFrame.fillna with axis=1 result unintuitive when passed dict",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 9,
    "deletions": 2,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1486,
    "reporter": "wesm",
    "created_at": "2012-06-17T00:34:34+00:00",
    "closed_at": "2012-06-19T20:54:46+00:00",
    "resolver": "wesm",
    "resolved_in": "7bc6455a47f0f6865fb839e5d6fa0c45bb6b8a62",
    "resolver_commit_num": 2125,
    "title": "fillna called with a Series not behave like a dict",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 8,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1491,
    "reporter": "turkeytest",
    "created_at": "2012-06-17T22:31:09+00:00",
    "closed_at": "2012-06-21T22:57:20+00:00",
    "resolver": "wesm",
    "resolved_in": "a6f553424b1aa8c433d6727d1622594ffe68ed26",
    "resolver_commit_num": 2139,
    "title": "DataFrame constructor (dict of tuples)",
    "body": "This might be an API change instead of an issue, but just in case, could you please confirm that this should crash?  \n\n\n\nwith this error: \n\n\n\nUsing a dictionary where the values are equal length tuples used to work in 0.7. \n\nThanks!\n",
    "labels": [],
    "comments": [
      "No, this is a bug. I'll fix it right now\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 9,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1493,
    "reporter": "wesm",
    "created_at": "2012-06-18T14:49:20+00:00",
    "closed_at": "2012-06-18T19:02:53+00:00",
    "resolver": "lodagro",
    "resolved_in": "6a98c3e586ca20cbd2707fb88d183937838c62d5",
    "resolver_commit_num": 35,
    "title": "Keywords not being passed on in DataFrame.boxplot",
    "body": "\n",
    "labels": [],
    "comments": [
      "Needs some more work, when using the vert=0 option, the labels are set on the x-axis iso y axis.\nJust passing along the keywords is not enough.\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "closed",
      "commented",
      "reopened"
    ],
    "changed_files": 1,
    "additions": 8,
    "deletions": 2,
    "changed_files_list": [
      "pandas/tools/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1496,
    "reporter": "wesm",
    "created_at": "2012-06-18T22:59:10+00:00",
    "closed_at": "2012-06-19T16:38:09+00:00",
    "resolver": "wesm",
    "resolved_in": "09ba4cea4c26344dfed3f92fe1915065aa0f9729",
    "resolver_commit_num": 2119,
    "title": "Improve date_range/bdate_range docstrings",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 33,
    "deletions": 9,
    "changed_files_list": [
      "pandas/tseries/index.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1498,
    "reporter": "wesm",
    "created_at": "2012-06-19T13:47:44+00:00",
    "closed_at": "2012-06-19T15:10:15+00:00",
    "resolver": "wesm",
    "resolved_in": "ed9e893b78e64a44057261f95d74c2779871ec8e",
    "resolver_commit_num": 2117,
    "title": "concat doesn't work around datetime64[ns] concatenate bug in NumPy 1.6",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 24,
    "deletions": 5,
    "changed_files_list": [
      "pandas/core/algorithms.py",
      "pandas/core/series.py",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1499,
    "reporter": "wesm",
    "created_at": "2012-06-19T15:14:07+00:00",
    "closed_at": "2012-06-19T16:33:55+00:00",
    "resolver": "wesm",
    "resolved_in": "ef8394582bf00b8089b4895b2e5beaa8799d9a6d",
    "resolver_commit_num": 2118,
    "title": "Series datetime64[ns] boxing issue",
    "body": "Boxing does not happen in `DatetimeEngine`\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 28,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/index.py",
      "pandas/src/datetime.pyx",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1503,
    "reporter": "petergx",
    "created_at": "2012-06-20T19:39:56+00:00",
    "closed_at": "2012-07-13T01:35:24+00:00",
    "resolver": "wesm",
    "resolved_in": "a66543bdbb86fd7fffb35ada285e9d9de4c185be",
    "resolver_commit_num": 2210,
    "title": "Improve repr for weekly Periods",
    "body": "It's totally possible I don't understand how to_timestamp should be used with a Period. But, I have a hunch there is a bug here.\n\n\n\n(found in 0.8b2)\n",
    "labels": [],
    "comments": [
      "I should improve the `__repr__` for weekly to show the whole timespan. \n\n```\nIn [18]: p = Period('01-Jan-2012', 'W')\n\nIn [19]: p.asfreq('D', 's')\nOut[19]: Period('26-Dec-2011', 'D')\n\nIn [20]: p.asfreq('D', 'e')\nOut[20]: Period('01-Jan-2012', 'D')\n\nIn [21]: p.asfreq('H', 's')\nOut[21]: Period('26-Dec-2011 00:00', 'H')\n\nIn [22]: p.asfreq('H', 'e')\nOut[22]: Period('01-Jan-2012 23:00', 'H')\n```\n\nHowever, `p.to_timestamp(how='e')` should possibly raise an exception, have to think about it\n",
      "Ahh my bad. I think an improved `__repr__` would be cool. Tho.. my confusion initially stemmed from the fact:\n\n`p.start_time == p.end_time == p.to_timestamp()`\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented"
    ],
    "changed_files": 2,
    "additions": 8,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/src/plib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1504,
    "reporter": "mrjbq7",
    "created_at": "2012-06-20T21:29:22+00:00",
    "closed_at": "2012-07-12T16:50:35+00:00",
    "resolver": "wesm",
    "resolved_in": "2dcae6e99a4803fab4bec11a09daf9a5a3a7fa0f",
    "resolver_commit_num": 2204,
    "title": "Faster rolling_min/rolling_max",
    "body": "See #50\n\nUsing a `min-heap` and `max-heap` to implement the `rolling_min` and `rolling_max` functions seems to be about 30% faster than the current implementation.  \n\nSomething sort of like this:\n\n\n",
    "labels": [],
    "comments": [
      "bottleneck (http://pypi.python.org/pypi/Bottleneck) has a very good rolling min/max implementation that's even better than using a heap, should just co-opt that code\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 166,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/src/moments.pyx",
      "pandas/stats/moments.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1505,
    "reporter": "gerigk",
    "created_at": "2012-06-21T11:10:29+00:00",
    "closed_at": "2012-06-21T23:02:54+00:00",
    "resolver": "wesm",
    "resolved_in": "2c511445b27ca61dfb2a4e9ca36901c36cb6a891",
    "resolver_commit_num": 2140,
    "title": "itertuples converts int to float for all numeric rows.",
    "body": "\n\nmixed dtype row leaves ints untouched\n\n\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 12,
    "deletions": 2,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1506,
    "reporter": "lodagro",
    "created_at": "2012-06-21T12:25:26+00:00",
    "closed_at": "2012-06-22T14:29:11+00:00",
    "resolver": "wesm",
    "resolved_in": "9d2d4bfb9bcb7a3003ab423aa17e922e6b0cf29d",
    "resolver_commit_num": 2142,
    "title": "DataFrame.boxplot() NaN handling",
    "body": "In case of NaN, `DataFrame.boxplot()` produces unexpected plot (see below df2, column A boxplot).\nIt is matplotlib related (using version 1.1.0).\n\n\n\n[screenshot](-7EplwgT9ltk/T-MUlkumNxI/AAAAAAAAANg/lQPskQy56XQ/s800/issue1506_screenshot1.png)\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 4,
    "deletions": 3,
    "changed_files_list": [
      "pandas/tools/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1507,
    "reporter": "lodagro",
    "created_at": "2012-06-21T12:53:48+00:00",
    "closed_at": "2012-06-29T00:12:49+00:00",
    "resolver": "lodagro",
    "resolved_in": "2b6bb03f9e14e070d40ef42a301bba2f73b325b1",
    "resolver_commit_num": 39,
    "title": "DataFrameGroupBy.boxplot() should not relay to DataFrame.boxplot()",
    "body": "matplotlib `boxplot()` does not create a new figure by default (as `hist()` does).\nSince `DataFrameGroupby.boxplot()` relays boxplot to `DataFrame.boxplot()`, one ends up with one messy plot.\nSee [screenshot](-Ax6O9Be-VLQ/T-MYeSyB7VI/AAAAAAAAANs/fGzupMhexw8/s583/issue1507_screenshot1.png)\nEither  `DataFrameGroupby.boxplot()` should not relay to `DataFrame.boxplot()` and handle this or other option is to change behavior of `DataFrame.boxplot()` and open a new figure before calling matplotlib boxplot.\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 99,
    "deletions": 0,
    "changed_files_list": [
      "pandas/core/groupby.py",
      "pandas/tests/test_graphics.py",
      "pandas/tools/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1511,
    "reporter": "dalejung",
    "created_at": "2012-06-22T17:22:40+00:00",
    "closed_at": "2012-06-28T22:40:21+00:00",
    "resolver": "wesm",
    "resolved_in": "0a963045afba688ef0c1468315356cce52acda4d",
    "resolver_commit_num": 2161,
    "title": "Inconsistent Behavior of cut when labels=False",
    "body": "\n\nI recall that there always used to be an implicit out of range level. It seems like the second example still acts like the (-inf, 0] exists. \n\nI could test the ind for nan to handle this, but wanted to make sure it wasn't a bug.\n",
    "labels": [],
    "comments": [
      "Thanks for catching this bug, indeed was caused by a typo and insufficient test coverage. Pushing a fix shortly\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 7,
    "deletions": 1,
    "changed_files_list": [
      "pandas/tools/tests/test_tile.py",
      "pandas/tools/tile.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1513,
    "reporter": "scouredimage",
    "created_at": "2012-06-22T18:06:09+00:00",
    "closed_at": "2012-06-22T19:26:59+00:00",
    "resolver": "wesm",
    "resolved_in": "836ecab757b3b952e6599c8dcc6b6c761b27db7a",
    "resolver_commit_num": 2144,
    "title": "DataFrame.drop loses MultiIndex",
    "body": "\n\nMultiIndex is gone! (2,2) and (3,3) are now plain tuples\n",
    "labels": [],
    "comments": [
      "Thanks-- I'll take a look\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 12,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/index.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1518,
    "reporter": "wesm",
    "created_at": "2012-06-24T14:22:36+00:00",
    "closed_at": "2012-06-25T22:32:57+00:00",
    "resolver": "wesm",
    "resolved_in": "eb7a751fa3396d1e194f319128be7dd4513512b7",
    "resolver_commit_num": 2152,
    "title": "Time zone handling / strftime issues",
    "body": "Moving issue here:\n\n-read-csv-input-local-datetime-strings-tz-convert-to-utc\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 22,
    "deletions": 23,
    "changed_files_list": [
      "pandas/core/format.py",
      "pandas/src/datetime.pyx",
      "pandas/tests/test_reshape.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1529,
    "reporter": "nicolashery",
    "created_at": "2012-06-25T22:54:25+00:00",
    "closed_at": "2012-06-28T22:30:46+00:00",
    "resolver": "wesm",
    "resolved_in": "d8a0b4fd0d39dcac4fb7a2297bf25cc442e12729",
    "resolver_commit_num": 2160,
    "title": "Parse Excel from in-memory file object",
    "body": "We came across a situation where we had a file object representing Excel data (came from HTTP POST but I'm thinking it could also come from MongoDB for example), and would've liked to pass it directly to Pandas to parse (vs saving it to disk and passing path to Pandas).\n\nCould this be possible?\n\nI saw that xlrd had `file_contents` as a possible argument of `open_workbook`:\n-excel/xlrd/blob/master/xlrd/__init__.py#L385\n\nMaybe `ExcelFile` in Pandas could take `path_or_buffer` as argument, and pass along the correct one to xlrd.\n#L1133\n\nDon't know if that could work for openpyxl also.\n\nThoughts?\n\nThanks!\nNicolas\n",
    "labels": [],
    "comments": [
      "Have you tried passing a file-like object (e.g. `StringIO`)?\n",
      "Thanks for the quick reply!\n\nIt seems that `open_workbook` calls `open_workbook_xls` which in turn calls `biff2_8_load` (https://github.com/python-excel/xlrd/blob/master/xlrd/book.py#L549)\n\nThey all have the same logic which basically is:\n\n``` python\nif file_contents:\n   filestr = file_contents\nelse:\n   f = open(filename, 'rb')\n   filestr = f.read()\n   f.close()\n```\n\nSo they don't check a single variable to see what type it is (`string`, ie a path, or `StringIO`). I guess Pandas would have to do it for them.\n",
      "A fair point. the `ExcelFile` class doesn't accept file-like objects on closer inspection. I'll have a look (or you can make a pull request if you get energetic)\n",
      "Looked at openpyxl, and they do the check \"file object vs path\" themselves (https://github.com/chronossc/openpyxl/blob/master/openpyxl/reader/excel.py#L43). So would mean only doing it for xlrd.\n\nTell you what, I feel energetic, so I'll try and look into it :)\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 68,
    "deletions": 26,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1532,
    "reporter": "leonbaum",
    "created_at": "2012-06-26T15:31:03+00:00",
    "closed_at": "2012-06-28T18:21:47+00:00",
    "resolver": "wesm",
    "resolved_in": "ce4ae9314847ee028cd528c9c8fd35b652bb3bd9",
    "resolver_commit_num": 2157,
    "title": "Segmentation fault with MultiIndex join()",
    "body": "With pandas from the current master branch and python-3.2.2, I'm getting a seg fault with the following minimal example:\n\n\n",
    "labels": [],
    "comments": [
      "also on python 2.7.3\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 26,
    "deletions": 19,
    "changed_files_list": [
      "doc/source/v0.8.0.txt",
      "pandas/core/index.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1534,
    "reporter": "wesm",
    "created_at": "2012-06-26T19:02:59+00:00",
    "closed_at": "2012-06-28T18:10:31+00:00",
    "resolver": "wesm",
    "resolved_in": "6904dba610be11872b6d8a39814fdf59d89e6b32",
    "resolver_commit_num": 2156,
    "title": "MultiIndex.from_tuples issue",
    "body": "from mailing list cc @ruidc \n\n\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 17,
    "deletions": 3,
    "changed_files_list": [
      "pandas/core/index.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1538,
    "reporter": "wesm",
    "created_at": "2012-06-27T14:42:11+00:00",
    "closed_at": "2012-07-12T16:27:48+00:00",
    "resolver": "wesm",
    "resolved_in": "b46b0014e024b17442ef2ca417a6c8aaa38c8d98",
    "resolver_commit_num": 2203,
    "title": "Add print option to not sparsify hierarchical levels",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 34,
    "deletions": 5,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/format.py",
      "pandas/core/frame.py",
      "pandas/core/index.py",
      "pandas/tests/test_index.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1543,
    "reporter": "JWCornV",
    "created_at": "2012-06-27T20:29:22+00:00",
    "closed_at": "2016-06-14T21:22:16+00:00",
    "resolver": "adrienemery",
    "resolved_in": "bd66592d7d1c10d88749c9fe42f770ded5d6a0d3",
    "resolver_commit_num": 0,
    "title": "Support for bimonthly/weekly timerules",
    "body": "Any change of getting bi-monthly (1st and 15th) of the month, or bi-weekly (every two weeks) supported?\n\nI could probably do this.\n",
    "labels": [
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "I'd be happy to accept a patch for bi-monthly. Bi-weekly is already supported but not in \"anchored form\":\n\n```\nIn [2]: date_range('6/26/2012', periods=10, freq='2W-TUE')\nOut[2]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2012-06-26 00:00:00, ..., 2012-10-30 00:00:00]\nLength: 10, Freq: 2W-TUE, Timezone: None\n\nIn [3]: list(date_range('6/26/2012', periods=10, freq='2W-TUE'))\nOut[3]: \n[<Timestamp: 2012-06-26 00:00:00>,\n <Timestamp: 2012-07-10 00:00:00>,\n <Timestamp: 2012-07-24 00:00:00>,\n <Timestamp: 2012-08-07 00:00:00>,\n <Timestamp: 2012-08-21 00:00:00>,\n <Timestamp: 2012-09-04 00:00:00>,\n <Timestamp: 2012-09-18 00:00:00>,\n <Timestamp: 2012-10-02 00:00:00>,\n <Timestamp: 2012-10-16 00:00:00>,\n <Timestamp: 2012-10-30 00:00:00>]\n```\n",
      "(Edited)  The Bi-unit form (multiplied by two) is already [supported](http://pandas.pydata.org/pandas-docs/version/0.16.2/timeseries.html#dateoffset-objects) for any `DateOffset` object.  What you asking for is the Semi-unit form (divided by two), which (AFAIK) is not supported.  Having a semi-\\* form would be useful for many business applications.  \n",
      "Any word on if this will be included in an upcoming release? If not I can put together a patch for Semi-Monthly.\n\nI propose:\n\n`SM` for Semi-Monthly Month End (15th and last day of month)\n`SMS` for Semi-Monthly Month Start (1st and 15th)\n",
      "This would be a straightforward extension. patches are welcome.\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 7,
    "additions": 750,
    "deletions": 13,
    "changed_files_list": [
      "asv_bench/benchmarks/timeseries.py",
      "doc/source/timeseries.rst",
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/tseries/offsets.py",
      "pandas/tseries/tests/test_frequencies.py",
      "pandas/tseries/tests/test_offsets.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1547,
    "reporter": "fabianp",
    "created_at": "2012-06-29T09:17:25+00:00",
    "closed_at": "2012-06-29T14:54:50+00:00",
    "resolver": "wesm",
    "resolved_in": "07cea900e0385805b5987b07305d9144483232cd",
    "resolver_commit_num": 2165,
    "title": "cannot use trace routines with Pandas",
    "body": "Because of the way local variables are used in read_cvs, setting a tracing routine will fail at that point. Here is a test case with an arbitrarily simple tracing routine:\n\n\n\nthis fails with a traceback\n\n\n\nSome real-world utilities that are affected because of this are profile (but not cProfile) and memory_profiler. \n",
    "labels": [],
    "comments": [
      "Good point. The use of `locals()` saves typing unpleasantness but is a bit too magical for tools like you describe\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 41,
    "deletions": 8,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1550,
    "reporter": "cgohlke",
    "created_at": "2012-06-30T04:39:12+00:00",
    "closed_at": "2012-07-12T16:14:55+00:00",
    "resolver": "wesm",
    "resolved_in": "c14a47034714ed7c789d8c448f3f82c388811c90",
    "resolver_commit_num": 2201,
    "title": "Pandas 0.8 test errors of win-amd64-py3.2",
    "body": "Test output: ~gohlke/pythonlibs/tests/20120629-win-amd64-py3.2/pandas_test.txt\nTest environment: ~gohlke/pythonlibs/tests/20120629-win-amd64-py3.2/versions.txt\n",
    "labels": [],
    "comments": [
      "Darn, that's annoying. I never ran Py3 tests with statsmodels installed.  If it doesn't cause you any problems, I'll push this off to the next minor release (out within a few weeks).\n",
      "No problem, it can wait. I just wanted to report.\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented"
    ],
    "changed_files": 3,
    "additions": 3,
    "deletions": 3,
    "changed_files_list": [
      "pandas/sparse/frame.py",
      "pandas/stats/tests/test_fama_macbeth.py",
      "pandas/stats/tests/test_ols.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1551,
    "reporter": "choffstein",
    "created_at": "2012-06-30T20:46:32+00:00",
    "closed_at": "2012-07-12T14:42:38+00:00",
    "resolver": "wesm",
    "resolved_in": "e147e0ff076967e1d822b26c9e495e6d5c7868b4",
    "resolver_commit_num": 2198,
    "title": "intersection between DatetimeIndex does not perform element-wise intersection",
    "body": "When I have two dataframes, df1 and df2, and df1 is missing an index of df2, performing the intersection only seems to compute matching slices -- but doesn't take into account missing inner-elements.  For example, below, see that `2010-07-14 00:00:00` is missing from df1, but in df2.  The resulting index intersection, however, contains that index.  \n\n\n",
    "labels": [],
    "comments": [
      "A little more poking and prodding and it seems to actually be performing a union?  Looks like the doc-string was actually ripped from the union ... so maybe a copy-paste issue somewhere in the code?\n"
    ],
    "events": [
      "subscribed",
      "commented"
    ],
    "changed_files": 3,
    "additions": 13,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1556,
    "reporter": "timmie",
    "created_at": "2012-07-01T10:48:39+00:00",
    "closed_at": "2012-09-11T19:36:44+00:00",
    "resolver": "lodagro",
    "resolved_in": "a83e6917748bd30ed92244349bce0770f1eac23e",
    "resolver_commit_num": 52,
    "title": "doc: link dialeg definition to python docs",
    "body": "-docs/stable/io.html#io-dialect\n\nto\n\n#csv.Dialect\n\nusing intersphinx\n",
    "labels": [],
    "comments": [
      "please add tags: community, doc\n",
      "Could you lend a helping hand?\n",
      "sure.\nBut I am quite busy till mid end of this week...\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "commented",
      "subscribed",
      "commented"
    ],
    "changed_files": 2,
    "additions": 6,
    "deletions": 7,
    "changed_files_list": [
      "doc/source/conf.py",
      "doc/source/io.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1561,
    "reporter": "wesm",
    "created_at": "2012-07-03T01:33:03+00:00",
    "closed_at": "2012-07-11T23:22:23+00:00",
    "resolver": "wesm",
    "resolved_in": "005b264c9eb59e737bf60edfabd3850125a4d500",
    "resolver_commit_num": 2194,
    "title": "datetime64 bug arising in Series.set_value",
    "body": "from the mailing list\n\n\n",
    "labels": [],
    "comments": [
      "When I try this, I get no error and this as output:\n\n``` python\nIn [1]: from pandas import Series\n\nIn [2]: from datetime import datetime\n\nIn [3]: s = Series().set_value(datetime(2001,1,1),1.).set_value(datetime(2001,1,2),float('nan'))\n\nIn [4]: s\nOut[4]: \n978307200000000000      1\n2001-01-02 00:00:00   NaN\n```\n\nI'm using the latest pandas and numpy. Looks like something is up with setting the first label in an index.\n",
      "You must be using NumPy 1.7dev. On NumPy 1.6 `numpy.concatenate` has a bug with the `datetime64` dtype\n",
      "I am, yeah. Does it still? The first row isn't a datetime\n",
      "The bug is manifesting in a different way on 1.7\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 21,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/series.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1562,
    "reporter": "wesm",
    "created_at": "2012-07-03T14:09:57+00:00",
    "closed_at": "2012-07-12T00:04:27+00:00",
    "resolver": "wesm",
    "resolved_in": "0e5fc4449462f2b5615b6286485223cde3033637",
    "resolver_commit_num": 2197,
    "title": "Date slicing bug",
    "body": "This throws an exception\n\n-slicing-on-a-timeseries-seems-inconsistent-with-list-slicing\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 11,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/src/engines.pyx",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1568,
    "reporter": "ruidc",
    "created_at": "2012-07-04T16:08:40+00:00",
    "closed_at": "2012-07-11T23:08:29+00:00",
    "resolver": "wesm",
    "resolved_in": "873409da0f1cd9b2b21f9b8fddd61e101261053f",
    "resolver_commit_num": 2193,
    "title": "sum with level=0 of MultiIndex with Series length 1 failing",
    "body": " pandas.Series([10.0], index=pandas.MultiIndex.from_tuples([(2, 3)])).sum(level=0) #level=None works, series length > 1 works\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 9,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1571,
    "reporter": "manuteleco",
    "created_at": "2012-07-06T08:22:42+00:00",
    "closed_at": "2012-07-13T02:47:43+00:00",
    "resolver": "wesm",
    "resolved_in": "5a25499b6d171bc881a5796840d5f9b83a69070e",
    "resolver_commit_num": 2212,
    "title": "to_datetime poor performance parsing string datetimes",
    "body": "Hi,\n\nI want to convert to datetime64 a Series that contains datetimes as strings. The format is '%Y-%m-%d %H:%M:%S' ('2012-07-06 10:05:58', for instance).\n\nCasting the strings array into a datetime64 array in numpy (or using Series.astype(\"datetime64\")) is fast, but it transforms the datetimes according to the local timezone, which is not the behavior I want in this case. Pandas to_datetime function does the parsing right, but it is much slower.\n\nHowever, it is also possible to do the parsing right and fast with numpy by appending the \"+0000\" timezone suffix to every string before parsing/casting to datetime64. So I wonder, is there any reason why to_datetime() runs much slower than this approach?\n\nThanks and regards.\n\nSome sample code to illustrate the issue:\n\n\n",
    "labels": [],
    "comments": [
      "I ran your file on my box and I get a performance difference of about >2x.\n\nIn [7]: %timeit using_to_datetime(test_data)\n1 loops, best of 3: 4 s per loop\n\nIn [8]: %timeit faking_tz(test_data)\n1 loops, best of 3: 1.44 s per loop\n\nIn [9]: %timeit concat_gmt_tz(test_data)\n1 loops, best of 3: 1.74 s per loop\n\nI think it's because pandas is using dateutil internally while numpy uses its own parser that's faster:\n\nIn [10]: from numpy.core._mx_datetime_parser import datetime_from_string as p2\n\nIn [11]: from dateutil.parser import parse as p1\n\nIn [12]: %timeit test_data.apply(p1)\n1 loops, best of 3: 3.75 s per loop\n\nIn [13]: %timeit test_data.apply(p2)\n1 loops, best of 3: 1.22 s per loop\n\nI'll see whether we can convert pandas to use the faster date parsing code\n",
      "Sorry, I should have said it before, but I'm using the '1.8.0.dev-6a06466' version for numpy and  '0.8.0' for pandas. It seems like I get much different performances due to recent improvements in numpy.\n\nThe results I get are these:\nto_datetime(): 8.36494483948\nfaking tz: 0.0932590007782\nRatio: 89.6958445799\n\nto_datetime(): 8.36494483948\nconcat tz: 0.0312120437622\nRatio: 268.003752116\n\nI also run some simple tests using both numpy '1.8.0.dev-6a06466' and '1.6.2' to compare performance:\n\nIn [2]: import numpy as np\nIn [3]: from datetime import datetime\nIn [4]: DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S\"\nIn [5]: str_datetime = datetime.now().strftime(DATETIME_FORMAT)\nIn [7]: %timeit np.datetime64(str_datetime)\n\nThe results are:\n- '1.8.0.dev-6a06466':\n     100000 loops, best of 3: 3.83 us per loop\n- '1.6.2':\n     10000 loops, best of 3: 95.9 us per loop\n\n~25 times faster in the newer version\n\nSo parsing data with the current develpment version for numpy seems to be significantly faster than using to_datetime. Maybe it could be possible for to_datetime to make use of the new numpy improvements in the future, or maybe try to apply the same optimizations. It would be really nice to be able to use to_datetime with a performance similar to that offered by numpy.\n\nThanks again and regards.\n",
      "Thanks for the feedback!\nI'll keep the issue open until we improve the performance on to_datetime.\n",
      "It should be straightforward to optimize `to_datetime` for ISO8601 format (what you're describing). Currently it is very general and handles a lot more date formats than NumPy does. Will let you know when one of us gets a chance to work on it. \n",
      "I was able to optimize the ISO8601 case and bring down the parsing time on 20000 strings from 1.87 seconds to 22.1 ms (85x improvement). Do you think this is adequate? I don't think things can get all that much faster than this.\n\n```\nIn [4]: to_datetime(strings)\nOut[4]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2000-01-01 00:00:00, ..., 2002-04-13 07:00:00]\nLength: 20000, Freq: None, Timezone: None\n\nIn [5]: timeit to_datetime(strings)\n1 loops, best of 3: 1.87 s per loop\n\nIn [6]: exit\n18:54 ~/code/pandas  ((3824af1...))$ git checkout master\nPrevious HEAD position was 3824af1... BUG: override ndarray.tolist in Index for MultiIndex compat, close #1576\nSwitched to branch 'master'\nYour branch is ahead of 'origin/master' by 1 commit.\n\n...\n\n18:55 ~/code/pandas  (master)$ ipy\nPython 2.7.2 |EPD 7.1-2 (64-bit)| (default, Jul  3 2011, 15:17:51) \nType \"copyright\", \"credits\" or \"license\" for more information.\n\nIPython 0.13 -- An enhanced Interactive Python.\n?         -> Introduction and overview of IPython's features.\n%quickref -> Quick reference.\nhelp      -> Python's own help system.\nobject?   -> Details about 'object', use 'object??' for extra details.\n\nWelcome to pylab, a matplotlib-based Python environment [backend: Qt4Agg].\nFor more information, type 'help(pylab)'.\n\nIn [1]: paste\nrng = date_range('1/1/2000', periods=20000, freq='h')\nstrings = [x.strftime('%Y-%m-%d %H:%M:%S') for x in rng]\n## -- End pasted text --\n\nIn [2]: timeit to_datetime(strings)\n10 loops, best of 3: 22.1 ms per loop\n\nIn [3]: 1870 / 22.1\nOut[3]: 84.61538461538461\n```\n",
      "Wow, definitely it is a huge improvement. It's even faster than doing .astype(\"datetime64\"). These are the results I get now in terms of performance:\n\nto_datetime(): 0.0160160064697\nfaking tz: 0.0268998146057\nRatio: 0.595394678532\n\nto_datetime(): 0.0160160064697\nconcat tz: 0.00913701057434\nRatio: 1.75287161369\n\nHowever, I've noticed the result values are different now (actually, the asserts in my sample code fail). It seems like it is making a transformation of the datetimes according to the local timezone so, for instance, \"2012-01-01 00:00:00\" becomes \"2011-12-31 23:00:00\" in my timezone (CET). This is coherent with the results yielded by .astype(\"datetime64\") and np.array([...], dtype=\"datetime64\").\n- Before:\n  to_datetime([\"2012-01-01 00:00:00\"])   ->   \"2012-01-01 00:00:00\"\n  Series([\"2012-01-01 00:00:00\"]).astype(\"datetime64\")   ->   \"2011-12-31 23:00:00\"\n- Now:\n  to_datetime([\"2012-01-01 00:00:00\"])   ->   \"2011-12-31 23:00:00\"\n  Series([\"2012-01-01 00:00:00\"]).astype(\"datetime64\")   ->   \"2011-12-31 23:00:00\"\n\nFor my application I would like to have a timezone-agnostic parsing utility just like the older to_datetime, but maybe it makes more sense that to_datetime behaves like it does now, I don't really know. This is a discussion I'm not fit to get in ;).\n\nPlease, let me know what you think about this.\nThanks for the amazing work you are all doing and sorry for the trouble ;)\n",
      "I am able to reproduce the issue. I'll try to figure out a fix\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "subscribed",
      "commented",
      "referenced",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 14,
    "deletions": 2,
    "changed_files_list": [
      "pandas/src/datetime.pyx",
      "pandas/src/datetime/np_datetime_strings.c",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1572,
    "reporter": "njsmith",
    "created_at": "2012-07-06T12:39:44+00:00",
    "closed_at": "2012-07-11T23:55:08+00:00",
    "resolver": "wesm",
    "resolved_in": "3ce416bbf9153f30a043eb799ee9f5d93315158a",
    "resolver_commit_num": 2196,
    "title": "DataFrame.__init__(..., dtype=dt) makes unnecessary copies",
    "body": "If I have a DataFrame `df` with homogenous type, then `DataFrame(df)` returns a view on the original dataframe. `DataFrame(df, dtype=current_type)` should be identical; but, instead, it makes an unnecessary copy.\n\n\n\nThe same thing seems to happen in the input is an ndarray -- `DataFrame(arr)` returns a view, `DataFrame(arr, dtype=arr.dtype)` returns a copy.\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 27,
    "deletions": 6,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1576,
    "reporter": "changhiskhan",
    "created_at": "2012-07-06T21:53:23+00:00",
    "closed_at": "2012-07-11T22:29:32+00:00",
    "resolver": "wesm",
    "resolved_in": "3824af12fb41ef6b488cde670daa90a751931f2e",
    "resolver_commit_num": 2191,
    "title": "Index.tolist returns empty list for MultiIndex",
    "body": "In [18]: idx\nOut[18]: \nMultiIndex\n[(0.12609189981717644, 1.103678121647419)\n (0.34279706720294867, -0.7021022763765951)\n (1.4611933034915323, -0.9892166873678242)\n (-1.8708717491582252, 1.0296013286162102)\n\nIn [19]: len(idx.tolist())\nOut[19]: 0\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 11,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/index.py",
      "pandas/tests/test_index.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1580,
    "reporter": "danse",
    "created_at": "2012-07-07T20:22:59+00:00",
    "closed_at": "2012-07-12T22:34:57+00:00",
    "resolver": "wesm",
    "resolved_in": "9a8ad655d1105c6ea381ba02017439819bdfd005",
    "resolver_commit_num": 2195,
    "title": "Weird results grouping data by day",
    "body": "Hi I was feeding pandas (0.8.0rc2) with dates and found some errors. The\namounts from following csv file are grouped by date, but the sums for some days\nare wrong:\n\n2011-02-02 resulting: 0   correct: 40\n2011-08-21 resulting: 3   correct: 133\n2012-10-22 resulting: 157 correct: 27\n\nThis is the script I am running:\n\n\n\nMaybe I'm using the time series methods in a wrong way.\n\nThe file with data is not too long, it is hosted here: \n",
    "labels": [],
    "comments": [
      "This is weird because all other sums look correct. Notice that the 2011-08-21 date has an entry out of order in the .csv file, but this is why I sort the frame before resampling.\n\nFor the 2012-02-02 entry, the behaviour is as such: the correct value (40) is shifted to the next day, and from that day to the end of the file, all entries are shifted by one day.\n",
      "I am using python 3.2.3\n",
      "By default, DataFrame.sort is NOT inplace.\n\ntry f.sort().resample('D', how='sum')\n",
      "This is definitely a bug; the resampling code was not checking for monotonicity (sortedness) in the data, thus the bug. I'm adding a check (and sorting if not), and this problem goes away. \n",
      "Thanks, using sorting not in place fixed the most of the errors, but the shift in resampling at a same point still remains. I reduced the data file to few rows. Processing the following rows:\n\n```\namount,date\n370,2012-02-01T00:00:00\n0,2012-02-01T22:27:48.633911\n20,2012-02-02T13:27:24.828871\n20,2012-02-02T21:35:41.482386\n20,2012-02-04T12:13:37.426859\n20,2012-02-04T21:42:11.164113\n200,2012-02-05T18:48:24.171116\n50,2012-02-06T19:11:07.339103\n18,2012-02-07T20:11:59.232420\n21,2012-02-11T09:16:25.231366\n40,2012-02-11T09:16:44.850074\n35,2012-02-13T00:28:00.666619\n40,2012-02-13T20:16:40.714301\n```\n\nWith the following script:\n\n```\nimport sys\nimport pandas\n\nf=pandas.read_csv(sys.stdin, index_col=1, parse_dates=True)\nf=f.sort().resample('D', how='sum')\nf['amount'] = f['amount'].fillna(0)\nf.to_csv(sys.stdout)\n```\n\nThe result is:\n\n```\n,amount\n2012-02-01 00:00:00,370.0\n2012-02-02 00:00:00,0.0\n2012-02-03 00:00:00,40.0\n2012-02-04 00:00:00,0.0\n2012-02-05 00:00:00,40.0\n2012-02-06 00:00:00,200.0\n2012-02-07 00:00:00,50.0\n2012-02-08 00:00:00,18.0\n2012-02-09 00:00:00,0.0\n2012-02-10 00:00:00,0.0\n2012-02-11 00:00:00,0.0\n2012-02-12 00:00:00,61.0\n2012-02-13 00:00:00,0.0\n2012-02-14 00:00:00,75.0\n```\n\nSo from 2012-02-02 all the sums are shifted by one day. I do not have permission to reopen the issue; should I fill in a new one?\n",
      "I think what you're really looking for is one of:\n\n```\nIn [9]: f.sort().resample('D', how='sum', kind='period')\nOut[9]: \n             amount\n01-Feb-2012     370\n02-Feb-2012      40\n03-Feb-2012     NaN\n04-Feb-2012      40\n05-Feb-2012     200\n06-Feb-2012      50\n07-Feb-2012      18\n08-Feb-2012     NaN\n09-Feb-2012     NaN\n10-Feb-2012     NaN\n11-Feb-2012      61\n12-Feb-2012     NaN\n13-Feb-2012      75\n```\n\nor \n\n```\nIn [11]: from pandas.tseries.tools import normalize_date\n\nIn [12]: f.groupby(normalize_date).sum()\nOut[12]: \n            amount\n2012-02-01     370\n2012-02-02      40\n2012-02-04      40\n2012-02-05     200\n2012-02-06      50\n2012-02-07      18\n2012-02-11      61\n2012-02-13      75\n```\n\nThe thing about the resampling algorithm is that it segments the data by bin edges, then has to assign a label to each bin. This actually gives you what you want as timestamps:\n\n```\nIn [16]: f.sort().resample('D', how='sum', label='left', closed='left')\nOut[16]: \n            amount\n2012-02-01     370\n2012-02-02      40\n2012-02-03     NaN\n2012-02-04      40\n2012-02-05     200\n2012-02-06      50\n2012-02-07      18\n2012-02-08     NaN\n2012-02-09     NaN\n2012-02-10     NaN\n2012-02-11      61\n2012-02-12     NaN\n2012-02-13      75\n```\n",
      "Thanks, kind=\"period\" will be perfect. normalize_data is not a solution because I need to fill the gaps. I prefer using kind=\"period\" rather than label left, closed left. I thought that it was a bin issue, but I was misleaded by the fact that the shift happened at a precise point in data. However on original data everything is correct using kind period. Many thanks! :)\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "commented",
      "commented",
      "subscribed",
      "commented",
      "subscribed",
      "closed",
      "commented",
      "reopened",
      "commented"
    ],
    "changed_files": 3,
    "additions": 18,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1581,
    "reporter": "wesm",
    "created_at": "2012-07-08T16:38:23+00:00",
    "closed_at": "2012-07-21T14:01:59+00:00",
    "resolver": "wesm",
    "resolved_in": "564f66f256adbb3e7ba51d284d50f341e2d44873",
    "resolver_commit_num": 2234,
    "title": "No datetime64 DataFrame column conversion of datetime.datetime with tzinfo",
    "body": "inspired by: -object-to-daterange/11384801#11384801\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 42,
    "deletions": 5,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/src/datetime.pyx",
      "pandas/tseries/tests/test_timezones.py",
      "pandas/tseries/tools.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1582,
    "reporter": "wesm",
    "created_at": "2012-07-08T16:44:45+00:00",
    "closed_at": "2012-07-11T20:52:56+00:00",
    "resolver": "wesm",
    "resolved_in": "58b5afa6966f97e516e914939ad668c4b52f9639",
    "resolver_commit_num": 2185,
    "title": "to_panel issues with integers",
    "body": "Case 1 (i'll look for the other)\n\n-pandas-to-panel-from-dataframe-returns-weird-numbers-for-binary-variab\n\npm10 = pds.read_csv('pm10.csv', index_col = [0,1], parse_dates=True)\npanel_exog = pm10.to_panel()['pass_ind']\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 12,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/reshape.py",
      "pandas/tests/test_panel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1583,
    "reporter": "wesm",
    "created_at": "2012-07-08T18:52:59+00:00",
    "closed_at": "2012-09-18T15:31:28+00:00",
    "resolver": "wesm",
    "resolved_in": "9c9f4cb0e7d1563988cb1543a28a9138ddb26535",
    "resolver_commit_num": 2398,
    "title": "DataFrame.to_csv with no \"leading comma\" option",
    "body": "cf -a-dataframe-in-pandas-for-output-to-csv\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 39,
    "deletions": 20,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1584,
    "reporter": "wesm",
    "created_at": "2012-07-08T19:27:44+00:00",
    "closed_at": "2012-07-13T03:03:22+00:00",
    "resolver": "wesm",
    "resolved_in": "9a0e52a7bdedf4c66e2a0d79dec06f8fe05cc830",
    "resolver_commit_num": 2213,
    "title": "Add option to disable adjustment factor in EWMA",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 32,
    "deletions": 20,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/src/moments.pyx",
      "pandas/stats/moments.py",
      "pandas/stats/tests/test_moments.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1585,
    "reporter": "mcobzarenco",
    "created_at": "2012-07-08T22:35:49+00:00",
    "closed_at": "2012-07-11T22:25:00+00:00",
    "resolver": "wesm",
    "resolved_in": "060ae8fdfafce9e15e730c1bf1ce11407d01cd71",
    "resolver_commit_num": 2190,
    "title": "Cannot slice a SparseDataFrame's columns.",
    "body": "Example:\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 23,
    "deletions": 12,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/sparse/frame.py",
      "pandas/sparse/tests/test_sparse.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1588,
    "reporter": "wesm",
    "created_at": "2012-07-09T19:47:42+00:00",
    "closed_at": "2012-07-09T20:19:52+00:00",
    "resolver": "wesm",
    "resolved_in": "0cd21ab5dced1ae48bca47ffe7d9b636b431ff3d",
    "resolver_commit_num": 2173,
    "title": "Resampling bug",
    "body": "This blows up inside the Cython binning method:\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 13,
    "deletions": 6,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/frequencies.py",
      "pandas/tseries/offsets.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1589,
    "reporter": "wesm",
    "created_at": "2012-07-09T21:26:43+00:00",
    "closed_at": "2012-07-11T21:50:46+00:00",
    "resolver": "wesm",
    "resolved_in": "772e39a9386404c764694d0393fa35df7cace171",
    "resolver_commit_num": 2188,
    "title": "Series/DataFrame.rank broken on all integer data",
    "body": "int64 should alias to generic, or perhaps generate a specialized int64 version\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 19,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/algorithms.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1591,
    "reporter": "petergx",
    "created_at": "2012-07-09T23:29:55+00:00",
    "closed_at": "2012-07-11T21:12:40+00:00",
    "resolver": "wesm",
    "resolved_in": "3b0f6985cf32147aefa93369c953a5d943d4735c",
    "resolver_commit_num": 2187,
    "title": "Cannot resample with 'D' and tz aware timeseries",
    "body": "\n\nyields..\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 7,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1596,
    "reporter": "lenolib",
    "created_at": "2012-07-10T16:08:49+00:00",
    "closed_at": "2012-07-11T22:16:42+00:00",
    "resolver": "wesm",
    "resolved_in": "77b49b909cc693bf7503cf8a23810c97538c5324",
    "resolver_commit_num": 2189,
    "title": "Resampling with how=list_of_funcs not returning dataframe",
    "body": "I would suggest that the observed behaviour is not the expected when passing a how=sequence of functions in resample.\nI would like to get a dataframe back where each column corresponds to applying each function to each time interval.\nIf you insert a value into the series, for example at 2012-06-12 01:01:00, you get that behaviour.\n\nIn [54]: stamps = [p for p in pd.DatetimeIndex(freq='60Min',start=pd.datetime(2012,6,12),periods=4)]\n\nIn [55]: stamps\nOut[55]: \n[Timestamp: 2012-06-12 00:00:00,\n Timestamp: 2012-06-12 01:00:00,\n Timestamp: 2012-06-12 02:00:00,\n Timestamp: 2012-06-12 03:00:00]\n\nIn [56]: series = pd.TimeSeries([1,2,3,4], index=stamps)\n\nIn [57]: series\nOut[57]: \n2012-06-12 00:00:00    1\n2012-06-12 01:00:00    2\n2012-06-12 02:00:00    3\n2012-06-12 03:00:00    4\n\nIn [58]: series.resample( '20Min', how=(np.mean,np.sum) )\nOut[58]: \n2012-06-12 00:00:00     1\n2012-06-12 00:20:00   NaN\n2012-06-12 00:40:00   NaN\n2012-06-12 01:00:00     2\n2012-06-12 01:20:00   NaN\n2012-06-12 01:40:00   NaN\n2012-06-12 02:00:00     3\n2012-06-12 02:20:00   NaN\n2012-06-12 02:40:00   NaN\n2012-06-12 03:00:00     4\nFreq: 20T\n\nIn [59]: \n",
    "labels": [],
    "comments": [
      "For my purposes, a temporary fix consists of shifting the last entry a microsecond before the even hour:\n\n> > > series[:-1].append(pd.Series(series[-1],index=[series.index[-1]-relativedelta(microseconds=1)])).resample( '20Min', how=(np.mean,np.size) )\n> > >                      mean  size\n> > > 2012-06-12 00:00:00     1     1\n> > > 2012-06-12 00:20:00   NaN     0\n> > > 2012-06-12 00:40:00   NaN     0\n> > > 2012-06-12 01:00:00     2     1\n> > > 2012-06-12 01:20:00   NaN     0\n> > > 2012-06-12 01:40:00   NaN     0\n> > > 2012-06-12 02:00:00     3     1\n> > > 2012-06-12 02:20:00   NaN     0\n> > > 2012-06-12 02:40:00   NaN     0\n> > > 2012-06-12 03:00:00     4     1\n",
      "The issue is that you are not aggregating:\n\n```\nIn [12]: series\nOut[12]: \n2012-06-12 00:00:00    1\n2012-06-12 01:00:00    2\n2012-06-12 02:00:00    3\n2012-06-12 03:00:00    4\n\nIn [13]: series.resample('2h', how=(np.mean, np.sum))\nOut[13]: \n                     mean  sum\n2012-06-12 00:00:00   1.0    1\n2012-06-12 02:00:00   2.5    5\n2012-06-12 04:00:00   4.0    4\n```\n\nI'll see if there's a way for me to change the behavior in the upsampling case\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "commented",
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 36,
    "deletions": 7,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/generic.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_offsets.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1601,
    "reporter": "wesm",
    "created_at": "2012-07-10T19:22:58+00:00",
    "closed_at": "2012-07-11T18:57:09+00:00",
    "resolver": "wesm",
    "resolved_in": "d4274d59fa625a1c0796285940371c910287867e",
    "resolver_commit_num": 2182,
    "title": "Year partial string indexing doesn't work right with freq='M' PeriodIndex",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 57,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1603,
    "reporter": "wesm",
    "created_at": "2012-07-10T20:12:13+00:00",
    "closed_at": "2012-07-11T17:57:19+00:00",
    "resolver": "wesm",
    "resolved_in": "9b67ac0b442e0329e6ed52165d2d168df32159c5",
    "resolver_commit_num": 2181,
    "title": "Panel .ix indexing bug",
    "body": "Major axis is DatetimeIndex\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 14,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/panel.py",
      "pandas/tests/test_panel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1606,
    "reporter": "wesm",
    "created_at": "2012-07-11T15:09:47+00:00",
    "closed_at": "2012-07-11T20:40:25+00:00",
    "resolver": "wesm",
    "resolved_in": "ca88112618ee6bbee827f922fd6c5dd1d157b7d4",
    "resolver_commit_num": 2184,
    "title": "Sparsify console output issue",
    "body": "Should not have that AAA hole\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 14,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/index.py",
      "pandas/tests/test_index.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1607,
    "reporter": "wesm",
    "created_at": "2012-07-11T17:41:35+00:00",
    "closed_at": "2012-07-11T19:39:40+00:00",
    "resolver": "wesm",
    "resolved_in": "9c6eadf6fa56fbde8fae4a26131cb28525df7ec5",
    "resolver_commit_num": 2183,
    "title": "Copy 0.7.0 what's new note re: integer indexes to gotchas page",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 72,
    "deletions": 17,
    "changed_files_list": [
      "doc/data/iris.data",
      "doc/source/gotchas.rst",
      "doc/source/indexing.rst",
      "pandas/tools/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1611,
    "reporter": "manuteleco",
    "created_at": "2012-07-12T08:36:49+00:00",
    "closed_at": "2012-07-12T15:12:29+00:00",
    "resolver": "wesm",
    "resolved_in": "221ab06f54ff39137e5a3881962f5b09a54c1815",
    "resolver_commit_num": 2199,
    "title": "Misleading __str__/__repr__ for DatatimeIndex objects with less than 3 items",
    "body": "Hi,\n\nthis isn't such a big issue, but I guess it's worth reporting anyway.\n\nI'm using the development version '0.8.1.dev-0e5fc44'. DatetimeIndex `__str__` and `__repr__` methods always return a string that contains a list-like representation of the contents in this fashion: [first_item, ..., last_item].\nThat format is used even when the DatetimeIndex has 1 or 2 elements, which should be represented like [only_item] and [first_item, last_item] respectively.\n\nExample:\n\n\n\nThanks and regards.\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 20,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1615,
    "reporter": "jseabold",
    "created_at": "2012-07-12T21:21:14+00:00",
    "closed_at": "2012-08-10T16:11:15+00:00",
    "resolver": "wesm",
    "resolved_in": "a2cd13abd114c2586342d2a00105a7605dfa7011",
    "resolver_commit_num": 2260,
    "title": "Trouble with factors when there are nans",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 25,
    "deletions": 12,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/rpy/common.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1620,
    "reporter": "manuteleco",
    "created_at": "2012-07-14T10:04:40+00:00",
    "closed_at": "2012-07-21T16:07:53+00:00",
    "resolver": "wesm",
    "resolved_in": "ae70acc7d63dc578a38e97add17d79067b655aea",
    "resolver_commit_num": 2233,
    "title": "__repr__ wrong column alignment with non-ascii characters",
    "body": "Hi,\n\nit seems that when DataFrame, Series and maybe other objects contain non-ascii characters inside non-unicode strings the `__repr__` method is not able to give the correct column alignment to its values. However, we see that this issue does not affect unicode strings. I'm using pandas '0.8.1.dev-70c3deb' in a Linux box.\n\nSample code:\n\n\n\nThis results in:\n\n\n\nThanks and regards.\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed",
      "assigned",
      "subscribed",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 27,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/format.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1622,
    "reporter": "wesm",
    "created_at": "2012-07-14T22:57:43+00:00",
    "closed_at": "2012-07-15T23:24:51+00:00",
    "resolver": "wesm",
    "resolved_in": "8cc9826b237826a0eeb7c27e97edcc17edbd6ae1",
    "resolver_commit_num": 2222,
    "title": "Bug resampling quarterly period data to annual",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 13,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/frequencies.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1624,
    "reporter": "wesm",
    "created_at": "2012-07-15T20:58:38+00:00",
    "closed_at": "2012-07-15T23:08:12+00:00",
    "resolver": "wesm",
    "resolved_in": "dcd3c57ef44aceeea5c97849c9e9ada38928fb9d",
    "resolver_commit_num": 2221,
    "title": "DatetimeIndex unnecessarily copies int64 array",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 19,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1628,
    "reporter": "idoko",
    "created_at": "2012-07-16T07:12:22+00:00",
    "closed_at": "2012-07-21T14:01:59+00:00",
    "resolver": "wesm",
    "resolved_in": "0da0066c2687898656bbf509ffd42f14b1c0a727",
    "resolver_commit_num": 2231,
    "title": "merge with empty DataFrame IndexError",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 29,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/src/join.pyx",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1644,
    "reporter": "wesm",
    "created_at": "2012-07-19T20:09:52+00:00",
    "closed_at": "2012-07-19T21:38:47+00:00",
    "resolver": "wesm",
    "resolved_in": "edaad31d9e7fafe6cea0b3482c7ee6d6a9ff8a84",
    "resolver_commit_num": 2226,
    "title": "Unconverted 'name' of Series object from DataFrame of time series row",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 24,
    "deletions": 5,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1645,
    "reporter": "wesm",
    "created_at": "2012-07-19T20:11:57+00:00",
    "closed_at": "2012-07-19T21:21:46+00:00",
    "resolver": "wesm",
    "resolved_in": "6a26249ab9426ba12e11616b58fdcb25d2210b6a",
    "resolver_commit_num": 2225,
    "title": "Business month start (BMS) bug",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 58,
    "deletions": 84,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/index.py",
      "pandas/tseries/offsets.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1646,
    "reporter": "wesm",
    "created_at": "2012-07-19T20:13:12+00:00",
    "closed_at": "2012-07-21T14:01:59+00:00",
    "resolver": "wesm",
    "resolved_in": "57538161b058560df05939db836d93714a896b55",
    "resolver_commit_num": 2230,
    "title": "Interpolate with method='values' and DatetimeIndex fails",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 31,
    "deletions": 6,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/series.py",
      "pandas/io/tests/test_yahoo.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1647,
    "reporter": "wesm",
    "created_at": "2012-07-19T20:13:44+00:00",
    "closed_at": "2012-07-19T21:49:13+00:00",
    "resolver": "wesm",
    "resolved_in": "01639307a3abc19b58ebd9e91d9b28744fe565a6",
    "resolver_commit_num": 2227,
    "title": "String timestamp aliases don't work with tz-aware data",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 11,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1649,
    "reporter": "tkf",
    "created_at": "2012-07-19T20:59:23+00:00",
    "closed_at": "2012-07-20T15:38:11+00:00",
    "resolver": "wesm",
    "resolved_in": "f5fb7096884831b9cb885c5c5a62c6c7ae6617fd",
    "resolver_commit_num": 2229,
    "title": "A bug in pandas.concat when the given dict contains None",
    "body": "\n\nChecked with the current master e1129b11b1fe0016748d3d279000a2a71db8dee5 and 0.7.3\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 28,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1656,
    "reporter": "wesm",
    "created_at": "2012-07-21T17:19:33+00:00",
    "closed_at": "2012-07-22T19:13:24+00:00",
    "resolver": "wesm",
    "resolved_in": "3839f647508f12046d720e1eccf3cedebfeaa7ab",
    "resolver_commit_num": 2241,
    "title": "Better vectorized string slicing / get notation",
    "body": "like:\n\n`df.col.str[:10]`\n\nversus current\n\n`df.col.str.slice(slice(0, 10))`\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 2,
    "additions": 22,
    "deletions": 3,
    "changed_files_list": [
      "pandas/core/strings.py",
      "pandas/tests/test_strings.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1659,
    "reporter": "wesm",
    "created_at": "2012-07-23T19:18:32+00:00",
    "closed_at": "2012-07-23T19:32:42+00:00",
    "resolver": "wesm",
    "resolved_in": "d6a99afe5d70c48a4b6f5d6c7fa31a2e390a7792",
    "resolver_commit_num": 2247,
    "title": "Add flags option to all string methods accepting regex string",
    "body": "Methods like findall/match need to be able to take `re.IGNORECASE`\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 70,
    "deletions": 47,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/strings.py",
      "pandas/tests/test_strings.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1661,
    "reporter": "wesm",
    "created_at": "2012-07-23T19:43:45+00:00",
    "closed_at": "2012-08-12T18:13:54+00:00",
    "resolver": "wesm",
    "resolved_in": "cf673a485df55a929e57cdb6ae6a2bd4ff19f7bc",
    "resolver_commit_num": 2283,
    "title": "Add option to Series.map for NA handling",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 4,
    "additions": 62,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/series.py",
      "pandas/src/inference.pyx",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1663,
    "reporter": "wesm",
    "created_at": "2012-07-23T23:23:29+00:00",
    "closed_at": "2017-04-20T01:02:13+00:00",
    "resolver": "jnothman",
    "resolved_in": "1b52b1218e973382ea958084c47c3f33eb0ed40c",
    "resolver_commit_num": 0,
    "title": "Styling in DataFrame.to_excel",
    "body": "from mailing list\n\n\n",
    "labels": [
      "Good as first PR"
    ],
    "comments": [
      "openpyxl supports styles but afaik you can't use them with the optimized writer than pandas uses by default. So instead of using sheet.append_row(row) one has to access each cell independently.\nYou can set the column width by column but attributes like colour or number format have to be set by cell and this slows everything down dramatically.\nIt's probably not too hard to add faster \"by column\" styles to openpyxl but unfortunately the project is very inactive (I added cell comments in May and never got any feedback for my pull request).\n",
      "I have been looking into this as well recently, and found [this blog post](http://informedguess.wordpress.com/2013/01/13/pandas-to-excel-with-styles/) and [related gist](https://gist.github.com/dmvianna/4602492) on the matter. \n\n@gerigk can you add a link to your PR?  I can't find it.\n\n@wesm interested in a PR on this?  What does it need to do to be accepted?\n",
      "@aflaxman see this as well: https://github.com/pydata/pandas/pull/7565\n\nalways interested in a PR!\n\npls write tests / code / update doc-strings\n\nsee here: https://github.com/pydata/pandas/wiki\n",
      "Ha ha, I just looked at the day and month, not the year.  Does #7565 expose something analogous to the style parameter in https://gist.github.com/dmvianna/4602492#file-xlpandas2-py-L50 ?  I couldn't find it with a quick search.\n",
      "excel has multiple engines (xlsxwriter, xlwt, openpyxl), so is different for each. \n",
      "closed by using this PR: https://github.com/pydata/pandas/pull/7565\n",
      "@jreback #7565 didn't really cover this, still need to decide how we'd actually want to expose styling, I'm thinking per-column styles could get us 90% of the way there. @aflaxman - if you have ideas for how an API ought to work, please feel free to comment here or in a new issue.\n",
      "@jtratner sure\n",
      "For my purposes, it would get 90% of the way to have a few defaults for highlighting the first row and/or the first column, and bonus points for zebra-striping a la excel.  But the case that inspired me to comment on this ticket was a series of tables where there were one or two cells that I need to highlight, and for that I think some sort of table-painter interface would be best.  Pandas already has plenty of functionality for this, so maybe just allowing an optional style dict in the `.to_excel` functions, e.g.\n\n```\ndf.to_excel(path, style=style_df)\n```\n\nwhere `style_df.shape = df.shape`, and non-empty entries of `style_df` are style dicts a la the examples here https://github.com/pydata/pandas/pull/7565#issuecomment-54767851\n\nI'll try it out and report back here.\n",
      "Here is [a notebook with a start in the direction I am thinking](http://nbviewer.ipython.org/gist/aflaxman/962619c69f07a61152de).  It makes this:\n![image](https://cloud.githubusercontent.com/assets/51236/4774674/8810c952-5baf-11e4-9c1b-1780fa945df0.png)\n\nComments welcome!\n",
      "cc @neirbowj\n\nthis is using the new openpyxl2 stuff?\n",
      "Yup, it sure looks that way. :smile: \n",
      "yes, speaking of which, is the style dictionary documented any more than in the #7565 comments now?\n",
      "@aflaxman: No, it is not. However, the short, short version of the docs would go something like this: any keyword or literal value that the native openpyxl v2 style interface accepts, the pandas style dict should accept as well. Symbolic constants are the main exception that I can think of off hand (e.g. `{'color':'RED'}`).\n\nSo, for example, the default font, per [the openpyxl docs](https://openpyxl.readthedocs.org/en/latest/styles.html), would look like this:\n\n``` python\n\"\"\"\"\nExcerpt --\n    font=Font(name='Calibri',\n        size=11,\n        bold=False,\n        italic=False,\n        vertAlign=None,\n        underline='none',\n        strike=False,\n        color='FF000000')\n\"\"\"\"\nfont = {\n    'font': {\n        'name': 'Calibri',\n        'size': 11,\n        'bold': False,\n        'italic': False,\n        'vertAlign': None,\n        'underline': 'none',\n        'strike': False,\n        'color': 'FF000000',\n    }\n}\n```\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "cross-referenced",
      "labeled",
      "commented",
      "closed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "reopened",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 13,
    "additions": 1670,
    "deletions": 380,
    "changed_files_list": [
      ".gitignore",
      "doc/source/_static/style-excel.png",
      "doc/source/style.ipynb",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/frame.py",
      "pandas/io/formats/common.py",
      "pandas/io/formats/css.py",
      "pandas/io/formats/excel.py",
      "pandas/io/formats/format.py",
      "pandas/io/formats/style.py",
      "pandas/tests/io/formats/test_css.py",
      "pandas/tests/io/formats/test_to_excel.py",
      "pandas/tests/io/test_excel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1671,
    "reporter": "wesm",
    "created_at": "2012-07-24T15:36:53+00:00",
    "closed_at": "2012-08-11T21:47:43+00:00",
    "resolver": "wesm",
    "resolved_in": "770af7d1306733e331dba95ef7ac836d40e35051",
    "resolver_commit_num": 2270,
    "title": "Improper handling of Series in Series constructor",
    "body": "This looks like a regression that did not have a unit test (pretty sure this used to do the right thing)\n\n\n\ncc @hughdbrown\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 28,
    "deletions": 7,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1672,
    "reporter": "wesm",
    "created_at": "2012-07-24T22:24:09+00:00",
    "closed_at": "2012-07-24T22:31:02+00:00",
    "resolver": "wesm",
    "resolved_in": "f5a74d4ca37226f4a62c2291db5ee08658706a48",
    "resolver_commit_num": 2249,
    "title": "Strings like 5-2001 broken in parser",
    "body": "It's my fault\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 20,
    "deletions": 18,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tools.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1673,
    "reporter": "killinc",
    "created_at": "2012-07-25T02:54:17+00:00",
    "closed_at": "2012-09-26T21:13:17+00:00",
    "resolver": "wesm",
    "resolved_in": "9a0c453c1c5e2563b2998ee28abf0adddba7ea9c",
    "resolver_commit_num": 2267,
    "title": "bug with time fields when using timezones",
    "body": "\n\nThe reason that this is an important use case is that often you have data stored in UTC but want to sample at a particular time of day in another region.\n\nFor some dataframe y whose index is timezone aware, I would like to be able to say x = y[y.index.hour == 1]\n\nInstead I need to write:\n\n\n\nthis is ok but the list comprehension step takes ages because it is created a new timestamp for each underlying numpy datetime...\n",
    "labels": [],
    "comments": [
      "Does anyone know if there is a workaround for this that doesn't take forever to execute? Any help would be greatly appreciated.\n",
      "sorry closed the issue by mistake. bit of a noob on github.\n",
      "further investigation shows that this is an issue when using Hongkong but not when using US/Eastern.\nIs this weirdness caused by the fact that the initial offset for the Hong Kong tzinfo is a non integer hour. ie look at:\n    pytz.timezone('Hongkong')\nevaluates to: <DstTzInfo 'Hongkong' LMT+7:37:00 STD>\nis this the root cause?\n",
      "Almost definitely. I think the code assumes the offsets are integers\n",
      "this no longer returns an error in 9.0rc1 but... the values it returns for dr.hour seem to be wrong. It looks like it is returning the number of hours in the pytz offset. Same is true if you look at the minutes field.\n",
      "reopening so I can check\n",
      "This is what I get on master:\n\nIn [5]: dr\nOut[5]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2012-01-01 00:00:00, ..., 2012-01-10 00:00:00]\nLength: 10, Freq: D, Timezone: Hongkong\n\nIn [6]: dr.hour\nOut[6]: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)\n",
      "Thanks. Pretty sure I fixed it since cutting rc1\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "closed",
      "reopened",
      "commented",
      "commented",
      "commented",
      "closed",
      "commented",
      "reopened",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 16,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/src/datetime.pyx",
      "pandas/tseries/tests/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1674,
    "reporter": "killinc",
    "created_at": "2012-07-25T06:20:39+00:00",
    "closed_at": "2012-08-12T03:24:09+00:00",
    "resolver": "wesm",
    "resolved_in": "188ddd7dd291dd6b5212500d44df3e3569f4d5fb",
    "resolver_commit_num": 2278,
    "title": "issue with changing frequencies when timezones are involved",
    "body": "the following code fails:\n\n\n\nit appears that the asfreq code checks whether the timezone of the start and end date are the same. In this particular example, the daylight savings offsets are different at the beginning and end so the test for equality fails. Is this intended behaviour or a bug?\n",
    "labels": [],
    "comments": [
      "Yes-- it's a bug\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 56,
    "deletions": 31,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timezones.py",
      "pandas/tseries/tools.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1677,
    "reporter": "wesm",
    "created_at": "2012-07-25T19:51:50+00:00",
    "closed_at": "2012-08-10T15:58:00+00:00",
    "resolver": "wesm",
    "resolved_in": "8db3848f3ed70485248d4df2d5ea78f38db30200",
    "resolver_commit_num": 2259,
    "title": "GroupBy bug with time series",
    "body": "from mailing list\n\n\n``````\n",
    "labels": [],
    "comments": [
      "Here is another minimal working example of a related (same?) failure. Note this runs correctly in version 0.7.3-1 and not in 0.8.x\n\n```\nfrom datetime import datetime\nimport numpy as np\nfrom pandas import DataFrame, date_range\n\ndf = DataFrame(np.random.rand(100), index=date_range(\"1/1/2000\", periods=100))\nmonthly_group = df.groupby(lambda x: (x.year,x.month))\ntry:\n    print monthly_group.groups\nexcept TypeError as e:\n    print \"Failed to list the groups: error was %s\" % e\nprint monthly_group.mean()\n```\n",
      "That actually looks like a different bug. I've just fixed the first one but not the latter\n"
    ],
    "events": [
      "subscribed",
      "assigned",
      "commented",
      "commented"
    ],
    "changed_files": 4,
    "additions": 26,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/index.py",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1678,
    "reporter": "wesm",
    "created_at": "2012-07-25T22:03:04+00:00",
    "closed_at": "2012-08-13T01:54:02+00:00",
    "resolver": "wesm",
    "resolved_in": "e47bf261038e5469a90b28905611961c352f1168",
    "resolver_commit_num": 2297,
    "title": ".ix with MultiIndex indexing buglet",
    "body": "from mailing list:\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 20,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/indexing.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tests/test_panel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1679,
    "reporter": "wesm",
    "created_at": "2012-07-26T02:58:38+00:00",
    "closed_at": "2012-08-12T03:39:45+00:00",
    "resolver": "wesm",
    "resolved_in": "745a49623fbce3a5b0ea761c726eb1d0e3ac155f",
    "resolver_commit_num": 2279,
    "title": "What should Series do when passed generator?",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 16,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/series.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1680,
    "reporter": "wesm",
    "created_at": "2012-07-26T15:10:18+00:00",
    "closed_at": "2012-08-12T03:57:56+00:00",
    "resolver": "wesm",
    "resolved_in": "ee9321f10384599aa959cdfa5af65f6c536afc46",
    "resolver_commit_num": 2281,
    "title": "Series with datetime64 dtype causes failure in dict passed to DataFrame",
    "body": "works:\n\n\n\ndoes not work:\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 1,
    "additions": 8,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1681,
    "reporter": "wesm",
    "created_at": "2012-07-26T15:14:08+00:00",
    "closed_at": "2012-08-12T17:50:07+00:00",
    "resolver": "wesm",
    "resolved_in": "e32ccf9e0fb90e2129d2ecaba1e621a729458d9e",
    "resolver_commit_num": 2282,
    "title": "Concat datetime64 bug",
    "body": "\n",
    "labels": [],
    "comments": [
      "this fails:\n\n```\ndr = pandas.date_range('2011/1/1', '2012/1/1', freq='W-FRI')\nts = pandas.TimeSeries(dr)\nd = pandas.DataFrame({'A': 'foo', 'B': ts}, index=dr)\n```\n"
    ],
    "events": [
      "subscribed",
      "assigned",
      "closed",
      "reopened",
      "commented"
    ],
    "changed_files": 3,
    "additions": 14,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1682,
    "reporter": "wesm",
    "created_at": "2012-07-26T15:15:52+00:00",
    "closed_at": "2012-08-10T19:14:17+00:00",
    "resolver": "wesm",
    "resolved_in": "edfc6db356834dfd3f1326a0ae4556694deb41d8",
    "resolver_commit_num": 2266,
    "title": "DatetimeIndex with time zone loses tzinfo in DataFrame constructor",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 13,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/internals.py",
      "pandas/tseries/tests/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1683,
    "reporter": "wesm",
    "created_at": "2012-07-26T15:17:33+00:00",
    "closed_at": "2012-08-12T01:05:09+00:00",
    "resolver": "wesm",
    "resolved_in": "65dde70f4e2c54ddc3ed93fa54c72268571b7bd7",
    "resolver_commit_num": 2275,
    "title": "Tz-aware DatetimeIndex shift issue",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 14,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1685,
    "reporter": "JoeGermuska",
    "created_at": "2012-07-26T20:27:10+00:00",
    "closed_at": "2012-08-06T19:57:43+00:00",
    "resolver": "lodagro",
    "resolved_in": "be94b91ac2b4ad13b7e18cb9cdcbf685874b8361",
    "resolver_commit_num": 42,
    "title": "plotting._stringify not unicode safe",
    "body": "/Users/germuska/.virtualenvs/data/lib/python2.6/site-packages/pandas/tools/plotting.pyc in _stringify(x)\n   1073         return '|'.join(str(y) for y in x)\n   1074     else:\n-> 1075         return str(x)\n   1076 \n   1077 \n\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\u2019' in position 1: ordinal not in range(128)\n\nIf labels can't be unicode, perhaps can a clearer error be thrown?\n",
    "labels": [],
    "comments": [
      "Matplotlib supports unicode so we don't have to limit pandas plotting functions either I think.\nWould you be interested in making a pull request on this? You can use the stringify functionality in pandas.core.common as a reference\n",
      "fixed\n\n``` python\nIn [13]: df\nOut[13]: \nc0     bar   \nc1       \u0394  \u0395\ni0 i1        \n\u03b1  0     8  5\n   1     6  5\n\u03b2  2     2  4\n   3     8  7\n\u03b3  4     1  8\n   5     7  7\n\u03b4  6     9  3\n   7     5  7\n\nIn [14]: df.plot(title=u'\\u03A3')\nOut[14]: <matplotlib.axes.AxesSubplot at 0xa01cbac>\n```\n\n![plot](http://i.imgur.com/d7Hb3.png)\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "commented",
      "subscribed",
      "commented"
    ],
    "changed_files": 2,
    "additions": 33,
    "deletions": 16,
    "changed_files_list": [
      "pandas/tests/test_graphics.py",
      "pandas/tools/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1686,
    "reporter": "taavi",
    "created_at": "2012-07-26T22:38:57+00:00",
    "closed_at": "2012-08-12T19:55:27+00:00",
    "resolver": "wesm",
    "resolved_in": "dbffb6cf88756cb671e78cff2690a57345e6dfdc",
    "resolver_commit_num": 2287,
    "title": "Performance of DataFrame.resample with a lot of non-unique datetime index values",
    "body": "I've noticed that the first time I resample a DataFrame with a datetime index, it's REALLY slow compared to doing exactly the same thing in pure Python.\n\nMy use case is analyzing logs from a web application server. The index datetimes indicate when an event happened, and there are many per second (so non-unique), and millions of rows per day (I'm totally fine with having separate days in separate dataframes). Once I've done a resample to a particular frequency (`.resample(\"T\")`), I can do another resample with a different `how=` and it's nice and snappy, so I suspect there's an inefficiency somewhere in the code that figures out the resampled row groupings, and that those groupings are cached.\n\nFrom iPython (for %time):\n\n\n\nThanks!\n",
    "labels": [],
    "comments": [
      "That's pandas 0.8.1, numpy 1.6.2, ipython 0.13, Python 2.6.1 (OSX 10.6.8) on a Core2Duo 2.26GHz.\n\nThe timings only get worse as `datelist` grows longer.\n",
      "So, using `line_profiler`, I tracked it down to this function (called 3 times, once each from `_get_annual_rule`, `_get_quarterly_rule`, and `_get_monthly_rule`):\n\n```\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n   783                                               @profile\n   784                                               def month_position_check(self):\n   785                                                   # TODO: cythonize this, very slow\n   786         3            8      2.7      0.0          calendar_end = True\n   787         3            5      1.7      0.0          business_end = True\n   788         3            4      1.3      0.0          calendar_start = True\n   789         3            4      1.3      0.0          business_start = True\n   790                                           \n   791         3           17      5.7      0.0          years = self.fields['Y']\n   792         3            7      2.3      0.0          months = self.fields['M']\n   793         3            9      3.0      0.0          days = self.fields['D']\n   794         3         7771   2590.3      0.4          weekdays = self.index.dayofweek\n   795                                           \n   796         3          130     43.3      0.0          from calendar import monthrange\n   797     30003        92505      3.1      4.5          for y, m, d, wd in zip(years, months, days, weekdays):\n   798     30000        98499      3.3      4.8              wd = datetime(y, m, d).weekday()\n   799                                           \n   800     30000        49471      1.6      2.4              if calendar_start:\n   801         3          161     53.7      0.0                  calendar_start &= d == 1\n   802     30000        47977      1.6      2.3              if business_start:\n   803         3           84     28.0      0.0                  business_start &= d == 1 or (d <= 3 and wd == 0)\n   804                                           \n   805     30000      1344334     44.8     65.5              _, daysinmonth = monthrange(y, m)\n   806     30000       300513     10.0     14.7              cal = d == daysinmonth\n   807     30000        59340      2.0      2.9              if calendar_end:\n   808         3           23      7.7      0.0                  calendar_end &= cal\n   809     30000        50104      1.7      2.4              if business_end:\n   810         3           43     14.3      0.0                  business_end &= cal or (daysinmonth - d < 3 and wd == 4)\n   811                                           \n   812         3           14      4.7      0.0          if calendar_end:\n   813                                                       return 'ce'\n   814         3            5      1.7      0.0          elif business_end:\n   815                                                       return 'be'\n   816         3            5      1.7      0.0          elif calendar_start:\n   817                                                       return 'cs'\n   818         3            4      1.3      0.0          elif business_start:\n   819                                                       return 'bs'\n   820                                                   else:\n   821         3            6      2.0      0.0              return None\n```\n\nSo that code chunk gets executed 3x per data point. That seems\u2026not scalable. I do have a patch (worked with Erik Welch at the PyOhio sprints) to make the example above go from 2.4s to 1.0s, which helps. :) I'll try to attach it to this bug request, but might end up just having to create a new pull request.\n\nAnd after that I guess I should learn some Cython? :)\n",
      "Given the code comment about Cython, maybe it's worthwhile keeping this issue open for now, despite the extra speed provided from #1699?\n",
      "Well, sorry about this one. I see the problem immediately (it's easier if you use cProfile) and will fix\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "unsubscribed",
      "commented",
      "referenced",
      "cross-referenced",
      "commented",
      "referenced",
      "commented"
    ],
    "changed_files": 2,
    "additions": 10,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/frequencies.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1688,
    "reporter": "taavi",
    "created_at": "2012-07-27T02:59:56+00:00",
    "closed_at": "2012-08-11T22:48:10+00:00",
    "resolver": "wesm",
    "resolved_in": "2279c92763a7dc86ddc21d91b9af4a8a38b924a0",
    "resolver_commit_num": 2273,
    "title": "AssertionError with df.resample(how=\"median\")",
    "body": "I've reproduced _something_ using `how=\"median\"`, perhaps #1648. It seems to hit when there are discontinuities in the resampling (i.e. minutes with no records when downsampling).\n\nBoth pandas 0.8.1 and 0.8.2.dev-f5a74d4 don't like it:\n\n\n",
    "labels": [],
    "comments": [
      "`how=\"mean\"` works:\n\n```\nIn [5]: df.resample(\"T\", how=\"mean\")\nOut[5]: \n                      0\n2012-01-01 00:00:00   1\n2012-01-01 00:01:00 NaN\n2012-01-01 00:02:00 NaN\n2012-01-01 00:03:00 NaN\n2012-01-01 00:04:00 NaN\n2012-01-01 00:05:00   2\n```\n\nI have no idea what numpy is up to, though:\n\n```\nIn [6]: import numpy as np\n\nIn [7]: df.resample(\"T\", how=lambda x: np.percentile(x, 50))\nOut[7]: \nEmpty DataFrame\nColumns: array([], dtype=int64)\nIndex: array([], dtype=object)\n```\n\nI can't seem to make it misbehave here, but with some Actual Data with data absent for several of the resampled intervals, I can get numpy to explode with the same stack trace as:\n\n``` python\nnp.percentile([], 50)\n# ...\n# ValueError: operands could not be broadcast together with shapes (0) (2) \n```\n\nI can certainly work around that in the lambda by short-circuiting on an empty list-like thing being passed in, but I don't think I should have to.\n\nFor this _particular_ case, I can just do this with expected results:\n\n```\nIn [8]: df.resample(\"T\", how=np.median)\nOut[8]: \n                      0\n2012-01-01 00:00:00   1\n2012-01-01 00:01:00 NaN\n2012-01-01 00:02:00 NaN\n2012-01-01 00:03:00 NaN\n2012-01-01 00:04:00 NaN\n2012-01-01 00:05:00   2\n```\n\nI can't fathom why `np.median(\u2026)` is different from `np.percentile(\u2026, 50)`. Maybe that's just a numpy thing? :)\n\nThanks!\n"
    ],
    "events": [
      "subscribed",
      "cross-referenced",
      "commented"
    ],
    "changed_files": 4,
    "additions": 24,
    "deletions": 4,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/src/reduce.pyx",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1692,
    "reporter": "mcobzarenco",
    "created_at": "2012-07-27T14:15:52+00:00",
    "closed_at": "2012-08-12T21:23:56+00:00",
    "resolver": "wesm",
    "resolved_in": "3c5621bdd2b0d533a818abfa22923fe22e01da0a",
    "resolver_commit_num": 2291,
    "title": "Timezone Conversion Bug",
    "body": "It seems that it is not possible to use static timezones with pandas:\n\n\n\n",
    "labels": [],
    "comments": [
      "I keep having to write workarounds for the above, I was curios if it's a real bug rather than me screwing something up. Any help would be greatly appreciated.\n"
    ],
    "events": [
      "subscribed",
      "commented"
    ],
    "changed_files": 3,
    "additions": 29,
    "deletions": 7,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/src/datetime.pyx",
      "pandas/tseries/tests/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1693,
    "reporter": "khughitt",
    "created_at": "2012-07-27T14:25:01+00:00",
    "closed_at": "2012-08-10T18:35:52+00:00",
    "resolver": "wesm",
    "resolved_in": "4cafeb0cf902d99154e76e12a48580d3914183fe",
    "resolver_commit_num": 2264,
    "title": "read_csv: Tz-aware datetime.datetime cannot be converted to datetime64",
    "body": "This may be related to some of the other TimeZone issues.\n\nThis worked in 0.8, but now raises an error in 0.8.1:\n\n\n\nError:\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 43,
    "deletions": 12,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/datetime.pyx",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1694,
    "reporter": "manuteleco",
    "created_at": "2012-07-27T15:27:56+00:00",
    "closed_at": "2012-08-10T19:05:54+00:00",
    "resolver": "wesm",
    "resolved_in": "cb5a31f5f7b64f9411610cb918ca2591c05c1a7e",
    "resolver_commit_num": 2265,
    "title": "Unexpected behavior on Series.irow (Series.iget) with non-unique index",
    "body": "Hi,\n\n`Series.irow(i)` (and also `Series.iget(i)`), with `i` being an integer, returns more than one value on Series with non-unique indexes if the index at location `i` has duplicates. This is not the behavior I expected and it is also different from the behavior shown by `DataFrame.irow(i)`, since it only returns the row located at position `i` regardless of any existing index duplicates. Also the documentation for `Series.irow`/`Series.iget` specifies that an \"int\" parameter should return a \"scalar\" value, so I guess this might be a bug.\n\nHere is some sample code I've run using pandas '0.8.2.dev-f5a74d4':\n\n\n\nand its DataFrame counterpart:\n\n\n\nThanks and regards.\n",
    "labels": [],
    "comments": [
      "Yes, it's a bug. Slated for 0.8.2\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 6,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/series.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1695,
    "reporter": "wesm",
    "created_at": "2012-07-27T16:16:11+00:00",
    "closed_at": "2012-08-11T22:24:04+00:00",
    "resolver": "wesm",
    "resolved_in": "9f8ee68d7b5b7099997d73261b9048e4b03932da",
    "resolver_commit_num": 2272,
    "title": "New rolling_max/min impl's do not handle min_periods option",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 11,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/src/moments.pyx",
      "pandas/stats/tests/test_moments.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1696,
    "reporter": "lbolla",
    "created_at": "2012-07-27T16:19:33+00:00",
    "closed_at": "2012-08-12T19:05:36+00:00",
    "resolver": "wesm",
    "resolved_in": "d270af97e9ef5e19646d8f407216360c03609e42",
    "resolver_commit_num": 2285,
    "title": "NaN-columns is plotted as all 0 column in pandas",
    "body": "-columns-is-plotted-as-a-all-0-column-in-pandas\n\nA column full of NaNs is plotted as full of zeros.\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed",
      "assigned",
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 28,
    "deletions": 6,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/strings.py",
      "pandas/tests/test_strings.py",
      "pandas/tools/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1697,
    "reporter": "ichipper",
    "created_at": "2012-07-27T21:03:25+00:00",
    "closed_at": "2012-08-12T00:53:09+00:00",
    "resolver": "wesm",
    "resolved_in": "48a31946871c37322eca3d83ce3b9d285bf5c4e9",
    "resolver_commit_num": 2274,
    "title": "Bug/unexpected behaviour when using groupby and aggregation functions with DataFrame",
    "body": "Here is the bug to reproduce the bug/unexpected behavior:\n\n\n# \n\nWhen running the code, we can see that df1 is:\n\n\n\nAnd df1 is selected from subblocks of df:\n\n\n\nAfter grouping df1 by the first level of multiindex of the columns,\nwe can see df1_group.groups is:\n\n`{'f2': [('f2', 's1'), ('f2', 's2')], 'f3': [('f3', 's1'), ('f3', 's2')]}`\n\nHowever, when apply a sum function to aggregate the columns inside each group, as in the example code,\ndf1_group.sum() results in:\n\n\n\nIt seems it tries to do the aggregation using the columns of df instead of df1 so the columns of the resulting dataframe \ninclude the label 'f1', which doesn't exist in df1.\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 23,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1698,
    "reporter": "kdebrab",
    "created_at": "2012-07-28T19:57:26+00:00",
    "closed_at": "2012-08-12T03:48:53+00:00",
    "resolver": "wesm",
    "resolved_in": "fdcc10672aea5553857d34722396b2be6b7aa4c0",
    "resolver_commit_num": 2280,
    "title": "Interpolating with method='time' incorrect for resolution higher than daily",
    "body": "The following code (pandas 0.8.0b2):\n\n\n\ncorrectly returns:\n2012-01-01     0\n2012-01-02     1\n2012-01-13    12\n2012-01-14    13\n2012-01-25    24\n2012-01-26    25\n2012-02-06    36\n2012-02-07    36\n\nBut when exchanging days for hours:\n\n\n\nthe result is not correct:\n2011-01-01 00:00:00     0\n2011-01-01 01:00:00    12\n2011-01-01 12:00:00    12\n2011-01-01 13:00:00    12\n2011-01-02 00:00:00    24\n2011-01-02 01:00:00    36\n2011-01-02 12:00:00    36\n2011-01-02 13:00:00    36\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 21,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/series.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1700,
    "reporter": "changhiskhan",
    "created_at": "2012-07-29T14:07:20+00:00",
    "closed_at": "2012-08-10T17:26:23+00:00",
    "resolver": "wesm",
    "resolved_in": "0c72d8be2f92008beadf10afc920b2df9c12a044",
    "resolver_commit_num": 2263,
    "title": "DataFrame repr fails for nonunique columns",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 11,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1701,
    "reporter": "wesm",
    "created_at": "2012-07-30T00:38:49+00:00",
    "closed_at": "2012-07-30T00:48:21+00:00",
    "resolver": "wesm",
    "resolved_in": "eec8a83f448432e42d9ef4672abe2a3b36b2c816",
    "resolver_commit_num": 2250,
    "title": "GroupBy.apply doesn't pass group key name onto index in all cases",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 15,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1703,
    "reporter": "kdebrab",
    "created_at": "2012-07-30T10:15:27+00:00",
    "closed_at": "2012-08-12T20:53:07+00:00",
    "resolver": "wesm",
    "resolved_in": "ee6db80d20b0bb330b9be595128e26d56f90979e",
    "resolver_commit_num": 2290,
    "title": "Incorrect result when using to_datetime() on PeriodIndex",
    "body": "Pandas 0.8.1:\n\n\n\nreturns incorrectly:\n\n\n\nThe correct result is obtained though with to_timestamp() instead of to_datetime():\n\n\n\nreturns correctly:\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 14,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/index.py",
      "pandas/tseries/index.py",
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1705,
    "reporter": "kdebrab",
    "created_at": "2012-07-30T10:29:53+00:00",
    "closed_at": "2012-08-12T20:18:39+00:00",
    "resolver": "wesm",
    "resolved_in": "4e364414174742a7a7e7a322400f7a96cfa93c6d",
    "resolver_commit_num": 2288,
    "title": "Incorrect behaviour when using periods in a hierarchical index",
    "body": "Pandas 0.8.1:\n\n\n\nincorrectly returns:\n\n\n\nThe correct result is obtained though when explicitly building the MultiIndex:\n\n\n\ncorrectly returns:\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 6,
    "additions": 42,
    "deletions": 17,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/categorical.py",
      "pandas/core/index.py",
      "pandas/tseries/index.py",
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1706,
    "reporter": "gerigk",
    "created_at": "2012-07-30T12:49:57+00:00",
    "closed_at": "2012-08-12T01:48:40+00:00",
    "resolver": "wesm",
    "resolved_in": "d0e9034c80c768638149559182ce80f12886a6ec",
    "resolver_commit_num": 2277,
    "title": "encode/decode method for string module (like in np.char.encode )",
    "body": "This is already in included in numpy but for convenience it would be nice to have this in pandas.\n\n\n\nis nicer than for example\n\n\n\nI find myself having to decode and encode a lot depending on what other libraries accept/return. \n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 33,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/strings.py",
      "pandas/tests/test_strings.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1707,
    "reporter": "eriknw",
    "created_at": "2012-07-30T19:55:06+00:00",
    "closed_at": "2012-08-12T01:31:39+00:00",
    "resolver": "wesm",
    "resolved_in": "603e5ae9f8c6dc7bd17e86f556c01820c6ffed61",
    "resolver_commit_num": 2276,
    "title": "Can't save empty Series or DataFrame to hdf5 with HDFStore",
    "body": "With pandas 0.8.1 (and pytables 2.3.1), trying to save an empty Series or DataFrame when using HDFStore results in an exception after some (but not all) data has been written to the hdf5 file.\n\n\n\nHere is the traceback:\n\n\n\nAnd here I show an issue that arises from only some of the data being written:\n\n\n\nOh, and just for the record, all tests in \"io/tests/test_pytables.py\" succeed for me.\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 39,
    "deletions": 16,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/pytables.py",
      "pandas/io/tests/test_pytables.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1708,
    "reporter": "kdebrab",
    "created_at": "2012-07-30T20:33:54+00:00",
    "closed_at": "2012-08-10T16:55:00+00:00",
    "resolver": "wesm",
    "resolved_in": "eb3c4e60b392f6a01999b5b6102e143abff75970",
    "resolver_commit_num": 2262,
    "title": "Unexpected result with union, intersection and diff on Index objects",
    "body": "Pandas 0.8.1:\n\n\n\ncorrectly returns:\n\n\n\nBut when building the same Index objects by specifying their frequency:\n\n\n\nunexpectedly results in:\n\n\n\nThe same issue occurs when directly calling the intersection() method and similar unexpected results occur for union (|, +) and diff (-) operators.\n\nFor information, combining append() and order() methods as an alternative for the union operators does give the correct result, independently how index_1 and index_2 are built:\n\n\n\ncorrectly results in:\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 10,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1709,
    "reporter": "zachcp",
    "created_at": "2012-07-30T21:07:09+00:00",
    "closed_at": "2012-10-21T18:08:42+00:00",
    "resolver": "wesm",
    "resolved_in": "9c502071acd2d7f6be34d0502b7a2370ee8fdfa5",
    "resolver_commit_num": 2492,
    "title": "include iris dataset",
    "body": "I just noticed the very nice radviz graphing feature added to pandas 0.8.1 (-docs/dev/visualization.html#scatter-plot-matrix). Most of the graphs in the pandas documentation use randomly generated graphs, however the radviz examples uses the iris data set.  The iris dataset, however, is not included as part of the pandas package to a  new user cannot immediately reproduce, and play with, the example graph. \n\nWould it be possible to include the iris dataset as part of the pandas package? alternatively, could the the example use a dataset that is hosted somewhere online?\n\nbest,\nzach cp\n",
    "labels": [],
    "comments": [
      "It is included in the test suite. It's not accessible like data sets in statsmodels, though\n",
      "close?\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 13,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/dsintro.rst",
      "doc/source/io.rst",
      "doc/source/visualization.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1711,
    "reporter": "mcobzarenco",
    "created_at": "2012-07-31T12:32:20+00:00",
    "closed_at": "2012-08-12T18:48:09+00:00",
    "resolver": "wesm",
    "resolved_in": "bd13307df8822db0bcd960fc29b61463b0f7d2bc",
    "resolver_commit_num": 2284,
    "title": "pandas.Series.plot ignores color in **kwarg",
    "body": "E.g.:\n\n\n\nWhose output is:\n\n![Series Plot]()\n\nThis used to work correctly in pandas 0.7.2 \n",
    "labels": [],
    "comments": [
      "The mailing list has a related topic: https://groups.google.com/forum/?fromgroups#!topic/pydata/AMKPFPvNgcE\nLooking at the code it is clear that a series will always be plotted in blue unless the color is specified in the style argument.\n\n``` python\npandas.Series(randn(10)).plot(style='red')\n```\n\n![series plot](http://i.imgur.com/rTH99.png)\n",
      "That is indeed a bug.\nFor now you can use `style='r'`. We'll update this issue once we put in a fix. Thanks.\n\nOn Jul 31, 2012, at 8:32 PM, Marius reply@reply.github.com wrote:\n\n> E.g.:\n> \n> ``` python\n> pandas.Series(randn(10)).plot(color='red')\n> ```\n> \n> Whose output is:\n> \n> ![Series Plot](http://i.imgur.com/dYVV2.png)\n> \n> This used to work correctly in pandas 0.7.2\n> \n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/pydata/pandas/issues/1711\n",
      "I fixed this in my view.\nThe example below now behaves as expected:\n\n``` python\nIn [28]: df = pandas.DataFrame(np.random.randn(4,4), index=list('abcd'))\n\nIn [29]: df.plot(style='d--', color='g')  # green dashed lines with diamonds\nOut[29]: <matplotlib.axes.AxesSubplot at 0xae729ac>\n\nIn [30]: pandas.Series(np.random.randn(10)).plot(color='g')\nOut[30]: <matplotlib.axes.AxesSubplot at 0xae729ac>\n```\n\nI have on question though. \n\n``` python\nIn [22]: close('all')\n\nIn [23]: pandas.Series(np.random.randn(10)).plot()\nOut[23]: <matplotlib.axes.AxesSubplot at 0xab0a82c>\n\nIn [24]: hold(True)\n\nIn [25]: pandas.Series(np.random.randn(10)).plot()\nOut[25]: <matplotlib.axes.AxesSubplot at 0xab0a82c>\n```\n\nThis will result in a plot with two lines both blue. In other words there is no color cycling when plotting multiple Series on a single plot. This behavior is different from mpl, is this intentional?\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "unsubscribed",
      "subscribed",
      "commented",
      "commented",
      "subscribed",
      "commented"
    ],
    "changed_files": 2,
    "additions": 39,
    "deletions": 11,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tools/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1717,
    "reporter": "gerigk",
    "created_at": "2012-08-01T10:52:31+00:00",
    "closed_at": "2012-08-09T03:11:50+00:00",
    "resolver": "wesm",
    "resolved_in": "8cde377c37d4f10abcb4a76b28458afe842aa273",
    "resolver_commit_num": 2257,
    "title": "groupby.first() fails for np.datetime64 columns",
    "body": "numpy 1.7 dev and pandas-0.8.2.dev_f5a74d4-py2.7-linux-x86_64\n\n\n",
    "labels": [],
    "comments": [
      "A weird thing is that this succeeds\n\n```\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame([(3,np.datetime64('2012-07-03 00:00:00')),(3,np.datetime64('2012-07-04 00:00:00'))], columns = ['a', 'date'])\ndf.date = df.date.astype('M8[ns]')\nprint df.dtypes\ndf.date = pd.to_datetime(df.date).astype(object)\ndf.groupby('a').first()\n```\n\nbut without the .astype(object) it does not succeed. Actually the values are pandas.lib.timestamp type but the series stays datetime64[ns]. \n"
    ],
    "events": [
      "subscribed",
      "commented"
    ],
    "changed_files": 4,
    "additions": 28,
    "deletions": 10,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/series.py",
      "pandas/src/inference.pyx",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1719,
    "reporter": "wesm",
    "created_at": "2012-08-02T01:25:10+00:00",
    "closed_at": "2012-08-09T02:26:52+00:00",
    "resolver": "wesm",
    "resolved_in": "4fb1bd6ee7876fe6d2a58ab223a1a0d643f33dc0",
    "resolver_commit_num": 2255,
    "title": "concat with axis=1, join='outer' not working correctly",
    "body": "-concatouter-not-doing-union\n\n\n",
    "labels": [],
    "comments": [
      "Just to add my 2 cents on this, it looks like this has not much to to do with DataFrame.concat and happens when you use the fast union for DatetimeIndex objects. Taking inspiration from the original example:\n\n``` python\nimport pandas\nimport pandas.util.testing as put\nts = put.makeTimeSeries(4)\ntsWithGaps = ts[::2]\nindex = ts.index\nindexWithGaps = tsWithGaps.index\nindex\nindexWithGaps\nindex.union(indexWithGaps)\nindexWithGaps.union(index)\npandas.Index.union(indexWithGaps, index)\n```\n\nand the output:\n\n```\nIn [7]: index\nOut[7]:\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2000-01-03 00:00:00, ..., 2000-01-06 00:00:00]\nLength: 4, Freq: B, Timezone: None\n\nIn [8]: indexWithGaps\nOut[8]:\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2000-01-03 00:00:00, 2000-01-05 00:00:00]\nLength: 2, Freq: 2B, Timezone: None\n\nIn [9]: index.union(indexWithGaps)\nOut[9]:\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2000-01-03 00:00:00, ..., 2000-01-06 00:00:00]\nLength: 4, Freq: B, Timezone: None\n\nIn [10]: indexWithGaps.union(index)\nOut[10]:\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2000-01-03 00:00:00, ..., 2000-01-06 00:00:00]\nLength: 3, Freq: 2B, Timezone: None\n\nIn [11]: pandas.Index.union(indexWithGaps, index)\nOut[11]:\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2000-01-03 00:00:00, ..., 2000-01-06 00:00:00]\nLength: 4, Freq: None, Timezone: None\n```\n\nJust for sake of clarity the problem is in [10] where the union of the indices has length 3 instead of length 4 (the Jan-4 timestamp is missing if you want to know). [11] shows what happened in the non fast union case and is fine. On top of that it's slightly weird that indexWithGaps.union(index) has a 2B frequency.\n\nLooking at the existing issues this probably has some overlap with #1708. Interestingly enough this indexing problem doesn't seem to affect aligning timeseries, i.e. both ts.align(tsWithGaps) and tsWithGaps.align(ts) work fine. On the other hand, constructing dataframe is affected as the original bug report showed:\n\n```\nIn [84]: pandas.DataFrame(collections.OrderedDict([('ts', ts), ('tsWithGaps', tsWithGaps)]))\nOut[84]:\n                  ts  tsWithGaps\n2000-01-03 -0.045699   -0.045699\n2000-01-04 -1.611032         NaN\n2000-01-05 -1.055301   -1.055301\n2000-01-06  1.024215         NaN\n\nIn [85]: pandas.DataFrame(collections.OrderedDict([('tsWithGaps', tsWithGaps), ('ts', ts)]))\nOut[85]:\n                  ts  tsWithGaps\n2000-01-03 -0.045699   -0.045699\n2000-01-05 -1.055301   -1.055301\n2000-01-06  1.024215         NaN\n```\n\ni.e. the dataframe constructed depends on which order you provide the timeseries.\n",
      "This was fixed in 5382985\n"
    ],
    "events": [
      "subscribed",
      "commented",
      "subscribed",
      "commented"
    ],
    "changed_files": 1,
    "additions": 1,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1720,
    "reporter": "wesm",
    "created_at": "2012-08-02T01:27:47+00:00",
    "closed_at": "2012-08-12T21:57:29+00:00",
    "resolver": "wesm",
    "resolved_in": "69d1f7550bde120dd1b30fb274743c42b3575517",
    "resolver_commit_num": 2293,
    "title": "DataFrame.to_records failure with DatetimeIndex",
    "body": "-to-records-gives-an-error-when-typeindex-is-datetimeindex\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 16,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1721,
    "reporter": "tschm",
    "created_at": "2012-08-02T09:00:24+00:00",
    "closed_at": "2012-08-09T02:24:57+00:00",
    "resolver": "wesm",
    "resolved_in": "8156ab9d3d75d4de78dd0eef139b789f62b90313",
    "resolver_commit_num": 2254,
    "title": "np.fix crashes with pandas time series",
    "body": "\n\n`print np.fix(x)`\n\ncrashes with:\n\n\n\nI assume this should work?\nThomas\n",
    "labels": [],
    "comments": [
      "I wasn't aware of this syntax. I thought `y[:]` was the standard practice; learn something new every day\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "commented"
    ],
    "changed_files": 3,
    "additions": 17,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/series.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1722,
    "reporter": "gerigk",
    "created_at": "2012-08-02T11:26:00+00:00",
    "closed_at": "2012-08-09T01:05:08+00:00",
    "resolver": "wesm",
    "resolved_in": "d5f5f17c14eaf5fd518e62d48c550eb30bde4673",
    "resolver_commit_num": 2251,
    "title": "Confusing docstring",
    "body": "I just came across this one while browsing the code.\nMaybe this should be Index.intersection in the docstring ?\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 1,
    "deletions": 1,
    "changed_files_list": [
      "pandas/tseries/index.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1723,
    "reporter": "dhirschfeld",
    "created_at": "2012-08-02T12:22:31+00:00",
    "closed_at": "2012-08-09T01:09:31+00:00",
    "resolver": "wesm",
    "resolved_in": "0fca7964fd8cc197ff86b0cb44015bec7f45bdd9",
    "resolver_commit_num": 2252,
    "title": "Add DatetimeIndex.day_of_year Method",
    "body": "PeriodIndex has a day_of_year method, but DatetimeIndex doesn't. This makes it hard to write generic code which can take either.\n\nGiven they both have dayofyear attributes it may be best to remove the day_of_year method so that there is only one obvious way to do it.\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 11,
    "deletions": 6,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1726,
    "reporter": "dalejung",
    "created_at": "2012-08-03T03:49:52+00:00",
    "closed_at": "2012-08-13T01:08:30+00:00",
    "resolver": "wesm",
    "resolved_in": "fc9217042220300d179cc0e97ef045a3e26777d1",
    "resolver_commit_num": 2296,
    "title": "Error with closed='left' and _adjust_bin_edges",
    "body": "\n\nWill result in an error. I tracked it down to \n\n\n\nI'm not sure why the day is added when closed='left'. Even on datasets that don't start on a Monday, and thus don't error, it'll cause the intervals to start on Tuesday.\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 17,
    "deletions": 5,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1727,
    "reporter": "grsr",
    "created_at": "2012-08-03T10:13:51+00:00",
    "closed_at": "2012-08-12T21:34:54+00:00",
    "resolver": "wesm",
    "resolved_in": "29846da521b1311ce2ac40ab818b7c6cd9f51fdb",
    "resolver_commit_num": 2292,
    "title": "inconsistent results building a DataFrame from a dict of Series with MultiIndexes",
    "body": "I am trying to build a DataFrame from a dict of Series objects which have (not necessarily exactly matching) MultiIndex indices, sometimes I don't get any results for some particular data point and so I create an empty Series object and add that to the dict, as I need there to be a column present even without any data (later I convert all NAs to 0). This seems to work sometimes, but other times I get an error message that implies all the Series need hierarchical indices, it seems to depend on the order in which the Series are added to the dict. See example session below, the first time I create the DataFrame it behaves just as I'd like, giving me a df index that is the union of all the indices in the populated Series and supplying NaNs wherever there is no data, but the second time it blows up. Perhaps I shouldn't be relying on this behaviour but it seems that the results should at least be consistent.\n\nAny tips on how to solve this in a cleaner way also very welcome. Thanks.\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 15,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/index.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1728,
    "reporter": "CRP",
    "created_at": "2012-08-03T14:54:44+00:00",
    "closed_at": "2012-08-05T19:45:44+00:00",
    "resolver": "lodagro",
    "resolved_in": "5771612634e23dbaf4aa4cb42d79d19c458851fa",
    "resolver_commit_num": 41,
    "title": "to_html issues",
    "body": "- doc string mentions a \"justify\" parameter that apparently does not exist and provokes an errore if used\n- setting index=False produces no visible effect\n",
    "labels": [],
    "comments": [
      "Is this related to the following bug?\n\nhttps://github.com/pydata/pandas/issues/1000\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "assigned",
      "commented",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 157,
    "deletions": 36,
    "changed_files_list": [
      "pandas/core/format.py",
      "pandas/core/frame.py",
      "pandas/tests/test_format.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1730,
    "reporter": "kdebrab",
    "created_at": "2012-08-04T22:16:02+00:00",
    "closed_at": "2012-08-09T01:25:34+00:00",
    "resolver": "wesm",
    "resolved_in": "5382985cd62e1d6cf70d03b3dc65b81534f7d43e",
    "resolver_commit_num": 2253,
    "title": "Unexpected result when combining Series into a DataFrame",
    "body": "Pandas 0.8.1:\n\n\n\nunexpectedly returns:\n\n\n\nThe correct result though is obtained with:\n\n\n\ncorrectly returns:\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 10,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1734,
    "reporter": "wesm",
    "created_at": "2012-08-04T23:00:11+00:00",
    "closed_at": "2012-08-10T16:49:40+00:00",
    "resolver": "wesm",
    "resolved_in": "5b3853da574e4b6c0bee4d6d5c170e33acb1aba6",
    "resolver_commit_num": 2261,
    "title": "Fix statsmodels import in pandas.stats.var or remove module",
    "body": "cc @blais \n",
    "labels": [],
    "comments": [
      "Just add a try: block for \nimport statsmodels.tsa.vector_ar.api\n"
    ],
    "events": [
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "subscribed"
    ],
    "changed_files": 2,
    "additions": 2,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/stats/var.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1736,
    "reporter": "lodagro",
    "created_at": "2012-08-06T20:07:04+00:00",
    "closed_at": "2012-08-12T20:37:34+00:00",
    "resolver": "wesm",
    "resolved_in": "1dd2c4f272062b1ae5163c9cdc0c54aeef756d82",
    "resolver_commit_num": 2289,
    "title": "Series/DataFrame repr fails for MultiIndex level names with unicode",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 16,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/index.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1744,
    "reporter": "blais",
    "created_at": "2012-08-08T10:30:03+00:00",
    "closed_at": "2012-09-18T20:33:40+00:00",
    "resolver": "wesm",
    "resolved_in": "dfe7a55c1032521c1c3817a10fd2c8e51b8b4cbd",
    "resolver_commit_num": 2404,
    "title": "Support creating a DataFrame from a list of dictionaries providing an index column",
    "body": "Hey Wes,\n\nHere's a use case that I think is not covered by Pandas.\nIt's a use case for creation of a DataFrame object from a\nlist of dicts.\n\nI extract \"documents\" (dicts) from a MongoDB database.\nFrom these dicts, one of the keys is meant to be used as \nthe index.\n\nWhile I can do something like\n\n  df = DataFrame(documents, columns=['order_id', 'time', 'quantity'])\n\nthe index that is added is an arbitrary one.\nI'd like to use 'order_id' as my index, in this example.\nThe only way for me to do that at the moment is to create\nthe index explicitly, e.g. like this:\n\n  df = DataFrame(documents, columns=['order_id', 'time', 'quantity'],\n                 index=[o['order_id'] for o in documents])\n\nThis is a bit of a PIA. It would be nice if one could just\nspecify the index key/column name, e.g.\n\n  df = DataFrame(documents, columns=['order_id', 'time', 'quantity'], indexcol='order_id')\n\nThat's my use case almost everywhere.\n",
    "labels": [],
    "comments": [
      "You can use a different constructor `DataFrame.from_records`\n\n``` python\nIn [36]: def create_dict(order_id):\n   ....:      return {'order_id': order_id, 'quantity': np.random.randint(1, 10), 'price': np.random.randint(1, 10)}\n\nIn [37]: documents = [create_dict(i) for i in range(10)]\n\nIn [38]: documents.append({'order_id': 10, 'quantity': 5})   # demo missing data\n\nIn [39]: df = pandas.DataFrame.from_records(documents).set_index('order_id')\n\nIn [40]: df\nOut[40]:\n          price  quantity\norder_id\n0             2         2\n1             2         3\n2             1         8\n3             2         1\n4             3         7\n5             9         5\n6             7         4\n7             7         5\n8             7         8\n9             6         3\n10          NaN         5\n\nIn [41]: df = pandas.DataFrame.from_records(documents, columns=['order_id', 'quantity', 'price'], index='order_id')\n\nIn [42]: df\nOut[42]:\n    quantity  price\n0          2      2\n1          3      2\n2          8      1\n3          1      2\n4          7      3\n5          5      9\n6          4      7\n7          5      7\n8          8      7\n9          3      6\n10         5    NaN\n```\n\nLast one seems not to set the index name.\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 3,
    "additions": 20,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1745,
    "reporter": "beaumartinez",
    "created_at": "2012-08-08T11:55:36+00:00",
    "closed_at": "2012-08-10T15:23:45+00:00",
    "resolver": "wesm",
    "resolved_in": "aea7c4522bd7beffd0df80efee818873110609fa",
    "resolver_commit_num": 2258,
    "title": "Creating a DataFrame from Series with unsorted indeces gives completely bad data",
    "body": "Creating a DataFrame from Series with unsorted indeces gives completely bad data.\n\nI've [written a module]() that reproduces the bug\u2014unless this is by design. Give it a try.\n\n(I'm running `pandas==0.8.1`, Python 2.7.2, and OS X Mountain Lion.)\n",
    "labels": [],
    "comments": [
      "It turns out [it's by design](http://pandas.sourceforge.net/timeseries.html#time-series-related-instance-methods)\u2014\n\n> While pandas does not force you to have a sorted date index, some of these methods may have unexpected or incorrect behavior if the dates are unsorted. So please be careful\n",
      "No, what you show in the script looks like a bug, let me have a look.\n",
      "Definitely a bug. I'll sort out a fix right now once I figure out what's happening\n"
    ],
    "events": [
      "commented",
      "closed",
      "reopened",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 12,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1746,
    "reporter": "njsmith",
    "created_at": "2012-08-08T13:05:47+00:00",
    "closed_at": "2012-08-09T19:39:21+00:00",
    "resolver": "lodagro",
    "resolved_in": "b956be013a1d1c029189c55173a543a405d986e0",
    "resolver_commit_num": 44,
    "title": "Extremely slow repr() on small data frame",
    "body": "I have a small data frame here (obtained by slicing a much larger data frame with a MultiIndex):\n\n\n\nIts repr() looks like this:\n\n\n\nProducing this repr takes a bizarrely long time, it's _very_ noticeable in interactive use:\n\n\n\nThe object is available here: ~njs/tmp/slow-repr-x.pickle\n",
    "labels": [],
    "comments": [
      "Seems related to the index\n\n``` python\nIn [68]: time repr(x)\nCPU times: user 3.91 s, sys: 0.10 s, total: 4.01 s\nWall time: 3.99 s\n...\n\nIn [69]: time repr(x.reset_index())\nCPU times: user 0.02 s, sys: 0.01 s, total: 0.03 s\nWall time: 0.03 s\n```\n",
      "Some profiling and a little code change ... \n\n``` python\nIn [11]: time repr(x)\nCPU times: user 0.03 s, sys: 0.00 s, total: 0.03 s\nWall time: 0.04 s\n```\n"
    ],
    "events": [
      "commented",
      "commented"
    ],
    "changed_files": 1,
    "additions": 4,
    "deletions": 3,
    "changed_files_list": [
      "pandas/core/index.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1749,
    "reporter": "lodagro",
    "created_at": "2012-08-09T12:50:40+00:00",
    "closed_at": "2012-08-12T22:16:08+00:00",
    "resolver": "wesm",
    "resolved_in": "c03019897fe56aa46e42414a098361231029c82d",
    "resolver_commit_num": 2295,
    "title": "Series __repr__ Exception when series hold DataFrames",
    "body": "from [stackoverflow](-stacking-dataframes-generated-by-apply)\n\n\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 16,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/src/util.pxd",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1750,
    "reporter": "manuteleco",
    "created_at": "2012-08-09T23:37:20+00:00",
    "closed_at": "2012-08-13T02:45:01+00:00",
    "resolver": "wesm",
    "resolved_in": "ded51c8c257856abb8737b6bb248f332608b2637",
    "resolver_commit_num": 2298,
    "title": "Assignment to DataFrame through .ix with non-unique MultiIndex",
    "body": "Hi,\n\nI think there might be a bug in the assignment to a DataFrame object through .ix when non-unique MultiIndexes are involved. The next sample code describes this issue:\n\n\n\nThanks and regards.\nManu.\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 4,
    "additions": 36,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/index.py",
      "pandas/core/indexing.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1755,
    "reporter": "jseabold",
    "created_at": "2012-08-10T18:42:45+00:00",
    "closed_at": "2012-08-10T22:10:21+00:00",
    "resolver": "wesm",
    "resolved_in": "2a10485b5346965869d4b80725e145849b5d4b3f",
    "resolver_commit_num": 2269,
    "title": "isnull returns a boolean for array-like inputs",
    "body": "In numpy\n\n\n\nIn pandas\n\n\n\nI would expect these to return arrays for array-like input. Is there any reason not to?\n",
    "labels": [],
    "comments": [
      "nope, indeed not\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 3,
    "additions": 11,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/common.py",
      "pandas/tests/test_common.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1756,
    "reporter": "wesm",
    "created_at": "2012-08-10T21:08:32+00:00",
    "closed_at": "2012-08-10T22:05:19+00:00",
    "resolver": "wesm",
    "resolved_in": "937f115c17e50665f290a2c33ceadac29c536a19",
    "resolver_commit_num": 2268,
    "title": "Time zone conversion routines assume monotonic timestamps",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 26,
    "deletions": 11,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1757,
    "reporter": "njsmith",
    "created_at": "2012-08-10T22:23:29+00:00",
    "closed_at": "2012-08-13T03:20:13+00:00",
    "resolver": "wesm",
    "resolved_in": "24c5b8fa63afa3149ed9d2ebbad1d56ba33c4e27",
    "resolver_commit_num": 2299,
    "title": "MultiIndexes containing >=1000000 elements do not work",
    "body": "With current master, any attempt to index into a Series (or whatever) with a MultiIndex and >=1000000 (= _SIZE_CUTOFF) elements simply raises an error:\n\n\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 47,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/src/engines.pyx",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1758,
    "reporter": "manuteleco",
    "created_at": "2012-08-12T09:21:02+00:00",
    "closed_at": "2012-08-12T22:01:28+00:00",
    "resolver": "wesm",
    "resolved_in": "ef105e79e202687b74d1f1464ca67e841e19ee40",
    "resolver_commit_num": 2294,
    "title": "Wrong defaults in the documentation for \"merge\"",
    "body": "Hi,\n\nas of pandas '0.8.2.dev-ee9321f', the merge operation is using False as default value for the optional arguments \"left_index\" and \"right_index\". However, its docstring states that the default value for both arguments is True instead.\n\nThanks and regards.\nManu.\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 1,
    "additions": 2,
    "deletions": 2,
    "changed_files_list": [
      "pandas/core/frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1759,
    "reporter": "wesm",
    "created_at": "2012-08-12T20:03:52+00:00",
    "closed_at": "2012-09-18T20:22:29+00:00",
    "resolver": "wesm",
    "resolved_in": "428de741456502f8f38c19f48720a57061bb1399",
    "resolver_commit_num": 2403,
    "title": "pandas.unique needs to be more careful on index types",
    "body": "e.g.\n\n\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 5,
    "additions": 43,
    "deletions": 6,
    "changed_files_list": [
      "pandas/core/algorithms.py",
      "pandas/core/common.py",
      "pandas/core/frame.py",
      "pandas/core/series.py",
      "pandas/tests/test_algos.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1761,
    "reporter": "jankatins",
    "created_at": "2012-08-13T12:19:34+00:00",
    "closed_at": "2012-08-13T21:50:47+00:00",
    "resolver": "wesm",
    "resolved_in": "4083689b8058c8de3f59ec297e1b1580ec993eeb",
    "resolver_commit_num": 2301,
    "title": "DataFrame.corr() throws error on simple data",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 11,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1763,
    "reporter": "lesteve",
    "created_at": "2012-08-13T13:43:47+00:00",
    "closed_at": "2012-08-17T18:37:41+00:00",
    "resolver": "wesm",
    "resolved_in": "3bd9d15cc6e4cd327ca8c62060a7347f7eea3a61",
    "resolver_commit_num": 2302,
    "title": "pandas.DatetimeIndex.isin always returns an array of False",
    "body": "pandas version: 0.8.2.dev-742d7fb\n\n\n\nIt works fine with pandas 0.7.3, i.e. it returns an array of True instead. This works fine as well with a pandas.Int64Index. `index.isin(index)` just calls `pandas.lib.ismember(index._array_values(), set(index))` but I am afraid I lack the skills to understand what goes wrong in the cython code.\n\nJust for some background, the original problem I bumped into:\n\n\n\nI tracked it down to `ts.ix._get_item_iterable` which has a special case for when the index has duplicates which use pandas.TimeSeries.isin.\n\nAs an aside it looks like pandas.core.common._asarray_tuplesafe(index) is wrong when using numpy 1.6 so that ts.ix[ts.index] still would return an empty array if this bug was fixed, but I guess that's a numpy 1.6 datetime64 bug which I have seen mentioned in a few other places.\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 34,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1772,
    "reporter": "dalejung",
    "created_at": "2012-08-16T03:01:34+00:00",
    "closed_at": "2012-09-11T01:11:58+00:00",
    "resolver": "wesm",
    "resolved_in": "54b54f8c36ff186e48eba8dd7f2d76ae06478c1c",
    "resolver_commit_num": 2373,
    "title": "Error with downsampling intraday data where end.time() < start.time()",
    "body": "Simple Example\n\n\n\nLong example:\n%20binning%20error.ipynb\n\nTracking it down, it appears that the problem is that `_get_range_edges` carries the time over when downsampling intraday data. So when `generate_range` is called during the `DatetimeIndex` creation, the final bin doesn't pass the `while cur <= end` check. \n\nThinking about it,  there are two issues. \n\n1) `generate_range` should never output an index that doesn't include end. Maybe something\n\n\n\n2) `_generate_range_edges` should generate a range that is perfectly divisible by the freq. For the downsampling, we'd have to change the time by adjusting the end time or just zeroing both out. I don't know how many rely on this behavior though. \n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 3,
    "additions": 26,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1773,
    "reporter": "spearsem",
    "created_at": "2012-08-16T13:00:45+00:00",
    "closed_at": "2012-09-04T20:28:13+00:00",
    "resolver": "lodagro",
    "resolved_in": "cfe674ec635e175d268b08f8feb2a14c25f8f05c",
    "resolver_commit_num": 49,
    "title": "DataFrame.drop_duplicates works literally only with list of column names, but fails when used on output of DataFrame.columns",
    "body": "`DataFrame.drop_duplicates()` does not properly handle array objects returned by `DataFrame.columns` (whether or not you use `DataFrame.columns.values` to get a NumPy array). If you compute \n\n\n\nthen it works, but this is needless overkill, especially when dealing with a large number of columns. Below is an example from IPython.\n\n\n\nFWIW:\n\n\n",
    "labels": [],
    "comments": [
      "fixed \n\n``` python\nIn [2]: dfrm = pandas.DataFrame({\"A\":[1,2,1,2,1,2], \"B\":[3,4,3,4,3,4], \"C\":[1,2,1,2,1,3]})\n\nIn [3]: dfrm.drop_duplicates(dfrm.columns)\nOut[3]: \n   A  B  C\n0  1  3  1\n1  2  4  2\n5  2  4  3\n```\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 2,
    "additions": 10,
    "deletions": 9,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1776,
    "reporter": "timcera",
    "created_at": "2012-08-17T17:05:32+00:00",
    "closed_at": "2012-09-09T03:25:23+00:00",
    "resolver": "wesm",
    "resolved_in": "7a83180fa3fc9297cdb253abd80d5977e68dfbcf",
    "resolver_commit_num": 2356,
    "title": "Suggest using ISO 8601 for display of Period and PeriodIndex",
    "body": "As of pandas 0.8.1, the Period and PeriodIndex display uses the English abbreviations for the month.  Suggest that ISO 8601, would be a better choice.\n\nAnnual Period is fine...\n\n> z =PeriodIndex([Period('2012'), Period('2013')])\n> \n> z\n> Out[182]: \n> <class 'pandas.tseries.period.PeriodIndex'>\n> freq: A-DEC\n> [2012, ..., 2013]\n> length: 2\n\nMonthly though...\n\n> In [185]: z =PeriodIndex([Period('2012-01'), Period('2012-02')])\n> \n> In [186]: z\n> Out[186]: \n> <class 'pandas.tseries.period.PeriodIndex'>\n> freq: M\n> [Jan-2012, ..., Feb-2012]\n> length: 2\n\nI think that 'z' instead should be:\n\n> In [186]: z\n> Out[186]: \n> <class 'pandas.tseries.period.PeriodIndex'>\n> freq: M\n> [2012-01, ..., 2012-02]\n> length: 2\n\nHourly currently is\n\n> z =PeriodIndex([Period('2012-01-01 00', freq='H'), Period('2012-01-01 01', freq='H')])\n> \n> z\n> Out[193]: \n> <class 'pandas.tseries.period.PeriodIndex'>\n> freq: H\n> [01-Jan-2012 00:00, ..., 01-Jan-2012 01:00]\n> length: 2\n\nThe ISO 8601 standard representation of hourly Periods would be:\n\n> In [186]: z\n> Out[186]: \n> <class 'pandas.tseries.period.PeriodIndex'>\n> freq: M\n> [2012-01-01 00, ..., 2012-01-01 01]\n> length: 2\n\nAdvantages:\nNot English centric\nDirectly sortable chronologically without parsing\n\nKindest regards,\nTim\n",
    "labels": [],
    "comments": [
      "Agreed\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 3,
    "additions": 10,
    "deletions": 6,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/src/plib.pyx",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1777,
    "reporter": "ijmcf",
    "created_at": "2012-08-17T17:39:32+00:00",
    "closed_at": "2012-08-18T01:56:03+00:00",
    "resolver": "wesm",
    "resolved_in": "befedb5294e1865312fc7810e05c293519489730",
    "resolver_commit_num": 2303,
    "title": "DatetimeIndex from timezone-aware datetimes fails",
    "body": "If you try to create a DatetimeIndex from a list of timezone aware datetimes, you get the exception: \"Tz-aware datetime.datetime cannot be converted to datetime64 unless utc=True\"\n\nFor example:\n\n> > > import pandas, datetime, pytz\n> > > d = [datetime.datetime(2012, 8, 19, tzinfo=pytz.timezone('US/Eastern'))]\n> > > pandas.DatetimeIndex(d)\n",
    "labels": [],
    "comments": [
      "This is similar to issue 1676 raised by kdebrab, but that raises a different exception, so I created a new issue for this one. \n",
      "Confirmed it's still a bug in master\n"
    ],
    "events": [
      "commented",
      "commented"
    ],
    "changed_files": 6,
    "additions": 75,
    "deletions": 14,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/index.py",
      "pandas/src/datetime.pyx",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timezones.py",
      "pandas/tseries/tools.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1778,
    "reporter": "ijmcf",
    "created_at": "2012-08-17T17:41:15+00:00",
    "closed_at": "2012-09-04T01:36:38+00:00",
    "resolver": "wesm",
    "resolved_in": "eae588d2b20af5ece4e98c35b37252144e6589be",
    "resolver_commit_num": 2304,
    "title": "In a DatetimeIndex using a non-static timezone over a time range that crosses the DST line, the time shifts around",
    "body": "For example:\n\n> > > dr = pandas.date_range('03/06/2012 00:00', periods=25, freq='W-FRI', tz='US/Eastern')\n> > > print dr\n> > > <class 'pandas.tseries.index.DatetimeIndex'>\n> > > [2012-03-09 00:00:00, ..., 2012-08-24 01:00:00]\n> > > Length: 25, Freq: W-FRI, Timezone: US/Eastern\n\nThis is midnight at the start, but 1am at the end. Shouldn't it be midnight all along, regardless of DST?\n",
    "labels": [],
    "comments": [
      "Yeah, it should be. Thanks for pointing it out\n",
      "Fixed this one too\n",
      "I found an instance where this bug is still occurring:\n\n> > > pandas.date_range('2012-11-02', periods=10, tz=pytz.timezone('US/Eastern'))\n\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2012-11-02 00:00:00, ..., 2012-11-10 23:00:00]\nLength: 10, Freq: D, Timezone: US/Eastern\n",
      "thanks, reopened the issue\n",
      "Fixed in git master\n"
    ],
    "events": [
      "commented",
      "closed",
      "commented",
      "commented",
      "reopened",
      "commented",
      "referenced"
    ],
    "changed_files": 6,
    "additions": 93,
    "deletions": 36,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/index.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_plotting.py",
      "pandas/tseries/tests/test_timezones.py",
      "pandas/tseries/tools.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1783,
    "reporter": "dalejung",
    "created_at": "2012-08-19T04:16:28+00:00",
    "closed_at": "2012-09-18T17:21:48+00:00",
    "resolver": "wesm",
    "resolved_in": "852a99459c2a7d47fc7fb0574cdad8359a42fade",
    "resolver_commit_num": 2401,
    "title": "Default empty DataFrame to dtype=object",
    "body": "I ran into the following :\n\n\n\nThe problem being that an empty dataframe's dtype defaults to float. I've adjusted for it by checking the len of data and setting the dtype to object when at 0. But I feel like an empty DataFrame should act like an empty list, valid for all list operations. Which in lieu of #549, means casting the widest net possible with dtype=object. \n",
    "labels": [],
    "comments": [
      "Taking a look at this. I'm amazed (disturbed, a little?) so far how little code I'm having to touch (and how few tests are breaking as a result) to make this change\n",
      "`<MrBurns>`Excellent...`</MrBurns>`\n\nI think the empty DataFrame Constructor is a corner case that short-circuits with a len check. So I can't imagine much depends on the float behavior. \n"
    ],
    "events": [
      "commented",
      "commented"
    ],
    "changed_files": 6,
    "additions": 55,
    "deletions": 21,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/internals.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1788,
    "reporter": "lodagro",
    "created_at": "2012-08-20T14:31:12+00:00",
    "closed_at": "2012-08-22T18:38:29+00:00",
    "resolver": "lodagro",
    "resolved_in": "95f30cab5ddb7c73cfd64a6fc0aeb68db97a0a4b",
    "resolver_commit_num": 47,
    "title": "pct_change() fails on series holding int64",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 2,
    "additions": 13,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/series.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1790,
    "reporter": "ijmcf",
    "created_at": "2012-08-20T18:06:19+00:00",
    "closed_at": "2012-09-04T01:45:05+00:00",
    "resolver": "wesm",
    "resolved_in": "d67f9d62ce7105b1680e75877df83dc3ce04896b",
    "resolver_commit_num": 2322,
    "title": "Creating DatetimeIndex from timezone-aware datetimes shifts the times",
    "body": "Creating a DatetimeIndex from a list of timezone-aware datetimes shift the times according to the offset of the timezone.\n\nUsing an easy way to get a list of datetimes by creating a DatetimeIndex and casting it to a list:\ndr = pandas.date_range('2012-06-02', periods=10, tz=pytz.timezone('US/Eastern'))\n\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2012-06-02 00:00:00, ..., 2012-06-11 00:00:00]\nLength: 10, Freq: D, Timezone: US/Eastern\n\npandas.DatetimeIndex(list(a))\n\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2012-06-02 04:00:00, ..., 2012-06-11 04:00:00]\nLength: 10, Freq: None, Timezone: US/Eastern\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 2,
    "additions": 11,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1791,
    "reporter": "jseabold",
    "created_at": "2012-08-20T18:17:04+00:00",
    "closed_at": "2012-09-09T17:16:52+00:00",
    "resolver": "wesm",
    "resolved_in": "553a2a956f2627532f9c2dc63b87f065b284a8be",
    "resolver_commit_num": 2358,
    "title": "lib.isnullobj fails on list of strings input",
    "body": "Was playing around with the new string functions and noticed this. Don't really know if it's a bug or expected\n\n\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 5,
    "additions": 22,
    "deletions": 4,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/common.py",
      "pandas/core/frame.py",
      "pandas/tests/test_common.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1793,
    "reporter": "ludaavics",
    "created_at": "2012-08-20T22:21:21+00:00",
    "closed_at": "2012-08-21T09:13:17+00:00",
    "resolver": "lodagro",
    "resolved_in": "34a0646093035087e4b900cb3ac5720b642d4a08",
    "resolver_commit_num": 45,
    "title": "Typo",
    "body": "-docs/dev/timeseries.html#time-span-representation\n\n\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 1,
    "additions": 1,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/timeseries.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1796,
    "reporter": "lodagro",
    "created_at": "2012-08-21T20:21:47+00:00",
    "closed_at": "2012-09-18T21:29:51+00:00",
    "resolver": "wesm",
    "resolved_in": "b8e7a359570d2bb59edf9aae7787f5b57b2c16c3",
    "resolver_commit_num": 2407,
    "title": "df.xs() level argument behavior changed.",
    "body": "When using xs in combination with a MultiIndex, before it was not needed to specify the _level_ argument if _key_ started from level 0 and referred to consecutive levels (see also the examples described in #1684), now this no longer works and _level_ needs to be defined in case _key_ is a list.\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "cross-referenced",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 23,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/index.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1803,
    "reporter": "sadruddin",
    "created_at": "2012-08-22T15:12:03+00:00",
    "closed_at": "2012-12-02T02:13:09+00:00",
    "resolver": "wesm",
    "resolved_in": "6abbbc1c090732d570f5fee7ee6a89bbb92c8174",
    "resolver_commit_num": 2665,
    "title": "Can't assign to subsets of MultiIndex columns DataFrames using partial labels",
    "body": "Assigning a value (scalar or Series) to the subset of a dataframe with MultiIndex columns addressed by a partial label doesn't work.\n\n\n\nThe same thing in the rows dimension works as expected.\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 6,
    "additions": 40,
    "deletions": 6,
    "changed_files_list": [
      "LICENSES/NUMPY_LICENSE",
      "LICENSES/PSF_LICENSE",
      "LICENSES/SCIPY_LICENSE",
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1805,
    "reporter": "twiecki",
    "created_at": "2012-08-23T17:01:24+00:00",
    "closed_at": "2012-09-08T02:28:47+00:00",
    "resolver": "minrk",
    "resolved_in": "7eaf5ca6daec0159c8d7df344579012e12758ee3",
    "resolver_commit_num": 0,
    "title": "Include .c files in repo.",
    "body": "`pip install git+`\n\ndoes currently not work because the .pyx files are not being cythonized (not sure why). An easy fix is to include .c files in the git repo which should make it easier for people to deploy.\n\nI used a simple try: import cython in setup.py that cythonizes if cython is installed and uses the .c files otherwise:\n\n-devs/hddm/blob/develop/setup.py#L4\n",
    "labels": [],
    "comments": [
      "I don't want to include the .c files in the repo because of the diff noise. @minrk do you know what could be wrong?\n",
      "I might.  @twiecki - do you have vanilla setuptools or distribute?\n\nI encountered this or similar in pyzmq, where old-style setuptools will actually explicitly convert all your `.pyx` extensions back to `.c` if _pyrex_ is not importable.  [my workaround](https://github.com/zeromq/pyzmq/commit/e9f623981c77f60663656559d38d2fd1427ff18e).  distribute doesn't have this problem.\n\nI can confirm on my system that a virtualenv created with setuptools cannot run the install above, but one with distribute instead can, so it's probably the same issue.\n",
      "@minrk Yes, it's in a virtualenv. Not sure what it was created with (used virtualenvwrapper), it might have both actually.\n",
      "Try this, then:\n\n```\npip install git+https://github.com/minrk/pandas.git@badsetuptools\n```\n",
      "(if it works, I'll do a PR)\n",
      "@minrk: yes, that did the trick\n\ncythoning pandas/src/generated.pyx to pandas/src/generated.c\n\nThanks!\n\nOn Thu, Aug 23, 2012 at 2:34 PM, Min RK notifications@github.com wrote:\n\n> (if it works, I'll do a PR)\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/pydata/pandas/issues/1805#issuecomment-7979354.\n",
      "See #1806\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 1,
    "additions": 8,
    "deletions": 0,
    "changed_files_list": [
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1807,
    "reporter": "jseabold",
    "created_at": "2012-08-23T20:25:06+00:00",
    "closed_at": "2012-09-04T02:16:37+00:00",
    "resolver": "wesm",
    "resolved_in": "dbfdb073030a8ef868bcd5583433136c555c153f",
    "resolver_commit_num": 2323,
    "title": "Sort broken after unique -> Series with object array",
    "body": "I have no idea why this fails.\n\n\n\nbut this doesn't\n\n\n",
    "labels": [],
    "comments": [
      "the unique method in hashtable sets the base to be something other than an ndarray.\n@wesm is setting uniques.base necessary? I have it setup locally right now so sort checks whether base is an instance of ndarray, but if uniques.base isn't necessary I'll just get rid of those in hashtable.pyx and we should be good to go.\n",
      "Well, the issue is ownership of the data buffer. I think there is a way to transfer ownership of the buffer that doesn't require setting `uniques.base`. See http://gael-varoquaux.info/blog/?p=157 (this is where I got the method from that I'm using as I recall)( \n",
      "Having a look at this. If you resize the databuffer and manually set the array flags to own the data, it should work\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented"
    ],
    "changed_files": 5,
    "additions": 62,
    "deletions": 15,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/src/hashtable.pyx",
      "pandas/src/numpy_helper.h",
      "pandas/src/util.pxd",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1815,
    "reporter": "jmloser",
    "created_at": "2012-08-27T02:57:38+00:00",
    "closed_at": "2012-09-10T23:06:27+00:00",
    "resolver": "wesm",
    "resolved_in": "f953727f6c42c2fe258aaa34611cf10a28deacdc",
    "resolver_commit_num": 2371,
    "title": "concat() drops index when used on series with a PeriodIndex",
    "body": "New to this, sorry if I'm just missing something...\n\n\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 5,
    "additions": 80,
    "deletions": 14,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/index.py",
      "pandas/tseries/index.py",
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1821,
    "reporter": "lodagro",
    "created_at": "2012-08-28T20:28:40+00:00",
    "closed_at": "2012-09-18T21:09:23+00:00",
    "resolver": "wesm",
    "resolved_in": "684e9dd448b183506b6d68b8efa18bd1fddf998c",
    "resolver_commit_num": 2406,
    "title": "DatetimeIndex, df.ix[date] and df.ix[[date]] failure",
    "body": "from [stackoverflow](-a-subset-of-a-pandas-dataframe-indexed-by-datetimeindex-with-a-list-of)\n\nThe original frame can not be shared, but following code reproduces the issue.\n\n\n\nrunning this script outputs:\n\n\n\nfor n = 1000000 it gets worse\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 3,
    "additions": 42,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/src/engines.pyx",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1823,
    "reporter": "wesm",
    "created_at": "2012-08-29T21:13:23+00:00",
    "closed_at": "2012-09-09T03:07:58+00:00",
    "resolver": "wesm",
    "resolved_in": "b66dc9cf1cd40a9ce667c749e519de7dae902ae9",
    "resolver_commit_num": 2354,
    "title": "Panel truncate failure",
    "body": "user reported this error (may be for all panels)\n\n\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 15,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/internals.py",
      "pandas/tests/test_panel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1826,
    "reporter": "wesm",
    "created_at": "2012-08-30T01:14:01+00:00",
    "closed_at": "2012-09-09T03:40:45+00:00",
    "resolver": "wesm",
    "resolved_in": "5fba03bda35e906d73b74853da31e02c93a71798",
    "resolver_commit_num": 2357,
    "title": "Better error message in Panel.__setitem__ for TypeError",
    "body": "`else` case never handled; can least to unboundlocalerror\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 6,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/panel.py",
      "pandas/tests/test_panel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1827,
    "reporter": "timcera",
    "created_at": "2012-08-30T18:01:58+00:00",
    "closed_at": "2012-08-30T21:04:44+00:00",
    "resolver": "lodagro",
    "resolved_in": "b17d4bbacab2bf2aafb4dbe6a7966a0f605b4f79",
    "resolver_commit_num": 48,
    "title": "WEB DOC: 'Not Interpolated' and 'Interpolated' plots on http://pandas.pydata.org/pandas-docs/stable/missing_data.html",
    "body": "The 'Not Interpolated' and 'Interpolated' plots on -docs/stable/missing_data.html appear identical.  Suspect that the 'Not Interpolated' should have gaps?\n",
    "labels": [],
    "comments": [
      "The plot that should be there is not generated, i expect on old plot with the same name to be on the server and this one is displayed.\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 1,
    "additions": 0,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/missing_data.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1829,
    "reporter": "lodagro",
    "created_at": "2012-08-30T20:10:48+00:00",
    "closed_at": "2012-09-08T02:48:26+00:00",
    "resolver": "wesm",
    "resolved_in": "35322447d88a5e9b2c4e834dd434f12db297829d",
    "resolver_commit_num": 2335,
    "title": "Add args/kwds arguments to Series.apply(), like for DataFrame.apply()",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 15,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/series.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1833,
    "reporter": "snth",
    "created_at": "2012-09-03T11:07:26+00:00",
    "closed_at": "2012-09-09T01:33:37+00:00",
    "resolver": "wesm",
    "resolved_in": "dc0db65e0f6158c381469ebc32fcbf8e08ca362b",
    "resolver_commit_num": 2352,
    "title": "Bug in DataFrame.duplicated() when dealing with datetime64",
    "body": "The following looks like a bug to me as DataFrame.duplicated() gives different results on what should be identical inputs. To me it looks like the problem is with the datetime64 values because if you look at the output of `dates.values` it's clear that the last 4 values are duplicates.\n\nPlease see the code below that reproduces the problem:\n\n\n\nI remember reading somewhere that there are problems with datetime64 in numpy 1.6 but I don't understand what coercions are taking place behind the scenes. Also, if someone could please explain to me why the dates in `dates.values` above are wrong and how to avoid this, I would appreciate it.\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 21,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1840,
    "reporter": "erg",
    "created_at": "2012-09-04T18:35:53+00:00",
    "closed_at": "2012-09-07T18:47:41+00:00",
    "resolver": "wesm",
    "resolved_in": "821da97126b53508f0df8c13972f7c0d28780a52",
    "resolver_commit_num": 2324,
    "title": "pandas.rolling_std() gives nans where it probably shouldn't",
    "body": "The output differs from bottleneck, which also gives nans, but fewer.\n\nThe Factor version never outputs nans and otherwise matches the pandas version.\n\n\n",
    "labels": [],
    "comments": [
      "Same bug report on bottleneck:\nhttps://github.com/kwgoodman/bottleneck/issues/50\n",
      "Bottleneck is fixing their version of this bug...;)\n",
      "The bottleneck bug was taking the square root of a negative number. Probably the problem here.\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 35,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/stats/moments.py",
      "pandas/stats/tests/test_moments.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1841,
    "reporter": "erg",
    "created_at": "2012-09-04T21:42:29+00:00",
    "closed_at": "2012-09-08T02:36:27+00:00",
    "resolver": "wesm",
    "resolved_in": "c3f19de5af07378284e69fe722df0d6d40d434eb",
    "resolver_commit_num": 2331,
    "title": "ewma docs don't explain time_rule",
    "body": "Not much else to say--should add an explanation in `_ewm_doc`.\n\n\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 1,
    "additions": 2,
    "deletions": 0,
    "changed_files_list": [
      "pandas/stats/moments.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1842,
    "reporter": "lodagro",
    "created_at": "2012-09-05T08:00:07+00:00",
    "closed_at": "2012-09-10T19:52:55+00:00",
    "resolver": "lodagro",
    "resolved_in": "f4fe06511e056c4990dd0388cf5f927b259f5a46",
    "resolver_commit_num": 50,
    "title": "df.plot(x=..., y=...) fails for mixed-integer column names",
    "body": "\n",
    "labels": [],
    "comments": [
      "Is it ok if change the x, y api \nfrom _reference by label only if label is string, otherwise use position_ \nto _reference by label and have positional fallback (except for all integer column names)_ ?\n",
      "I don't have a strong preference. @changhiskhan you have any opinion?\n",
      "I think that makes sense because it's more consistent with DataFrame indexing convention.\n",
      "OK then. @lodagro go for it\n",
      "Any reason why sort is called on x?\n\n``` python\n   if x is not None:\n        frame = frame.set_index(x).sort_index()\n```\n\n``` python\nIn [149]: x = [0, 1, 1, 0]\n\nIn [150]: y = [0, 0, 1, 1]\n\nIn [151]: df = pandas.DataFrame({'x': x, 'y': y})\n\nIn [152]: df.plot(x='x', y='y')\nOut[152]: <matplotlib.axes.AxesSubplot at 0xb41d44c>\n\nIn [153]: plot(x,y)\nOut[153]: [<matplotlib.lines.Line2D at 0xb5357ac>]\n```\n",
      "I don't think the sort_index is necessary. Though I think use_index should be set to True if x is not None.\n"
    ],
    "events": [
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "assigned",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 24,
    "deletions": 16,
    "changed_files_list": [
      "pandas/core/index.py",
      "pandas/tests/test_graphics.py",
      "pandas/tools/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1848,
    "reporter": "John-Colvin",
    "created_at": "2012-09-06T14:47:53+00:00",
    "closed_at": "2012-09-09T03:17:31+00:00",
    "resolver": "wesm",
    "resolved_in": "a0b59f8d62b206052f383318012d691d9a324ba3",
    "resolver_commit_num": 2355,
    "title": "lib.infer_dtype_list(MultiIndex) crashes python",
    "body": "If lib.infer_dtype_list is called with a MultiIndex, it crashes python (on OS X 10.7 at least)\n\nThis crash occurs when writing a dataframe with a multiindex to a pytables table (_write_table)\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 13,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/pytables.py",
      "pandas/io/tests/test_pytables.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1850,
    "reporter": "mrjbq7",
    "created_at": "2012-09-06T23:02:13+00:00",
    "closed_at": "2012-09-08T02:34:47+00:00",
    "resolver": "wesm",
    "resolved_in": "cd9655c6b0ae1d1230bb9459042cf4b02e998464",
    "resolver_commit_num": 2330,
    "title": "pandas.rolling_apply: \"Out of bounds on buffer access\"",
    "body": "Looks like `pandas.rolling_apply` doesn't like a window larger than the length of the array:\n\n\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 14,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/src/moments.pyx",
      "pandas/stats/tests/test_moments.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1851,
    "reporter": "codeape2",
    "created_at": "2012-09-07T12:24:09+00:00",
    "closed_at": "2012-09-08T02:13:31+00:00",
    "resolver": "wesm",
    "resolved_in": "bdf58cc8cbd096ae2e6123d3e6e2da774cdef517",
    "resolver_commit_num": 2328,
    "title": "python-dateutils < 2 requirement for python 2. Is it necessary?",
    "body": "The most recent version (2.1) supports both 2.x and 3.x.\n\nPandas works for me using version 2.1:\n\n\n\nI suggest change setup.py.\n",
    "labels": [],
    "comments": [
      "Yes-- will do so, thanks\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 2,
    "additions": 2,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1854,
    "reporter": "lodagro",
    "created_at": "2012-09-07T13:53:20+00:00",
    "closed_at": "2012-09-15T17:20:41+00:00",
    "resolver": "lodagro",
    "resolved_in": "f5536682d1f6b63910184c5944428c6c19a18ebf",
    "resolver_commit_num": 53,
    "title": "indicate __repr__ truncation by ...",
    "body": "When a column is wider than 50 chars (controllable by `pandas.setprintoptions(max_colwidth=...)`) it is truncated in the `__repr__`, indicate this by adding `...` at the end of the truncated strings.\n\nsee also #1852\n",
    "labels": [],
    "comments": [],
    "events": [
      "referenced"
    ],
    "changed_files": 2,
    "additions": 25,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/format.py",
      "pandas/tests/test_format.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1857,
    "reporter": "mlew",
    "created_at": "2012-09-07T20:46:20+00:00",
    "closed_at": "2012-09-10T21:57:01+00:00",
    "resolver": "changhiskhan",
    "resolved_in": "ddcc30a7e6f7761638d0936e21bfae1144424e1c",
    "resolver_commit_num": 43,
    "title": "Period.start_time returns the same value as Period.end_time",
    "body": "If you create a Period object with a non-Day frequency, the start_time property incorrectly returns the end_time value.\n\nex:\n\n\n\nThe expected behavior would return a `<Timestamp: 2012-08-14 00:00:00>` object for the Period.start_time property.\n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned",
      "cross-referenced"
    ],
    "changed_files": 4,
    "additions": 61,
    "deletions": 7,
    "changed_files_list": [
      "pandas/io/sql.py",
      "pandas/tseries/frequencies.py",
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1858,
    "reporter": "wesm",
    "created_at": "2012-09-07T21:53:23+00:00",
    "closed_at": "2012-11-28T00:26:47+00:00",
    "resolver": "wesm",
    "resolved_in": "6a7c11cc2655f22dbf33265d190318f90762e302",
    "resolver_commit_num": 2630,
    "title": "Explicit column dtype specification in read_* functions",
    "body": "e.g. columns with values like `01001` are getting converted to int\n\nexample from mailing list:\n\n\n",
    "labels": [],
    "comments": [
      "This is done in c-parser (`dtype={'oid': object}`) but needs a unit test\n",
      "This works now:\n\n```\nIn [11]: df = read_clipboard(delim_whitespace=True, dtype={'oid': 'O', 'did': 'O', 'mode': 'O'}); df\nOut[11]: \n     oid    did mode             ox             oy      dx      dy\n0  01001  01001   01  272311.659358  176751.822655  272675  176375\n1  01001  01001   01  272311.659358  176751.822655  272375  176375\n2  01001  01001   01  272311.659358  176751.822655  272125  176675\n3  01001  01001   06  272311.659358  176751.822655  272675  177125\n4  01001  01001   06  272311.659358  176751.822655  272675  176375\n```\n\nThis needs to be able to accept more than just format strings though (e.g. 'f8'). I'll do that then close this issue.\n"
    ],
    "events": [
      "commented",
      "commented"
    ],
    "changed_files": 2,
    "additions": 17,
    "deletions": 3,
    "changed_files_list": [
      "pandas/io/tests/test_cparser.py",
      "pandas/src/parser.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1859,
    "reporter": "wesm",
    "created_at": "2012-09-07T22:23:57+00:00",
    "closed_at": "2012-09-08T02:02:01+00:00",
    "resolver": "wesm",
    "resolved_in": "ad33f4e251b16cdabec415a2a8e67d892750e651",
    "resolver_commit_num": 2327,
    "title": "Series.str.split doesn't work with no arguments",
    "body": "Inconsistent with builtin string API\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 18,
    "deletions": 6,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/strings.py",
      "pandas/tests/test_strings.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1861,
    "reporter": "jreback",
    "created_at": "2012-09-08T13:04:34+00:00",
    "closed_at": "2012-09-09T01:22:33+00:00",
    "resolver": "wesm",
    "resolved_in": "3a1de719f995854cdf8818f22ac73b281cfb6242",
    "resolver_commit_num": 2351,
    "title": "deprecated from pandas.stats import misc fails in 0.8.1",
    "body": "I believe this is a deprecated import (I used it to access bucket_series)\nexists currently in master 0.9.dev as well\n\nPython 2.7.3 (default, Jun 21 2012, 07:50:29) \n\n> > > import pandas\n> > > print pandas.**version**\n> > > 0.8.1\n> > > from pandas.stats import misc\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File \"/usr/local/lib/python2.7/site-packages/pandas-0.8.1-py2.7-linux-x86_64.egg/pandas/stats/misc.py\", line 7, in <module>\n> > >     from pandas.tools.tile import quantileTS\n> > > ImportError: cannot import name quantileTS\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 1,
    "additions": 0,
    "deletions": 1,
    "changed_files_list": [
      "pandas/stats/misc.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1866,
    "reporter": "wesm",
    "created_at": "2012-09-09T00:40:21+00:00",
    "closed_at": "2012-09-10T02:39:13+00:00",
    "resolver": "wesm",
    "resolved_in": "59767d57a2ff0190118ff2519a2c092d7d93b7d6",
    "resolver_commit_num": 2365,
    "title": "Doc build bug",
    "body": "@changhiskhan there's some bug showing up in the docs but not in the main test suite. Can you have a look?\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "mentioned",
      "subscribed",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 38,
    "deletions": 5,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/inference.pyx",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1870,
    "reporter": "wesm",
    "created_at": "2012-09-09T02:54:08+00:00",
    "closed_at": "2016-06-17T00:09:32+00:00",
    "resolver": "jreback",
    "resolved_in": "6d8c04ce64d975c2fd8c902c0e5df343805bd112",
    "resolver_commit_num": 4050,
    "title": "ordered_merge improvements",
    "body": "-like-asof-join-for-timeseries-data-in-pandas/12336039#12336039\n",
    "labels": [],
    "comments": [
      "It would be great to be able to perform ordered left joins using a time-series index as well.\n",
      "+1 vote for \"ordered left joins.\"\n",
      "+1 vote for \"ordered left joins.\n",
      "Any thoughts on this? A kdb-style asof-join is very useful in time-series analysis. The rows of the right-hand table would just be picked via a DataFrame.asof (see #2941).\n",
      "http://pandas.pydata.org/pandas-docs/stable/merging.html?highlight=ordered_merge#merging-ordered-data\n\n(unless something else was meant here). This has been in for a while.\n",
      "@jreback Unlike the pd.ordered_merge(), the kdb aj() only keeps the rows from the left-hand table.\n\nhttp://code.kx.com/wiki/Reference/aj\n\nI.e., as with a regular left-join, the table doesn't get more rows; it just appends columns from the right-hand table.\n\nActually, it's very similar to a left-join, but instead of searching for equal keys, it searches for the greatest key from the right-hand table that is less-than-or-equal-to the key from the left-hand table. This is exactly what the asof function does in pandas, so we're part of the way there already.\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 30,
    "additions": 1975,
    "deletions": 278,
    "changed_files_list": [
      "doc/source/api.rst",
      "doc/source/merging.rst",
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/__init__.py",
      "pandas/algos.pyx",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/series.py",
      "pandas/hashtable.pyx",
      "pandas/indexes/category.py",
      "pandas/src/join.pyx",
      "pandas/tests/frame/test_asof.py",
      "pandas/tests/series/test_asof.py",
      "pandas/tests/series/test_timeseries.py",
      "pandas/tools/merge.py",
      "pandas/tools/tests/data/allow_exact_matches.csv",
      "pandas/tools/tests/data/allow_exact_matches_and_tolerance.csv",
      "pandas/tools/tests/data/asof.csv",
      "pandas/tools/tests/data/asof2.csv",
      "pandas/tools/tests/data/cut_data.csv",
      "pandas/tools/tests/data/quotes.csv",
      "pandas/tools/tests/data/quotes2.csv",
      "pandas/tools/tests/data/tolerance.csv",
      "pandas/tools/tests/data/trades.csv",
      "pandas/tools/tests/data/trades2.csv",
      "pandas/tools/tests/test_merge_asof.py",
      "pandas/tools/tests/test_merge_ordered.py",
      "pandas/tools/tests/test_tile.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1881,
    "reporter": "jreback",
    "created_at": "2012-09-10T13:27:05+00:00",
    "closed_at": "2012-09-17T18:03:39+00:00",
    "resolver": "wesm",
    "resolved_in": "815256a284e6a2c32e69e5d04028d09b096b9152",
    "resolver_commit_num": 2393,
    "title": "Backwards incompatibility in io/pytables with existing data prior to 0.8 causes data corruption",
    "body": "trying to append to a table that was created before 0.8 (e.g. before the kind of datetime64 existed) causes corrupted data (essentially the index is converted to datetime64, but since its in nanoseconds, existing datetimes are not evaluated correctly). easiest to convert existing data (read in and just write it out), so this patch is simply an exception which is raised if incompatble kinds are detected.\n\n\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 17,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/pytables.py",
      "pandas/io/tests/test_pytables.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1884,
    "reporter": "erg",
    "created_at": "2012-09-10T18:34:27+00:00",
    "closed_at": "2012-09-13T01:22:46+00:00",
    "resolver": "wesm",
    "resolved_in": "8743be59612138c2e20d6303766063631c9537e7",
    "resolver_commit_num": 2381,
    "title": "pandas.rolling_std() first value is nan",
    "body": "The window is 3, but we want a std at `min_periods=1`.  The one-period standard deviation is trivially 0.\n\n\n\nThe pathological case:\n\n\n\nMaybe it's because pandas is taking the unbiased std for `N-1` where `N = 1`, so it's dividing by zero?\n",
    "labels": [],
    "comments": [
      "```\n\nIn [32]: import bottleneck as bn\n\nIn [33]: bn.move_std(np.array([1,2,3], dtype='double'), 1)\nOut[33]: array([ 0.,  0.,  0.])\n```\n\nBottleneck has a parameter you can use to set the degrees of freedom. Maybe that's a feature worth implementing?\n\n```\nIn [36]: ?bn.move_std\nType:       builtin_function_or_method\nString Form:<built-in function move_std>\nDocstring:\nmove_std(arr, int window, int axis=-1, int ddof=0)\n\nMoving window standard deviation along the specified axis.\n\nUnlike bn.nanstd, which uses a more rubust two-pass algorithm, move_std\nuses a faster one-pass algorithm.\n\nAn example of a one-pass algorithm:\n\n    >>> np.sqrt((arr*arr).mean() - arr.mean()**2)\n\nAn example of a two-pass algorithm:    \n\n    >>> np.sqrt(((arr - arr.mean())**2).mean())\n\nNote in the two-pass algorithm the mean must be found (first pass) before\nthe squared deviation (second pass) can be found.\n\nParameters\n----------\narr : ndarray\n    Input array.\nwindow : int\n    The number of elements in the moving window.\naxis : int, optional\n    The axis over which to perform the moving standard deviation. By\n    default the moving standard deviation is taken over the last axis\n    (axis=-1). An axis of None is not allowed.\nddof : int, optional\n    Means Delta Degrees of Freedom. The divisor used in calculations\n    is ``N - ddof``, where ``N`` represents the number of elements.\n    By default `ddof` is zero.\n\nReturns\n-------\ny : ndarray\n    The moving standard deviation of the input array along the specified\n    axis. The output has the same shape as the input. \n\nExamples\n--------\n>>> arr = np.array([1.0, 2.0, 3.0, 4.0])\n>>> bn.move_std(arr, window=2)\narray([ nan,  1.5,  2.5,  3.5])\n```\n",
      "Having configurable ddof would be nice. Leaving it for the next release though\n",
      "I think the first value being nan is a bug. Maybe this part could get fixed for the release?\n",
      "A fair point. I suppose the `nobs == 1` case should always yield 0\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 4,
    "additions": 21,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/src/moments.pyx",
      "pandas/stats/moments.py",
      "pandas/stats/tests/test_moments.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1887,
    "reporter": "lodagro",
    "created_at": "2012-09-10T20:42:21+00:00",
    "closed_at": "2012-09-10T22:09:37+00:00",
    "resolver": "wesm",
    "resolved_in": "26e7e69e81d0d04dfeffecde5ac5d1046e131f47",
    "resolver_commit_num": 2369,
    "title": "Series repr issue (numpy.int64)",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 14,
    "deletions": 7,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/format.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1888,
    "reporter": "jreback",
    "created_at": "2012-09-10T21:22:38+00:00",
    "closed_at": "2012-09-17T20:23:06+00:00",
    "resolver": "wesm",
    "resolved_in": "325afdff78316b37f751a5d809d9b4df88318221",
    "resolver_commit_num": 2395,
    "title": "panel ix change in 0.8?",
    "body": "seems p.ix[:,[-1],:] throws an exception (from take)\np.ix[:,-1:,:] is the workaround\nmaybe add to the docs?\n\nipython session follows:\n9/10/12 panel test ix change\n\n\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 33,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/indexing.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1890,
    "reporter": "andreas-h",
    "created_at": "2012-09-11T11:49:09+00:00",
    "closed_at": "2012-10-31T20:21:18+00:00",
    "resolver": "wesm",
    "resolved_in": "33bb84b88b62ec0ced687cfad3862e098389511c",
    "resolver_commit_num": 2375,
    "title": "`TimeSeries.plot` ignores `color` kwarg",
    "body": "I know this looks like a duplicate of #1636, but even with pandas 0.8.1, I don't get the expected plotting behaviour:\n\n\n",
    "labels": [],
    "comments": [
      "thanks for the report. indeed this was not fixed by #1636 and has been remedied here\n",
      "Is it possoble this bug is not yet fixed? I think it is for a normal series, but not for a **time**series as mentioned in the title. An example in the same way as above:\n\n```\nIn [9]: pandas.Series(np.arange(12) + 1).plot(color='green')  # this line is indeed green now\nOut[9]: <matplotlib.axes.AxesSubplot at 0x55d1e70>\n\nIn [12]: pandas.Series(np.arange(12) + 1, index=pandas.date_range('1/1/2000', periods=12)).plot(color='green')  # this is still blue\nOut[12]: <matplotlib.axes.AxesSubplot at 0x58a8850>\n```\n\nAnd a bigger example (the case where I experienced it): http://nbviewer.ipython.org/3857689/\n",
      "It is still a problem. I've submitted a patch, which is waiting for review. A work around is to add style='b' to the plot statement. eg:\n`pandas.Series(np.arange(12) + 1, index=pandas.date_range('1/1/2000', periods=12)).plot(style='b', color='green')`\n",
      "OK, thanks!\n",
      "reopened til patch is merged\n",
      "merged the PR. I think we're good to go\n"
    ],
    "events": [
      "closed",
      "commented",
      "referenced",
      "referenced",
      "commented",
      "commented",
      "commented",
      "reopened",
      "commented",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 13,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tests/test_graphics.py",
      "pandas/tools/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1893,
    "reporter": "jseabold",
    "created_at": "2012-09-11T16:36:54+00:00",
    "closed_at": "2013-03-31T20:37:46+00:00",
    "resolver": "wesm",
    "resolved_in": "d212666b5eebeb8b6c471782636dea384d6d0683",
    "resolver_commit_num": 2855,
    "title": "Inplace returns self?",
    "body": "Is there any reason that whenever I'm doing inplace=True I always get self returned? Obviously not a huge deal, but this is kind of wart-y IMO. I wouldn't expect inplace to return anything.\n",
    "labels": [],
    "comments": [
      "Good idea actually. Useful to make the user more explicit about invoking side-effects.\n",
      "I go back and forth on it. At some point I'd decided it was better to have a consistent API w.r.t. return values (vs. `None` in the `inplace=True` case), e.g. whether or not `inplace=True`, you can always count on getting a reference back to the modified object. \n",
      "Sure, but this is different than the in-place operations in python and in numpy. When I was first using pandas, I was really thrown by the default, make copies everywhere and return an object, especially when python and numpy lead me to expect in-place operation (e.g., sort, assigment to a view). I was bit by not catching this often (kind of like mpl returns things that you don't really need). Then I discovered the inplace keyword. Great, because I almost always want to do things in-place and avoid all the extra typing of assignment, though I have to use the keyword now everywhere. Just seems unnecessary to return the object since I explicitly asked for inplace and I already have the reference to it. Just noise at the interpreter when working interactively.\n",
      "Just as a follow-up, when writing notebooks I have to put semi-colons after every line where I do in-place operations so it doesn't barf the returned self.\n",
      "Wes, your argument confuses me. Do you consider the `inplace` option a special case or not? Also, I just learned that ipython keeps unassigned memory objects alive for the history (the `_` thingie). Is this true for notebooks as well and could this be an argument for really not returning an object when doing things `inplace`?\n",
      "I think the proposal on the table is to always return `None` when using `inplace=True`. Moving this to 0.10\n",
      "Note that not returning `self` breaks the [fluent interface](https://en.wikipedia.org/wiki/Fluent_interface): a.opA().opB().opC()\n",
      "Well, inplace is optional, so the easy solution is don't use inplace if you want to in turn do an operation on the returned object.\n",
      "Another place where Python's eager evaluation can be a weakness\n",
      "those are two orthogonal considerations. one (arguably) should not force the other.\n",
      "Fair enough, but pandas is still an outlier in this respect compared to many of the methods in numpy and python itself. Admittedly, my argument for a (default) inplace not returning self is because I want to save myself typing and improve readability of output at the interpreter for teaching, presenting, or demonstrating. I don't often have serious concerns about memory use and readability of scripts.\n\nWhat's the alternative? Having options where inplace can be 'true', 'false', or 'return'?\n",
      "Or you could provide some kind of chainable, inplace interface. I'm thinking a lot lately about building a DSL layer around pandas so you could do things like:\n\n```\nframe do {\n    .dropna axis=1\n    ab_diff = a - b       \n} group by key1 key2  {\n    max(ab_diff)\n    std(a)\n}\n```\n\nand have that be as fast and memory-efficient as possible. And then you could easily chain \"in place operations\" and get what you expect\n",
      "I fail to see the 'orthogonality' (maybe because 'orthogonal' is linguistically overrated, IMHO). Your claim that these design questions would be independent (i.e. 'orthogonal'), supports the use of something like `obj.opA().obB(inplace=True).obC()`. I even don't want to start thinking about what I just did there and which of all the objects flying around has what content now. The cleanest interface for me is: When I do `inplace`, it effects my original object, if not, it is safe.\n",
      "- Actually, the result returned from the code you quoted would contain the expected result of those operations.\n- since you didn't use inplace in the first operation, all the interim results should be GC'd anyway.\n- Actually, this could be a useful idiom:\n\n``` python\nobj.expensive_op(inplace=true).ViewOp1()\nobj.ViewOp2()\n```\n\nis not that bad IMHO.\n- IPython's history behaviour is a tooling issue, and can probably be disabled when needed.\n- Seems to me that the reason you don't want to think about what you did there is mostly\n  because it's clear to you that you might be misusing the API.\n",
      "> Seems to me that the reason you don't want to think about what you did there is mostly because it's clear to you that you might be misusing the API.\n\nSure. But I understood you in the way that this kind of usage is what you suggest should be allowed? I'm sorry if I still misunderstand... Or maybe you are actually saying, even so it's powerful and harder to understand, it still should be allowed and possible?\n",
      "I think:\n1) powerful AND harder to understand is not a bad thing. I agree that I would not consider that\ncommon usage, just acceptable usage.\n2) Agreed that inplace=True with None as return value makes the semantics clearer. \n3) I do not agree that 2) justifies giving up the convenience of fluent interfaces, which ofcourse\nare useful when all your ops are inplace=True, as well.\n4) chaining inplace=True and regular operations is specialized enough that I suspect\nsomeone who would use it would not be confused by it, but actually be a power-user \nmaxing-out.\n5) unless the first op is inplace=True, you won't even clobber anything your code already\nhas a reference to. \n6) I believe the issues raised are real concerns,  which can be solved by other means however, without \ngiving up the fluent interface.\n",
      "About GC: \nIf I were to do a `_ = obj.op(inplace=True)` just to avoid the printout messing up my notebook, the GC won't work. Is there an alternative to that? Ah, but that's not a new object, so this is okay, right?\n",
      "Another interesting case:\n\n``` python\nobj.clone().MuchMemoryOp(inplace=True).MMO2(inplace=True).MMO3(inplace=True)\n```\n\nthat looks useful to me if I'd like to keep obj _and_ save memory.\nIn fact I wonder why I can't find the `df.clone()` method .\n**edit**: there it is - `df.copy()`\n",
      "you could 'abuse' `df.astype()` to enforce getting a new array.\n",
      "jseabold suggested ending your lines with semicolons, I guess they took that from Mathematica. \nThat's not too obtrusive I think.\nDoesn't work with qtconsole though, which I usually use.\n",
      "regarding GC, `Out` is much more of a problem in that regard,\nI would have expected\n\n``` ipython\nget_ipython().history_length=0\n```\n\nto do the trick, but it doesn't.\n",
      "I tried to say that above, but then my memory failed me that it is the `Out` part that is the problem. Wes is warning about that in his book as well. So, is that another argument for returning `None`? Because one would not want to give up the history just to avoid caching huge objects, right?\n",
      "I think that's better addresed in IPython, though,  and I find the input history far more useful \nfor productivity then the output history, so maybe giving just that up is acceptable if you\nwork with such huge objects frequently. if that's currently implemented now - I don't know.\n",
      "Alright I made a game-time decision to do this-- I think it will make people more mindful about mutation.\n",
      "FYI @njsmith just posted this to numpy ML re: fluent interface. Actively discouraged by Guido.\n\nhttp://mail.python.org/pipermail/python-dev/2003-October/038855.html\n",
      "I would argue that his argument doesn't apply, since in pandas you are  explicitly \nspecifying `inplace=true`, and so \"intimate knowledge required\" is no reason. \nThat's just my love of argument talking though, The decision has been made and \nif it's turned out to be in-line with guido - so much the better.\n\nin what context was this quoted? can't find the post on the numpy ml archive...\n",
      "Response to a suggestion that fill could return self for arrays. \n\nhttp://mail.scipy.org/pipermail/numpy-discussion/2013-January/065158.html\n",
      "Was there any deprecation warning for this? I have some code that works fine in 0.9.0 and I just switched systems to one with 0.10.0 and things were silently broken. Took me a minute to figure out what was going on, and it looks like there are other problems that I'm now having to hunt down.\n",
      "It's possible to detect when someone uses an inplace call on the RHS of a statement by \nusing stack inspection and the ast module (to issue a warning), but that's rather a lot of voodoo.\n",
      "Huh?\n\nhttps://github.com/pydata/pandas/blob/36043e887ae644d72424a50379c0a9c565d98be3/pandas/core/frame.py#L2753\n\n```\nif inplace is True:\n    warn(\"In release XYZ, inplace will no longer return self.\", FutureWarning)\n    new_obj = self\n```\n",
      "wouldn't that issue a warning whenever someone uses an inplace operation regardless? that would be annoying.\n",
      "You say annoying, I say polite. Plus users can always filter warnings. My code broke when upgrading from one minor release to the next. I don't think that's acceptable in a mature project.\n",
      "I agree it's poor form. I'm going to add a deprecation warning in 0.10.1\n",
      "We're agreed on back-compat being crucial.\n\nIMO adding a warning that will trigger every time, even when the usage if fine is annyoing.\nand it would have to stay in place at least till after 0.11. \nI suggested a way to issue a warning only when there is actual misuse.\n\nYou opened an issue a while back concerning a warning given whenever you loaded\npandas in ipython (due to some mpl/pkg_resources deal). I sympethized.\n",
      "Only trigger at the first usage\n",
      "I can do the AST hack to make it less annoying\n",
      "@y-p oh ok sure. That's up to y'all. Seems like overkill to me, but whatever. The last time we tried to creatively trigger deprecation warnings in statsmodels it ended up being much more trouble than it was worth.\n",
      "it's not terrible either way, but **\"Warning: that MIGHT have been a bug\"** seems clumsy...\n",
      "I added deprecation warnings throughout. Users will forgive us: better than code breaking. We'll remove it in a few months in 0.11\n",
      "Guess I'm late to this thread, but having the warning appearing ALL THE TIME (even though I already fixed it so it's not relevant) is causing false positive errors with the infrastructure that monitors our jobs and emails us when errors/warnings are thrown: https://github.com/pydata/pandas/issues/2841 . After reading this thread, I understand why you did what you did; but can we at least get some kind of global option to disable the warning? \n",
      "The python `warnings` module let's you do that out of the box.\nhttp://docs.python.org/2/library/warnings.html#warnings.filterwarnings\n",
      "Yes, but that turns off ALL futureWarnings, even those from other libraries, which are relevant to us.  (Or alternatively, you can wrap it around individual statements, but then you have to repeat that around everywhere you call the .set_index function). We don't want to disable all warnings, only the ones that are not relevant to us.\n  I was thinking of something like:\n\npandas.warnings.disable(\"1893\") #where 1893 is the number of this particular pandas warning\n",
      "read the docs. you can filter by module and line number.\n",
      "Well, aren't there a bunch of different places where you can use inplace=True? (set_index, drop_duplicates, fillna, etc, etc). Each of those is a different line of code, so wouldn't we have to repeat the filter a dozen times for each of those dozen lines of code, even though they're really all conceptually the same warning? \n",
      "No you wouldn't, please see the link.\n",
      "I did read the docs. How would you recommend doing the following? What am I missing?\n\nwarnings.filterwarnings('ignore', module='pandas', lineno=2762) #filter out set_index warning\ndf.set_index('col', inplace=True) #good, no warning.\ndf.sort('someCol', inplace=True) #oops, now we get warning on line 3192.\nwarnings.filterwarnins('ignore', module='pandas', lineno=3192) #filter out sort warning\ndf['someCol'].fillna(0,inplace=True) #oops, now warning on line 2479\nwarngins.filterwarnings('ignore', module='pandas', lineno=2479) #filter out fillna warning\ndf.drop_duplicates(inplace=True) #oops, now warning on line 3034.\n#etc, etc, for every function that takes inplace=True\n\nI see no way to filter those errors except one by one. Do you? (Yes, you could filter out all FutureWarnings from all of pandas, but then that hides other unrelated warnings that we do need to fix. We want to filter out only the inplace=True warnings)\n",
      "If module-wide supression is not option, I have no better way without touching the code.\nAssuming you can, it might be quickest to just revert the single commit that did this.\n",
      "The usual solution is to specify a regex that matches the pandas warning\nmessage text. (I assume this is pretty constant even across different\ntriggering locations.)\nOn 11 Feb 2013 07:26, \"tavistmorph\" notifications@github.com wrote:\n\n> I did read the docs. How would you recommend doing the following? What am\n> I missing?\n> \n> warnings.filterwarnings('ignore', module='pandas', lineno=2762) #filter\n> out set_index warning\n> df.set_index('col', inplace=True) #good, no warning.\n> df.sort('someCol', inplace=True) #oops, now we get warning on line 3192.\n> warnings.filterwarnins('ignore', module='pandas', lineno=3192) #filter out\n> sort warning\n> df['someCol'].fillna(0,inplace=True) #oops, now warning on line 2479\n> warngins.filterwarnings('ignore', module='pandas', lineno=2479) #filter\n> out fillna warning\n> df.drop_duplicates(inplace=True) #oops, now warning on line 3034.\n> #etc, etc, for every function that takes inplace=True\n> \n> I see no way to filter those errors except one by one. Do you? (Yes, you\n> could filter out all FutureWarnings from all of pandas, but then that hides\n> other unrelated warnings that we do need to fix. We want to filter out only\n> the inplace=True warnings)\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/pydata/pandas/issues/1893#issuecomment-13384764..\n",
      "Specifying a regex on the error message is fragile too -- it'll break if the next release changes the wording of the error message. The other python convention of filtering on line number is fragile too, cause the next release will add/remove code and change the line numbers.\n\nI'm just saying, why not give pandas a more robust way of disabling warnings, maybe something like the C++ #pragma warning macro ( http://msdn.microsoft.com/en-us/library/2c8f766e%28v=vs.80%29.aspx ) Something like:\n\n   pandas.warnings.disable(\"InPlaceReturnsObjectIsBeingDeprecated\") \n   pandas.warnings.disable(\"SomeOtherSpecificWarning\")\n\nOr alternatively, instead of a name, maybe that takes a number specifying the warning. Gives maximum flexibility, robustness, and safety when upgrading to the next version.\n\nI've split that off to a differnet issue (https://github.com/pydata/pandas/issues/2842) so we can end debate on this thread and resume it there.\n",
      "In practice, the chance of the next release changing the wording of the\nmessage is nil (partly because it would break such scripts, also because\nthis warning will be removed entirely within a release or two). I see your\npoint and perhaps it's worth filing a second issue over, but for now you\nshould probably grit your teeth and filter on the message :-)\nOn 11 Feb 2013 08:34, \"tavistmorph\" notifications@github.com wrote:\n\n> Specifying a regex on the error message is fragile too -- it'll break if\n> the next release changes the wording of the error message. The other python\n> convention of filtering on line number is fragile too, cause the next\n> release will add/remove code and change the line numbers.\n> \n> I'm just saying, why not give pandas a more robust way of disabling\n> warnings, maybe something like the C++ #pragma warning macro (\n> http://msdn.microsoft.com/en-us/library/2c8f766e%28v=vs.80%29.aspx )\n> Something like:\n> \n> pandas.warnings.disable(\"InPlaceReturnsObjectIsBeingDeprecated\")\n> pandas.warnings.disable(\"SomeOtherSpecificWarning\")\n> \n> Or alternatively, instead of a name, maybe that takes a number specifying\n> the warning. Gives maximum flexibility, robustness, and safety when\n> upgrading to the next version.\n> \n> Perhaps we should split this off into a different issue?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/pydata/pandas/issues/1893#issuecomment-13388577..\n",
      "@wesm, now that 0.11 is coming, can b904029 be reverted? it might cut down \nthe [incoming](https://github.com/pydata/pandas/issues/2841) [complaints](https://github.com/pydata/pandas/issues/2912).\n",
      "Yeah. There is a big test case where we can change all of the checks to assert that the return value is None and then go through and fix things\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "commented",
      "closed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "reopened",
      "referenced",
      "commented",
      "referenced",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 22,
    "deletions": 74,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/core/series.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1895,
    "reporter": "changhiskhan",
    "created_at": "2012-09-11T16:57:31+00:00",
    "closed_at": "2012-09-13T02:12:23+00:00",
    "resolver": "wesm",
    "resolved_in": "82b0d2a4493c5577d44cfc57a1e390baaa225b2d",
    "resolver_commit_num": 2383,
    "title": "DatetimeIndex min failure",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 3,
    "additions": 37,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1896,
    "reporter": "changhiskhan",
    "created_at": "2012-09-11T17:17:03+00:00",
    "closed_at": "2012-09-13T01:47:19+00:00",
    "resolver": "wesm",
    "resolved_in": "7526823d3419e4921143fa1b0ea388eb0dc82c4a",
    "resolver_commit_num": 2382,
    "title": "Mixed type DataFrame.diff converts to object",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 34,
    "deletions": 18,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1897,
    "reporter": "erg",
    "created_at": "2012-09-11T19:23:24+00:00",
    "closed_at": "2012-09-12T14:53:40+00:00",
    "resolver": "erg",
    "resolved_in": "15717e7bf9b249efd9e16bad4104c1578fcc642d",
    "resolver_commit_num": 0,
    "title": "pandas.rolling_min/max fail if window size is larger than input array",
    "body": "This is a regression since July 12.\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 2,
    "additions": 54,
    "deletions": 27,
    "changed_files_list": [
      "pandas/src/moments.pyx",
      "pandas/stats/tests/test_moments.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1900,
    "reporter": "erg",
    "created_at": "2012-09-12T00:38:07+00:00",
    "closed_at": "2012-09-17T19:14:26+00:00",
    "resolver": "wesm",
    "resolved_in": "68251c278a1957a9f7407df02e819a1dcbfb78bd",
    "resolver_commit_num": 2394,
    "title": "pandas.ewma doesn't handle empty inputs correctly",
    "body": "\n\nI think it should output an empty array instead, like `rolling_sum`.\n\n\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 4,
    "additions": 12,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/src/moments.pyx",
      "pandas/stats/moments.py",
      "pandas/stats/tests/test_moments.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1906,
    "reporter": "wesm",
    "created_at": "2012-09-13T01:09:29+00:00",
    "closed_at": "2012-09-13T03:04:20+00:00",
    "resolver": "wesm",
    "resolved_in": "67121af2dcaf07acded45c624b25f3292aaf46c4",
    "resolver_commit_num": 2384,
    "title": "DataFrame formatting column name truncation problem",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 4,
    "additions": 31,
    "deletions": 5,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/format.py",
      "pandas/tests/test_frame.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1908,
    "reporter": "ukch",
    "created_at": "2012-09-13T09:36:40+00:00",
    "closed_at": "2012-12-18T00:34:06+00:00",
    "resolver": "changhiskhan",
    "resolved_in": "38333f42abc2c030223daedd4b2d8a861a3f7225",
    "resolver_commit_num": 206,
    "title": "DataFrame.to_records converts dates wrongly",
    "body": "Possibly related to #1720:\n\nWhen converting a DataFrame to a recarray using `df.to_records`, date indexes are incorrectly converted.\n\n\n\nNotice the dates have been converted to 1970, even though the original dates were in 2012.\n",
    "labels": [],
    "comments": [
      "``` python\n>>> pandas.__version__\n'0.9.0.dev-a83e691'\n>>> numpy.__version__\n'1.6.2'\n```\n",
      "I have found that converting the index to Python datetime values (using `index.topydatetime()`) yields the expected value.\n",
      "It's a display/repr issue in NumPy 1.6 unfortunately. The actual nanosecond timestamps have not been altered\n",
      "I am pretty sure this is not simply a display/repr issue. See the following output:\n\n``` python\n>>> recs[0][0]\n1970-01-16 224:00:00\n>>> recs[0][0].astype(datetime.datetime)\ndatetime.datetime(1970, 1, 16, 224, 0)\n```\n\nI noticed this problem while trying to convert a DataFrame object into a PostgreSQL table using the `psycopg2` library. The values generated by psycopg2 when passed the above datetime-converted objects were for dates in 1970.\n",
      "All caused by the same NumPy 1.6 bug. Maybe a solution is to add an option to `to_records` which sidesteps NumPy to properly convert the values to `datetime.datetime`\n",
      "+1\n",
      "A new contributor who can reproduce this on their system should be able to write an implementation of this fairly quickly. (They would need to have a buggy version of NumPy, which may be very common! Otherwise, they would need to be familiar with 'pip' or other ways of installing/changing the version of NumPy installed.)\n\nA question: should skipping NumPy be the default mode?\n\nIt should be possible to write a test case that checks that NumPy is skipped with the new argument to to_records, so it seems to me that the pull request should include a test.\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 14,
    "deletions": 7,
    "changed_files_list": [
      "pandas/core/common.py",
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1911,
    "reporter": "louist87",
    "created_at": "2012-09-14T02:35:46+00:00",
    "closed_at": "2012-09-17T23:30:28+00:00",
    "resolver": "wesm",
    "resolved_in": "de31cbecd8831195bab36fc0ddd61eb34393425d",
    "resolver_commit_num": 2396,
    "title": "Output for small decimal numbers is misleading",
    "body": "I think that [this](-do-i-convert-a-column-from-a-pandas-dataframe-from-str-scientific-notation) question on stackoverflow sums up the problem fairly concisely.\n\nIn a nutshell, the `0` and `-0` output for very small decimal numbers is misleading and looks a lot like an error.  It might be better just to show scientific notation directly.\n",
    "labels": [],
    "comments": [
      "I have to look at what to do here. Note that setting engineering format helps:\n\n```\nIn [5]: pd.set_eng_float_format()\n\nIn [6]: pd.read_table('http://dl.dropbox.com/u/6160029/gradStat_mmn.tdf').head()\nOut[6]: \n   Subject Group Local Global  Attn         mean\n0        1  DSub     S      S  Attn  325.290E-24\n1        1  DSub     S      S  Dist  601.010E-24\n2        1  DSub     D      S  Attn  421.570E-24\n3        1  DSub     D      S  Dist  830.810E-24\n4        1  DSub     S      D  Attn  298.350E-24\n```\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 3,
    "additions": 27,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/format.py",
      "pandas/tests/test_format.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1913,
    "reporter": "lbeltrame",
    "created_at": "2012-09-15T08:18:33+00:00",
    "closed_at": "2012-09-17T17:52:37+00:00",
    "resolver": "wesm",
    "resolved_in": "92e500e345257d088fc729bd36ea9ae5ade830b1",
    "resolver_commit_num": 2392,
    "title": "Constructing a Series with a set returns a set and not a Series",
    "body": "With latest master:\n\n\n",
    "labels": [],
    "comments": [
      "I'm not sure what would be the correct behavior, IMO (return a Series or raise an exception).\n",
      "It should probably raise an exception because sets are unordered and, unlike dicts, are not a mapping\n",
      "Where is the check made? I might take a stab at it if time permits.\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 9,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/series.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1917,
    "reporter": "jseabold",
    "created_at": "2012-09-16T18:28:47+00:00",
    "closed_at": "2012-09-17T16:09:22+00:00",
    "resolver": "wesm",
    "resolved_in": "46fdbb728e5ee527d99bf912d4e359e5617a8443",
    "resolver_commit_num": 2389,
    "title": "drop_duplicates regression with variable name",
    "body": "I just updated to master and now this doesn't work.\n\n\n\nI believe cfe674e is the offending commit. A string is iterable but we don't want to iterate over it.\n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned",
      "assigned"
    ],
    "changed_files": 2,
    "additions": 10,
    "deletions": 10,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1923,
    "reporter": "lbeltrame",
    "created_at": "2012-09-17T09:06:36+00:00",
    "closed_at": "2012-11-28T03:26:04+00:00",
    "resolver": "wesm",
    "resolved_in": "537e6a643cc591a6744f8d6f19aa9d679d364d2c",
    "resolver_commit_num": 2635,
    "title": "Have the possibility for Series.unique() to return a Series rather than an array",
    "body": "I admit I haven't looked at the code so there may be reasons for this, but I've found myself in the need of squeezing out duplicates from a Series but keeping the results as a Series.\n\nSeries.unique() however returns an array, so in my code I have to construct a Series twice:\n\n\n\nIs this by design? If so, feel free to close this bug.\n",
    "labels": [],
    "comments": [
      "Well it's a good question. I guess the main issue is what index you should assign (default 0 to N-1 would be the only reasonable one probably, otherwise the index values where the unique values occurred).\n",
      "In data luned\u00ec 17 settembre 2012 05:35:34, Wes McKinney ha scritto:\n\n> assign (default 0 to N-1 would be the only reasonable one probably,\n> otherwise the index values where the unique values occurred).\n\nI don't have strong opinions on either, any would be a very good improvement \nover the current behavior IMO.\n\n## \n\nLuca Beltrame - KDE Forums team\nKDE Science supporter\nGPG key ID: 6E1A4E79\n",
      "I think the second option (indices of the unique entries) would be helpful\nsince it is easy to simply reset the index to 0...n-1 but much more\nexpensive to get the indices where they occur in case I am interested.\nbut probably if you added this users would then ask for an option to\nspecify whether I get the index of the first, last or whatever occurence of\nthe unique values ;-)\n\nOn Mon, Sep 17, 2012 at 3:21 PM, Luca Beltrame notifications@github.comwrote:\n\n> In data luned\u00ec 17 settembre 2012 05:35:34, Wes McKinney ha scritto:\n> \n> > assign (default 0 to N-1 would be the only reasonable one probably,\n> > otherwise the index values where the unique values occurred).\n> \n> I don't have strong opinions on either, any would be a very good\n> improvement\n> over the current behavior IMO.\n> \n> ## \n> \n> Luca Beltrame - KDE Forums team\n> KDE Science supporter\n> GPG key ID: 6E1A4E79\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/pydata/pandas/issues/1923#issuecomment-8613623.\n",
      "I agree it would be helpful. But more expensive to compute. Have to think about it\n",
      "I don't think unique should return a Series with a meaningless integer index, seems harmful/confusing if the original Series also had an integer index.\nAs for computing the indices of occurrences, how about an optional `return_loc` parameter where it's None by default (returns ndarray) and can be \"all\", \"first\", \"last\".\n",
      "like how about this:\n\n```\ns.unique() -> no index\ns.unique(index='first') -> Series\ns.unique(index='last') -> Series\n```\n",
      "yeah, exactly what I was thinking\n",
      "s.unique() --> keep method as it is, a faster alternative to np.unique() --- no index\n\nAdd drop_duplicates() to Series?:\ns.drop_duplicates(take_last=...) --> Series, index behavior like for DataFrame.drop_duplicates()\n",
      "That's not a bad idea either\n",
      "Maybe drop_duplicates to get first or last and then a separate method to get a reverse mapping of all indices for each unique value?\n",
      "See `DataFrame.duplicated`, which returns a boolean array\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 4,
    "additions": 67,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/series.py",
      "pandas/tests/test_series.py",
      "vb_suite/reindex.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1929,
    "reporter": "btel",
    "created_at": "2012-09-18T09:46:35+00:00",
    "closed_at": "2012-09-18T19:43:31+00:00",
    "resolver": "wesm",
    "resolved_in": "d99a119c76559d127823c7fa0f6098b53b94bb30",
    "resolver_commit_num": 2402,
    "title": "to_html: sparsify mutliindex keys in dataframe rows",
    "body": "The sparsify option of DataFrame.to_html currently works only for columns. It would be useful to add this option also for rows, similarly to what is already done for data frame representation in console.\n\nExample:\n\n\n\ncurrently produces the following output:\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><strong>X</strong></td>\n      <td><strong>a</strong></td>\n      <td> 0</td>\n    </tr>\n    <tr>\n      <td><strong>X</strong></td>\n      <td><strong>b</strong></td>\n      <td> 1</td>\n    </tr>\n    <tr>\n      <td><strong>Y</strong></td>\n      <td><strong>a</strong></td>\n      <td> 2</td>\n    </tr>\n  </tbody>\n</table>\n\n\nThis should be changed to:\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td rowspan=\"2\" valign=\"top\"><strong>X</strong></td>\n      <td><strong>a</strong></td>\n      <td> 0</td>\n    </tr>\n    <tr>\n      <td><strong>b</strong></td>\n      <td> 1</td>\n    </tr>\n    <tr>\n      <td><strong>Y</strong></td>\n      <td><strong>a</strong></td>\n      <td> 2</td>\n    </tr>\n  </tbody>\n</table>\n\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 140,
    "deletions": 27,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/format.py",
      "pandas/tests/test_format.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1940,
    "reporter": "edschofield",
    "created_at": "2012-09-20T04:09:28+00:00",
    "closed_at": "2012-09-20T15:36:46+00:00",
    "resolver": "wesm",
    "resolved_in": "c69c4f08d1daab70b72d79597f27ff1e72525dc0",
    "resolver_commit_num": 2422,
    "title": "Regression in date formatting in IPython Notebook (v0.9-dev)",
    "body": "This is a possible regression in Pandas v0.9.0-667220a versus v0.8.1:\n\nThe following works, in both v0.8.1 and v0.9.0-667220a:\n\n\n\nHowever, with v0.9.0-667220a, the graphical IPython Notebook output of the dates is garbled. In it, the dates appear as follows:\n\n\n\n(I can attach a screenshot if someone can tell me how to upload or attach a file to an issue using GitHub...)\n",
    "labels": [],
    "comments": [
      "confirmed. looking into this\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 97,
    "additions": 4251,
    "deletions": 782,
    "changed_files_list": [
      ".travis.yml",
      "LICENSE",
      "NP_LICENSE.txt",
      "README.rst",
      "RELEASE.rst",
      "doc/make.py",
      "doc/source/api.rst",
      "doc/source/basics.rst",
      "doc/source/computation.rst",
      "doc/source/conf.py",
      "doc/source/index.rst",
      "doc/source/install.rst",
      "doc/source/io.rst",
      "doc/source/missing_data.rst",
      "doc/source/related.rst",
      "doc/source/timeseries.rst",
      "doc/source/v0.7.3.txt",
      "doc/source/v0.8.0.txt",
      "pandas/__init__.py",
      "pandas/core/algorithms.py",
      "pandas/core/common.py",
      "pandas/core/datetools.py",
      "pandas/core/format.py",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/indexing.py",
      "pandas/core/internals.py",
      "pandas/core/panel.py",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/core/strings.py",
      "pandas/io/data.py",
      "pandas/io/parsers.py",
      "pandas/io/pytables.py",
      "pandas/io/sql.py",
      "pandas/io/tests/test_date_converters.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/io/tests/test_pytables.py",
      "pandas/sparse/frame.py",
      "pandas/sparse/tests/test_sparse.py",
      "pandas/src/datetime.pxd",
      "pandas/src/datetime.pyx",
      "pandas/src/datetime/np_datetime.c",
      "pandas/src/datetime/np_datetime.h",
      "pandas/src/datetime/np_datetime_strings.c",
      "pandas/src/engines.pyx",
      "pandas/src/groupby.pyx",
      "pandas/src/hashtable.pyx",
      "pandas/src/inference.pyx",
      "pandas/src/moments.pyx",
      "pandas/src/numpy_helper.h",
      "pandas/src/offsets.pyx",
      "pandas/src/period.c",
      "pandas/src/plib.pyx",
      "pandas/src/reduce.pyx",
      "pandas/src/tseries.pyx",
      "pandas/src/util.pxd",
      "pandas/stats/misc.py",
      "pandas/stats/moments.py",
      "pandas/stats/ols.py",
      "pandas/stats/tests/test_moments.py",
      "pandas/stats/tests/test_ols.py",
      "pandas/tests/test_algos.py",
      "pandas/tests/test_common.py",
      "pandas/tests/test_format.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_graphics.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_series.py",
      "pandas/tests/test_strings.py",
      "pandas/tools/merge.py",
      "pandas/tools/plotting.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tseries/converter.py",
      "pandas/tseries/frequencies.py",
      "pandas/tseries/index.py",
      "pandas/tseries/offsets.py",
      "pandas/tseries/period.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_frequencies.py",
      "pandas/tseries/tests/test_offsets.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_plotting.py",
      "pandas/tseries/tests/test_resample.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tests/test_timezones.py",
      "pandas/tseries/tests/test_util.py",
      "pandas/tseries/tools.py",
      "pandas/util/decorators.py",
      "scripts/gen_release_notes.py",
      "setup.py",
      "vb_suite/binary_ops.py",
      "vb_suite/make.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1941,
    "reporter": "gerigk",
    "created_at": "2012-09-20T08:32:31+00:00",
    "closed_at": "2012-09-20T09:02:29+00:00",
    "resolver": "wesm",
    "resolved_in": "c69c4f08d1daab70b72d79597f27ff1e72525dc0",
    "resolver_commit_num": 2422,
    "title": "nosetest errors when \"xlrd\" isn't installed",
    "body": "Maybe this is because I have openpyxl but not xlrd installed.\n\nAnyway, I guess the tests should be skipped instead of throwing an error.\n\n\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 97,
    "additions": 4251,
    "deletions": 782,
    "changed_files_list": [
      ".travis.yml",
      "LICENSE",
      "NP_LICENSE.txt",
      "README.rst",
      "RELEASE.rst",
      "doc/make.py",
      "doc/source/api.rst",
      "doc/source/basics.rst",
      "doc/source/computation.rst",
      "doc/source/conf.py",
      "doc/source/index.rst",
      "doc/source/install.rst",
      "doc/source/io.rst",
      "doc/source/missing_data.rst",
      "doc/source/related.rst",
      "doc/source/timeseries.rst",
      "doc/source/v0.7.3.txt",
      "doc/source/v0.8.0.txt",
      "pandas/__init__.py",
      "pandas/core/algorithms.py",
      "pandas/core/common.py",
      "pandas/core/datetools.py",
      "pandas/core/format.py",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/indexing.py",
      "pandas/core/internals.py",
      "pandas/core/panel.py",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/core/strings.py",
      "pandas/io/data.py",
      "pandas/io/parsers.py",
      "pandas/io/pytables.py",
      "pandas/io/sql.py",
      "pandas/io/tests/test_date_converters.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/io/tests/test_pytables.py",
      "pandas/sparse/frame.py",
      "pandas/sparse/tests/test_sparse.py",
      "pandas/src/datetime.pxd",
      "pandas/src/datetime.pyx",
      "pandas/src/datetime/np_datetime.c",
      "pandas/src/datetime/np_datetime.h",
      "pandas/src/datetime/np_datetime_strings.c",
      "pandas/src/engines.pyx",
      "pandas/src/groupby.pyx",
      "pandas/src/hashtable.pyx",
      "pandas/src/inference.pyx",
      "pandas/src/moments.pyx",
      "pandas/src/numpy_helper.h",
      "pandas/src/offsets.pyx",
      "pandas/src/period.c",
      "pandas/src/plib.pyx",
      "pandas/src/reduce.pyx",
      "pandas/src/tseries.pyx",
      "pandas/src/util.pxd",
      "pandas/stats/misc.py",
      "pandas/stats/moments.py",
      "pandas/stats/ols.py",
      "pandas/stats/tests/test_moments.py",
      "pandas/stats/tests/test_ols.py",
      "pandas/tests/test_algos.py",
      "pandas/tests/test_common.py",
      "pandas/tests/test_format.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_graphics.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_series.py",
      "pandas/tests/test_strings.py",
      "pandas/tools/merge.py",
      "pandas/tools/plotting.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tseries/converter.py",
      "pandas/tseries/frequencies.py",
      "pandas/tseries/index.py",
      "pandas/tseries/offsets.py",
      "pandas/tseries/period.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_frequencies.py",
      "pandas/tseries/tests/test_offsets.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_plotting.py",
      "pandas/tseries/tests/test_resample.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tests/test_timezones.py",
      "pandas/tseries/tests/test_util.py",
      "pandas/tseries/tools.py",
      "pandas/util/decorators.py",
      "scripts/gen_release_notes.py",
      "setup.py",
      "vb_suite/binary_ops.py",
      "vb_suite/make.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1942,
    "reporter": "CRP",
    "created_at": "2012-09-20T12:56:09+00:00",
    "closed_at": "2012-09-20T19:15:21+00:00",
    "resolver": "wesm",
    "resolved_in": "c69c4f08d1daab70b72d79597f27ff1e72525dc0",
    "resolver_commit_num": 2422,
    "title": "assignment with index broken",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 97,
    "additions": 4251,
    "deletions": 782,
    "changed_files_list": [
      ".travis.yml",
      "LICENSE",
      "NP_LICENSE.txt",
      "README.rst",
      "RELEASE.rst",
      "doc/make.py",
      "doc/source/api.rst",
      "doc/source/basics.rst",
      "doc/source/computation.rst",
      "doc/source/conf.py",
      "doc/source/index.rst",
      "doc/source/install.rst",
      "doc/source/io.rst",
      "doc/source/missing_data.rst",
      "doc/source/related.rst",
      "doc/source/timeseries.rst",
      "doc/source/v0.7.3.txt",
      "doc/source/v0.8.0.txt",
      "pandas/__init__.py",
      "pandas/core/algorithms.py",
      "pandas/core/common.py",
      "pandas/core/datetools.py",
      "pandas/core/format.py",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/indexing.py",
      "pandas/core/internals.py",
      "pandas/core/panel.py",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/core/strings.py",
      "pandas/io/data.py",
      "pandas/io/parsers.py",
      "pandas/io/pytables.py",
      "pandas/io/sql.py",
      "pandas/io/tests/test_date_converters.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/io/tests/test_pytables.py",
      "pandas/sparse/frame.py",
      "pandas/sparse/tests/test_sparse.py",
      "pandas/src/datetime.pxd",
      "pandas/src/datetime.pyx",
      "pandas/src/datetime/np_datetime.c",
      "pandas/src/datetime/np_datetime.h",
      "pandas/src/datetime/np_datetime_strings.c",
      "pandas/src/engines.pyx",
      "pandas/src/groupby.pyx",
      "pandas/src/hashtable.pyx",
      "pandas/src/inference.pyx",
      "pandas/src/moments.pyx",
      "pandas/src/numpy_helper.h",
      "pandas/src/offsets.pyx",
      "pandas/src/period.c",
      "pandas/src/plib.pyx",
      "pandas/src/reduce.pyx",
      "pandas/src/tseries.pyx",
      "pandas/src/util.pxd",
      "pandas/stats/misc.py",
      "pandas/stats/moments.py",
      "pandas/stats/ols.py",
      "pandas/stats/tests/test_moments.py",
      "pandas/stats/tests/test_ols.py",
      "pandas/tests/test_algos.py",
      "pandas/tests/test_common.py",
      "pandas/tests/test_format.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_graphics.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_series.py",
      "pandas/tests/test_strings.py",
      "pandas/tools/merge.py",
      "pandas/tools/plotting.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tseries/converter.py",
      "pandas/tseries/frequencies.py",
      "pandas/tseries/index.py",
      "pandas/tseries/offsets.py",
      "pandas/tseries/period.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_frequencies.py",
      "pandas/tseries/tests/test_offsets.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_plotting.py",
      "pandas/tseries/tests/test_resample.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tests/test_timezones.py",
      "pandas/tseries/tests/test_util.py",
      "pandas/tseries/tools.py",
      "pandas/util/decorators.py",
      "scripts/gen_release_notes.py",
      "setup.py",
      "vb_suite/binary_ops.py",
      "vb_suite/make.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1943,
    "reporter": "lodagro",
    "created_at": "2012-09-20T14:30:31+00:00",
    "closed_at": "2013-04-30T17:49:54+00:00",
    "resolver": "wesm",
    "resolved_in": "c69c4f08d1daab70b72d79597f27ff1e72525dc0",
    "resolver_commit_num": 2422,
    "title": "Selecting multiple columns from DataFrame with duplicate column labels failure.",
    "body": "\n",
    "labels": [],
    "comments": [
      "It won't reorder, but it will at least work\n",
      "I still get this error with a long dataframe (pandas.**version** == '0.10.0'). Renaming the column makes it work. Unfortuantelly I can't produce a small reproduceable example: using the above code it simple works :-(\n\n```\nIn [85]: journals = journals[[\"sid\", \"title\",  \"SNIP2_2009\" , \"SJR2_2009\", \"SNIP2_2010\", \"SJR2_2010\", \"SNIP2_2011\", \"SJR2_2011\"]]\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n<ipython-input-85-7a986a34fc84> in <module>()\n----> 1 journals = journals[[\"sid\", \"title\",  \"SNIP2_2009\" , \"SJR2_2009\", \"SNIP2_2010\", \"SJR2_2010\", \"SNIP2_2011\", \"SJR2_2011\"]]\n\nC:\\portabel\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in __getitem__(self, key)\n   1932             if com._is_bool_indexer(key):\n   1933                 key = np.asarray(key, dtype=bool)\n-> 1934             return self._getitem_array(key)\n   1935         elif isinstance(self.columns, MultiIndex):\n   1936             return self._getitem_multilevel(key)\n\nC:\\portabel\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _getitem_array(self, key)\n   1968                         raise KeyError(\"No column(s) named: %s\" %\n   1969                                        com.pprint_thing(k))\n-> 1970                 return self.take(mask.nonzero()[0], axis=1)\n   1971 \n   1972     def _slice(self, slobj, axis=0):\n\nC:\\portabel\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in take(self, indices, axis)\n   2850             else:\n   2851                 new_columns = self.columns.take(indices)\n-> 2852                 return self.reindex(columns=new_columns)\n   2853         else:\n   2854             new_values = com.take_2d(self.values,\n\nC:\\portabel\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in reindex(self, index, columns, method, level, fill_value, limit, copy)\n   2505         if columns is not None:\n   2506             frame = frame._reindex_columns(columns, copy, level,\n-> 2507                                            fill_value, limit)\n   2508 \n   2509         if index is not None:\n\nC:\\portabel\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _reindex_columns(self, new_columns, copy, level, fill_value, limit)\n   2592                          limit=None):\n   2593         new_columns, indexer = self.columns.reindex(new_columns, level=level,\n-> 2594                                                     limit=limit)\n   2595         return self._reindex_with_indexers(None, None, new_columns, indexer,\n   2596                                            copy, fill_value)\n\nC:\\portabel\\Python27\\lib\\site-packages\\pandas\\core\\index.pyc in reindex(self, target, method, level, limit)\n    873             else:\n    874                 indexer = self.get_indexer(target, method=method,\n--> 875                                            limit=limit)\n    876         return target, indexer\n    877 \n\nC:\\portabel\\Python27\\lib\\site-packages\\pandas\\core\\index.pyc in get_indexer(self, target, method, limit)\n    792 \n    793         if not self.is_unique:\n--> 794             raise Exception('Reindexing only valid with uniquely valued Index '\n    795                             'objects')\n    796 \n\nException: Reindexing only valid with uniquely valued Index objects\n```\n",
      "looks like your data is mixed type but the original issue is not.\n",
      "@wesm I moved this to 0.10.2\n\nNot enough time for 0.10.1\n",
      "pushing to 0.12 when can deal with this in block manager, see #3012\n",
      "this was actually fixed in 0.9.0 and test for it in test_frames...so closing...UOD (unless otherwise directed)\n"
    ],
    "events": [
      "closed",
      "commented",
      "referenced",
      "referenced",
      "commented",
      "reopened",
      "assigned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "commented"
    ],
    "changed_files": 97,
    "additions": 4251,
    "deletions": 782,
    "changed_files_list": [
      ".travis.yml",
      "LICENSE",
      "NP_LICENSE.txt",
      "README.rst",
      "RELEASE.rst",
      "doc/make.py",
      "doc/source/api.rst",
      "doc/source/basics.rst",
      "doc/source/computation.rst",
      "doc/source/conf.py",
      "doc/source/index.rst",
      "doc/source/install.rst",
      "doc/source/io.rst",
      "doc/source/missing_data.rst",
      "doc/source/related.rst",
      "doc/source/timeseries.rst",
      "doc/source/v0.7.3.txt",
      "doc/source/v0.8.0.txt",
      "pandas/__init__.py",
      "pandas/core/algorithms.py",
      "pandas/core/common.py",
      "pandas/core/datetools.py",
      "pandas/core/format.py",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/indexing.py",
      "pandas/core/internals.py",
      "pandas/core/panel.py",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/core/strings.py",
      "pandas/io/data.py",
      "pandas/io/parsers.py",
      "pandas/io/pytables.py",
      "pandas/io/sql.py",
      "pandas/io/tests/test_date_converters.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/io/tests/test_pytables.py",
      "pandas/sparse/frame.py",
      "pandas/sparse/tests/test_sparse.py",
      "pandas/src/datetime.pxd",
      "pandas/src/datetime.pyx",
      "pandas/src/datetime/np_datetime.c",
      "pandas/src/datetime/np_datetime.h",
      "pandas/src/datetime/np_datetime_strings.c",
      "pandas/src/engines.pyx",
      "pandas/src/groupby.pyx",
      "pandas/src/hashtable.pyx",
      "pandas/src/inference.pyx",
      "pandas/src/moments.pyx",
      "pandas/src/numpy_helper.h",
      "pandas/src/offsets.pyx",
      "pandas/src/period.c",
      "pandas/src/plib.pyx",
      "pandas/src/reduce.pyx",
      "pandas/src/tseries.pyx",
      "pandas/src/util.pxd",
      "pandas/stats/misc.py",
      "pandas/stats/moments.py",
      "pandas/stats/ols.py",
      "pandas/stats/tests/test_moments.py",
      "pandas/stats/tests/test_ols.py",
      "pandas/tests/test_algos.py",
      "pandas/tests/test_common.py",
      "pandas/tests/test_format.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_graphics.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_series.py",
      "pandas/tests/test_strings.py",
      "pandas/tools/merge.py",
      "pandas/tools/plotting.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tseries/converter.py",
      "pandas/tseries/frequencies.py",
      "pandas/tseries/index.py",
      "pandas/tseries/offsets.py",
      "pandas/tseries/period.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_frequencies.py",
      "pandas/tseries/tests/test_offsets.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_plotting.py",
      "pandas/tseries/tests/test_resample.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tests/test_timezones.py",
      "pandas/tseries/tests/test_util.py",
      "pandas/tseries/tools.py",
      "pandas/util/decorators.py",
      "scripts/gen_release_notes.py",
      "setup.py",
      "vb_suite/binary_ops.py",
      "vb_suite/make.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1946,
    "reporter": "adamgreenhall",
    "created_at": "2012-09-21T22:58:18+00:00",
    "closed_at": "2012-09-23T17:37:09+00:00",
    "resolver": "wesm",
    "resolved_in": "c69c4f08d1daab70b72d79597f27ff1e72525dc0",
    "resolver_commit_num": 2422,
    "title": "hours are wrong for DatetimeIndex with timezone set",
    "body": "Using  `hours` on a basic hourly `DatetimeIndex` with a time zone gives the incorrect hours. \n\n\n\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2011-10-02 00:00:00, ..., 2011-10-02 09:00:00]\nLength: 10, Freq: None, Timezone: America/Atikokan\n\n['2011-10-02 00:00:00-05:00', '2011-10-02 01:00:00-05:00', '2011-10-02 02:00:00-05:00', '2011-10-02 03:00:00-05:00', '2011-10-02 04:00:00-05:00', '2011-10-02 05:00:00-05:00', '2011-10-02 06:00:00-05:00', '2011-10-02 07:00:00-05:00', '2011-10-02 08:00:00-05:00', '2011-10-02 09:00:00-05:00']\n\n[ 9  9 19 19 23  0  1  2  3  4]\n",
    "labels": [],
    "comments": [
      "work-around \n`numpy.array([t.hour for t in data.index])`\n",
      "Looking...\n"
    ],
    "events": [
      "commented",
      "commented"
    ],
    "changed_files": 97,
    "additions": 4251,
    "deletions": 782,
    "changed_files_list": [
      ".travis.yml",
      "LICENSE",
      "NP_LICENSE.txt",
      "README.rst",
      "RELEASE.rst",
      "doc/make.py",
      "doc/source/api.rst",
      "doc/source/basics.rst",
      "doc/source/computation.rst",
      "doc/source/conf.py",
      "doc/source/index.rst",
      "doc/source/install.rst",
      "doc/source/io.rst",
      "doc/source/missing_data.rst",
      "doc/source/related.rst",
      "doc/source/timeseries.rst",
      "doc/source/v0.7.3.txt",
      "doc/source/v0.8.0.txt",
      "pandas/__init__.py",
      "pandas/core/algorithms.py",
      "pandas/core/common.py",
      "pandas/core/datetools.py",
      "pandas/core/format.py",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/indexing.py",
      "pandas/core/internals.py",
      "pandas/core/panel.py",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/core/strings.py",
      "pandas/io/data.py",
      "pandas/io/parsers.py",
      "pandas/io/pytables.py",
      "pandas/io/sql.py",
      "pandas/io/tests/test_date_converters.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/io/tests/test_pytables.py",
      "pandas/sparse/frame.py",
      "pandas/sparse/tests/test_sparse.py",
      "pandas/src/datetime.pxd",
      "pandas/src/datetime.pyx",
      "pandas/src/datetime/np_datetime.c",
      "pandas/src/datetime/np_datetime.h",
      "pandas/src/datetime/np_datetime_strings.c",
      "pandas/src/engines.pyx",
      "pandas/src/groupby.pyx",
      "pandas/src/hashtable.pyx",
      "pandas/src/inference.pyx",
      "pandas/src/moments.pyx",
      "pandas/src/numpy_helper.h",
      "pandas/src/offsets.pyx",
      "pandas/src/period.c",
      "pandas/src/plib.pyx",
      "pandas/src/reduce.pyx",
      "pandas/src/tseries.pyx",
      "pandas/src/util.pxd",
      "pandas/stats/misc.py",
      "pandas/stats/moments.py",
      "pandas/stats/ols.py",
      "pandas/stats/tests/test_moments.py",
      "pandas/stats/tests/test_ols.py",
      "pandas/tests/test_algos.py",
      "pandas/tests/test_common.py",
      "pandas/tests/test_format.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_graphics.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_series.py",
      "pandas/tests/test_strings.py",
      "pandas/tools/merge.py",
      "pandas/tools/plotting.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tseries/converter.py",
      "pandas/tseries/frequencies.py",
      "pandas/tseries/index.py",
      "pandas/tseries/offsets.py",
      "pandas/tseries/period.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_frequencies.py",
      "pandas/tseries/tests/test_offsets.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_plotting.py",
      "pandas/tseries/tests/test_resample.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tests/test_timezones.py",
      "pandas/tseries/tests/test_util.py",
      "pandas/tseries/tools.py",
      "pandas/util/decorators.py",
      "scripts/gen_release_notes.py",
      "setup.py",
      "vb_suite/binary_ops.py",
      "vb_suite/make.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1955,
    "reporter": "blounsbury",
    "created_at": "2012-09-23T22:50:58+00:00",
    "closed_at": "2012-09-29T15:36:39+00:00",
    "resolver": "wesm",
    "resolved_in": "44a2734bf97a16ed1c74af10d5cfb4c71376b473",
    "resolver_commit_num": 2434,
    "title": "pip install pandas fails on Mageia",
    "body": "As title says, I get an error trying to pip install pandas on Mageia. Unfortunately, there is no pre-built pandas package in the Mageia repositories. Don't know if I should post the issue here with pandas or if it is Mageia related. The error is:\n\n\n\nI have the necessary pandas dependencies, as shown in the pip.log:\n\n\n\nThanks,\nBob\n",
    "labels": [],
    "comments": [
      "That's weird. What does\n\n```\nimport numpy as np\nprint np.get_include()\n```\n\nprint out? (and what files are in that include directory?)\n",
      "Agree, very strange.\n\n```\n>>> print np.get_include()\n/usr/lib/python2.7/site-packages/numpy/core/include\n\n$ ls /usr/lib/python2.7/site-packages/numpy/core/include\nls: cannot access /usr/lib/python2.7/site-packages/numpy/core/include: No such file or directory\n```\n\nThere is no 'include' directory???\n",
      "Looks like 'core/include' is included in the 'python-numpy-devel' package on Mageia. \n\nNext error message...\n\n```\nbuild/temp.linux-i686-2.7/pandas/src/tseries.o: In function `__pyx_pf_6pandas_3lib_104roll_skew':\n\n/tmp/pip-build/pandas/pandas/src/tseries.c:69639: undefined reference to `sqrt'\n\n/tmp/pip-build/pandas/pandas/src/tseries.c:69648: undefined reference to `sqrt'\n\ncollect2: ld returned 1 exit status\n\nerror: command 'gcc' failed with exit status 1\n```\n",
      "Also weird. Can you paste the full console output including the compiler commands?\n",
      "```\n\n$ sudo pip install pandas\n[sudo] password for bob: \nDownloading/unpacking pandas\n  Running setup.py egg_info for package pandas\n\n    warning: no files found matching 'setupegg.py'\n    no previously-included directories found matching 'doc/build'\n    warning: no previously-included files matching '*.so' found anywhere in distribution\n    warning: no previously-included files matching '*.pyd' found anywhere in distribution\n    warning: no previously-included files matching '*.pyc' found anywhere in distribution\n    warning: no previously-included files matching '.git*' found anywhere in distribution\n    warning: no previously-included files matching '.DS_Store' found anywhere in distribution\n    warning: no previously-included files matching '*.png' found anywhere in distribution\nRequirement already satisfied (use --upgrade to upgrade): python-dateutil<2 in /usr/lib/python2.7/site-packages (from pandas)\nRequirement already satisfied (use --upgrade to upgrade): pytz in /usr/lib/python2.7/site-packages (from pandas)\nRequirement already satisfied (use --upgrade to upgrade): numpy>=1.6 in /usr/lib/python2.7/site-packages (from pandas)\nInstalling collected packages: pandas\n  Running setup.py install for pandas\n    building 'pandas.lib' extension\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -I/usr/include/ncursesw -DNDEBUG -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -g -fPIC -I/usr/lib/python2.7/site-packages/numpy/core/include -I/usr/include/python2.7 -c pandas/src/tseries.c -o build/temp.linux-i686-2.7/pandas/src/tseries.o\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -I/usr/include/ncursesw -DNDEBUG -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -g -fPIC -I/usr/lib/python2.7/site-packages/numpy/core/include -I/usr/include/python2.7 -c pandas/src/datetime/np_datetime.c -o build/temp.linux-i686-2.7/pandas/src/datetime/np_datetime.o\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -I/usr/include/ncursesw -DNDEBUG -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -g -fPIC -I/usr/lib/python2.7/site-packages/numpy/core/include -I/usr/include/python2.7 -c pandas/src/datetime/np_datetime_strings.c -o build/temp.linux-i686-2.7/pandas/src/datetime/np_datetime_strings.o\n    pandas/src/datetime/np_datetime_strings.c: In function \u2018make_iso_8601_datetime\u2019:\n    pandas/src/datetime/np_datetime_strings.c:1149:5: warning: format \u2018%Ld\u2019 expects argument of type \u2018long long int\u2019, but argument 4 has type \u2018long int\u2019 [-Wformat]\n    pandas/src/datetime/np_datetime_strings.c:1149:5: warning: format \u2018%Ld\u2019 expects argument of type \u2018long long int\u2019, but argument 4 has type \u2018long int\u2019 [-Wformat]\n    gcc -pthread -shared -Wl,--as-needed -Wl,--no-undefined -Wl,-z,relro -Wl,-O1 -Wl,--build-id -Wl,--enable-new-dtags build/temp.linux-i686-2.7/pandas/src/tseries.o build/temp.linux-i686-2.7/pandas/src/datetime/np_datetime.o build/temp.linux-i686-2.7/pandas/src/datetime/np_datetime_strings.o -L/usr/lib -lpython2.7 -o build/lib.linux-i686-2.7/pandas/lib.so\n    build/temp.linux-i686-2.7/pandas/src/tseries.o: In function `__pyx_f_6pandas_3lib_Log2':\n    /tmp/pip-build/pandas/pandas/src/tseries.c:42837: undefined reference to `log'\n    build/temp.linux-i686-2.7/pandas/src/tseries.o: In function `__pyx_pf_6pandas_3lib_101nancorr':\n    /tmp/pip-build/pandas/pandas/src/tseries.c:68275: undefined reference to `sqrt'\n    build/temp.linux-i686-2.7/pandas/src/tseries.o: In function `__pyx_f_6pandas_3lib_Log2':\n    /tmp/pip-build/pandas/pandas/src/tseries.c:42837: undefined reference to `log'\n    build/temp.linux-i686-2.7/pandas/src/tseries.o: In function `Log2':\n    /tmp/pip-build/pandas/pandas/src/skiplist.h:40: undefined reference to `log'\n    /tmp/pip-build/pandas/pandas/src/skiplist.h:40: undefined reference to `log'\n    /tmp/pip-build/pandas/pandas/src/skiplist.h:40: undefined reference to `log'\n    build/temp.linux-i686-2.7/pandas/src/tseries.o: In function `__pyx_pf_6pandas_3lib_104roll_skew':\n    /tmp/pip-build/pandas/pandas/src/tseries.c:69639: undefined reference to `sqrt'\n    /tmp/pip-build/pandas/pandas/src/tseries.c:69648: undefined reference to `sqrt'\n    collect2: ld returned 1 exit status\n    error: command 'gcc' failed with exit status 1\n    Complete output from command /usr/bin/python -c \"import setuptools;__file__='/tmp/pip-build/pandas/setup.py';exec(compile(open(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-yJBG1p-record/install-record.txt --single-version-externally-managed:\n    running install\n\nrunning build\n\nrunning build_py\n\ncopying pandas/version.py -> build/lib.linux-i686-2.7/pandas\n\nrunning build_ext\n\nbuilding 'pandas.lib' extension\n\ngcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -I/usr/include/ncursesw -DNDEBUG -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -g -fPIC -I/usr/lib/python2.7/site-packages/numpy/core/include -I/usr/include/python2.7 -c pandas/src/tseries.c -o build/temp.linux-i686-2.7/pandas/src/tseries.o\n\ngcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -I/usr/include/ncursesw -DNDEBUG -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -g -fPIC -I/usr/lib/python2.7/site-packages/numpy/core/include -I/usr/include/python2.7 -c pandas/src/datetime/np_datetime.c -o build/temp.linux-i686-2.7/pandas/src/datetime/np_datetime.o\n\ngcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -I/usr/include/ncursesw -DNDEBUG -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -g -fPIC -I/usr/lib/python2.7/site-packages/numpy/core/include -I/usr/include/python2.7 -c pandas/src/datetime/np_datetime_strings.c -o build/temp.linux-i686-2.7/pandas/src/datetime/np_datetime_strings.o\n\npandas/src/datetime/np_datetime_strings.c: In function \u2018make_iso_8601_datetime\u2019:\n\npandas/src/datetime/np_datetime_strings.c:1149:5: warning: format \u2018%Ld\u2019 expects argument of type \u2018long long int\u2019, but argument 4 has type \u2018long int\u2019 [-Wformat]\n\npandas/src/datetime/np_datetime_strings.c:1149:5: warning: format \u2018%Ld\u2019 expects argument of type \u2018long long int\u2019, but argument 4 has type \u2018long int\u2019 [-Wformat]\n\ngcc -pthread -shared -Wl,--as-needed -Wl,--no-undefined -Wl,-z,relro -Wl,-O1 -Wl,--build-id -Wl,--enable-new-dtags build/temp.linux-i686-2.7/pandas/src/tseries.o build/temp.linux-i686-2.7/pandas/src/datetime/np_datetime.o build/temp.linux-i686-2.7/pandas/src/datetime/np_datetime_strings.o -L/usr/lib -lpython2.7 -o build/lib.linux-i686-2.7/pandas/lib.so\n\nbuild/temp.linux-i686-2.7/pandas/src/tseries.o: In function `__pyx_f_6pandas_3lib_Log2':\n\n/tmp/pip-build/pandas/pandas/src/tseries.c:42837: undefined reference to `log'\n\nbuild/temp.linux-i686-2.7/pandas/src/tseries.o: In function `__pyx_pf_6pandas_3lib_101nancorr':\n\n/tmp/pip-build/pandas/pandas/src/tseries.c:68275: undefined reference to `sqrt'\n\nbuild/temp.linux-i686-2.7/pandas/src/tseries.o: In function `__pyx_f_6pandas_3lib_Log2':\n\n/tmp/pip-build/pandas/pandas/src/tseries.c:42837: undefined reference to `log'\n\nbuild/temp.linux-i686-2.7/pandas/src/tseries.o: In function `Log2':\n\n/tmp/pip-build/pandas/pandas/src/skiplist.h:40: undefined reference to `log'\n\n/tmp/pip-build/pandas/pandas/src/skiplist.h:40: undefined reference to `log'\n\n/tmp/pip-build/pandas/pandas/src/skiplist.h:40: undefined reference to `log'\n\nbuild/temp.linux-i686-2.7/pandas/src/tseries.o: In function `__pyx_pf_6pandas_3lib_104roll_skew':\n\n/tmp/pip-build/pandas/pandas/src/tseries.c:69639: undefined reference to `sqrt'\n\n/tmp/pip-build/pandas/pandas/src/tseries.c:69648: undefined reference to `sqrt'\n\ncollect2: ld returned 1 exit status\n\nerror: command 'gcc' failed with exit status 1\n\n----------------------------------------\nCommand /usr/bin/python -c \"import setuptools;__file__='/tmp/pip-build/pandas/setup.py';exec(compile(open(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-yJBG1p-record/install-record.txt --single-version-externally-managed failed with error code 1 in /tmp/pip-build/pandas\nStoring complete log in /root/.pip/pip.log\n```\n",
      "OK, it looks like your platform requires an explicit linking to the math library (`-lm`). I need to test and see if this causes an issue on Windows but will otherwise make the change...\n",
      "Great. Thanks.\n\nIn the meantime, I'll submit a request to add 'python-pandas?' to the Mageia repositories.\n",
      "master revision should build now (added math library to link)\n",
      "Still not working with version below.\n\n```\n[bob@darkstar pandas-0.9.0rc2]$ sudo python setup.py install\nrunning install\nrunning bdist_egg\nrunning egg_info\ncreating pandas.egg-info\nwriting requirements to pandas.egg-info/requires.txt\nwriting pandas.egg-info/PKG-INFO\nwriting top-level names to pandas.egg-info/top_level.txt\nwriting dependency_links to pandas.egg-info/dependency_links.txt\nwriting manifest file 'pandas.egg-info/SOURCES.txt'\nreading manifest file 'pandas.egg-info/SOURCES.txt'\nreading manifest template 'MANIFEST.in'\nwarning: no files found matching 'setupegg.py'\nno previously-included directories found matching 'doc/build'\nwarning: no previously-included files matching '*.so' found anywhere in distribution\nwarning: no previously-included files matching '*.pyd' found anywhere in distribution\nwarning: no previously-included files matching '*.pyc' found anywhere in distribution\nwarning: no previously-included files matching '.git*' found anywhere in distribution\nwarning: no previously-included files matching '.DS_Store' found anywhere in distribution\nwarning: no previously-included files matching '*.png' found anywhere in distribution\nwriting manifest file 'pandas.egg-info/SOURCES.txt'\ninstalling library code to build/bdist.linux-i686/egg\nrunning install_lib\nrunning build_py\ncreating build\ncreating build/lib.linux-i686-2.7\ncreating build/lib.linux-i686-2.7/pandas\ncopying pandas/setup.py -> build/lib.linux-i686-2.7/pandas\ncopying pandas/info.py -> build/lib.linux-i686-2.7/pandas\ncopying pandas/version.py -> build/lib.linux-i686-2.7/pandas\ncopying pandas/__init__.py -> build/lib.linux-i686-2.7/pandas\ncreating build/lib.linux-i686-2.7/pandas/compat\ncopying pandas/compat/scipy.py -> build/lib.linux-i686-2.7/pandas/compat\ncopying pandas/compat/__init__.py -> build/lib.linux-i686-2.7/pandas/compat\ncreating build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/index.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/daterange.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/format.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/groupby.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/nanops.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/generic.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/series.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/datetools.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/api.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/categorical.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/internals.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/frame.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/panel.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/matrix.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/reshape.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/common.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/strings.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/indexing.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/algorithms.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/sparse.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/__init__.py -> build/lib.linux-i686-2.7/pandas/core\ncreating build/lib.linux-i686-2.7/pandas/io\ncopying pandas/io/date_converters.py -> build/lib.linux-i686-2.7/pandas/io\ncopying pandas/io/data.py -> build/lib.linux-i686-2.7/pandas/io\ncopying pandas/io/parsers.py -> build/lib.linux-i686-2.7/pandas/io\ncopying pandas/io/pytables.py -> build/lib.linux-i686-2.7/pandas/io\ncopying pandas/io/sql.py -> build/lib.linux-i686-2.7/pandas/io\ncopying pandas/io/__init__.py -> build/lib.linux-i686-2.7/pandas/io\ncreating build/lib.linux-i686-2.7/pandas/rpy\ncopying pandas/rpy/mass.py -> build/lib.linux-i686-2.7/pandas/rpy\ncopying pandas/rpy/common.py -> build/lib.linux-i686-2.7/pandas/rpy\ncopying pandas/rpy/vars.py -> build/lib.linux-i686-2.7/pandas/rpy\ncopying pandas/rpy/base.py -> build/lib.linux-i686-2.7/pandas/rpy\ncopying pandas/rpy/__init__.py -> build/lib.linux-i686-2.7/pandas/rpy\ncreating build/lib.linux-i686-2.7/pandas/sandbox\ncopying pandas/sandbox/qtpandas.py -> build/lib.linux-i686-2.7/pandas/sandbox\ncopying pandas/sandbox/__init__.py -> build/lib.linux-i686-2.7/pandas/sandbox\ncreating build/lib.linux-i686-2.7/pandas/sparse\ncopying pandas/sparse/array.py -> build/lib.linux-i686-2.7/pandas/sparse\ncopying pandas/sparse/series.py -> build/lib.linux-i686-2.7/pandas/sparse\ncopying pandas/sparse/api.py -> build/lib.linux-i686-2.7/pandas/sparse\ncopying pandas/sparse/frame.py -> build/lib.linux-i686-2.7/pandas/sparse\ncopying pandas/sparse/panel.py -> build/lib.linux-i686-2.7/pandas/sparse\ncopying pandas/sparse/list.py -> build/lib.linux-i686-2.7/pandas/sparse\ncopying pandas/sparse/__init__.py -> build/lib.linux-i686-2.7/pandas/sparse\ncreating build/lib.linux-i686-2.7/pandas/sparse/tests\ncopying pandas/sparse/tests/test_array.py -> build/lib.linux-i686-2.7/pandas/sparse/tests\ncopying pandas/sparse/tests/test_list.py -> build/lib.linux-i686-2.7/pandas/sparse/tests\ncopying pandas/sparse/tests/test_sparse.py -> build/lib.linux-i686-2.7/pandas/sparse/tests\ncopying pandas/sparse/tests/test_libsparse.py -> build/lib.linux-i686-2.7/pandas/sparse/tests\ncopying pandas/sparse/tests/__init__.py -> build/lib.linux-i686-2.7/pandas/sparse/tests\ncreating build/lib.linux-i686-2.7/pandas/stats\ncopying pandas/stats/math.py -> build/lib.linux-i686-2.7/pandas/stats\ncopying pandas/stats/fama_macbeth.py -> build/lib.linux-i686-2.7/pandas/stats\ncopying pandas/stats/api.py -> build/lib.linux-i686-2.7/pandas/stats\ncopying pandas/stats/misc.py -> build/lib.linux-i686-2.7/pandas/stats\ncopying pandas/stats/common.py -> build/lib.linux-i686-2.7/pandas/stats\ncopying pandas/stats/ols.py -> build/lib.linux-i686-2.7/pandas/stats\ncopying pandas/stats/var.py -> build/lib.linux-i686-2.7/pandas/stats\ncopying pandas/stats/moments.py -> build/lib.linux-i686-2.7/pandas/stats\ncopying pandas/stats/interface.py -> build/lib.linux-i686-2.7/pandas/stats\ncopying pandas/stats/plm.py -> build/lib.linux-i686-2.7/pandas/stats\ncopying pandas/stats/__init__.py -> build/lib.linux-i686-2.7/pandas/stats\ncreating build/lib.linux-i686-2.7/pandas/util\ncopying pandas/util/terminal.py -> build/lib.linux-i686-2.7/pandas/util\ncopying pandas/util/clipboard.py -> build/lib.linux-i686-2.7/pandas/util\ncopying pandas/util/misc.py -> build/lib.linux-i686-2.7/pandas/util\ncopying pandas/util/testing.py -> build/lib.linux-i686-2.7/pandas/util\ncopying pandas/util/counter.py -> build/lib.linux-i686-2.7/pandas/util\ncopying pandas/util/decorators.py -> build/lib.linux-i686-2.7/pandas/util\ncopying pandas/util/compat.py -> build/lib.linux-i686-2.7/pandas/util\ncopying pandas/util/py3compat.py -> build/lib.linux-i686-2.7/pandas/util\ncopying pandas/util/__init__.py -> build/lib.linux-i686-2.7/pandas/util\ncreating build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_stats.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_series.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_groupby.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_factor.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_format.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_common.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_multilevel.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_internals.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_algos.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_ndframe.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_graphics.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_reshape.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_tseries.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_index.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_frame.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_panel.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_strings.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/__init__.py -> build/lib.linux-i686-2.7/pandas/tests\ncreating build/lib.linux-i686-2.7/pandas/tools\ncopying pandas/tools/util.py -> build/lib.linux-i686-2.7/pandas/tools\ncopying pandas/tools/tile.py -> build/lib.linux-i686-2.7/pandas/tools\ncopying pandas/tools/pivot.py -> build/lib.linux-i686-2.7/pandas/tools\ncopying pandas/tools/describe.py -> build/lib.linux-i686-2.7/pandas/tools\ncopying pandas/tools/plotting.py -> build/lib.linux-i686-2.7/pandas/tools\ncopying pandas/tools/merge.py -> build/lib.linux-i686-2.7/pandas/tools\ncopying pandas/tools/__init__.py -> build/lib.linux-i686-2.7/pandas/tools\ncreating build/lib.linux-i686-2.7/pandas/tools/tests\ncopying pandas/tools/tests/test_pivot.py -> build/lib.linux-i686-2.7/pandas/tools/tests\ncopying pandas/tools/tests/test_tile.py -> build/lib.linux-i686-2.7/pandas/tools/tests\ncopying pandas/tools/tests/test_tools.py -> build/lib.linux-i686-2.7/pandas/tools/tests\ncopying pandas/tools/tests/__init__.py -> build/lib.linux-i686-2.7/pandas/tools/tests\ncopying pandas/tools/tests/test_merge.py -> build/lib.linux-i686-2.7/pandas/tools/tests\ncreating build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/frequencies.py -> build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/resample.py -> build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/index.py -> build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/util.py -> build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/tools.py -> build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/api.py -> build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/offsets.py -> build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/plotting.py -> build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/converter.py -> build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/period.py -> build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/interval.py -> build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/__init__.py -> build/lib.linux-i686-2.7/pandas/tseries\ncreating build/lib.linux-i686-2.7/pandas/tseries/tests\ncopying pandas/tseries/tests/test_plotting.py -> build/lib.linux-i686-2.7/pandas/tseries/tests\ncopying pandas/tseries/tests/test_cursor.py -> build/lib.linux-i686-2.7/pandas/tseries/tests\ncopying pandas/tseries/tests/test_frequencies.py -> build/lib.linux-i686-2.7/pandas/tseries/tests\ncopying pandas/tseries/tests/test_timezones.py -> build/lib.linux-i686-2.7/pandas/tseries/tests\ncopying pandas/tseries/tests/test_resample.py -> build/lib.linux-i686-2.7/pandas/tseries/tests\ncopying pandas/tseries/tests/test_daterange.py -> build/lib.linux-i686-2.7/pandas/tseries/tests\ncopying pandas/tseries/tests/test_util.py -> build/lib.linux-i686-2.7/pandas/tseries/tests\ncopying pandas/tseries/tests/test_timeseries.py -> build/lib.linux-i686-2.7/pandas/tseries/tests\ncopying pandas/tseries/tests/test_period.py -> build/lib.linux-i686-2.7/pandas/tseries/tests\ncopying pandas/tseries/tests/__init__.py -> build/lib.linux-i686-2.7/pandas/tseries/tests\ncopying pandas/tseries/tests/test_offsets.py -> build/lib.linux-i686-2.7/pandas/tseries/tests\ncreating build/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/test_pytables.py -> build/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/test_date_converters.py -> build/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/test_yahoo.py -> build/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/test_parsers.py -> build/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/test_sql.py -> build/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/__init__.py -> build/lib.linux-i686-2.7/pandas/io/tests\ncreating build/lib.linux-i686-2.7/pandas/stats/tests\ncopying pandas/stats/tests/test_fama_macbeth.py -> build/lib.linux-i686-2.7/pandas/stats/tests\ncopying pandas/stats/tests/test_moments.py -> build/lib.linux-i686-2.7/pandas/stats/tests\ncopying pandas/stats/tests/test_ols.py -> build/lib.linux-i686-2.7/pandas/stats/tests\ncopying pandas/stats/tests/common.py -> build/lib.linux-i686-2.7/pandas/stats/tests\ncopying pandas/stats/tests/test_math.py -> build/lib.linux-i686-2.7/pandas/stats/tests\ncopying pandas/stats/tests/__init__.py -> build/lib.linux-i686-2.7/pandas/stats/tests\ncopying pandas/stats/tests/test_var.py -> build/lib.linux-i686-2.7/pandas/stats/tests\ncopying pandas/io/tests/legacy.h5 -> build/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/test2.csv -> build/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/test1.csv -> build/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/test2.xls -> build/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/test.xls -> build/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/test3.xls -> build/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/test.xlsx -> build/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/salary.table -> build/lib.linux-i686-2.7/pandas/io/tests\ncreating build/lib.linux-i686-2.7/pandas/tests/data\ncopying pandas/tests/data/mindex_073.pickle -> build/lib.linux-i686-2.7/pandas/tests/data\ncopying pandas/tests/data/multiindex_v1.pickle -> build/lib.linux-i686-2.7/pandas/tests/data\ncopying pandas/tests/data/iris.csv -> build/lib.linux-i686-2.7/pandas/tests/data\ncopying pandas/tests/data/unicode_series.csv -> build/lib.linux-i686-2.7/pandas/tests/data\ncreating build/lib.linux-i686-2.7/pandas/tseries/tests/data\ncopying pandas/tseries/tests/data/series_daterange0.pickle -> build/lib.linux-i686-2.7/pandas/tseries/tests/data\ncopying pandas/tseries/tests/data/daterange_073.pickle -> build/lib.linux-i686-2.7/pandas/tseries/tests/data\ncopying pandas/tseries/tests/data/frame.pickle -> build/lib.linux-i686-2.7/pandas/tseries/tests/data\ncopying pandas/tseries/tests/data/series.pickle -> build/lib.linux-i686-2.7/pandas/tseries/tests/data\nrunning build_ext\nbuilding 'pandas._algos' extension\ncreating build/temp.linux-i686-2.7\ncreating build/temp.linux-i686-2.7/pandas\ncreating build/temp.linux-i686-2.7/pandas/src\ngcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -I/usr/include/ncursesw -DNDEBUG -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -g -fPIC -I/usr/lib/python2.7/site-packages/numpy/core/include -I/usr/include/python2.7 -c pandas/src/generated.c -o build/temp.linux-i686-2.7/pandas/src/generated.o\ngcc -pthread -shared -Wl,--as-needed -Wl,--no-undefined -Wl,-z,relro -Wl,-O1 -Wl,--build-id -Wl,--enable-new-dtags build/temp.linux-i686-2.7/pandas/src/generated.o -L/usr/lib -lpython2.7 -o build/lib.linux-i686-2.7/pandas/_algos.so\nbuilding 'pandas.lib' extension\ncreating build/temp.linux-i686-2.7/pandas/src/datetime\ngcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -I/usr/include/ncursesw -DNDEBUG -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -g -fPIC -I/usr/lib/python2.7/site-packages/numpy/core/include -I/usr/include/python2.7 -c pandas/src/tseries.c -o build/temp.linux-i686-2.7/pandas/src/tseries.o\ngcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -I/usr/include/ncursesw -DNDEBUG -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -g -fPIC -I/usr/lib/python2.7/site-packages/numpy/core/include -I/usr/include/python2.7 -c pandas/src/datetime/np_datetime.c -o build/temp.linux-i686-2.7/pandas/src/datetime/np_datetime.o\ngcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -I/usr/include/ncursesw -DNDEBUG -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -g -fPIC -I/usr/lib/python2.7/site-packages/numpy/core/include -I/usr/include/python2.7 -c pandas/src/datetime/np_datetime_strings.c -o build/temp.linux-i686-2.7/pandas/src/datetime/np_datetime_strings.o\npandas/src/datetime/np_datetime_strings.c: In function \u2018make_iso_8601_datetime\u2019:\npandas/src/datetime/np_datetime_strings.c:1151:5: warning: format \u2018%Ld\u2019 expects argument of type \u2018long long int\u2019, but argument 4 has type \u2018long int\u2019 [-Wformat]\npandas/src/datetime/np_datetime_strings.c:1151:5: warning: format \u2018%Ld\u2019 expects argument of type \u2018long long int\u2019, but argument 4 has type \u2018long int\u2019 [-Wformat]\ngcc -pthread -shared -Wl,--as-needed -Wl,--no-undefined -Wl,-z,relro -Wl,-O1 -Wl,--build-id -Wl,--enable-new-dtags build/temp.linux-i686-2.7/pandas/src/tseries.o build/temp.linux-i686-2.7/pandas/src/datetime/np_datetime.o build/temp.linux-i686-2.7/pandas/src/datetime/np_datetime_strings.o -L/usr/lib -lm -lpython2.7 -o build/lib.linux-i686-2.7/pandas/lib.so\nbuilding 'pandas._period' extension\ngcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -I/usr/include/ncursesw -DNDEBUG -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -g -fPIC -I/usr/lib/python2.7/site-packages/numpy/core/include -I/usr/include/python2.7 -c pandas/src/plib.c -o build/temp.linux-i686-2.7/pandas/src/plib.o\ngcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -I/usr/include/ncursesw -DNDEBUG -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -g -fPIC -I/usr/lib/python2.7/site-packages/numpy/core/include -I/usr/include/python2.7 -c pandas/src/datetime/np_datetime.c -o build/temp.linux-i686-2.7/pandas/src/datetime/np_datetime.o\ngcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -I/usr/include/ncursesw -DNDEBUG -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -g -fPIC -I/usr/lib/python2.7/site-packages/numpy/core/include -I/usr/include/python2.7 -c pandas/src/period.c -o build/temp.linux-i686-2.7/pandas/src/period.o\ngcc -pthread -shared -Wl,--as-needed -Wl,--no-undefined -Wl,-z,relro -Wl,-O1 -Wl,--build-id -Wl,--enable-new-dtags build/temp.linux-i686-2.7/pandas/src/plib.o build/temp.linux-i686-2.7/pandas/src/datetime/np_datetime.o build/temp.linux-i686-2.7/pandas/src/period.o -L/usr/lib -lpython2.7 -o build/lib.linux-i686-2.7/pandas/_period.so\nbuilding 'pandas._sparse' extension\ngcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -I/usr/include/ncursesw -DNDEBUG -O2 -g -pipe -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586 -mtune=generic -fasynchronous-unwind-tables -g -fPIC -I/usr/lib/python2.7/site-packages/numpy/core/include -I/usr/include/python2.7 -c pandas/src/sparse.c -o build/temp.linux-i686-2.7/pandas/src/sparse.o\ngcc -pthread -shared -Wl,--as-needed -Wl,--no-undefined -Wl,-z,relro -Wl,-O1 -Wl,--build-id -Wl,--enable-new-dtags build/temp.linux-i686-2.7/pandas/src/sparse.o -L/usr/lib -lpython2.7 -o build/lib.linux-i686-2.7/pandas/_sparse.so\nbuild/temp.linux-i686-2.7/pandas/src/sparse.o: In function `__pyx_f_6pandas_7_sparse___pow':\n/home/bob/Downloads/pandas-0.9.0rc2/pandas/src/sparse.c:13907: undefined reference to `pow'\n/home/bob/Downloads/pandas-0.9.0rc2/pandas/src/sparse.c:13907: undefined reference to `pow'\ncollect2: ld returned 1 exit status\nerror: command 'gcc' failed with exit status 1\n```\n",
      "Thanks-- missed the math.h usage in the sparse extension. Will fix\n\nSent from my mobile device\n\nOn Sep 27, 2012, at 7:26 PM, bobl2424 notifications@github.com wrote:\n\nStill not working with version below.\n\n[bob@darkstar pandas-0.9.0rc2]$ sudo python setup.py install\nrunning install\nrunning bdist_egg\nrunning egg_info\ncreating pandas.egg-info\nwriting requirements to pandas.egg-info/requires.txt\nwriting pandas.egg-info/PKG-INFO\nwriting top-level names to pandas.egg-info/top_level.txt\nwriting dependency_links to pandas.egg-info/dependency_links.txt\nwriting manifest file 'pandas.egg-info/SOURCES.txt'\nreading manifest file 'pandas.egg-info/SOURCES.txt'\nreading manifest template 'MANIFEST.in'\nwarning: no files found matching 'setupegg.py'\nno previously-included directories found matching 'doc/build'\nwarning: no previously-included files matching '_.so' found anywhere\nin distribution\nwarning: no previously-included files matching '_.pyd' found anywhere\nin distribution\nwarning: no previously-included files matching '_.pyc' found anywhere\nin distribution\nwarning: no previously-included files matching '.git_' found anywhere\nin distribution\nwarning: no previously-included files matching '.DS_Store' found\nanywhere in distribution\nwarning: no previously-included files matching '*.png' found anywhere\nin distribution\nwriting manifest file 'pandas.egg-info/SOURCES.txt'\ninstalling library code to build/bdist.linux-i686/egg\nrunning install_lib\nrunning build_py\ncreating build\ncreating build/lib.linux-i686-2.7\ncreating build/lib.linux-i686-2.7/pandas\ncopying pandas/setup.py -> build/lib.linux-i686-2.7/pandas\ncopying pandas/info.py -> build/lib.linux-i686-2.7/pandas\ncopying pandas/version.py -> build/lib.linux-i686-2.7/pandas\ncopying pandas/**init**.py -> build/lib.linux-i686-2.7/pandas\ncreating build/lib.linux-i686-2.7/pandas/compat\ncopying pandas/compat/scipy.py -> build/lib.linux-i686-2.7/pandas/compat\ncopying pandas/compat/**init**.py -> build/lib.linux-i686-2.7/pandas/compat\ncreating build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/index.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/daterange.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/format.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/groupby.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/nanops.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/generic.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/series.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/datetools.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/api.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/categorical.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/internals.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/frame.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/panel.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/matrix.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/reshape.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/common.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/strings.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/indexing.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/algorithms.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/sparse.py -> build/lib.linux-i686-2.7/pandas/core\ncopying pandas/core/**init**.py -> build/lib.linux-i686-2.7/pandas/core\ncreating build/lib.linux-i686-2.7/pandas/io\ncopying pandas/io/date_converters.py -> build/lib.linux-i686-2.7/pandas/io\ncopying pandas/io/data.py -> build/lib.linux-i686-2.7/pandas/io\ncopying pandas/io/parsers.py -> build/lib.linux-i686-2.7/pandas/io\ncopying pandas/io/pytables.py -> build/lib.linux-i686-2.7/pandas/io\ncopying pandas/io/sql.py -> build/lib.linux-i686-2.7/pandas/io\ncopying pandas/io/**init**.py -> build/lib.linux-i686-2.7/pandas/io\ncreating build/lib.linux-i686-2.7/pandas/rpy\ncopying pandas/rpy/mass.py -> build/lib.linux-i686-2.7/pandas/rpy\ncopying pandas/rpy/common.py -> build/lib.linux-i686-2.7/pandas/rpy\ncopying pandas/rpy/vars.py -> build/lib.linux-i686-2.7/pandas/rpy\ncopying pandas/rpy/base.py -> build/lib.linux-i686-2.7/pandas/rpy\ncopying pandas/rpy/**init**.py -> build/lib.linux-i686-2.7/pandas/rpy\ncreating build/lib.linux-i686-2.7/pandas/sandbox\ncopying pandas/sandbox/qtpandas.py -> build/lib.linux-i686-2.7/pandas/sandbox\ncopying pandas/sandbox/**init**.py -> build/lib.linux-i686-2.7/pandas/sandbox\ncreating build/lib.linux-i686-2.7/pandas/sparse\ncopying pandas/sparse/array.py -> build/lib.linux-i686-2.7/pandas/sparse\ncopying pandas/sparse/series.py -> build/lib.linux-i686-2.7/pandas/sparse\ncopying pandas/sparse/api.py -> build/lib.linux-i686-2.7/pandas/sparse\ncopying pandas/sparse/frame.py -> build/lib.linux-i686-2.7/pandas/sparse\ncopying pandas/sparse/panel.py -> build/lib.linux-i686-2.7/pandas/sparse\ncopying pandas/sparse/list.py -> build/lib.linux-i686-2.7/pandas/sparse\ncopying pandas/sparse/**init**.py -> build/lib.linux-i686-2.7/pandas/sparse\ncreating build/lib.linux-i686-2.7/pandas/sparse/tests\ncopying pandas/sparse/tests/test_array.py ->\nbuild/lib.linux-i686-2.7/pandas/sparse/tests\ncopying pandas/sparse/tests/test_list.py ->\nbuild/lib.linux-i686-2.7/pandas/sparse/tests\ncopying pandas/sparse/tests/test_sparse.py ->\nbuild/lib.linux-i686-2.7/pandas/sparse/tests\ncopying pandas/sparse/tests/test_libsparse.py ->\nbuild/lib.linux-i686-2.7/pandas/sparse/tests\ncopying pandas/sparse/tests/**init**.py ->\nbuild/lib.linux-i686-2.7/pandas/sparse/tests\ncreating build/lib.linux-i686-2.7/pandas/stats\ncopying pandas/stats/math.py -> build/lib.linux-i686-2.7/pandas/stats\ncopying pandas/stats/fama_macbeth.py -> build/lib.linux-i686-2.7/pandas/stats\ncopying pandas/stats/api.py -> build/lib.linux-i686-2.7/pandas/stats\ncopying pandas/stats/misc.py -> build/lib.linux-i686-2.7/pandas/stats\ncopying pandas/stats/common.py -> build/lib.linux-i686-2.7/pandas/stats\ncopying pandas/stats/ols.py -> build/lib.linux-i686-2.7/pandas/stats\ncopying pandas/stats/var.py -> build/lib.linux-i686-2.7/pandas/stats\ncopying pandas/stats/moments.py -> build/lib.linux-i686-2.7/pandas/stats\ncopying pandas/stats/interface.py -> build/lib.linux-i686-2.7/pandas/stats\ncopying pandas/stats/plm.py -> build/lib.linux-i686-2.7/pandas/stats\ncopying pandas/stats/**init**.py -> build/lib.linux-i686-2.7/pandas/stats\ncreating build/lib.linux-i686-2.7/pandas/util\ncopying pandas/util/terminal.py -> build/lib.linux-i686-2.7/pandas/util\ncopying pandas/util/clipboard.py -> build/lib.linux-i686-2.7/pandas/util\ncopying pandas/util/misc.py -> build/lib.linux-i686-2.7/pandas/util\ncopying pandas/util/testing.py -> build/lib.linux-i686-2.7/pandas/util\ncopying pandas/util/counter.py -> build/lib.linux-i686-2.7/pandas/util\ncopying pandas/util/decorators.py -> build/lib.linux-i686-2.7/pandas/util\ncopying pandas/util/compat.py -> build/lib.linux-i686-2.7/pandas/util\ncopying pandas/util/py3compat.py -> build/lib.linux-i686-2.7/pandas/util\ncopying pandas/util/**init**.py -> build/lib.linux-i686-2.7/pandas/util\ncreating build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_stats.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_series.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_groupby.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_factor.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_format.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_common.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_multilevel.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_internals.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_algos.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_ndframe.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_graphics.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_reshape.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_tseries.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_index.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_frame.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_panel.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/test_strings.py -> build/lib.linux-i686-2.7/pandas/tests\ncopying pandas/tests/**init**.py -> build/lib.linux-i686-2.7/pandas/tests\ncreating build/lib.linux-i686-2.7/pandas/tools\ncopying pandas/tools/util.py -> build/lib.linux-i686-2.7/pandas/tools\ncopying pandas/tools/tile.py -> build/lib.linux-i686-2.7/pandas/tools\ncopying pandas/tools/pivot.py -> build/lib.linux-i686-2.7/pandas/tools\ncopying pandas/tools/describe.py -> build/lib.linux-i686-2.7/pandas/tools\ncopying pandas/tools/plotting.py -> build/lib.linux-i686-2.7/pandas/tools\ncopying pandas/tools/merge.py -> build/lib.linux-i686-2.7/pandas/tools\ncopying pandas/tools/**init**.py -> build/lib.linux-i686-2.7/pandas/tools\ncreating build/lib.linux-i686-2.7/pandas/tools/tests\ncopying pandas/tools/tests/test_pivot.py ->\nbuild/lib.linux-i686-2.7/pandas/tools/tests\ncopying pandas/tools/tests/test_tile.py ->\nbuild/lib.linux-i686-2.7/pandas/tools/tests\ncopying pandas/tools/tests/test_tools.py ->\nbuild/lib.linux-i686-2.7/pandas/tools/tests\ncopying pandas/tools/tests/**init**.py ->\nbuild/lib.linux-i686-2.7/pandas/tools/tests\ncopying pandas/tools/tests/test_merge.py ->\nbuild/lib.linux-i686-2.7/pandas/tools/tests\ncreating build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/frequencies.py -> build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/resample.py -> build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/index.py -> build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/util.py -> build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/tools.py -> build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/api.py -> build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/offsets.py -> build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/plotting.py -> build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/converter.py -> build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/period.py -> build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/interval.py -> build/lib.linux-i686-2.7/pandas/tseries\ncopying pandas/tseries/**init**.py -> build/lib.linux-i686-2.7/pandas/tseries\ncreating build/lib.linux-i686-2.7/pandas/tseries/tests\ncopying pandas/tseries/tests/test_plotting.py ->\nbuild/lib.linux-i686-2.7/pandas/tseries/tests\ncopying pandas/tseries/tests/test_cursor.py ->\nbuild/lib.linux-i686-2.7/pandas/tseries/tests\ncopying pandas/tseries/tests/test_frequencies.py ->\nbuild/lib.linux-i686-2.7/pandas/tseries/tests\ncopying pandas/tseries/tests/test_timezones.py ->\nbuild/lib.linux-i686-2.7/pandas/tseries/tests\ncopying pandas/tseries/tests/test_resample.py ->\nbuild/lib.linux-i686-2.7/pandas/tseries/tests\ncopying pandas/tseries/tests/test_daterange.py ->\nbuild/lib.linux-i686-2.7/pandas/tseries/tests\ncopying pandas/tseries/tests/test_util.py ->\nbuild/lib.linux-i686-2.7/pandas/tseries/tests\ncopying pandas/tseries/tests/test_timeseries.py ->\nbuild/lib.linux-i686-2.7/pandas/tseries/tests\ncopying pandas/tseries/tests/test_period.py ->\nbuild/lib.linux-i686-2.7/pandas/tseries/tests\ncopying pandas/tseries/tests/**init**.py ->\nbuild/lib.linux-i686-2.7/pandas/tseries/tests\ncopying pandas/tseries/tests/test_offsets.py ->\nbuild/lib.linux-i686-2.7/pandas/tseries/tests\ncreating build/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/test_pytables.py ->\nbuild/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/test_date_converters.py ->\nbuild/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/test_yahoo.py ->\nbuild/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/test_parsers.py ->\nbuild/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/test_sql.py -> build/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/**init**.py -> build/lib.linux-i686-2.7/pandas/io/tests\ncreating build/lib.linux-i686-2.7/pandas/stats/tests\ncopying pandas/stats/tests/test_fama_macbeth.py ->\nbuild/lib.linux-i686-2.7/pandas/stats/tests\ncopying pandas/stats/tests/test_moments.py ->\nbuild/lib.linux-i686-2.7/pandas/stats/tests\ncopying pandas/stats/tests/test_ols.py ->\nbuild/lib.linux-i686-2.7/pandas/stats/tests\ncopying pandas/stats/tests/common.py ->\nbuild/lib.linux-i686-2.7/pandas/stats/tests\ncopying pandas/stats/tests/test_math.py ->\nbuild/lib.linux-i686-2.7/pandas/stats/tests\ncopying pandas/stats/tests/**init**.py ->\nbuild/lib.linux-i686-2.7/pandas/stats/tests\ncopying pandas/stats/tests/test_var.py ->\nbuild/lib.linux-i686-2.7/pandas/stats/tests\ncopying pandas/io/tests/legacy.h5 -> build/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/test2.csv -> build/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/test1.csv -> build/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/test2.xls -> build/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/test.xls -> build/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/test3.xls -> build/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/test.xlsx -> build/lib.linux-i686-2.7/pandas/io/tests\ncopying pandas/io/tests/salary.table -> build/lib.linux-i686-2.7/pandas/io/tests\ncreating build/lib.linux-i686-2.7/pandas/tests/data\ncopying pandas/tests/data/mindex_073.pickle ->\nbuild/lib.linux-i686-2.7/pandas/tests/data\ncopying pandas/tests/data/multiindex_v1.pickle ->\nbuild/lib.linux-i686-2.7/pandas/tests/data\ncopying pandas/tests/data/iris.csv -> build/lib.linux-i686-2.7/pandas/tests/data\ncopying pandas/tests/data/unicode_series.csv ->\nbuild/lib.linux-i686-2.7/pandas/tests/data\ncreating build/lib.linux-i686-2.7/pandas/tseries/tests/data\ncopying pandas/tseries/tests/data/series_daterange0.pickle ->\nbuild/lib.linux-i686-2.7/pandas/tseries/tests/data\ncopying pandas/tseries/tests/data/daterange_073.pickle ->\nbuild/lib.linux-i686-2.7/pandas/tseries/tests/data\ncopying pandas/tseries/tests/data/frame.pickle ->\nbuild/lib.linux-i686-2.7/pandas/tseries/tests/data\ncopying pandas/tseries/tests/data/series.pickle ->\nbuild/lib.linux-i686-2.7/pandas/tseries/tests/data\nrunning build_ext\nbuilding 'pandas._algos' extension\ncreating build/temp.linux-i686-2.7\ncreating build/temp.linux-i686-2.7/pandas\ncreating build/temp.linux-i686-2.7/pandas/src\ngcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wformat\n-Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector\n--param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586\n-mtune=generic -fasynchronous-unwind-tables -I/usr/include/ncursesw\n-DNDEBUG -O2 -g -pipe -Wformat -Werror=format-security\n-Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4\n-fomit-frame-pointer -march=i586 -mtune=generic\n-fasynchronous-unwind-tables -g -fPIC\n-I/usr/lib/python2.7/site-packages/numpy/core/include\n-I/usr/include/python2.7 -c pandas/src/generated.c -o\nbuild/temp.linux-i686-2.7/pandas/src/generated.o\ngcc -pthread -shared -Wl,--as-needed -Wl,--no-undefined -Wl,-z,relro\n-Wl,-O1 -Wl,--build-id -Wl,--enable-new-dtags\nbuild/temp.linux-i686-2.7/pandas/src/generated.o -L/usr/lib\n-lpython2.7 -o build/lib.linux-i686-2.7/pandas/_algos.so\nbuilding 'pandas.lib' extension\ncreating build/temp.linux-i686-2.7/pandas/src/datetime\ngcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wformat\n-Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector\n--param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586\n-mtune=generic -fasynchronous-unwind-tables -I/usr/include/ncursesw\n-DNDEBUG -O2 -g -pipe -Wformat -Werror=format-security\n-Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4\n-fomit-frame-pointer -march=i586 -mtune=generic\n-fasynchronous-unwind-tables -g -fPIC\n-I/usr/lib/python2.7/site-packages/numpy/core/include\n-I/usr/include/python2.7 -c pandas/src/tseries.c -o\nbuild/temp.linux-i686-2.7/pandas/src/tseries.o\ngcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wformat\n-Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector\n--param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586\n-mtune=generic -fasynchronous-unwind-tables -I/usr/include/ncursesw\n-DNDEBUG -O2 -g -pipe -Wformat -Werror=format-security\n-Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4\n-fomit-frame-pointer -march=i586 -mtune=generic\n-fasynchronous-unwind-tables -g -fPIC\n-I/usr/lib/python2.7/site-packages/numpy/core/include\n-I/usr/include/python2.7 -c pandas/src/datetime/np_datetime.c -o\nbuild/temp.linux-i686-2.7/pandas/src/datetime/np_datetime.o\ngcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wformat\n-Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector\n--param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586\n-mtune=generic -fasynchronous-unwind-tables -I/usr/include/ncursesw\n-DNDEBUG -O2 -g -pipe -Wformat -Werror=format-security\n-Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4\n-fomit-frame-pointer -march=i586 -mtune=generic\n-fasynchronous-unwind-tables -g -fPIC\n-I/usr/lib/python2.7/site-packages/numpy/core/include\n-I/usr/include/python2.7 -c pandas/src/datetime/np_datetime_strings.c\n-o build/temp.linux-i686-2.7/pandas/src/datetime/np_datetime_strings.o\npandas/src/datetime/np_datetime_strings.c: In function \u0091make_iso_8601_datetime\u0092:\npandas/src/datetime/np_datetime_strings.c:1151:5: warning: format\n\u0091%Ld\u0092 expects argument of type \u0091long long int\u0092, but argument 4 has\ntype \u0091long int\u0092 [-Wformat]\npandas/src/datetime/np_datetime_strings.c:1151:5: warning: format\n\u0091%Ld\u0092 expects argument of type \u0091long long int\u0092, but argument 4 has\ntype \u0091long int\u0092 [-Wformat]\ngcc -pthread -shared -Wl,--as-needed -Wl,--no-undefined -Wl,-z,relro\n-Wl,-O1 -Wl,--build-id -Wl,--enable-new-dtags\nbuild/temp.linux-i686-2.7/pandas/src/tseries.o\nbuild/temp.linux-i686-2.7/pandas/src/datetime/np_datetime.o\nbuild/temp.linux-i686-2.7/pandas/src/datetime/np_datetime_strings.o\n-L/usr/lib -lm -lpython2.7 -o build/lib.linux-i686-2.7/pandas/lib.so\nbuilding 'pandas._period' extension\ngcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wformat\n-Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector\n--param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586\n-mtune=generic -fasynchronous-unwind-tables -I/usr/include/ncursesw\n-DNDEBUG -O2 -g -pipe -Wformat -Werror=format-security\n-Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4\n-fomit-frame-pointer -march=i586 -mtune=generic\n-fasynchronous-unwind-tables -g -fPIC\n-I/usr/lib/python2.7/site-packages/numpy/core/include\n-I/usr/include/python2.7 -c pandas/src/plib.c -o\nbuild/temp.linux-i686-2.7/pandas/src/plib.o\ngcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wformat\n-Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector\n--param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586\n-mtune=generic -fasynchronous-unwind-tables -I/usr/include/ncursesw\n-DNDEBUG -O2 -g -pipe -Wformat -Werror=format-security\n-Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4\n-fomit-frame-pointer -march=i586 -mtune=generic\n-fasynchronous-unwind-tables -g -fPIC\n-I/usr/lib/python2.7/site-packages/numpy/core/include\n-I/usr/include/python2.7 -c pandas/src/datetime/np_datetime.c -o\nbuild/temp.linux-i686-2.7/pandas/src/datetime/np_datetime.o\ngcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wformat\n-Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector\n--param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586\n-mtune=generic -fasynchronous-unwind-tables -I/usr/include/ncursesw\n-DNDEBUG -O2 -g -pipe -Wformat -Werror=format-security\n-Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4\n-fomit-frame-pointer -march=i586 -mtune=generic\n-fasynchronous-unwind-tables -g -fPIC\n-I/usr/lib/python2.7/site-packages/numpy/core/include\n-I/usr/include/python2.7 -c pandas/src/period.c -o\nbuild/temp.linux-i686-2.7/pandas/src/period.o\ngcc -pthread -shared -Wl,--as-needed -Wl,--no-undefined -Wl,-z,relro\n-Wl,-O1 -Wl,--build-id -Wl,--enable-new-dtags\nbuild/temp.linux-i686-2.7/pandas/src/plib.o\nbuild/temp.linux-i686-2.7/pandas/src/datetime/np_datetime.o\nbuild/temp.linux-i686-2.7/pandas/src/period.o -L/usr/lib -lpython2.7\n-o build/lib.linux-i686-2.7/pandas/_period.so\nbuilding 'pandas._sparse' extension\ngcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wformat\n-Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector\n--param=ssp-buffer-size=4 -fomit-frame-pointer -march=i586\n-mtune=generic -fasynchronous-unwind-tables -I/usr/include/ncursesw\n-DNDEBUG -O2 -g -pipe -Wformat -Werror=format-security\n-Wp,-D_FORTIFY_SOURCE=2 -fstack-protector --param=ssp-buffer-size=4\n-fomit-frame-pointer -march=i586 -mtune=generic\n-fasynchronous-unwind-tables -g -fPIC\n-I/usr/lib/python2.7/site-packages/numpy/core/include\n-I/usr/include/python2.7 -c pandas/src/sparse.c -o\nbuild/temp.linux-i686-2.7/pandas/src/sparse.o\ngcc -pthread -shared -Wl,--as-needed -Wl,--no-undefined -Wl,-z,relro\n-Wl,-O1 -Wl,--build-id -Wl,--enable-new-dtags\nbuild/temp.linux-i686-2.7/pandas/src/sparse.o -L/usr/lib -lpython2.7\n-o build/lib.linux-i686-2.7/pandas/_sparse.so\nbuild/temp.linux-i686-2.7/pandas/src/sparse.o: In function\n`__pyx_f_6pandas_7_sparse___pow':\n/home/bob/Downloads/pandas-0.9.0rc2/pandas/src/sparse.c:13907:\nundefined reference to`pow'\n/home/bob/Downloads/pandas-0.9.0rc2/pandas/src/sparse.c:13907:\nundefined reference to `pow'\ncollect2: ld returned 1 exit status\nerror: command 'gcc' failed with exit status 1\n\n \u0097\nReply to this email directly or view it on\nGitHubhttps://github.com/pydata/pandas/issues/1955#issuecomment-8958512.\n",
      "I added another library link. Could you let me know if this fixes the issue? Unfortunately I don't have a platform to test on where this is needed\n",
      "Thanks! All seems well. Except that this statement from http://pandas.pydata.org/getpandas.html does not seem true:\n\n```\nStable windows binaries are built on a rotating basis every hour, as long \nas there have been code changes on github since the previous build. \nYou can find the latest builds here.\n```\n\nAnyways, I cloned the read-only git version. It installed successfully.\n",
      "Thanks. The dev builds have been disabled for the last couple of months, need to update the website\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "closed",
      "commented",
      "referenced",
      "commented",
      "commented",
      "referenced",
      "reopened",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 1,
    "deletions": 8,
    "changed_files_list": [
      "pandas/src/generate_code.py",
      "pandas/src/generated.pyx",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1971,
    "reporter": "hayd",
    "created_at": "2012-09-26T09:52:21+00:00",
    "closed_at": "2012-09-27T00:40:07+00:00",
    "resolver": "wesm",
    "resolved_in": "64e887823fb2e26f51c3a2490ffaeba4d24a2345",
    "resolver_commit_num": 2437,
    "title": "set_index breaks with multi keys but one empty",
    "body": "Migrated from StackOverflow: -index-on-multiple-columns-with-one-empty-column\n\n\n\n\n\n\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 25,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/index.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1976,
    "reporter": "elliottwaldron",
    "created_at": "2012-09-26T16:53:22+00:00",
    "closed_at": "2012-11-02T22:30:04+00:00",
    "resolver": "wesm",
    "resolved_in": "5f040c90d3063989fae2dddc3970c84fe9d01468",
    "resolver_commit_num": 2516,
    "title": "regex filter in DataFrame.filter",
    "body": "Hi pandas folks - \n\nBack to using pandas after having to take a hiatus in MATLAB.  One minor problem I ran into was the use of re.match instead of re.search (as is documented) in the DataFrame.filter method.   \n\nI assumed there was a good reason for the change, but for most of my use cases the re.search way was less verbose.   \n\nWe should update the method documentation anyway, but could you see supporting both re.match and re.search?\n\nExcellent package and am using in on a daily basis for both research and production code.\n\nBest,\n\nElliott   \n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 2,
    "additions": 20,
    "deletions": 6,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1978,
    "reporter": "wesm",
    "created_at": "2012-09-26T21:41:17+00:00",
    "closed_at": "2013-03-28T05:19:19+00:00",
    "resolver": "wesm",
    "resolved_in": "8a18af8d135cf5a5d13bee765f7db969ce573d87",
    "resolver_commit_num": 2847,
    "title": "qcut user-reported failure",
    "body": "from pystatsmodels mailing list\n\n\n",
    "labels": [],
    "comments": [
      "Leaving this issue open with no milestone until reproducible test case found\n",
      "I get this error with an all zeros timeseries:\n\n```\nx = Series(0, index=date_range('2011-01-01', end='2011-01-31'))\ncut(x, 10)\n```\n\nI think the series being `cut` needs more than one unique values for this error not to happen.\n",
      "from ML \"I hit this error, and there is an easy way to reproduce it... it is just a matter of having an array with multiple repeated values, with one of them being as large as the numbe rof elements on one category.\n\"\n",
      "Another example: \n\n```\nimport pandas as pd\nquantile = [0, 0.50, 0.75, 0.90, 0.95, 0.99, 1]\npd.qcut([0,0,0,0,0,0,0,0,0,0], quantile)\n```\n\nresults in \n\n```\nValueError: Categorical levels must be unique\n```\n\nNote: \n\n```\npd.__version__ \n'0.10.1'\n```\n"
    ],
    "events": [
      "cross-referenced",
      "assigned",
      "commented",
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 8,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tools/tests/test_tile.py",
      "pandas/tools/tile.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1979,
    "reporter": "sunfaquir",
    "created_at": "2012-09-26T21:59:04+00:00",
    "closed_at": "2012-11-03T17:08:52+00:00",
    "resolver": "wesm",
    "resolved_in": "99616eb3ff34f7dca1204136295b2c97c11978c2",
    "resolver_commit_num": 2523,
    "title": "qcut bin formatting bugs",
    "body": "I'm reading CSV tables and then use qcut to bin the continuous numbers. Given the number of bins, most of the time the qcut works fine. However, some times, it doesn't function correctly. The following are some weird cases I got in the result levels index. Sorry I can't give the experimental data  now because I'm running it on a very large data collections and currently I'm not allowed to public the data\n\n(1) When dealing with negative numbers, sometimes it gives [-117.-1,  1], This happens at the first bin index in my test case.\n(2) when bin# is two, it gives results like [1, 2005], (2005, 2005]\n(3)a bin result like this:  please note the value 0.993, and value 0.1\narray([[0.6, 0.991], (0.991, 0.992], (0.992, 0.993], (0.993, 0.993],\n       (0.993, 0.994], (0.994, 0.995], (0.995, 0.996], (0.996, 0.997],\n       (0.997, 0.998], (0.998, 0.1], (0.1, 1.2], (1.2, 1.5], (1.5, 2.1],\n       (2.1, 3.5], (3.5, 5.2], (5.2, 7.1], (7.1, 8.5], (8.5, 11],\n       (11, 13.7], (13.7, 65.8]], dtype=object)\n\n(2) and (3) might be caused by the same reason\n\nI'm using pandas 0.8.1 thanks.\n",
    "labels": [],
    "comments": [
      "related also #1978\n",
      "Could you give some sufficiently anonymized data that illustrates these issues? \n",
      "Sure.  please allow me some time to post it.\nFor case(3), using the following data as a column  tmpcol \npd.qcut(tmpcol, 20), I got the bin's levels as \n\narray([[0.6, 0.991], (0.991, 0.992], (0.992, 0.993], (0.993, 0.993],\n       (0.993, 0.994], (0.994, 0.995], (0.995, 0.996], (0.996, 0.997],\n       (0.997, 0.998], (0.998, 0.1], (0.1, 1.2], (1.2, 1.5], (1.5, 2.1],\n       (2.1, 3.5], (3.5, 5.2], (5.2, 7.1], (7.1, 8.5], (8.5, 11],\n       (11, 13.7], (13.7, 65.8]], dtype=object)\n\nNote  (0.993, 0.993], and (0.998, 0.1]\n",
      "1.001\n0.994\n0.9951\n0.9956\n0.9956\n0.9951\n0.9949\n1.001\n0.994\n0.9938\n0.9908\n0.9947\n0.992\n0.9912\n1.0002\n0.9914\n0.9928\n0.9892\n0.9917\n0.9955\n0.9892\n0.9912\n0.993\n0.9937\n0.9951\n0.9955\n0.993\n0.9961\n0.9914\n0.9906\n0.9974\n0.9934\n0.992\n0.9939\n0.9962\n0.9905\n0.9934\n0.9906\n0.9999\n0.9999\n0.9937\n0.9937\n0.9954\n0.9934\n0.9934\n0.9931\n0.994\n0.9939\n0.9954\n0.995\n0.9917\n0.9914\n0.991\n0.9911\n0.993\n0.9908\n0.9962\n0.9972\n0.9931\n0.9926\n0.9951\n0.9972\n0.991\n0.9931\n0.9927\n0.9934\n0.9903\n0.992\n0.9926\n0.9962\n0.9956\n0.9958\n0.9964\n0.9941\n0.9926\n0.9962\n0.9898\n0.9912\n0.9961\n0.9949\n0.9929\n0.9985\n0.9946\n0.9966\n0.9974\n0.9975\n0.9974\n0.9972\n0.9974\n0.9975\n0.9974\n0.9957\n0.99\n0.9899\n0.9916\n0.9969\n0.9979\n0.9913\n0.9956\n0.9979\n0.9975\n0.9962\n0.997\n1\n0.9975\n0.9974\n0.9962\n0.999\n0.999\n0.9927\n0.9959\n1\n0.9982\n0.9968\n0.9968\n0.994\n0.9914\n0.9911\n0.9982\n0.9982\n0.9934\n0.9984\n0.9952\n0.9952\n0.9928\n0.9912\n0.994\n0.9958\n0.9924\n0.9924\n0.994\n0.9958\n0.9979\n0.9982\n0.9961\n0.9979\n0.992\n0.9975\n0.9917\n0.9923\n0.9927\n0.9975\n0.992\n0.9947\n0.9921\n0.9905\n0.9918\n0.9951\n0.9917\n0.994\n0.9934\n0.9968\n0.994\n0.9919\n0.9966\n0.9979\n0.9979\n0.9898\n0.9894\n0.9894\n0.9898\n0.998\n0.9932\n0.9979\n0.997\n0.9972\n0.9974\n0.9896\n0.9968\n0.9958\n0.9906\n0.9917\n0.9902\n0.9918\n0.999\n0.9927\n0.991\n0.9972\n0.9931\n0.995\n0.9951\n0.9936\n1.001\n0.9979\n0.997\n0.9972\n0.9954\n0.9924\n0.9906\n0.9962\n0.9962\n1.001\n0.9928\n0.9942\n0.9942\n0.9942\n0.9942\n0.9961\n0.998\n0.9961\n0.9984\n0.998\n0.9973\n0.9949\n0.9924\n0.9972\n0.9958\n0.9968\n0.9938\n0.993\n0.994\n0.9918\n0.9958\n0.9944\n0.9912\n0.9961\n0.9939\n0.9961\n0.9989\n0.9938\n0.9939\n0.9971\n0.9912\n0.9936\n0.9929\n0.9998\n0.9938\n0.9969\n0.9938\n0.9998\n0.9972\n0.9976\n0.9976\n0.9979\n0.9979\n0.9979\n0.9979\n0.9972\n0.9918\n0.9982\n0.9985\n0.9944\n0.9903\n0.9934\n0.9975\n0.9923\n0.99\n0.9905\n0.9905\n0.996\n0.9964\n0.998\n0.9975\n0.9913\n0.9932\n0.9935\n0.9927\n0.9927\n0.9912\n0.9904\n0.9939\n0.9996\n0.9944\n0.9977\n0.9912\n0.9996\n0.9965\n0.9944\n0.9945\n0.9944\n0.9965\n0.9944\n0.9972\n0.9949\n0.9966\n0.9954\n0.9954\n0.9915\n0.9919\n0.9916\n0.99\n0.9909\n0.9938\n0.9982\n0.9988\n0.9961\n0.9978\n0.9979\n0.9979\n0.9979\n0.9979\n0.9945\n1\n0.9957\n0.9968\n0.9934\n0.9976\n0.9932\n0.997\n0.9923\n0.9914\n0.992\n0.9914\n0.9914\n0.9949\n0.9949\n0.995\n0.995\n0.9927\n0.9928\n0.9917\n0.9918\n0.9954\n0.9941\n0.9941\n0.9934\n0.9927\n0.9938\n0.9933\n0.9934\n0.9927\n0.9938\n0.9927\n0.9946\n0.993\n0.9946\n0.9976\n0.9944\n0.9978\n0.992\n0.9912\n0.9927\n0.9906\n0.9954\n0.9923\n0.9906\n0.991\n0.9972\n0.9945\n0.9934\n0.9964\n0.9948\n0.9962\n0.9931\n0.993\n0.9942\n0.9906\n0.9995\n0.998\n0.997\n0.9914\n0.992\n0.9924\n0.992\n0.9937\n0.9978\n0.9978\n0.9927\n0.994\n0.9935\n0.9968\n0.9941\n0.9942\n0.9978\n0.9923\n0.9912\n0.9923\n0.9927\n0.9931\n0.9941\n0.9927\n0.9931\n0.9934\n0.9936\n0.9893\n0.9893\n0.9919\n0.9924\n0.9927\n0.9919\n0.9924\n0.9975\n0.9969\n0.9936\n0.991\n0.9893\n0.9906\n0.9941\n0.995\n0.9983\n0.9983\n0.9916\n0.9957\n0.99\n0.9976\n0.992\n0.9917\n0.9917\n0.9993\n0.9908\n0.9917\n0.9976\n0.9934\n1\n0.9918\n0.992\n0.9896\n0.9932\n0.992\n0.9917\n0.9999\n0.998\n0.9918\n0.9918\n0.9999\n0.998\n0.9927\n0.9959\n0.9927\n0.9929\n0.9898\n0.9954\n0.9954\n0.9954\n0.9954\n0.9954\n0.9954\n0.9974\n0.9936\n0.9978\n0.9974\n0.9927\n0.9934\n0.9938\n0.9922\n0.992\n0.9935\n0.9906\n0.9934\n0.9934\n0.9913\n0.9938\n0.9898\n0.9975\n0.9975\n0.9937\n0.9914\n0.9982\n0.9982\n0.9929\n0.9971\n0.9921\n0.9931\n0.9924\n0.9929\n0.9982\n0.9892\n0.9956\n0.9924\n0.9971\n0.9956\n0.9982\n0.9973\n0.9932\n0.9976\n0.9962\n0.9956\n0.9932\n0.9976\n0.9992\n0.9983\n0.9937\n0.99\n0.9944\n0.9938\n0.9965\n0.9893\n0.9927\n0.994\n0.9928\n0.9964\n0.9917\n0.9972\n0.9964\n0.9954\n0.993\n0.9928\n0.9916\n0.9936\n0.9962\n0.9899\n0.9898\n0.996\n0.9907\n0.994\n0.9913\n0.9976\n0.9904\n0.992\n0.9976\n0.999\n0.9975\n0.9937\n0.9937\n0.998\n0.998\n0.9944\n0.9938\n0.9907\n0.9938\n0.9921\n0.9908\n0.9931\n0.9915\n0.9952\n0.9926\n0.9934\n0.992\n0.9918\n0.9942\n0.9942\n0.9942\n0.9901\n0.9898\n0.9902\n0.9934\n0.9906\n0.9898\n0.9896\n0.9922\n0.9947\n0.9945\n0.9976\n0.9976\n0.9976\n0.9987\n0.9987\n0.9976\n0.992\n0.9955\n0.9953\n0.9976\n0.992\n0.9952\n0.9983\n0.9933\n0.9958\n0.9922\n0.9928\n0.9976\n0.9976\n0.9916\n0.9901\n0.9976\n0.9901\n0.9916\n0.9982\n0.993\n0.9969\n0.991\n0.9953\n0.9924\n0.9969\n0.9928\n0.9945\n0.9967\n0.9944\n0.9928\n0.9929\n0.9948\n0.9976\n0.9912\n0.9987\n0.99\n0.991\n0.9933\n0.9933\n0.9899\n0.9912\n0.9912\n0.9976\n0.994\n0.9947\n0.9954\n0.993\n0.9954\n0.9963\n0.992\n0.9926\n0.995\n0.9983\n0.992\n0.9968\n0.9905\n0.9904\n0.9926\n0.9968\n0.9928\n0.9949\n0.9909\n0.9937\n0.9914\n0.9905\n0.9904\n0.9924\n0.9924\n0.9965\n0.9965\n0.9993\n0.9965\n0.9908\n0.992\n0.9978\n0.9978\n0.9978\n0.9978\n0.9912\n0.9928\n0.9928\n0.993\n0.9993\n0.9965\n0.9937\n0.9913\n0.9934\n0.9952\n0.9983\n0.9957\n0.9957\n0.9916\n0.9999\n0.9999\n0.9936\n0.9972\n0.9933\n0.9934\n0.9931\n0.9976\n0.9937\n0.9937\n0.991\n0.9979\n0.9971\n0.9969\n0.9968\n0.9961\n0.993\n0.9973\n0.9944\n0.9986\n0.9986\n0.9986\n0.9986\n0.9972\n0.9917\n0.992\n0.9932\n0.9936\n0.9915\n0.9922\n0.9934\n0.9952\n0.9972\n0.9934\n0.9958\n0.9944\n0.9908\n0.9958\n0.9925\n0.9966\n0.9972\n0.9912\n0.995\n0.9928\n0.9968\n0.9955\n0.9981\n0.991\n0.991\n0.991\n0.992\n0.9931\n0.997\n0.9948\n0.9923\n0.9976\n0.9938\n0.9984\n0.9972\n0.9922\n0.9935\n0.9944\n0.9942\n0.9944\n0.9997\n0.9977\n0.9912\n0.9982\n0.9982\n0.9983\n0.998\n0.9894\n0.9927\n0.9917\n0.9904\n0.993\n0.9941\n0.9943\n0.99855\n0.99345\n0.998\n0.9916\n0.9916\n0.99475\n0.99325\n0.9933\n0.9969\n1.0002\n0.9933\n0.9937\n0.99685\n0.99455\n0.9917\n0.99035\n0.9914\n0.99225\n0.99155\n0.9954\n0.99455\n0.9924\n0.99695\n0.99655\n0.9934\n0.998\n0.9971\n0.9948\n0.998\n0.9971\n0.99215\n0.9948\n0.9915\n0.99115\n0.9932\n0.9977\n0.99535\n0.99165\n0.9953\n0.9928\n0.9958\n0.9928\n0.9928\n0.9964\n0.9987\n0.9953\n0.9932\n0.9907\n0.99755\n0.99935\n0.9932\n0.9932\n0.9958\n0.99585\n1.00055\n0.9985\n0.99505\n0.992\n0.9988\n0.99175\n0.9962\n0.9962\n0.9942\n0.9927\n0.9927\n0.99985\n0.997\n0.9918\n0.99215\n0.99865\n0.9992\n1.0006\n0.99135\n0.99715\n0.9992\n1.0006\n0.99865\n0.99815\n0.99815\n0.99815\n0.9949\n0.99815\n0.99815\n0.99225\n0.99445\n0.99225\n0.99335\n0.99625\n0.9971\n0.9983\n0.99445\n0.99085\n0.9977\n0.9953\n0.99775\n0.99795\n0.99505\n0.9977\n0.9975\n0.99745\n0.9976\n0.99775\n0.9953\n0.9932\n0.99405\n1\n0.99785\n0.9939\n0.9939\n0.99675\n0.9939\n0.99675\n0.98965\n0.9971\n0.99445\n0.9945\n0.9939\n0.9958\n0.9956\n0.99055\n0.9959\n0.9925\n0.9963\n0.9935\n0.99105\n0.99045\n0.9963\n0.99155\n0.99085\n0.99085\n0.99085\n0.9924\n0.9924\n0.99975\n0.99975\n0.99315\n0.9917\n0.9917\n0.99845\n0.9921\n0.99975\n0.9909\n0.99315\n0.99855\n0.9934\n0.9978\n0.9934\n0.9949\n0.99855\n0.9986\n0.99725\n0.9946\n0.99255\n0.9996\n0.9939\n0.99\n0.9937\n0.9886\n0.9934\n1\n0.9994\n0.9926\n0.9956\n0.9978\n0.9915\n0.9939\n0.9932\n0.993\n0.9898\n0.9921\n0.9932\n0.9919\n0.993\n0.9953\n0.9928\n0.9928\n0.9976\n0.9906\n0.9918\n0.99185\n0.9918\n0.99185\n0.994\n0.9908\n0.9928\n0.9896\n0.9908\n0.9918\n0.9952\n0.9923\n0.9915\n0.9952\n0.9947\n0.9983\n0.9975\n0.995\n0.9944\n0.994\n0.9944\n0.9908\n0.99795\n0.9985\n0.99425\n0.99425\n0.9943\n0.9924\n0.9946\n0.9924\n0.995\n0.9919\n0.99\n0.9923\n0.9956\n0.9978\n0.9978\n0.9967\n0.9934\n0.9936\n0.9932\n0.9934\n0.998\n0.9978\n0.9929\n0.9974\n0.99685\n0.99495\n0.99745\n0.99505\n0.992\n0.9978\n0.9956\n0.9982\n0.99485\n0.9971\n0.99265\n0.9904\n0.9965\n0.9946\n0.99965\n0.9935\n0.996\n0.9942\n0.9936\n0.9965\n0.9928\n0.9928\n0.9965\n0.9936\n0.9938\n0.9926\n0.9926\n0.9983\n0.9983\n0.992\n0.9983\n0.9923\n0.9972\n0.9928\n0.9928\n0.9994\n0.991\n0.9906\n0.9894\n0.9898\n0.9994\n0.991\n0.9925\n0.9956\n0.9946\n0.9966\n0.9951\n0.9927\n0.9927\n0.9951\n0.9894\n0.9907\n0.9925\n0.9928\n0.9941\n0.9941\n0.9925\n0.9935\n0.9932\n0.9944\n0.9972\n0.994\n0.9956\n0.9927\n0.9924\n0.9966\n0.9997\n0.9936\n0.9936\n0.9952\n0.9952\n0.9928\n0.9911\n0.993\n0.9911\n0.9932\n0.993\n0.993\n0.9932\n0.9932\n0.9943\n0.9968\n0.9994\n0.9926\n0.9968\n0.9932\n0.9916\n0.9946\n0.9925\n0.9925\n0.9935\n0.9962\n0.9928\n0.993\n0.993\n0.9956\n0.9941\n0.9972\n0.9948\n0.9955\n0.9972\n0.9972\n0.9983\n0.9942\n0.9936\n0.9956\n0.9953\n0.9918\n0.995\n0.992\n0.9952\n1.001\n0.9924\n0.9932\n0.9937\n0.9918\n0.9934\n0.991\n0.9962\n0.9932\n0.9908\n0.9962\n0.9918\n0.9941\n0.9931\n0.9981\n0.9931\n0.9944\n0.992\n0.9966\n0.9956\n0.9956\n0.9949\n1.0002\n0.9942\n0.9923\n0.9917\n0.9931\n0.992\n1.0002\n0.9953\n0.9951\n0.9974\n0.9904\n0.9974\n0.9944\n1.0004\n0.9952\n0.9956\n0.995\n0.995\n0.9995\n0.9942\n0.9977\n0.992\n0.992\n0.9995\n0.9934\n1.0006\n0.9982\n0.9928\n0.9945\n0.9963\n0.9906\n0.9956\n0.9942\n0.9962\n0.9894\n0.995\n0.9908\n0.9914\n0.9938\n0.9977\n0.9922\n0.992\n0.9903\n0.9893\n0.9952\n0.9903\n0.9912\n0.9983\n0.9937\n0.9932\n0.9928\n0.9922\n0.9976\n0.9922\n0.9974\n0.998\n0.9931\n0.9911\n0.9944\n0.9937\n0.9974\n0.989\n0.992\n0.9928\n0.9918\n0.9936\n0.9944\n0.9988\n0.994\n0.9953\n0.9986\n0.9914\n0.9934\n0.996\n0.9937\n0.9921\n0.998\n0.996\n0.9933\n0.9933\n0.9959\n0.9936\n0.9953\n0.9938\n0.9952\n0.9959\n0.9959\n0.9937\n0.992\n0.9967\n0.9944\n0.9998\n0.9998\n0.9942\n0.9998\n0.9945\n0.9998\n0.9946\n0.9942\n0.9928\n0.9946\n0.9927\n0.9938\n0.9918\n0.9945\n0.9966\n0.9954\n0.9913\n0.9931\n0.9986\n0.9965\n0.9984\n0.9952\n0.9956\n0.9949\n0.9954\n0.996\n0.9931\n0.992\n0.9912\n0.9978\n0.9938\n0.9914\n0.9932\n0.9944\n0.9913\n0.9948\n0.998\n0.9998\n0.9964\n0.9992\n0.9948\n0.9998\n0.998\n0.9939\n0.992\n0.9922\n0.9955\n0.9917\n0.9917\n0.9954\n0.9986\n0.9955\n0.9917\n0.9907\n0.9922\n0.9958\n0.993\n0.9917\n0.9926\n0.9959\n0.9906\n0.9993\n0.993\n0.9906\n0.992\n0.992\n0.994\n0.9959\n0.9908\n0.9902\n0.9908\n0.9943\n0.9921\n0.9911\n0.9986\n0.992\n0.992\n0.9943\n0.9937\n0.993\n0.9902\n0.9928\n0.9896\n0.998\n0.9954\n0.9938\n0.9918\n0.9896\n0.9944\n0.9999\n0.9953\n0.992\n0.9925\n0.9981\n0.9952\n0.9927\n0.9927\n0.9911\n0.9936\n0.9959\n0.9946\n0.9948\n0.9955\n0.9951\n0.9952\n0.9946\n0.9946\n0.9944\n0.9938\n0.9963\n0.991\n1.0003\n0.9966\n0.9993\n1.0003\n0.9938\n0.9965\n0.9938\n0.9993\n0.9938\n1.0003\n0.9966\n0.9942\n0.9928\n0.991\n0.9911\n0.9977\n0.9927\n0.9911\n0.991\n0.9912\n0.9907\n0.9902\n0.992\n0.994\n0.9966\n0.993\n0.993\n0.993\n0.9966\n0.9942\n0.9925\n0.9925\n0.9928\n0.995\n0.9939\n0.9958\n0.9952\n1\n0.9948\n0.99\n0.9958\n0.9948\n0.9949\n0.997\n0.9927\n0.9938\n0.9949\n0.9953\n0.997\n0.9932\n0.9927\n0.9932\n0.9955\n0.9914\n0.991\n0.992\n0.9924\n0.9927\n0.9911\n0.9958\n0.9928\n0.9902\n0.994\n0.994\n0.9972\n1.0004\n0.991\n0.9918\n0.995\n0.9941\n0.9956\n0.9956\n0.9959\n0.9922\n0.9931\n0.9959\n0.9984\n0.9908\n0.991\n0.9928\n0.9936\n0.9941\n0.9924\n0.9917\n0.9906\n0.995\n0.9956\n0.9955\n0.9907\n1\n0.9953\n0.9911\n0.9922\n0.9951\n0.9948\n0.9906\n0.994\n0.9907\n0.9927\n0.9914\n0.9958\n1\n0.9984\n0.9941\n0.9944\n0.998\n0.998\n0.9902\n0.9911\n0.9929\n0.993\n0.9918\n0.992\n0.9932\n0.992\n0.994\n0.9923\n0.993\n0.9956\n0.9907\n0.99\n0.9918\n0.9926\n0.995\n0.99\n0.99\n0.9946\n0.9907\n0.9898\n0.9918\n0.9986\n0.9986\n0.9928\n0.9986\n0.9979\n0.994\n0.9937\n0.9938\n0.9942\n0.9944\n0.993\n0.9986\n0.9932\n0.9934\n0.9928\n0.9925\n0.9944\n0.9909\n0.9932\n0.9934\n1.0001\n0.992\n0.9916\n0.998\n0.9919\n0.9925\n0.9977\n0.9944\n0.991\n0.99\n0.9917\n0.9923\n0.9928\n0.9923\n0.9928\n0.9902\n0.9893\n0.9917\n0.9982\n1.0005\n0.9923\n0.9951\n0.9956\n0.998\n0.9928\n0.9938\n0.9914\n0.9955\n0.9924\n0.9911\n0.9917\n0.9917\n0.9932\n0.9955\n0.9929\n0.9955\n0.9958\n1.0012\n0.9968\n0.9911\n0.9924\n0.991\n0.9946\n0.9928\n0.9946\n0.9917\n0.9918\n0.9926\n0.9931\n0.9932\n0.9903\n0.9928\n0.9929\n0.9958\n0.9955\n0.9911\n0.9938\n0.9942\n0.9945\n0.9962\n0.992\n0.9927\n0.9948\n0.9945\n0.9942\n0.9952\n0.9942\n0.9958\n0.9918\n0.9932\n1.0004\n0.9972\n0.9998\n0.9918\n0.9918\n0.9964\n0.9936\n0.9931\n0.9938\n0.9934\n0.99\n0.9914\n0.9904\n0.994\n0.9938\n0.9933\n0.9909\n0.9942\n0.9945\n0.9954\n0.996\n0.9991\n0.993\n0.9942\n0.9934\n0.9939\n0.9937\n0.994\n0.9926\n0.9951\n0.9952\n0.9935\n0.9938\n0.9939\n0.9933\n0.9927\n0.998\n0.9997\n0.9981\n0.992\n0.9954\n0.992\n0.9997\n0.9981\n0.9943\n0.9941\n0.9936\n0.9996\n0.9932\n0.9926\n0.9936\n0.992\n0.9936\n0.9996\n0.993\n0.9924\n0.9928\n0.9926\n0.9952\n0.9945\n0.9945\n0.9903\n0.9932\n0.9953\n0.9936\n0.9912\n0.9962\n0.9965\n0.9932\n0.9967\n0.9953\n0.9963\n0.992\n0.991\n0.9958\n0.99\n0.991\n0.9958\n0.9938\n0.9996\n0.9946\n0.9974\n0.9945\n0.9946\n0.9974\n0.9957\n0.9931\n0.9947\n0.9953\n0.9931\n0.9946\n0.9978\n0.9989\n1.0004\n0.9938\n0.9934\n0.9978\n0.9956\n0.9982\n0.9948\n0.9956\n0.9982\n0.9926\n0.991\n0.9945\n0.9916\n0.9953\n0.9938\n0.9956\n0.9906\n0.9956\n0.9932\n0.9914\n0.9938\n0.996\n0.9906\n0.98815\n0.9942\n0.9903\n0.9906\n0.9935\n1.0024\n0.9968\n0.9906\n0.9941\n0.9919\n0.9928\n0.9958\n0.9932\n0.9957\n0.9937\n0.9982\n0.9928\n0.9919\n0.9956\n0.9957\n0.9954\n0.993\n0.9954\n0.9987\n0.9956\n0.9928\n0.9951\n0.993\n0.9928\n0.9926\n0.9938\n1.0001\n0.9933\n0.9952\n0.9934\n0.9988\n0.993\n0.9952\n0.9948\n0.9998\n0.9971\n0.9998\n0.9962\n0.9948\n0.99\n0.9942\n0.9965\n0.9912\n0.9978\n0.9928\n1.0103\n0.9956\n0.9936\n0.9929\n0.9966\n0.9964\n0.996\n0.9959\n0.9954\n0.9914\n1.0103\n1.0004\n0.9911\n0.9938\n0.9927\n0.9922\n0.9924\n0.9963\n0.9936\n0.9951\n0.9951\n0.9955\n0.9961\n0.9936\n0.992\n0.9944\n0.9944\n1.0008\n0.9962\n0.9986\n0.9986\n1\n0.9986\n0.9982\n1\n0.9949\n0.9915\n0.9951\n0.9986\n0.9927\n0.9955\n0.9952\n0.9928\n0.9982\n0.9914\n0.9927\n0.9918\n0.9944\n0.9969\n0.9955\n0.9954\n0.9955\n0.9921\n0.9934\n0.9998\n0.9946\n0.9984\n0.9924\n0.9939\n0.995\n0.9957\n0.9953\n0.9912\n0.9939\n0.9921\n0.9954\n0.9933\n0.9941\n0.995\n0.9977\n0.9912\n0.9945\n0.9952\n0.9924\n0.9986\n0.9953\n0.9939\n0.9929\n0.9988\n0.9906\n0.9914\n0.9978\n0.9928\n0.9948\n0.9978\n0.9946\n0.9908\n0.9954\n0.9906\n0.99705\n0.9982\n0.9932\n0.9977\n0.994\n0.9982\n0.9929\n0.9924\n0.9966\n0.9921\n0.9967\n0.9934\n0.9914\n0.99705\n0.9961\n0.9967\n0.9926\n0.99605\n0.99435\n0.9948\n0.9916\n0.997\n0.9961\n0.9967\n0.9961\n0.9955\n0.9922\n0.9918\n0.9955\n0.9941\n0.9955\n0.9955\n0.9924\n0.9973\n0.999\n0.9941\n0.9922\n0.9922\n0.9953\n0.9945\n0.9945\n0.9957\n0.9932\n0.9945\n0.9913\n0.9909\n0.9939\n0.991\n0.9954\n0.9943\n0.993\n1.0002\n0.9946\n0.9953\n0.9918\n0.9936\n0.9984\n0.9956\n0.9966\n0.9942\n0.9984\n0.9956\n0.9966\n0.9974\n0.9944\n1.0008\n0.9974\n1.0008\n0.9928\n0.9944\n0.9908\n0.9917\n0.9911\n0.9912\n0.9953\n0.9932\n0.9896\n0.9889\n0.9912\n0.9926\n0.9911\n0.9964\n0.9974\n0.9944\n0.9974\n0.9964\n0.9963\n0.9948\n0.9948\n0.9953\n0.9948\n0.9953\n0.9949\n0.9988\n0.9954\n0.992\n0.9984\n0.9954\n0.9926\n0.992\n0.9976\n0.9972\n0.991\n0.998\n0.9966\n0.998\n1.0007\n0.992\n0.9925\n0.991\n0.9934\n0.9955\n0.9944\n0.9981\n0.9968\n0.9946\n0.9946\n0.9981\n0.9946\n0.997\n0.9924\n0.9958\n0.994\n0.9958\n0.9984\n0.9948\n0.9932\n0.9952\n0.9924\n0.9945\n0.9976\n0.9976\n0.9938\n0.9997\n0.994\n0.9921\n0.9986\n0.9987\n0.9991\n0.9987\n0.9991\n0.9991\n0.9948\n0.9987\n0.993\n0.9988\n1\n0.9932\n0.9991\n0.9989\n1\n1\n0.9952\n0.9969\n0.9966\n0.9966\n0.9976\n0.99\n0.9988\n0.9942\n0.9984\n0.9932\n0.9969\n0.9966\n0.9933\n0.9916\n0.9914\n0.9966\n0.9958\n0.9926\n0.9939\n0.9953\n0.9906\n0.9914\n0.9958\n0.9926\n0.9991\n0.9994\n0.9976\n0.9966\n0.9953\n0.9923\n0.993\n0.9931\n0.9932\n0.9926\n0.9938\n0.9966\n0.9974\n0.9924\n0.9948\n0.9964\n0.9924\n0.9966\n0.9974\n0.9938\n0.9928\n0.9959\n1.0001\n0.9959\n1.0001\n0.9968\n0.9932\n0.9954\n0.9992\n0.9932\n0.9939\n0.9952\n0.9996\n0.9966\n0.9925\n0.996\n0.9996\n0.9973\n0.9937\n0.9966\n1.0017\n0.993\n0.993\n0.9959\n0.9958\n1.0017\n0.9958\n0.9979\n0.9941\n0.997\n0.9934\n0.9927\n0.9944\n0.9927\n0.9963\n1.0011\n1.0011\n0.9959\n0.9973\n0.9966\n0.9932\n0.9984\n0.999\n0.999\n0.999\n0.999\n0.999\n1.0006\n0.9937\n0.9954\n0.997\n0.9912\n0.9939\n0.999\n0.9957\n0.9926\n0.9994\n1.0004\n0.9994\n1.0004\n1.0004\n1.0002\n0.9922\n0.9922\n0.9934\n0.9926\n0.9941\n0.9994\n1.0004\n0.9924\n0.9948\n0.9935\n0.9918\n0.9948\n0.9924\n0.9979\n0.993\n0.994\n0.991\n0.993\n0.9922\n0.9979\n0.9937\n0.9928\n0.9965\n0.9928\n0.9991\n0.9948\n0.9925\n0.9958\n0.9962\n0.9965\n0.9951\n0.9944\n0.9916\n0.9987\n0.9928\n0.9926\n0.9934\n0.9944\n0.9949\n0.9926\n0.997\n0.9949\n0.9948\n0.992\n0.9964\n0.9926\n0.9982\n0.9955\n0.9955\n0.9958\n0.9997\n1.0001\n1.0001\n0.9918\n0.9918\n0.9931\n1.0001\n0.9926\n0.9966\n0.9932\n0.9969\n0.9925\n0.9914\n0.996\n0.9952\n0.9934\n0.9939\n0.9939\n0.9906\n0.9901\n0.9948\n0.995\n0.9953\n0.9953\n0.9952\n0.996\n0.9948\n0.9951\n0.9931\n0.9962\n0.9948\n0.9959\n0.9962\n0.9958\n0.9948\n0.9948\n0.994\n0.9942\n0.9942\n0.9948\n0.9964\n0.9958\n0.9932\n0.9986\n0.9986\n0.9988\n0.9953\n0.9983\n1\n0.9951\n0.9983\n0.9906\n0.9981\n0.9936\n0.9951\n0.9953\n1.0005\n0.9972\n1\n0.9969\n1.0001\n1.0001\n1.0001\n0.9934\n0.9969\n1.0001\n0.9902\n0.993\n0.9914\n0.9941\n0.9967\n0.9918\n0.998\n0.9967\n0.9918\n0.9957\n0.9986\n0.9958\n0.9948\n0.9918\n0.9923\n0.9998\n0.9998\n0.9914\n0.9939\n0.9966\n0.995\n0.9966\n0.994\n0.9972\n0.9998\n0.9998\n0.9982\n0.9924\n0.9972\n0.997\n0.9954\n0.9962\n0.9972\n0.9921\n0.9905\n0.9998\n0.993\n0.9941\n0.9994\n0.9962\n0.992\n0.9922\n0.994\n0.9897\n0.9954\n0.99\n0.9948\n0.9922\n0.998\n0.9944\n0.9944\n0.9986\n0.9986\n0.9986\n0.9986\n0.9986\n0.996\n0.9999\n0.9986\n0.9986\n0.996\n0.9951\n0.9999\n0.993\n0.9982\n0.992\n0.9963\n0.995\n0.9956\n0.997\n0.9936\n0.9935\n0.9963\n0.9967\n0.9912\n0.9981\n0.9966\n0.9967\n0.9963\n0.9935\n0.9902\n0.99\n0.996\n0.9966\n0.9962\n0.994\n0.996\n0.994\n0.9944\n0.9974\n0.996\n0.9922\n0.9917\n0.9918\n0.9936\n0.9938\n0.9918\n0.9939\n0.9917\n0.9981\n0.9941\n0.9928\n0.9952\n0.9898\n0.9914\n0.9981\n0.9957\n0.998\n0.9957\n0.9986\n0.9983\n0.9982\n0.997\n0.9947\n0.997\n0.9947\n0.99416\n0.99516\n0.99496\n0.9974\n0.99579\n0.9983\n0.99471\n0.9974\n0.99644\n0.99579\n0.99699\n0.99758\n0.9977\n0.99397\n0.9983\n0.99471\n0.99243\n0.9962\n1.00182\n0.99384\n0.99582\n0.9962\n0.9924\n0.99466\n0.99212\n0.99449\n0.99748\n0.99449\n0.99748\n0.99475\n0.99189\n0.99827\n0.99752\n0.99827\n0.99479\n0.99752\n0.99642\n1.00047\n0.99382\n0.99784\n0.99486\n0.99537\n0.99382\n0.99838\n0.99566\n0.99268\n0.99566\n0.99468\n0.9933\n0.99307\n0.99907\n0.99907\n0.99907\n0.99907\n0.99471\n0.99471\n0.99907\n0.99148\n0.99383\n0.99365\n0.99272\n0.99148\n0.99235\n0.99508\n0.9946\n0.99674\n0.99018\n0.99235\n0.99084\n0.99856\n0.99591\n0.9975\n0.9944\n0.99173\n0.99378\n0.99805\n0.99534\n0.99232\n0.99805\n0.99078\n0.99534\n0.99061\n0.99182\n0.9966\n0.9912\n0.99779\n0.99814\n0.99096\n0.99379\n0.99426\n0.99228\n0.99335\n0.99595\n0.99297\n0.99687\n0.99297\n0.99687\n0.99445\n0.9986\n0.99154\n0.9981\n0.98993\n1.00241\n0.99716\n0.99437\n0.9972\n0.99756\n0.99509\n0.99572\n0.99756\n0.99175\n0.99254\n0.99509\n0.99676\n0.9979\n0.99194\n0.99077\n0.99782\n0.99942\n0.99708\n0.99353\n0.99256\n0.99199\n0.9918\n0.99354\n0.99244\n0.99831\n0.99396\n0.99724\n0.99524\n0.9927\n0.99802\n0.99512\n0.99438\n0.99679\n0.99652\n0.99698\n0.99474\n0.99511\n0.99582\n0.99125\n0.99256\n0.9911\n0.99168\n0.9911\n0.99556\n1.00098\n0.99516\n0.99516\n0.99518\n0.99347\n0.9929\n0.99347\n0.99841\n0.99362\n0.99361\n0.9914\n0.99114\n0.9925\n0.99453\n0.9938\n0.9938\n0.99806\n0.9961\n1.00016\n0.9916\n0.99116\n0.99319\n0.99517\n0.99514\n0.99566\n0.99166\n0.99587\n0.99558\n0.99117\n0.99399\n0.99741\n0.99405\n0.99622\n1.00051\n0.99803\n0.99405\n0.99773\n0.99397\n0.99622\n0.99713\n0.99274\n1.00118\n0.99176\n0.9969\n0.99771\n0.99411\n0.99771\n0.99411\n0.99194\n0.99558\n0.99194\n0.99558\n0.99577\n0.99564\n0.99578\n0.99888\n1.00014\n0.99441\n0.99594\n0.99437\n0.99594\n0.9979\n0.99434\n0.99203\n0.998\n0.99316\n0.998\n0.99314\n0.99316\n0.99612\n0.99295\n0.99394\n0.99642\n0.99642\n0.99248\n0.99268\n0.99954\n0.99692\n0.99592\n0.99592\n0.99692\n0.99822\n0.99822\n0.99402\n0.99404\n0.99787\n0.99347\n0.99838\n0.99839\n0.99375\n0.99155\n0.9936\n0.99434\n0.9922\n0.99571\n0.99658\n0.99076\n0.99496\n0.9937\n0.99076\n0.99542\n0.99825\n0.99289\n0.99432\n0.99523\n0.99542\n0.9959\n0.99543\n0.99662\n0.99088\n0.99088\n0.99922\n0.9966\n0.99466\n0.99922\n0.99836\n0.99836\n0.99238\n0.99645\n1\n1\n0.99376\n1\n0.99513\n0.99556\n0.99556\n0.99543\n0.99886\n0.99526\n0.99166\n0.99691\n0.99732\n0.99573\n0.99656\n0.99112\n0.99214\n0.99165\n0.99004\n0.99463\n0.99683\n0.99004\n0.99596\n0.99898\n0.99114\n0.99508\n0.99306\n0.99898\n0.99508\n0.99114\n0.99342\n0.99345\n0.99772\n0.99239\n0.99502\n0.99502\n0.99479\n0.99207\n0.99497\n0.99828\n0.99542\n0.99542\n0.99228\n0.99706\n0.99497\n0.99669\n0.99828\n0.99269\n0.99196\n0.99662\n0.99475\n0.99544\n0.99944\n0.99475\n0.99544\n0.9966\n0.99066\n0.9907\n0.99066\n0.998\n0.9907\n0.99066\n0.99307\n0.99106\n0.99696\n0.99106\n0.99307\n0.99167\n0.99902\n0.98992\n0.99182\n0.99556\n0.99582\n0.99182\n0.98972\n0.99352\n0.9946\n0.99273\n0.99628\n0.99582\n0.99553\n0.98914\n0.99354\n0.99976\n0.99808\n0.99808\n0.99808\n0.99808\n0.99808\n0.99808\n0.9919\n0.99808\n0.99499\n0.99655\n0.99615\n0.99296\n0.99482\n0.99079\n0.99366\n0.99434\n0.98958\n0.99434\n0.99938\n0.99059\n0.99835\n0.98958\n0.99159\n0.99159\n0.98931\n0.9938\n0.99558\n0.99563\n0.98931\n0.99691\n0.9959\n0.99159\n0.99628\n0.99076\n0.99678\n0.99678\n0.99678\n0.99089\n0.99537\n1.0002\n0.99628\n0.99089\n0.99678\n0.99076\n0.99332\n0.99316\n0.99272\n0.99636\n0.99202\n0.99148\n0.99064\n0.99884\n0.99773\n1.00013\n0.98974\n0.99773\n1.00013\n0.99112\n0.99136\n0.99132\n0.99642\n0.99488\n0.99527\n0.99578\n0.99352\n0.99199\n0.99198\n0.99756\n0.99578\n0.99561\n0.99347\n0.98936\n0.99786\n0.99705\n0.9942\n0.9948\n0.99116\n0.99688\n0.98974\n0.99542\n0.99154\n0.99118\n0.99044\n0.9914\n0.9979\n0.98892\n0.99114\n0.99188\n0.99583\n0.98892\n0.98892\n0.99704\n0.9911\n0.99334\n0.99334\n0.99094\n0.99014\n0.99304\n0.99652\n0.98944\n0.99772\n0.99367\n0.99304\n0.99183\n0.99126\n0.98944\n0.99577\n0.99772\n0.99652\n0.99428\n0.99388\n0.99208\n0.99256\n0.99388\n0.9925\n0.99904\n0.99216\n0.99208\n0.99428\n0.99165\n0.99924\n0.99924\n0.99924\n0.9956\n0.99562\n0.9972\n0.99924\n0.9958\n0.99976\n0.99976\n0.99296\n0.9957\n0.9958\n0.99579\n0.99541\n0.99976\n0.99518\n0.99168\n0.99276\n0.99085\n0.99873\n0.99172\n0.99312\n0.99276\n0.9972\n0.99278\n0.99092\n0.9962\n0.99053\n0.99858\n0.9984\n0.99335\n0.99053\n0.9949\n0.9962\n0.99092\n0.99532\n0.99727\n0.99026\n0.99668\n0.99727\n0.9952\n0.99144\n0.99144\n0.99015\n0.9914\n0.99693\n0.99035\n0.99693\n0.99035\n0.99006\n0.99126\n0.98994\n0.98985\n0.9971\n0.99882\n0.99477\n0.99478\n0.99576\n0.99578\n0.99354\n0.99244\n0.99084\n0.99612\n0.99356\n0.98952\n0.99612\n0.99084\n0.99244\n0.99955\n0.99374\n0.9892\n0.99144\n0.99352\n0.99352\n0.9935\n0.99237\n0.99144\n0.99022\n0.99032\n1.03898\n0.99587\n0.99587\n0.99587\n0.99976\n0.99354\n0.99976\n0.99552\n0.99552\n0.99587\n0.99604\n0.99584\n0.98894\n0.9963\n0.993\n0.98894\n0.9963\n0.99068\n0.98964\n0.99604\n0.99584\n0.9923\n0.99437\n0.993\n0.99238\n0.99801\n0.99802\n0.99566\n0.99067\n0.99066\n0.9929\n0.9934\n0.99067\n0.98912\n0.99066\n0.99228\n0.98912\n0.9958\n0.99052\n0.99312\n0.9968\n0.99502\n0.99084\n0.99573\n0.99256\n0.9959\n0.99084\n0.99084\n0.99644\n0.99526\n0.9954\n0.99095\n0.99188\n0.9909\n0.99256\n0.9959\n0.99581\n0.99132\n0.98936\n0.99136\n0.99142\n0.99232\n0.99232\n0.993\n0.99311\n0.99132\n0.98993\n0.99208\n0.99776\n0.99839\n0.99574\n0.99093\n0.99156\n0.99278\n0.9924\n0.98984\n0.99035\n0.9924\n0.99165\n0.9923\n0.99278\n0.99008\n0.98964\n0.99156\n0.9909\n0.98984\n0.9889\n0.99178\n0.99076\n0.9889\n0.99046\n0.98999\n0.98946\n0.98976\n0.99046\n0.99672\n0.99482\n0.98945\n0.98883\n0.99362\n0.99075\n0.99436\n0.98988\n0.99158\n0.99265\n0.99195\n0.99168\n0.9918\n0.99313\n0.9895\n0.9932\n0.99848\n0.9909\n0.99014\n0.9952\n0.99652\n0.99848\n0.99104\n0.99772\n0.9922\n0.99076\n0.99622\n0.9902\n0.99114\n0.9938\n0.99594\n0.9902\n0.99035\n0.99032\n0.99558\n0.99622\n0.99076\n0.99413\n0.99043\n0.99043\n0.98982\n0.98934\n0.9902\n0.99449\n0.99629\n0.9948\n0.98984\n0.99326\n0.99834\n0.99555\n0.98975\n0.99216\n0.99216\n0.99834\n0.9901\n0.98975\n0.99573\n0.99326\n0.99215\n0.98993\n0.99218\n0.99555\n0.99564\n0.99564\n0.99397\n0.99576\n0.99601\n0.99564\n0.99397\n0.98713\n0.99308\n0.99308\n0.99582\n0.99494\n0.9929\n0.99471\n0.9929\n0.9929\n0.99037\n0.99304\n0.99026\n0.98986\n0.99471\n0.98951\n0.99634\n0.99368\n0.99792\n0.99026\n0.99362\n0.98919\n0.99835\n0.99835\n0.99038\n0.99104\n0.99038\n0.99286\n0.99296\n0.99835\n0.9954\n0.9914\n0.99286\n0.99604\n0.99604\n0.99119\n0.99007\n0.99507\n0.99596\n0.99011\n0.99184\n0.99469\n0.99469\n0.99406\n0.99305\n0.99096\n0.98956\n0.9921\n0.99496\n0.99406\n0.99406\n0.9888\n0.98942\n0.99082\n0.98802\n17.3\n1.4\n1.3\n1.6\n5.25\n2.4\n14.6\n11.8\n1.5\n1.8\n7.7\n2\n1.8\n1.4\n16.7\n8.1\n8\n4.7\n8.1\n2.1\n16.7\n6.4\n1.5\n7.6\n1.5\n12.4\n1.3\n1.7\n8.1\n7.1\n7.6\n2.3\n6.5\n1.4\n12.7\n1.6\n1.1\n1.2\n6.5\n4.6\n0.6\n10.6\n4.6\n4.8\n2.7\n12.6\n0.6\n9.2\n6.6\n7\n8.45\n11.1\n18.15\n18.15\n4.1\n4.1\n4.6\n18.15\n4.9\n8.3\n1.4\n11.5\n1.8\n1.6\n2.4\n4.9\n1.8\n4.3\n4.4\n1.4\n1.6\n1.3\n5.2\n5.6\n5.3\n4.9\n2.4\n1.6\n2.1\n1.4\n7.1\n1.6\n10.7\n11.1\n10.7\n1.6\n1.6\n1.5\n1.5\n1.6\n1.6\n8\n7.7\n2.7\n15.1\n15.1\n8.9\n6\n12.3\n13.1\n6.7\n12.3\n2.3\n11.1\n1.5\n6.7\n6\n15.2\n10.2\n13.1\n10.7\n17.1\n17.1\n17.1\n1.9\n10.7\n17.1\n1.2\n1.2\n3.1\n1.5\n10.7\n4.9\n12.6\n10.7\n4.9\n12.15\n12\n1.7\n2.6\n1.4\n1.9\n16.9\n16.9\n2.1\n7\n7.1\n5.9\n7.1\n8.7\n13.2\n15.3\n15.3\n13.2\n2.7\n10.65\n10\n6.8\n15.6\n13.2\n5.1\n3\n15.3\n2.1\n1.9\n8.6\n8.75\n3.6\n4.7\n1.3\n1.8\n9.7\n4\n2.4\n4.7\n18.8\n1.8\n1.8\n12.8\n12.8\n12.8\n12.8\n12.8\n7.8\n16.75\n12.8\n12.8\n7.8\n5.4\n16.75\n1.3\n10.1\n3.8\n10.9\n6.6\n9.8\n11.7\n1.2\n1.4\n9.6\n12.2\n2.6\n10.7\n4.9\n12.2\n9.6\n1.4\n1.1\n1\n8.2\n11.3\n7.3\n2.3\n8.2\n2.1\n2\n10\n15.75\n3.9\n2\n1.5\n1.6\n1.4\n1.5\n1.4\n2\n13.8\n1.3\n3.8\n6.9\n2.2\n1.6\n13.8\n10.8\n12.8\n10.8\n15.3\n12.1\n12\n11.6\n9.2\n11.6\n9.2\n2.8\n1.6\n6.1\n8.5\n7.8\n14.9\n6.2\n8.5\n8.2\n7.8\n10.6\n11.2\n11.6\n7.1\n14.9\n6.2\n1.7\n7.7\n17.3\n1.4\n7.7\n7.7\n3.4\n1.6\n1.4\n1.4\n10.4\n1.4\n10.4\n4.1\n2.8\n15.7\n10.9\n15.7\n6.5\n10.9\n5.9\n17.3\n1.4\n13.5\n8.5\n6.2\n1.4\n14.95\n7.7\n1.3\n7.7\n1.3\n1.3\n1.3\n15.6\n15.6\n15.6\n15.6\n4.9\n5\n15.6\n6.5\n1.4\n2.7\n1.2\n6.5\n6.4\n6.9\n7.2\n10.6\n3.5\n6.4\n2.3\n12.05\n7\n11.8\n1.4\n5\n2.2\n14.6\n1.6\n1.3\n14.6\n2.8\n1.6\n3.3\n6.3\n8.1\n1.6\n10.6\n11.8\n1.7\n8.1\n1.4\n1.3\n1.8\n7.2\n1.1\n11.95\n1.1\n11.95\n2.2\n12.7\n1.4\n10.6\n1.9\n17.8\n10.2\n4.8\n9.8\n8.4\n7.2\n4.8\n8.4\n4.5\n1.4\n7.2\n11\n11.1\n2.6\n2\n10.1\n13.3\n11.4\n1.3\n1.4\n1.4\n7\n2\n1.2\n12.9\n5\n10.1\n3.75\n1.7\n12.6\n1.3\n1.6\n7.6\n8.1\n14.9\n6\n6\n7.2\n3\n1.2\n2\n4.9\n2\n8.9\n16.45\n2\n1.9\n5.1\n4.4\n5.8\n4.4\n12.9\n1.3\n1.3\n1.2\n2.7\n1.7\n8.2\n1.5\n1.5\n12.9\n3.9\n17.75\n4.9\n1.6\n1.4\n2\n2\n8.2\n2.1\n1.8\n8.5\n4.45\n5.8\n13\n2.7\n7.3\n19.1\n8.8\n2.7\n7.4\n2.3\n6.85\n11.4\n0.9\n19.35\n7.9\n11.75\n7.7\n3\n7.7\n3\n1.5\n7.5\n1.5\n7.5\n8.3\n7.05\n8.4\n13.9\n17.5\n5.6\n9.4\n4.8\n9.4\n9.7\n6.3\n1.6\n14.6\n2.5\n14.6\n2.6\n2.5\n8.2\n1.5\n2.3\n10\n10\n1.6\n1.6\n16\n10.4\n7.4\n7.4\n10.4\n16.05\n16.05\n2.6\n2.5\n10.8\n1.2\n12.1\n11.95\n1.7\n0.8\n1.4\n1.3\n6.3\n10.3\n15.55\n1.5\n1.5\n1.4\n1.5\n7.9\n13\n1\n4.85\n7.1\n7.9\n7.5\n7.6\n10.3\n1.7\n1.7\n19.95\n7.7\n5.3\n19.95\n12.7\n12.7\n1.5\n11.3\n18.1\n18.1\n7\n18.1\n6.4\n1.4\n1.4\n3.1\n14.1\n7.7\n5.2\n11.6\n10.4\n7.5\n11.2\n0.8\n1.4\n4.7\n3.1\n4\n11.3\n3.1\n8.1\n14.8\n1.4\n8.1\n3.5\n14.8\n8.1\n1.4\n1.5\n1.5\n12.8\n1.6\n7.1\n7.1\n11.2\n1.7\n6.7\n17.3\n8.6\n8.6\n1.5\n12.1\n6.7\n10.7\n17.3\n1.8\n1.4\n7.5\n4.8\n7.1\n16.9\n4.8\n7.1\n11.3\n1.1\n1.2\n1.1\n12.9\n1.2\n1.1\n1.2\n2.3\n10\n2.3\n1.2\n1.4\n14.9\n1.8\n1.8\n7\n8.6\n1.8\n1.1\n1.3\n4.9\n1.9\n10.4\n10\n8.6\n1.7\n1.7\n18.95\n12.8\n12.8\n12.8\n12.8\n12.8\n12.8\n0.7\n12.8\n1.4\n13.3\n8.5\n1.5\n11.7\n5\n1.2\n2.1\n1.4\n2.1\n16\n1.1\n15.3\n1.4\n2.8\n2.8\n0.9\n2.5\n8.1\n8.2\n0.9\n11.1\n7.8\n2.8\n10.1\n3.2\n14.2\n14.2\n14.2\n2.9\n6\n20.4\n10.1\n2.9\n14.2\n3.2\n0.95\n1.7\n1.7\n9\n1.3\n1.4\n2.4\n16\n11.4\n14.35\n2.1\n11.4\n14.35\n1.1\n1.1\n1.2\n15.8\n5.2\n5.2\n9.6\n5.2\n1.2\n0.8\n14.45\n9.6\n6.9\n3.4\n2.3\n11\n5.95\n5.1\n5.4\n1.2\n12.6\n1\n6.6\n1.5\n1\n1.1\n6.6\n8.2\n2\n1.4\n2\n7.5\n2\n2\n13.3\n2.85\n5.6\n5.6\n1\n3.2\n1\n7.1\n2.4\n11.2\n9.5\n1\n1.8\n2.6\n2.4\n8\n11.2\n7.1\n3.3\n10.3\n1.2\n1.6\n10.3\n9.65\n16.4\n1.5\n1.2\n3.3\n5\n16.3\n16.3\n16.3\n6.5\n6.4\n10.2\n16.3\n7.4\n13.7\n13.7\n1.3\n7.4\n7.4\n7.45\n7.2\n13.7\n10.4\n1.1\n6.5\n4.6\n13.9\n5.2\n1.7\n6.5\n16.4\n3.6\n1.5\n12.4\n1.7\n6.2\n6.2\n2.6\n1.7\n9.3\n12.4\n1.5\n9.1\n12\n4.8\n12.3\n12\n2.7\n3.6\n3.6\n4.3\n1.8\n11.8\n1.8\n11.8\n1.8\n1.4\n6.6\n1.55\n0.7\n6.4\n11.8\n4.3\n5.1\n5.8\n5.9\n1.3\n1.4\n1.2\n7.4\n10.8\n1.8\n7.4\n1.2\n1.4\n14.4\n1.7\n3.6\n3.6\n10.05\n10.05\n10.5\n1.9\n3.6\n1.65\n1.9\n65.8\n6.85\n7.4\n7.4\n20.2\n11\n20.2\n6.2\n6.2\n6.85\n8\n8.2\n2.2\n10.1\n7.2\n2.2\n10.1\n1.6\n1.3\n8\n8.2\n5.3\n14\n7.2\n1.6\n11.8\n9.6\n6.1\n2.7\n3.6\n1.7\n1.6\n2.7\n1\n0.9\n1.6\n1\n10.6\n2\n1.2\n6.2\n9.2\n5\n6.3\n3.3\n8\n1.2\n1.2\n16.2\n11.6\n7.2\n1.1\n3.4\n1.4\n3.3\n8\n9.3\n2.3\n0.9\n3.5\n1.7\n1.3\n1.3\n5.6\n7.4\n2.3\n1\n1.5\n10\n14.9\n9.3\n1\n1\n5.9\n5\n1.25\n3.9\n5\n0.8\n1\n5.9\n1.6\n1.3\n1\n1.1\n1.25\n1.4\n1.2\n5\n1.4\n1.7\n1.8\n1.6\n1.5\n1.7\n13.9\n5.9\n2.1\n1.1\n6.7\n2.7\n6.7\n3.95\n7.75\n10.6\n1.6\n2.5\n0.7\n11.1\n5.15\n4.7\n9.7\n1.7\n1.4\n2\n7.5\n9.7\n0.8\n13.1\n1.1\n2.2\n8.9\n1.1\n0.9\n1.7\n6.9\n1.1\n1\n1\n7.6\n8.9\n2.2\n1.2\n1\n1\n3.1\n1.95\n2.2\n8.75\n11.9\n2.7\n5.45\n6.3\n14.4\n7.8\n1.6\n9.1\n9.1\n14.4\n1.3\n1.6\n11.3\n6.3\n0.7\n1.25\n0.7\n7.8\n10.3\n10.3\n7.8\n8.7\n8.3\n10.3\n7.8\n1.2\n8.3\n8.3\n6.2\n5\n1.8\n1.6\n1.8\n1.8\n2.9\n6\n0.9\n1.1\n1.6\n5.45\n14.05\n8\n13.1\n4.9\n1.3\n2.2\n14.9\n14.9\n0.95\n1.4\n0.95\n1.7\n5.6\n14.9\n7.1\n1.2\n9.6\n11.4\n11.4\n7.9\n5\n11.1\n8\n3.8\n10.55\n10.2\n10.2\n9.8\n6.3\n1.1\n4.5\n6.3\n10.9\n9.8\n9.8\n0.8\n0.8\n1.2\n1.3\n9.8\n10.2\n10.9\n6.3\n6.3\n1.2\n0.9\n1.1\n4.5\n3.7\n18.1\n1.35\n5.5\n3.1\n12.85\n19.8\n8.25\n12.85\n3.8\n6.9\n8.25\n11.7\n4.6\n4\n19.8\n12.85\n1.2\n8.9\n11.7\n6.2\n14.8\n14.8\n10.8\n1.6\n8.3\n8.4\n2.5\n3.5\n17.2\n2.1\n12.2\n11.8\n16.8\n17.2\n1.1\n14.7\n5.5\n6.1\n1.2\n1.3\n8.7\n1.7\n8.7\n10.2\n4.5\n5.9\n1.7\n1.4\n5.4\n7.9\n1.1\n7\n7\n7.6\n7\n12.3\n15.3\n12.3\n1.2\n2.3\n6.1\n7.6\n10.2\n4.1\n2.9\n8.5\n1.5\n3.1\n7.9\n3.5\n4.9\n1.1\n7\n1.2\n4.5\n2.6\n9.9\n4.5\n9.5\n1.5\n3.2\n2.6\n11.2\n3.2\n2.3\n4.9\n4.9\n1.4\n1.5\n6.7\n2.1\n4.3\n10.9\n7\n2.3\n2.5\n2.6\n3.2\n2.5\n14.7\n4.5\n2.2\n1.9\n1.6\n17.3\n4.2\n4.2\n2.5\n1.9\n1.4\n0.8\n8\n1.6\n1.7\n5.5\n17.3\n8.6\n6.9\n2.1\n2.2\n1.5\n2.5\n17.6\n4.2\n2.9\n4.8\n11.9\n0.9\n1.3\n6.4\n4.3\n11.9\n8.1\n1.3\n0.9\n17.2\n17.2\n17.2\n8.7\n17.2\n8.7\n7.5\n17.2\n4.6\n3.7\n2.2\n7.4\n15.1\n7.4\n4.8\n7.9\n1\n15.1\n7.4\n4.8\n4.6\n1.4\n6.2\n6.1\n5.1\n6.3\n0.9\n2.3\n6.6\n7.5\n8.6\n11.9\n2.3\n7.1\n4.3\n1.1\n1\n7.9\n1\n1\n1\n7.3\n1.7\n1.3\n6.4\n1.8\n1.5\n3.8\n7.9\n1\n1.2\n5.3\n9.1\n6.5\n9.1\n6.3\n5.1\n6.5\n2.4\n9.1\n7.5\n5\n6.75\n1.2\n1.6\n16.05\n5\n12.4\n0.95\n4.6\n1.7\n1\n1.3\n5\n2.5\n2.6\n2.1\n12.75\n1.1\n12.4\n3.7\n2.65\n2.5\n8.2\n7.3\n1.1\n6.6\n7\n14.5\n11.8\n3\n3.7\n6\n4.6\n2.5\n3.3\n1\n1.1\n1.4\n3.3\n8.55\n2.5\n6.7\n3.8\n4.5\n4.6\n4.2\n11.3\n5.5\n4.2\n2.2\n14.5\n14.5\n14.5\n14.5\n14.5\n14.5\n1.5\n18.75\n3.6\n1.4\n5.1\n10.5\n2\n2.6\n9.2\n1.8\n5.7\n2.4\n1.9\n1.4\n0.9\n4.6\n1.4\n9.2\n1.4\n1.8\n2.3\n2.3\n4.4\n6.4\n2.9\n2.8\n2.9\n4.4\n8.2\n1\n2.9\n7\n1.8\n1.5\n7\n8.2\n7.6\n2.3\n8.7\n1\n2.9\n6.7\n5\n1.9\n2\n1.9\n8.5\n12.6\n5.2\n2.1\n1.1\n1.3\n1.1\n9.2\n1.2\n1.1\n8.3\n1.8\n1.4\n15.7\n4.35\n1.8\n1.6\n2\n5\n1.8\n1.3\n1\n1.4\n8.1\n8.6\n3.7\n5.7\n2.35\n13.65\n13.65\n13.65\n15.2\n4.6\n1.2\n4.6\n6.65\n13.55\n13.65\n9.8\n10.3\n6.7\n15.2\n9.9\n7.2\n1.1\n8.3\n11.25\n12.8\n9.65\n12.6\n12.2\n8.3\n11.25\n1.3\n9.9\n7.2\n1.1\n1.1\n4.8\n1.1\n1.4\n1.7\n10.6\n1.4\n1.1\n5.55\n2.1\n1.7\n9\n1.7\n1.8\n4.7\n11.3\n3.6\n6.9\n3.6\n4.9\n6.95\n1.9\n4.7\n11.3\n1.8\n11.3\n8.2\n8.3\n9.55\n8.4\n7.8\n7.8\n10.2\n5.5\n7.8\n7.4\n3.3\n5\n3.3\n5\n1.3\n1.2\n7.4\n7.8\n9.9\n0.7\n4.6\n5.6\n9.5\n14.8\n4.6\n2.1\n11.6\n1.2\n11.6\n2.1\n20.15\n4.7\n4.3\n14.5\n4.9\n14.55\n14.55\n10.05\n4.9\n14.5\n14.55\n15.25\n3.15\n1.3\n5.2\n1.1\n7.1\n8.8\n18.5\n8.8\n1.4\n1.2\n5\n1.6\n18.75\n6\n9.4\n9.7\n4.75\n6\n5.35\n5.35\n6.8\n6.9\n1.4\n0.9\n1.2\n1.3\n2.6\n12\n9.85\n3.85\n2\n1.6\n7.8\n1.9\n2\n10.3\n1.1\n12\n3.85\n9.85\n2\n4\n1.1\n10.4\n6.1\n1.8\n10.4\n4.7\n4\n1.1\n6.4\n8.15\n6.1\n4.8\n1.2\n1.1\n1.4\n7.4\n1.8\n1\n15.5\n15.5\n8.4\n2.4\n3.95\n19.95\n2\n3\n15.5\n8.4\n14.3\n4.2\n1.4\n3\n4.9\n2.4\n14.3\n10.7\n11\n1.4\n1.2\n12.9\n10.8\n1.3\n2\n1.8\n1.2\n7.5\n9.7\n3.8\n7.2\n9.7\n6.3\n6.3\n0.8\n8.6\n6.3\n3.1\n7.2\n7.1\n6.4\n14.7\n7.2\n7.1\n1.9\n1.2\n4.8\n1.2\n3.4\n4.3\n8.5\n1.8\n1.8\n19.5\n8.5\n19.9\n8.3\n1.8\n1.1\n16.65\n16.65\n16.65\n0.9\n6.1\n10.2\n0.9\n16.65\n3.85\n4.4\n4.5\n3.2\n4.5\n4.4\n9.7\n4.2\n4.2\n1.1\n9.7\n4.2\n5.6\n4.2\n1.6\n1.6\n1.1\n14.6\n2.6\n1.2\n7.25\n6.55\n7\n1.5\n1.4\n7.25\n1\n4.2\n17.5\n17.5\n17.5\n1.5\n1.3\n3.9\n4.2\n7.6\n1\n1.1\n11.8\n1.4\n9.7\n12.9\n1.6\n7.2\n7.1\n1.9\n8.8\n7.2\n1.4\n14.3\n14.3\n8.8\n1.4\n1.8\n14.3\n7.2\n1.2\n11.8\n0.9\n12.6\n26.05\n4.7\n12.6\n1.2\n26.05\n6.1\n11.8\n0.9\n5.6\n5.3\n5.7\n8\n8\n17.6\n8\n8.8\n1.5\n1.4\n4.8\n2.4\n3.7\n4.9\n5.7\n5.7\n4.9\n2\n5.1\n4.5\n3.2\n6.65\n1.6\n4\n17.75\n1.4\n17.75\n7.2\n5.7\n8.5\n11.4\n5.4\n2.7\n4.3\n1.2\n1.8\n1.3\n5.7\n2.7\n11.7\n4.3\n11\n1.6\n11.6\n6.2\n1.8\n1.2\n1\n2.4\n1.2\n8.2\n18.8\n9.6\n12.9\n9.2\n1.2\n12.9\n8\n12.9\n1.6\n12\n2.5\n9.2\n4.4\n8.8\n9.6\n8\n18.8\n1.3\n1.2\n12.9\n1.2\n1.6\n1.5\n18.15\n13.1\n13.1\n13.1\n13.1\n1\n1.6\n11.8\n1.4\n1\n13.1\n10.6\n10.4\n1.1\n7.4\n1.2\n3.4\n18.15\n8\n2.5\n2\n2\n6.9\n1.2\n9.4\n2.9\n6.9\n5.4\n1.3\n20.8\n10.3\n1.3\n1.6\n13.1\n1.8\n8\n1.6\n1.4\n14.7\n14.7\n14.7\n14.7\n14.7\n14.7\n14.7\n1.8\n10.6\n12.5\n6.8\n14.7\n2.9\n1.4\n1.4\n2.1\n7.4\n2.9\n1.4\n1.4\n7.4\n5\n2.5\n6.1\n2.7\n2.1\n12.9\n12.9\n12.9\n13.7\n12.9\n2.4\n9.8\n13.7\n1.3\n12.1\n6.1\n7.7\n6.1\n1.4\n7.7\n12.1\n6.8\n9.2\n8.3\n17.4\n2.7\n12.8\n8.2\n8.1\n8.2\n8.3\n8\n11.8\n12\n1.7\n17.4\n13.9\n10.7\n2\n2.2\n1.3\n1.1\n2\n6.4\n1.3\n1.1\n10.7\n6.4\n6.3\n6.4\n15.1\n2\n2\n2.2\n12.1\n8.8\n8.8\n5.1\n6.8\n6.8\n3.7\n12.2\n5.7\n8.1\n2.5\n4\n6.8\n1\n5.1\n5.8\n10.6\n3.5\n3.5\n16.4\n4.8\n3.3\n1.2\n1.2\n4.8\n3.3\n2.5\n8.7\n1.6\n4\n2.5\n16.2\n9\n16.2\n1.4\n7\n9\n3.1\n1.5\n4.6\n4.8\n4.6\n1.5\n2.7\n6.3\n7.2\n7.2\n12.4\n6.6\n6.6\n4\n4.8\n1.3\n7.2\n11.1\n12.4\n9.8\n6.6\n13.3\n11.7\n8\n1.6\n16.55\n1.5\n10.2\n6.6\n17.8\n17.8\n1.5\n7.4\n17.8\n2\n7.4\n2\n17.8\n12.1\n8.2\n1.5\n8.7\n3.5\n6.4\n2.1\n7.7\n12.3\n1.3\n8.7\n3.5\n1.1\n2.8\n3.5\n1.9\n3.8\n3.8\n2.4\n4.8\n4.8\n6.2\n1.3\n3.8\n1.5\n4.8\n1.9\n6.2\n7.9\n1.6\n1.4\n2.6\n14.8\n2.4\n0.9\n0.9\n1.2\n9.9\n3.9\n15.6\n15.6\n1.5\n1.6\n7.8\n5.6\n1.3\n16.7\n7.95\n6.7\n1.1\n6.3\n8.9\n1\n1.5\n6.6\n6.2\n6.3\n2.1\n2.2\n5.4\n8.9\n1\n17.9\n2.6\n1.3\n17.9\n2.6\n2.3\n4.3\n7.1\n7.1\n11.9\n11.7\n5.8\n3.8\n12.4\n6.5\n7.1\n7.6\n7.9\n2.8\n10.6\n2.8\n1.5\n7.6\n7.9\n1.7\n7.6\n7.5\n1.7\n1.7\n12.1\n4.5\n1.7\n8\n7.6\n8.6\n8.6\n14.6\n1.6\n8.6\n14.6\n1.1\n3.7\n8.9\n8.9\n4.7\n8.9\n3.1\n5.8\n5.8\n5.8\n1\n15.8\n1.5\n5.2\n1.5\n2.5\n1\n15.8\n5.9\n3.1\n3.1\n5.8\n11.5\n18\n4.8\n8.5\n1.6\n18\n4.8\n5.9\n1.1\n8.5\n13.1\n4.1\n2.9\n13.1\n1.1\n1.5\n7.75\n1.15\n1\n17.8\n5.7\n17.8\n7.4\n1.4\n1.4\n1\n4.4\n1.6\n7.9\n15.5\n15.5\n15.5\n15.5\n17.55\n13.5\n13.5\n1.3\n15.5\n11.6\n7.9\n15.5\n17.55\n11.6\n13.15\n1.9\n13.5\n1.3\n6.1\n6.1\n1.9\n1.9\n1.6\n11.3\n8.4\n8.3\n8.4\n12.2\n8\n1.3\n12.7\n1.3\n10.5\n12.5\n9.6\n1.5\n1.5\n7.8\n10.8\n12.5\n8.6\n1.2\n14.5\n3.7\n1.1\n1.1\n3.8\n4.6\n10.2\n7.9\n2.4\n10.7\n4.9\n10.7\n1.1\n7.9\n5.6\n2.4\n14.2\n9.5\n9.5\n4.1\n4.7\n1.4\n0.9\n20.3\n3.5\n2.7\n1.2\n1.2\n2\n1.1\n1.5\n1.2\n18.1\n18.1\n3.6\n3.5\n12.1\n17.45\n12.1\n3\n1.6\n5.7\n5.6\n6.8\n15.6\n6\n1.8\n8.6\n8.6\n11.5\n7.8\n2.4\n5\n8.6\n1.5\n5.4\n11.9\n11.9\n9\n10\n11.9\n11.9\n15.5\n5.4\n15\n1.4\n9.4\n3.7\n15\n1.4\n6.5\n1.4\n6.3\n13.7\n13.7\n13.7\n13.7\n13.7\n13.7\n1.5\n1.6\n1.4\n3.5\n1\n1.4\n1.5\n13.7\n1.6\n5.2\n1.4\n11.9\n2.4\n3.2\n1.7\n4.2\n15.4\n13\n5.6\n9.7\n2.5\n4\n15.4\n1.2\n2\n1.2\n5.1\n1.4\n1.2\n6.5\n1.3\n6.5\n2.7\n1.3\n7.4\n12.9\n1.3\n1.2\n2.6\n2.3\n1.3\n10.5\n2.6\n14.4\n1.2\n3.1\n1.7\n6\n11.8\n6.2\n1.4\n12.1\n12.1\n12.1\n3.9\n4.6\n12.1\n1.2\n8.1\n3.9\n1.1\n6.5\n10.1\n10.7\n3.2\n12.4\n5.2\n5\n2.5\n9.2\n6.9\n2\n15\n15\n1.2\n15\n1.8\n10.8\n3.9\n4.2\n2\n13.5\n13.3\n2.2\n1.4\n1.6\n2.2\n14.8\n1.8\n14.8\n1.3\n9.9\n5.1\n5.1\n1.5\n1.5\n11.1\n5.25\n2.3\n7.9\n8\n1.4\n5.25\n2.3\n2.3\n3.5\n13.7\n9.9\n15.4\n16\n16\n16\n16\n2.4\n5.5\n2.3\n16.8\n16\n17.8\n17.8\n6.8\n6.8\n6.8\n6.8\n1.6\n4.7\n11.8\n17.8\n15.7\n5.8\n15.7\n9\n15.7\n5.8\n8.8\n10.2\n6.6\n6.5\n8.9\n11.1\n4.2\n1.6\n7.4\n11.5\n1.6\n2\n4.8\n9.8\n1.9\n4.2\n1.6\n7.3\n5.4\n10.4\n1.9\n7.3\n5.4\n7.7\n11.5\n1.2\n2.2\n1\n8.2\n8.3\n8.2\n9.3\n8.1\n8.2\n8.3\n13.9\n13.9\n13.9\n13.9\n13.9\n13.9\n13.9\n2\n13.9\n15.7\n1.2\n1.5\n1.2\n3.2\n1.2\n2.6\n13.2\n10.4\n5.7\n2.5\n1.6\n1.4\n7.4\n2.5\n5.6\n3.6\n7.5\n5.8\n1.6\n1.5\n2.9\n11.2\n9.65\n10.1\n3.2\n11.2\n11.45\n9.65\n4.5\n2.7\n3.5\n1.7\n2.1\n4.8\n5\n2.6\n6.6\n5\n7.3\n5\n1.7\n2.6\n8.2\n8.2\n5\n1.2\n7.1\n9.5\n15.8\n15.5\n15.8\n17.05\n12.7\n12.3\n11.8\n11.8\n11.8\n12.3\n11.8\n13.6\n5.2\n6.2\n7.9\n7.9\n3.3\n2.8\n7.9\n3.3\n6.3\n4.9\n10.4\n4.9\n10.4\n16\n6.3\n2.2\n17.3\n17.3\n17.3\n17.3\n2.2\n2.2\n17.3\n6.6\n6.5\n12.3\n5\n2.8\n13.6\n2.8\n5.4\n10.9\n1.7\n9.15\n4.5\n9.15\n1.4\n5.9\n16.4\n1.2\n16.4\n5.9\n7.8\n7.8\n2.8\n2.9\n2.5\n12.8\n12.2\n7.7\n2.8\n2.9\n17.3\n19.3\n19.3\n19.3\n2.7\n6.4\n17.3\n2.4\n2.8\n1.7\n15.4\n15.4\n4.1\n6.6\n1.2\n2.1\n1\n1.1\n1.4\n1.6\n9.8\n1.9\n1.3\n7.9\n7.9\n4.5\n22.6\n7.9\n3.5\n1.2\n4.5\n2\n7.8\n0.9\n2.9\n2.9\n3.5\n4.2\n9.7\n10.5\n1.1\n16.1\n1.1\n8.1\n6.2\n7.7\n2.4\n16.3\n2.3\n8.4\n8.5\n6\n1.1\n1.75\n2.6\n1.3\n2.1\n1.1\n1.1\n2.8\n9\n2.8\n2.2\n5.1\n3.5\n12.7\n7.5\n2\n3.5\n14.3\n9.8\n12.7\n12.7\n5.1\n3.5\n12.7\n12.9\n12.9\n1.3\n10.5\n1.5\n12.7\n12.9\n1.2\n6.2\n8.8\n3.9\n1.3\n9.1\n9.1\n3.9\n1.8\n2.1\n1.4\n14.7\n9.1\n1.9\n1.8\n9.6\n3.9\n1.3\n11.8\n1.9\n12\n7.9\n9.3\n4.6\n2.2\n10.2\n10.6\n1.4\n9.1\n11.1\n9.1\n4.4\n2.8\n1.1\n1.3\n1.2\n3.3\n9.7\n2.3\n1.1\n11.4\n1.2\n14.7\n13.8\n1.3\n6.3\n7.9\n2\n11.8\n1.2\n10\n5.2\n1.2\n7.2\n9.9\n5.3\n13.55\n2.2\n9.9\n4.3\n13\n13.55\n1\n1.1\n6.9\n13.4\n4.6\n9.9\n3\n5.8\n12.9\n3.2\n0.8\n2.5\n2.4\n7.2\n7.3\n6.3\n4.25\n1.2\n2\n4.25\n4.7\n4.5\n1.4\n4.1\n5.3\n4.2\n6.65\n8.2\n2.6\n2.6\n2\n12.2\n2.3\n8.2\n5\n10.7\n10.8\n1.7\n1.3\n1.7\n12.7\n1.3\n1.2\n1.3\n5.7\n3.4\n1.1\n1\n1\n1.65\n6.8\n6.8\n4.9\n1.4\n2.5\n10.8\n10.8\n10.8\n10.8\n2.8\n1.3\n2\n1.1\n8.2\n6\n6.1\n8.2\n8.8\n6.1\n6\n1.2\n11.4\n1.3\n1.3\n6.2\n3.2\n4.5\n9.9\n6.2\n11.4\n1.3\n1.3\n0.9\n0.7\n1\n1\n10.4\n1.3\n12.5\n12.5\n12.5\n12.5\n19.25\n1.1\n12.5\n19.25\n9\n1.2\n9\n1.3\n12.8\n12.8\n7.6\n7.6\n1.4\n8.3\n9\n1.85\n12.55\n1.4\n1.8\n4\n12.55\n9\n3\n1.85\n7.9\n2.6\n1.2\n7.1\n7.9\n1.3\n10.7\n7.7\n8.4\n10.7\n12.7\n1.8\n7.7\n10.5\n1.6\n1.85\n10.5\n10.5\n1\n1.2\n1.7\n1.6\n9\n1.9\n1.2\n1.5\n3.9\n3.6\n1.2\n5\n2.9\n10.4\n11.4\n18.35\n18.4\n1.2\n7.1\n1.3\n1.5\n10.2\n2.2\n3.5\n3.5\n3.9\n7.4\n7.4\n11\n1.5\n3.9\n5.4\n1.5\n5\n1.2\n13\n13\n13\n13\n8.6\n1.7\n1.2\n1.2\n1.2\n2\n19.4\n0.8\n6.3\n6.4\n12.1\n12.1\n12.9\n2.4\n4.3\n4.2\n12.9\n1.7\n2.2\n12.1\n3.4\n7.4\n7.3\n1.1\n1.1\n1.4\n14.5\n8\n1.1\n1.1\n2.2\n5.8\n0.9\n6.4\n10.9\n7.3\n8.3\n1.3\n3.3\n1\n1.1\n1\n5.1\n3.2\n12.6\n3.7\n1.7\n5.1\n1\n1.3\n1.5\n4.6\n10.3\n6.1\n6.1\n1.2\n10.3\n9.9\n1.6\n1.1\n1.5\n1.2\n1.5\n1.1\n11.5\n7.8\n7.4\n1.45\n8.9\n1.1\n1\n2.5\n1.1\n2.4\n2.3\n5.1\n2.5\n8.9\n2.5\n8.9\n1.6\n1.4\n3.9\n13.7\n13.7\n9.2\n7.8\n7.6\n7.7\n3\n1.3\n4\n1.1\n2\n1.9\n1.4\n4.5\n10.1\n6.6\n1.9\n12.4\n1.6\n2.5\n1.2\n2.5\n0.8\n0.9\n8.1\n8.1\n11.75\n1.3\n1.9\n8.3\n8.1\n5.7\n1.9\n1.2\n11.75\n2.2\n0.9\n1.3\n1.6\n8\n1.2\n1.1\n0.8\n",
      "cool, can you give me the qcut function calls and arguments that fail?\n",
      "For case(3), using the above data as a column tmpcol \npd.qcut(tmpcol, 20), I got the bin's levels as\n\narray([[0.6, 0.991], (0.991, 0.992], (0.992, 0.993], (0.993, 0.993],\n(0.993, 0.994], (0.994, 0.995], (0.995, 0.996], (0.996, 0.997],\n(0.997, 0.998], (0.998, 0.1], (0.1, 1.2], (1.2, 1.5], (1.5, 2.1],\n(2.1, 3.5], (3.5, 5.2], (5.2, 7.1], (7.1, 8.5], (8.5, 11],\n(11, 13.7], (13.7, 65.8]], dtype=object)\n\nNote (0.993, 0.993], and (0.998, 0.1], seems two kind of different errors\n\ncase (2)  can be replicated by executing  qcut on any column containing 2 distinct values \nRan into case(1) a couple of weeks  and  I can hardly remember which data I was using. \n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "assigned"
    ],
    "changed_files": 5,
    "additions": 85,
    "deletions": 9,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tools/tests/cut_data.csv",
      "pandas/tools/tests/test_tile.py",
      "pandas/tools/tile.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1987,
    "reporter": "lbeltrame",
    "created_at": "2012-09-28T15:29:03+00:00",
    "closed_at": "2012-09-29T21:33:39+00:00",
    "resolver": "wesm",
    "resolved_in": "48fadc189c6f8c85e6cb2ba18d654837cff69e52",
    "resolver_commit_num": 2441,
    "title": "When a series contains all strings, converting to dtype \"int\" generates bogus int values",
    "body": "This works:\n\n\n\nThis doesn't:\n\n\n\nThis with RC2 state from git.\n",
    "labels": [],
    "comments": [
      "That's no good. I'll look into it\n",
      "Irritating NumPy brokenness:\n\n```\nIn [3]: arr = np.array([\"car\", \"house\", \"tree\",\"1\"], dtype=object)\n\nIn [4]: arr\nOut[4]: array([car, house, tree, 1], dtype=object)\n\nIn [5]: arr.astype(int)\nOut[5]: array([4454445056, 4454445056, 4482056920,          1])\n```\n"
    ],
    "events": [
      "commented",
      "commented"
    ],
    "changed_files": 4,
    "additions": 27,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/common.py",
      "pandas/src/tseries.pyx",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1990,
    "reporter": "jseabold",
    "created_at": "2012-09-29T17:31:42+00:00",
    "closed_at": "2012-09-29T21:03:14+00:00",
    "resolver": "wesm",
    "resolved_in": "f82d931f53bc2688715016887e41290dbde7da29",
    "resolver_commit_num": 2440,
    "title": "NaN comparison in merge",
    "body": "Parking this here so I don't forget about it. I assume that the NaNs here are not comparing equal, so we get \"duplicate\" rows on a merge, which I think is a bug.\n\n\n\nWorked around this by filling in the NaNs before the merge, but then I have to convert them back to NaNs later.\n",
    "labels": [],
    "comments": [
      "Hm, I was fairly surprised by this. Having a look under the hood\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 4,
    "additions": 40,
    "deletions": 4,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 1993,
    "reporter": "ldkphd",
    "created_at": "2012-09-30T10:47:35+00:00",
    "closed_at": "2012-10-01T01:19:11+00:00",
    "resolver": "wesm",
    "resolved_in": "0523d98aee41f0579524cc0a40895eee26e4369d",
    "resolver_commit_num": 2445,
    "title": "date_range export to csv weird behavior",
    "body": "Hello,\n\nWorking with 0.8.1\n\ndate_range works nicely, I am using the technique to reduce sparse GPS measurements to a fixed time step.\n\nprocedure:\nstep 1: create date_range (\"10S\"), all datetime within a single day (input from gpx)\nstep 2: outer join with original data\nstep 3: interpolate the locations\n\nEverything works fine, .to_string() results in results as expected. \n\nWhen I export the result to_csv(), the date_range object is exported with the correct datetime, but the date part is reduced to to january 1970. How is this possible?\n\nCan somebody get me on the road again?\n\nLuc\n",
    "labels": [],
    "comments": [
      "More NumPy 1.6 buggy crap. I'll fix. thanks\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 3,
    "additions": 15,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2000,
    "reporter": "wesm",
    "created_at": "2012-10-01T00:14:26+00:00",
    "closed_at": "2012-10-01T18:59:29+00:00",
    "resolver": "wesm",
    "resolved_in": "c0c7402f9071d8114e8c849aa4cc46ca499a02c0",
    "resolver_commit_num": 2449,
    "title": "Change default column names in read_* functions when header=False",
    "body": "In retrospect I think using R's convention of `X.1`, ..., `X.N` was a bad move. Maybe `range(N)` would be better? Will cause API breakage but not the end of the world\n",
    "labels": [],
    "comments": [
      "@changhiskhan do you have an opinion? \n",
      "FWIW, I don't like the dots because you lose attribute access, but I do like the `X`. I believe we grab and use the column names in statsmodels.\n",
      "I think the API breakage will be worth it in exchange for attribute access.\nMaybe just X1 .... XN or X_1 ... X_N?\n",
      "Or start at 0?\n",
      "Maybe more Pythonic to start at 0, but I don't think people really think of DataFrame columns positionally. No strong opinion either way.\n",
      "I vote for starting at 0 since there's no compelling reason to break consistency here.\nOtherwise it's a little weird that DataFrame(data) starts numbering the columns at 0 but pd.read_csv('frame.csv') gives columns that start at 1.\n",
      "OK. I'll bite for 'X0', ... , 'XN'. Hopefully will not cause too much damage\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 4,
    "additions": 44,
    "deletions": 39,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/tests/test_format.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2008,
    "reporter": "PhE",
    "created_at": "2012-10-02T15:16:42+00:00",
    "closed_at": "2012-11-05T01:09:51+00:00",
    "resolver": "wesm",
    "resolved_in": "81661af85e68d340ad7834b22d36f0be3cd04d41",
    "resolver_commit_num": 2533,
    "title": "set_index breaks depending on column name or number",
    "body": "I get a very strange behaviour with pandas current dev version (tagged as '0.9.0rc2') :\nCalling set_index() on a DataFrame breaks depending on the columns name I use !!!\n\nThis code  fails :\n\n\n\nBut If I change the indexed column names to 'a' and 'x', It works :\n\n\n\nAt first I thought It was related to column name .... but If I keep my exotic column name and remove one column (one not used by set_index), it works !! :\n\n\n",
    "labels": [],
    "comments": [
      "on master:\n\n``` python\nIn [9]: df3.set_index(['0001-INTERNAL_PRODUCT_CODE', '1050-LOCALISED_DESCRIPTION'])\nOut[9]: \n                                                       0004-PACKAGING_LANGUAGE_CODE  0024-PRODUCT_BAR_CODED  0027-LOCAL_COMMERCIAL_AUTHORIZATION Source system\n0001-INTERNAL_PRODUCT_CODE 1050-LOCALISED_DESCRIPTION                                                                                                         \nA0006000                   nan                                                  NaN                     NaN                                  NaN             A\n\n```\n\nHowever, this way you are using NaN in the index, which is a no-no, see also #1971.\n",
      "Can this be closed?\n",
      "Don't think so. Looks like this is broken on master now. Another instance of na support issues with MultiIndex.\nIt's not dependent on the column name. It's the call to Index.summary that craps out and the format in the difference use cases differ.\n",
      "I'll have a look. After food, though\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "assigned"
    ],
    "changed_files": 4,
    "additions": 27,
    "deletions": 4,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/index.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tests/test_panel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2011,
    "reporter": "jseabold",
    "created_at": "2012-10-02T21:35:48+00:00",
    "closed_at": "2012-10-05T00:55:19+00:00",
    "resolver": "wesm",
    "resolved_in": "5722a57f6755b90abd71acfdf71a62fc86f6f253",
    "resolver_commit_num": 2459,
    "title": "io.data._sanitize_dates behavior vs. docs",
    "body": "Docs say default start for DataReader should be 2010-1-1, but it returns 1 year ago from today. I prefer the docs. Thoughts?\n\n\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 2,
    "additions": 3,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/data.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2024,
    "reporter": "gerigk",
    "created_at": "2012-10-05T09:33:52+00:00",
    "closed_at": "2012-11-28T03:07:50+00:00",
    "resolver": "wesm",
    "resolved_in": "96545d0ee7a5a5c9446d4fd8c3f055b3a4d13751",
    "resolver_commit_num": 2634,
    "title": "pd.merge fails if columns of only one side are hierarchical (even if index is equal)",
    "body": "\n\nalso this fails\n\n\n\nand this\n\n\n",
    "labels": [],
    "comments": [
      "it it possible somehow to use the agg({'a': [f1, f1], 'b': [f3, f4]}) syntax without creating the hierarchical columns? \nof course, I could reset the index after doing this step to something like column-name_function-name   and then I could easily join with the other dataframe but this feels wrong.\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 3,
    "additions": 23,
    "deletions": 14,
    "changed_files_list": [
      "pandas/core/index.py",
      "pandas/tests/test_index.py",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2034,
    "reporter": "bmu",
    "created_at": "2012-10-07T12:51:41+00:00",
    "closed_at": "2012-12-09T19:12:37+00:00",
    "resolver": "wesm",
    "resolved_in": "1e5c9d05c5bb5b8e171d84afb8d6c8176e8982e9",
    "resolver_commit_num": 2714,
    "title": "Default column names for `read_csv` and for data frames created from other data",
    "body": "If you read data from a file with `read_csv` the default column names of the resulting data frame are set to `X.1` to `X.N` (and to `X1` to `XN` for versions >= 0.9), which are strings.  \n\nIf you create a data frame from exiting arrays or lists or something the column names default to `0` to `N` and are integers.\n\nShould these default names be the same? I think this would be more consistent and would avoid confusion (Not sure how this is handled in `R` if this a part of this design decision). \n\nSee also [this question]() and the answers on stackoverflow.\n",
    "labels": [],
    "comments": [
      "We made the decision for the default names to be X0, X1, ... X_{N-1} from read_csv. I'd thought about 0 through N - 1 also, but the X# was a convention from R that users seemed comfortable with\n",
      "@wesm Its about the difference between the default names with\n\n```\nIn [39]: df = pd.DataFrame(np.arange(100).reshape(10, 10))\n\nIn [40]: df\nOut[40]: \n    0   1   2   3   4   5   6   7   8   9\n0   0   1   2   3   4   5   6   7   8   9\n1  10  11  12  13  14  15  16  17  18  19\n2  20  21  22  23  24  25  26  27  28  29\n3  30  31  32  33  34  35  36  37  38  39\n4  40  41  42  43  44  45  46  47  48  49\n5  50  51  52  53  54  55  56  57  58  59\n6  60  61  62  63  64  65  66  67  68  69\n7  70  71  72  73  74  75  76  77  78  79\n8  80  81  82  83  84  85  86  87  88  89\n9  90  91  92  93  94  95  96  97  98  99\n\nIn [41]: df.to_csv('df.txt', header=False)\n\nIn [42]: df = pd.DataFrame(np.arange(100).reshape(10, 10))\n\nIn [43]: df.columns\nOut[43]: Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int64)\n\nIn [44]: df.to_csv('df.txt', header=False)\n\nIn [45]: df = pd.read_csv('df.txt', index_col=0, header=None)\n\nIn [46]: df.columns\nOut[46]: Index([X1, X2, X3, X4, X5, X6, X7, X8, X9, X10], dtype=object)\n```\n\nIt would be more consistent if it would be the same index (X1 ...XN) when creating a DataFrame from an array. This would less confusing when indexing a DataFrame by column names.\n",
      "Well, I often think the column names should just be integers 0 through N-1. Making default DataFrame column names X0 through X{N-1} is not going to happen, but perhaps the parser should be consistent with the range(N) behavior. I'd feel like a real jerk changing this again given that I just changed it. =/\n",
      "reopening for 0.10 and further consideration\n",
      "@jseabold I hate to bring this up again and break APIs again, but I'm thinking it might be simpler on everyone to just make the default column names range(N) vs. the X0, ... I don't really know how disruptive this would be.\n",
      "As far as I can see a DataFrame should also be dict like (and you need default keys as no keys are given), so there is nothing wrong to give default names for the keys and 'X0', ... would be an option. You can still access it in a list like way using the index. so from this point of view it wouldn't be \"not pythonic\". further more attribute access would be better with 'X0', ...\n\nSo whats wrong with using 'x0', ... as the default for every initialization of a DataFrame without column names given?   \n",
      "It would be very inconsistent with default indexing on other axes; plus it would break probably 90% of people's code, whereas changing X0, X1, ... to 0, 1, ... would break about < 1% of people's code in practice. Indeed one of the reasons we chose the X0, ... was to facilitate attribute access. \n"
    ],
    "events": [
      "commented",
      "closed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "reopened",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented"
    ],
    "changed_files": 5,
    "additions": 53,
    "deletions": 27,
    "changed_files_list": [
      "pandas/core/series.py",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/tests/test_format.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2041,
    "reporter": "lodagro",
    "created_at": "2012-10-08T19:19:54+00:00",
    "closed_at": "2012-11-01T14:10:01+00:00",
    "resolver": "wesm",
    "resolved_in": "396c3abbb9ceb230ae396b35d3088244dc2f08f4",
    "resolver_commit_num": 2502,
    "title": "read_csv treats both inf and -inf as very large negative number",
    "body": "from [mailing list](#!topic/pydata/Wy3eBEnoDrM), same discussion led to #2026\n\n\n",
    "labels": [],
    "comments": [
      "Here is a test and a patch for this bug: https://github.com/aflaxman/pandas/commit/d11eea217ff382cc3258f21bc938f9537f5ed684\n\nI'm planning to make a pull request which combines it with work on #2026 and #1919.\n"
    ],
    "events": [
      "commented",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 11,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/inference.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2042,
    "reporter": "ludaavics",
    "created_at": "2012-10-08T21:41:52+00:00",
    "closed_at": "2012-10-31T21:02:21+00:00",
    "resolver": "wesm",
    "resolved_in": "56fac890ed44608ba2b122a94942d9c8c04a236f",
    "resolver_commit_num": 2495,
    "title": "Dot doesn't accept arrays",
    "body": "\n\nI would guess replace `rvals = other.values` with\n\n\n\nunless you specifically intended for this to fail?\n\nThanks,\nLudovic\n",
    "labels": [],
    "comments": [
      "bump\n",
      "Updated this to go into 0.9.1 bugfix release\n"
    ],
    "events": [
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 41,
    "deletions": 8,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2043,
    "reporter": "changhiskhan",
    "created_at": "2012-10-08T22:28:29+00:00",
    "closed_at": "2012-11-02T14:22:33+00:00",
    "resolver": "wesm",
    "resolved_in": "db9446f788e55ca3916ce1abab15b6067ab2218a",
    "resolver_commit_num": 2505,
    "title": "Merge doc still refers to Index uniqueness as requirement",
    "body": "\"Note that if using the index from either the left or right DataFrame (or both) using the left_index / right_index options, the join operation is no longer a many-to-many join by construction, as the index values are necessarily unique. There will be some examples of this below.\"\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 1,
    "additions": 0,
    "deletions": 5,
    "changed_files_list": [
      "doc/source/merging.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2057,
    "reporter": "bluefir",
    "created_at": "2012-10-11T17:20:52+00:00",
    "closed_at": "2012-11-19T04:59:35+00:00",
    "resolver": "wesm",
    "resolved_in": "19dc2847afd74329664d6c42d98b75dcd70c14fa",
    "resolver_commit_num": 2586,
    "title": "transform in groupby throws TypeError when run with python -O option",
    "body": "I have a script that works fine when run without any options but generates the following traceback when run with 'python -O':\n\n  File \"C:\\Python27\\lib\\site-packages\\pandas\\core\\groupby.py\", line 1745, in transform\n    return self._transform_item_by_item(obj, wrapper)\n  File \"C:\\Python27\\lib\\site-packages\\pandas\\core\\groupby.py\", line 1777, in _transform_item_by_item\n    raise TypeError('Transform function invalid for data types')\nTypeError: Transform function invalid for data types\n\nI have Python 2.7.3 and pandas 0.9.0:\n\nPython 2.7.3 (default, Apr 10 2012, 23:24:47) [MSC v.1500 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n> > > import pandas\n> > > pandas.**version**\n> > > '0.9.0'\n",
    "labels": [],
    "comments": [
      "That's not cool. Do you have a self-contained reproduction you could post here?\n",
      "Well, sort of. I tried to reproduce and discovered another strange behavior. Here is the code for the strange behavior:\n\n```\nimport numpy as np\nfrom pandas import DataFrame, MultiIndex\n\ndef quantiles(df, q=0.5):\n    print('Entered quantiles() with shape ' + str(df.shape))\n\n    print('Calculating quantiles ' + str(q) + ' for each column')\n    qtls = df.quantile(q)\n\n    print('Building output data frame')\n    df_zeros = DataFrame(np.zeros(df.shape), index=df.index, columns=df.columns)\n    df_out = df_zeros.add(qtls, axis='columns')\n\n    print('Output shape ' + str(df_out.shape))\n    return df_out\n\n\nmidx = MultiIndex(levels=[[1, 2], ['a', 'b', 'c', 'd', 'e']],\n                  labels=[[0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n                          [0, 1, 2, 3, 4, 0, 1, 2, 3, 4]],\n                  names=['date', 'id'])\ndf = DataFrame(np.random.randn(10, 2), index=midx, columns=['col1', 'col2'])\nprint('\\nInput data frame: ')\nprint(df.to_string())\nprint('\\nCalculating medians for each column:')\nqtls = df.groupby(level='date').transform(quantiles)\nprint('\\nData frame with medians:')\nprint('Shape ' + str(qtls.shape))\nprint(qtls.to_string())\n```\n\n'python TestGroupbyTransformO.py' produces the expected oucome:\n\n---begin console output-------------------------------------\n\nInput data frame:\n             col1      col2\ndate id\n1    a   0.025334  0.468002\n     b   1.307855  1.094578\n     c   0.454256  0.711495\n     d  -1.450975 -0.858718\n     e   0.851123  0.878828\n2    a   1.726560 -0.936486\n     b   0.911542 -0.177365\n     c  -1.078583  1.797866\n     d   0.595278  1.683337\n     e  -1.718456 -1.041106\n\nCalculating medians for each column:\nEntered quantiles() with shape (5L,)\nCalculating quantiles 0.5 for each column\nBuilding output data frame\nEntered quantiles() with shape (5L,)\nCalculating quantiles 0.5 for each column\nBuilding output data frame\nEntered quantiles() with shape (5, 2)\nCalculating quantiles 0.5 for each column\nBuilding output data frame\nOutput shape (5, 2)\nEntered quantiles() with shape (5L,)\nCalculating quantiles 0.5 for each column\nBuilding output data frame\nEntered quantiles() with shape (5L,)\nCalculating quantiles 0.5 for each column\nBuilding output data frame\nEntered quantiles() with shape (5, 2)\nCalculating quantiles 0.5 for each column\nBuilding output data frame\nOutput shape (5, 2)\n\nData frame with medians:\nShape (10, 2)\n             col1      col2\ndate id\n1    a   0.454256  0.711495\n     b   0.454256  0.711495\n     c   0.454256  0.711495\n     d   0.454256  0.711495\n     e   0.454256  0.711495\n2    a   0.595278 -0.177365\n     b   0.595278 -0.177365\n     c   0.595278 -0.177365\n     d   0.595278 -0.177365\n     e   0.595278 -0.177365\n\n---end console output-------------------------------------------\n\n'python -O TestGroupbyTransformO.py' produces this:\n\n---begin console output------------------------------------------\n\nInput data frame:\n             col1      col2\ndate id\n1    a   0.824534  0.258803\n     b  -0.807477 -0.046351\n     c  -0.243443 -0.887152\n     d  -1.430488  1.675248\n     e  -0.430917  1.466759\n2    a   1.101497  0.738619\n     b  -2.010792 -0.152976\n     c  -1.757038  1.234569\n     d  -0.081311 -1.690532\n     e   0.696795  0.442808\n\nCalculating medians for each column:\nEntered quantiles() with shape (5L,)\nCalculating quantiles 0.5 for each column\nBuilding output data frame\nEntered quantiles() with shape (5L,)\nCalculating quantiles 0.5 for each column\nBuilding output data frame\nEntered quantiles() with shape (5, 2)\nCalculating quantiles 0.5 for each column\nBuilding output data frame\nOutput shape (5, 2)\nEntered quantiles() with shape (5L,)\nCalculating quantiles 0.5 for each column\nBuilding output data frame\nEntered quantiles() with shape (5L,)\nCalculating quantiles 0.5 for each column\nBuilding output data frame\nEntered quantiles() with shape (5, 2)\nCalculating quantiles 0.5 for each column\nBuilding output data frame\nOutput shape (5, 2)\n\nData frame with medians:\nShape (10, 2)\nTraceback (most recent call last):\n  File \"TestGroupbyTransformO.py\", line 29, in <module>\n    print(qtls.to_string())\n  File \"C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py\", line 1267, in to_st\nring\n    formatter.to_string(force_unicode=force_unicode)\n  File \"C:\\Python27\\lib\\site-packages\\pandas\\core\\format.py\", line 279, in to_st\nring\n    strcols = self._to_str_columns(force_unicode)\n  File \"C:\\Python27\\lib\\site-packages\\pandas\\core\\format.py\", line 214, in _to_s\ntr_columns\n    str_columns = self._get_formatted_column_labels()\n  File \"C:\\Python27\\lib\\site-packages\\pandas\\core\\format.py\", line 355, in _get_\nformatted_column_labels\n    dtypes = self.frame.dtypes\n  File \"C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py\", line 1386, in dtype\ns\n    return self.apply(lambda x: x.dtype)\n  File \"C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py\", line 3763, in apply\n\n```\nreturn self._apply_standard(f, axis)\n```\n\n  File \"C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py\", line 3831, in _appl\ny_standard\n    k = res_index[i]\nUnboundLocalError: local variable 'i' referenced before assignment\n\n---end console output------------------------------------------------------------------------------------\n\nThis is not what I observe in my more complex problem. It has a bigger frame and a more complicated function, but it does calculate several quantiles per column during the first steps (I tried to isolate those steps and discovered the above behavior). Normal run produces something like this:\n\n---begin console output-----------------------------------\n...\n\nEntered with (1996L,)\nEntered with (1996L,)\nEntered with (1996, 21)\nAssertions done\nCalculating quantiles\nQuantiles calculated\nBounds calculated\nOutliers detected\nOutput shape (1996, 21)\nEntered with (1996L,)\nEntered with (1996L,)\nEntered with (1996, 21)\nAssertions done\nCalculating quantiles\nQuantiles calculated\nBounds calculated\nOutliers detected\nOutput shape (1996, 21)\nEntered with (1996L,)\nEntered with (1996L,)\nEntered with (1996, 21)\nAssertions done\nCalculating quantiles\nQuantiles calculated\nBounds calculated\nOutliers detected\nOutput shape (1996, 21)\nEntered with (1996L,)\nEntered with (1996L,)\nEntered with (1996, 21)\nAssertions done\nCalculating quantiles\nQuantiles calculated\nBounds calculated\nOutliers detected\nOutput shape (1996, 21)\n...\n---end console output------------------------------\n\nThe -O run produces this:\n\n---begin console output-----------------------------\n...\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nEntered with (1996L,)\nAssertions done\nCalculating quantiles\nTraceback (most recent call last):\n  File \"InefficiencyScores.py\", line 390, in <module>\n    returns_daily_no_outliers = returns_daily.groupby(level=field_date).transfor\nm(f_shrink_outliers)\n  File \"C:\\Python27\\lib\\site-packages\\pandas\\core\\groupby.py\", line 1745, in tra\nnsform\n    return self._transform_item_by_item(obj, wrapper)\n  File \"C:\\Python27\\lib\\site-packages\\pandas\\core\\groupby.py\", line 1777, in _tr\nansform_item_by_item\n    raise TypeError('Transform function invalid for data types')\nTypeError: Transform function invalid for data types\n\n---end console output-----------------------------\n\nAs you can see, in my program the -O run never enters with the full shape (1996, 21) and doesn't seem to even get beyond quantile calculations:\n\n```\nprint('Calculating quantiles')\n\n# Calculate sample quantiles\nq25 = df_out.quantile(q=0.25, axis=axis)\nq75 = df_out.quantile(q=0.75, axis=axis)\nif symmetric:\n    midpoint = (q75 + q25) / 2.\nelse:\n    midpoint = df_out.quantile(q=0.5, axis=axis)\n\nprint('Quantiles calculated')\n```\n\nI realize it's convoluted but I hope it helps. The original program is more complex and so far I haven't been able to simply reproduce the behavior I observe. But I did find another puzzling behavior! :-)\n",
      "wow, this is annoying. Apparently `python -O` removes `assert` statements in code \n",
      "Yep, among other things. But that's the point! Faster code. -OO also removes docstrings. I am not sure how much it helps though. If it's hard to fix, fughetaboutit :-)\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 7,
    "deletions": 5,
    "changed_files_list": [
      "pandas/core/common.py",
      "pandas/core/internals.py",
      "pandas/tools/merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2061,
    "reporter": "wesm",
    "created_at": "2012-10-11T23:40:07+00:00",
    "closed_at": "2012-11-01T14:19:31+00:00",
    "resolver": "wesm",
    "resolved_in": "c091a8a270c7ea25af8d51ffb9bf84513041ddce",
    "resolver_commit_num": 2503,
    "title": "Documentation flow issue",
    "body": "CHAPTER 6\nESSENTIAL BASIC FUNCTIONALITY\n\nAs the reader moves from section 6.1 to 6.2 to 6.3.1\n\nThe df variable does not trace through as  the tutorial would expect.\n\nI think it needs to insert something like the following \n\n\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 1,
    "additions": 0,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/basics.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2068,
    "reporter": "wesm",
    "created_at": "2012-10-14T19:08:42+00:00",
    "closed_at": "2012-10-31T23:10:18+00:00",
    "resolver": "wesm",
    "resolved_in": "53cf5a9fbeccca89c450b158ac20f28755645656",
    "resolver_commit_num": 2496,
    "title": "Better prevent UnboundLocalError in DataFrame.apply",
    "body": "cf. -pandas-unbound-local-error-while-calling-a-function-df-apply\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 1,
    "additions": 1,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2070,
    "reporter": "abielr",
    "created_at": "2012-10-15T04:23:59+00:00",
    "closed_at": "2013-01-21T17:17:58+00:00",
    "resolver": "wesm",
    "resolved_in": "7c6e30aae2ecab2775c87d656a36335b16236a2b",
    "resolver_commit_num": 2809,
    "title": "Cannot aggregate by mean when using PeriodIndex and high-frequency series does cross between bins",
    "body": "When using a Series indexed by a PeriodIndex and downsampling, resampling fails when using how='mean' and where the series to be resampled does not span multiple lower-frequency bins. \n\nFor example:\n\n\n\nFails because the period range is entirely contained within a single year. I've been able to replicate this going from quarterly to annual, or monthly to quarterly, etc. As of 0.9.1-dev, crashes Python without an exception as Cython function group_mean_bin() attempts to index into an empty bins array.\n\n\n\nI don't know how fine-grained pandas is right now when aggregating partially-filled periods, but it could be nice to have an option to return a NaN when the higher-frequency window is only partially filled. For example, suppose we sum daily to monthly and take a percent change across months, and either the recording started partway through the first month or data is only available partway through the last month. Then the first or last period percent change will possibly show a dramatic swing, and the user may not realize its simply an artifact of the data availability, as opposed to a truly interesting move in the underlying process. When running alot of automated aggregations the user may wish to not aggregate any partially filled periods in order to protect themselves from reaching a false conclusion about the time-series trend at the beginning or end of the series.\n",
    "labels": [],
    "comments": [
      "This issue seems like it can be resolved by uncommenting the Cython decorators at the top function group_mean_bin in src/groupby.pyx:\n\n```\n@cython.boundscheck(False)\n```\n\nThis was the only case where I could see these commented out; didn't know if it was just an oversight or some testing was in progress at a point in the past.\n",
      "I noticed when I install the pre-built binaries for Python 2.7 on Windows I don't run into quite the same error; in that case Python doesn't crash but the call to resample() nonetheless returns an NaN. I built the dev version using MinGW after which I noticed unexpected failures popping up elsewhere, so the compilation process may not have been good. I will also try building with Visual C++. In any case, the original problem of taking the mean when the data is all within one period is still outstanding.\n",
      "thanks i will have a look\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 11,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2071,
    "reporter": "lbeltrame",
    "created_at": "2012-10-15T09:23:44+00:00",
    "closed_at": "2012-11-15T15:46:12+00:00",
    "resolver": "wesm",
    "resolved_in": "25cc4e1f3117f5d2c30182983b31eb4113138aff",
    "resolver_commit_num": 2573,
    "title": "c-parser branch: Iteration over an open file handle makes the parser fail",
    "body": "An example is better than words:\n\n\n\nThis works with the python engine. Notice that the handle is not really iterated through: when debugging I noticed that after iterator usage, the handle keeps on staying at the same file line (IOW the parser is not iterating on it at all).\n",
    "labels": [],
    "comments": [
      "In general terms, it seems that _any_ iteration on the open file handle breaks things:\n\n``` python\nIn [26]: with open(\"test.txt\") as handle:\n    for line in handle:\n        print line\n        if \"CCC\" in line:\n            break\n    res = pandas.read_table(handle, squeeze=True, header=None)\n   ....:     \nAAA\n\nBBB\n\nCCC\n\n\nIn [27]: res\nOut[27]: \nEmpty DataFrame\nColumns: array([], dtype=object)\nIndex: array([], dtype=object)\n\n```\n\nWhile this is not the case for the Python parser:\n\n``` python\n\nIn [28]: with open(\"test.txt\") as handle:\n    for line in handle:\n        print line\n        if \"CCC\" in line:\n            break\n    res = pandas.read_table(handle, squeeze=True, header=None, engine=\"python\")\n   ....:     \nAAA\n\nBBB\n\nCCC\n\n\nIn [29]: res\nOut[29]: \n0     DDD\n1     EEE\n2     FFF\n3     GGG\n4    None\nName: X0\n```\n",
      "Gave it a go again after the merge to master, this time the parser simply segfaults with this case (100% reproducible).\n",
      "And here's a backtrace:\n\n``` gdb\nProgram received signal SIGSEGV, Segmentation fault.\nbuffer_rd_bytes (source=0xf068c0, nbytes=<optimized out>, bytes_read=0x7fffffffb5c8, status=0x7fffffffb5c4) at pandas/src/parser/io.c:128\n128         if (!PyBytes_Check(result)) {\n(gdb) bt\n#0  buffer_rd_bytes (source=0xf068c0, nbytes=<optimized out>, bytes_read=0x7fffffffb5c8, status=0x7fffffffb5c4) at pandas/src/parser/io.c:128\n#1  0x00007fffeeba2637 in parser_buffer_bytes (self=self@entry=0x6e6c10, nbytes=<optimized out>) at pandas/src/parser/parser.c:493\n#2  0x00007fffeeba2d1f in _tokenize_helper (self=0x6e6c10, nrows=nrows@entry=1, all=all@entry=0) at pandas/src/parser/parser.c:1188\n#3  0x00007fffeeba2dd7 in tokenize_nrows (self=<optimized out>, nrows=nrows@entry=1) at pandas/src/parser/parser.c:1218\n#4  0x00007fffeeb7f9bd in __pyx_f_6pandas_7_parser_10TextReader__tokenize_rows (__pyx_v_self=0x6dcd20, __pyx_v_nrows=1) at pandas/src/parser.c:5893\n#5  0x00007fffeeb81a21 in __pyx_f_6pandas_7_parser_10TextReader__get_header (__pyx_v_self=0x6dcd20) at pandas/src/parser.c:4946\n#6  0x00007fffeeb8306b in __pyx_pf_6pandas_7_parser_10TextReader___cinit__ (__pyx_v_verbose=0x7ffff7d90a80 <_Py_ZeroStruct>, __pyx_v_skip_footer=0x61e9b0, __pyx_v_skiprows=0x7fffef08fde8, __pyx_v_low_memory=0x7ffff7d90a60 <_Py_TrueStruct>, \n    __pyx_v_use_unsigned=0x7ffff7d90a80 <_Py_ZeroStruct>, __pyx_v_compact_ints=0x7ffff7d90a80 <_Py_ZeroStruct>, __pyx_v_na_values=0x7fffef08f878, __pyx_v_na_filter=0x7ffff7d90a60 <_Py_TrueStruct>, \n    __pyx_v_warn_bad_lines=0x7ffff7d90a60 <_Py_TrueStruct>, __pyx_v_error_bad_lines=0x7ffff7d90a60 <_Py_TrueStruct>, __pyx_v_usecols=0x7ffff7da4e20 <_Py_NoneStruct>, __pyx_v_dtype=0x7ffff7da4e20 <_Py_NoneStruct>, \n    __pyx_v_thousands=0x7ffff7da4e20 <_Py_NoneStruct>, __pyx_v_decimal=<optimized out>, __pyx_v_encoding=0x7ffff7da4e20 <_Py_NoneStruct>, __pyx_v_quoting=0x61e9b0, __pyx_v_quotechar=0x7ffff6b3ff08, \n    __pyx_v_doublequote=0x7ffff7d90a60 <_Py_TrueStruct>, __pyx_v_escapechar=0x7ffff7da4e20 <_Py_NoneStruct>, __pyx_v_skipinitialspace=0x7ffff7d90a80 <_Py_ZeroStruct>, __pyx_v_as_recarray=0x7ffff7d90a80 <_Py_ZeroStruct>, \n    __pyx_v_factorize=0x0, __pyx_v_converters=0xf4bf10, __pyx_v_compression=<optimized out>, __pyx_v_delim_whitespace=<optimized out>, __pyx_v_tokenize_chunksize=<optimized out>, __pyx_v_memory_map=<optimized out>, \n    __pyx_v_names=0x7ffff7da4e20 <_Py_NoneStruct>, __pyx_v_header=<optimized out>, __pyx_v_delimiter=0x7ffff7da4e20 <_Py_NoneStruct>, __pyx_v_source=0x7ffff7f385d0, __pyx_v_self=<optimized out>, __pyx_v_comment=<optimized out>, \n    __pyx_v_buffer_lines=<optimized out>) at pandas/src/parser.c:3601\n#7  __pyx_pw_6pandas_7_parser_10TextReader_1__cinit__ (__pyx_v_self=__pyx_v_self@entry=0x6dcd20, __pyx_args=__pyx_args@entry=0x7ffff7eec310, __pyx_kwds=__pyx_kwds@entry=0xf4ccc0) at pandas/src/parser.c:2481\n#8  0x00007fffeeb8699e in __pyx_tp_new_6pandas_7_parser_TextReader (t=<optimized out>, a=0x7ffff7eec310, k=0xf4ccc0) at pandas/src/parser.c:19587\n\n```\n",
      "Thanks. I'll have a look\n",
      "The underlying problem is that the new parser relies on being able to call `read` on the file handle you pass. however, after iterating, this causes:\n\n```\nValueError: Mixing iteration and read methods would lose data\n```\n\nThe case where calling `read` fails was not handled in the C code, so I did that and here's the new error message. Best I can do for this somewhat unusual case\n\n```\n/home/wesm/code/pandas/pandas/_parser.so in pandas._parser.TextReader.__cinit__ (pandas/src/parser.c:3624)()\n\n/home/wesm/code/pandas/pandas/_parser.so in pandas._parser.TextReader._get_header (pandas/src/parser.c:4594)()\n\n/home/wesm/code/pandas/pandas/_parser.so in pandas._parser.TextReader._tokenize_rows (pandas/src/parser.c:5967)()\n\n/home/wesm/code/pandas/pandas/_parser.so in pandas._parser.raise_parser_error (pandas/src/parser.c:14702)()\n\nCParserError: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n```\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 6,
    "additions": 54,
    "deletions": 14,
    "changed_files_list": [
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/parser.pyx",
      "pandas/src/parser/io.c",
      "pandas/src/parser/parser.c",
      "pandas/src/parser/parser.h"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2078,
    "reporter": "cpcloud",
    "created_at": "2012-10-16T21:15:38+00:00",
    "closed_at": "2012-11-02T21:55:14+00:00",
    "resolver": "wesm",
    "resolved_in": "b568c7459f51b2665e2e7f85aa27322768814d70",
    "resolver_commit_num": 2514,
    "title": "\"greater than\" comparison method of doesn't perform as expected on Micro objects",
    "body": "Using IPython:\n\n\n\nThey can't both be `False`. Giving the arguments to `>` to `cmp` works as expected. Not sure where the issue lies since neither `Micro` nor any of its superclasses implements any of the comparison methods except `__eq__` and `__ne__`.\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 35,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/offsets.py",
      "pandas/tseries/tests/test_offsets.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2079,
    "reporter": "ghost",
    "created_at": "2012-10-17T05:41:41+00:00",
    "closed_at": "2012-11-05T16:56:02+00:00",
    "resolver": "wesm",
    "resolved_in": "b1b85aef43f08d736204ffd37c06ed9ba2e8cbc6",
    "resolver_commit_num": 2534,
    "title": "BUG: df fails when columns arg is a list containing dupes",
    "body": "\n\n5e6db32e is a failing test for this.\n\nit looks like `_to_sdict` threads down to a call to `_convert_object_array` which builds a dict\nkeyed on column names, so dupe columns  get squashed and you end up with a mismatch \nbetween the length of the `columns` arg to `df.__init__` and the data.\n_to_sdict is not used for ndarrays so this doesn't haoppen, I was able to reuse \n`_init_ndarray` for the case of `columns` being a flat list and have things work as expected.\n\nstill, too much code touching this, better left to the core devs to decide how to handle this.\n",
    "labels": [],
    "comments": [
      "Fixing this is quite an undertaking since there's a lot of existing constructor code that assumes unique column names. I'm on it; probably get it sorted out over next day or so\n",
      "Should this work?\n\n``` python\npd.DataFrame.from_items([('a',['foo']),('a',['bar'])],columns=['a','a'])\nOut[6]: \n     a    a\n0  bar  bar\n```\n",
      "Also, forgive the nitpick, but since **sdict** is now abandoned, it would be good to rename the methods\nthat reference it, enhance readability...\n\n``` python\ndef _list_to_sdict(data, columns, coerce_float=False):\ndef _list_of_series_to_sdict(data, columns, coerce_float=False):\ndef _list_of_dict_to_sdict(data, columns, coerce_float=False):\n```\n",
      "Yeah that should work. \n"
    ],
    "events": [
      "cross-referenced",
      "commented",
      "closed",
      "commented",
      "commented",
      "reopened",
      "commented",
      "referenced"
    ],
    "changed_files": 8,
    "additions": 123,
    "deletions": 108,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/core/internals.py",
      "pandas/core/panel.py",
      "pandas/stats/tests/test_ols.py",
      "pandas/tests/test_frame.py",
      "pandas/tseries/period.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2083,
    "reporter": "wesm",
    "created_at": "2012-10-19T14:45:21+00:00",
    "closed_at": "2012-11-01T00:10:29+00:00",
    "resolver": "wesm",
    "resolved_in": "37868c43549a3fdc4c403c3d06e2132c1a30b2b9",
    "resolver_commit_num": 2497,
    "title": "min/max on datetime64-dtype Series should yield Timestamp",
    "body": "xref: -results-of-min-and-max-methods-of-pandas-series-made-of-timestamp\n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 3,
    "additions": 49,
    "deletions": 4,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/nanops.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2087,
    "reporter": "ktii",
    "created_at": "2012-10-20T12:33:17+00:00",
    "closed_at": "2012-11-04T23:03:17+00:00",
    "resolver": "wesm",
    "resolved_in": "06f74e5d06c4766ec59564374640c4758a72409d",
    "resolver_commit_num": 2531,
    "title": "Series.diff() not exact for huge numbers: approximation or mistake?",
    "body": "Series.diff() is not exact for huge numbers. This might be due to some quick approximation or just a bug/mistake.\n\nIn [40]: a = 10000000000000000\n\nIn [41]: log10(a)\nOut[41]: 16.0\n\nIn [42]: b = a + 1\n\nIn [43]: s = Series([a,b])\n\nIn [44]: s.diff()\nOut[44]: \n0   NaN\n1     0\n\nnumpy.diff() is not to blame:\n\nIn [45]: v = s.values\n\nIn [46]: v\nOut[46]: array([10000000000000000, 10000000000000001], dtype=int64)\n\nIn [47]: diff(v)\nOut[47]: array([1], dtype=int64)\n\nOne less digit is fine:\n\nIn [48]: a = 1000000000000000\n\nIn [48]: log10(a)\nOut[48]: 15.0\n\nIn [49]: b = a + 1\n\nIn [50]: s = Series([a,b])\n\nIn [51]: s.diff()\nOut[51]: \n0   NaN\n1     1\n\nWhy do I need these huge numbers? Certain timestamps (in my case VMS) have this many digits (tenth of microseconds since the year 1858 if I remember correctly).\n",
    "labels": [],
    "comments": [
      "This is a casualty of how NA values are represented in integer arrays (by upcasting the data to floating point). `diff` should be more careful in working with integer dtypes in this case\n"
    ],
    "events": [
      "commented",
      "assigned"
    ],
    "changed_files": 2,
    "additions": 8,
    "deletions": 23,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/common.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2090,
    "reporter": "wesm",
    "created_at": "2012-10-20T15:41:16+00:00",
    "closed_at": "2012-11-17T21:31:20+00:00",
    "resolver": "wesm",
    "resolved_in": "41c984425a31ba9e1b2b6fd253065056278e2f15",
    "resolver_commit_num": 2582,
    "title": "Parser unit test",
    "body": "This needs (problem and solution) to be converted to a unit test somewhere. Perhaps wait until c-parser branch has been merged into mainline to avoid merging unpleasantness.\n\n-pandas-mixed-boolean-yes-true-and-nan-columns\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 1,
    "additions": 15,
    "deletions": 1,
    "changed_files_list": [
      "pandas/io/tests/test_parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2095,
    "reporter": "wesm",
    "created_at": "2012-10-20T20:19:49+00:00",
    "closed_at": "2012-11-01T14:00:10+00:00",
    "resolver": "wesm",
    "resolved_in": "6f02df9d29128034ac90007e7b60f7fead5cee4e",
    "resolver_commit_num": 2501,
    "title": "Incorrect handling of datetime64 values in structured arrays",
    "body": "see:\n\n\n\ncf -parsing-datetime-column-from-sqlite-database\n",
    "labels": [],
    "comments": [
      "Similarly with timedelta64:\n\n```\n>>> np.array([1100, 20], dtype='timedelta64[s]')\n 36 array([0:18:20, 0:00:20], dtype=timedelta64[s])\n>>> print pd.DataFrame({'x': np.array([1100, 20], dtype='timedelta64[s]')}).to_string()\n      x\n0  1100\n1    20\n```\n",
      "It's even worse with timedelta64 because I can find no way to convince DataFrame to just let it through unmolested. Is there any way to do this? I've tried\n\n```\n>>> orig\n 74 array([0:18:20, 0:00:20], dtype=timedelta64[s])\n>>> df['x']\n 75 \n0    1100\n1      20\nName: x\n>>> df['x'] = orig\n>>> df['x']\n 77 \n0    1100\n1      20\nName: x\n>>> df['x'] = df['x'].astype('timedelta64[s]')\n>>> df['x']\n 79 \n0    1100\n1      20\nName: x\n```\n",
      "DataFrame's preference is to coerce datetime64 values to nanoseconds. I have no test coverage at all for timedelta64-- where is this data originating for you? I will do what I can\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 20,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/internals.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2096,
    "reporter": "wesm",
    "created_at": "2012-10-20T20:23:37+00:00",
    "closed_at": "2012-11-02T23:06:06+00:00",
    "resolver": "wesm",
    "resolved_in": "d9035b149b309791fc46493de7910b9143d11c61",
    "resolver_commit_num": 2518,
    "title": "Bug with boolean indexing on empty DataFrame",
    "body": "xref -edge-case-on-boolean-index-into-empty-dataframe\n\n\n\nEmpty DataFrame\n\n\n",
    "labels": [],
    "comments": [
      "This is caused by an upstream bug in NumPy (http://github.com/numpy/numpy/issues/2704). I will work around\n"
    ],
    "events": [
      "assigned",
      "commented"
    ],
    "changed_files": 3,
    "additions": 23,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/indexing.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2100,
    "reporter": "wesm",
    "created_at": "2012-10-22T00:32:03+00:00",
    "closed_at": "2012-11-01T00:20:38+00:00",
    "resolver": "wesm",
    "resolved_in": "0a04723027919a143ff52c42b0d8aec5a919cecf",
    "resolver_commit_num": 2498,
    "title": "Odd unstack failure",
    "body": "Set day,time, smoker as index, unstack(2)\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 3,
    "additions": 25,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/reshape.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2101,
    "reporter": "jseabold",
    "created_at": "2012-10-22T13:14:30+00:00",
    "closed_at": "2012-11-03T00:22:54+00:00",
    "resolver": "wesm",
    "resolved_in": "1b23b6fda648ec864ecc7865b385ba525f5d6182",
    "resolver_commit_num": 2519,
    "title": "Bug in set_index or drop",
    "body": "I'm not sure if the bug is in the drop or that it shouldn't have let me set this MultiIndex since it's non-unique. var1 is just a combination of var2 and var3. In any event the result given back by this is garbage. I wanted to drop all the rows where the count of var1 == 1. Since there's not to my knowledge a way to drop variables without setting them to an index, I tried to do this without thinking\n\n\n",
    "labels": [],
    "comments": [
      "Garbage indeed.\n\nMO <--> #2064\n\n``` python\ndf.drop(df.index[df['var1'].isin(drop_idx)])\n\n  var1 var2 var3  var4\n0  x-a    x    a   1.5\n1  x-a    x    a   1.2\n2  z-c    z    c   3.1\n3  x-a    x    a   4.1\n4  x-b    x    b   5.1\n5  x-b    x    b   4.1\n6  x-b    x    b   2.2\n7  y-a    y    a   1.2\n8  z-b    z    b   2.1\n```\n",
      "Ah, nice one-liner. Missed your comment on my other issue.\n",
      "@lodagro  I think it needs to be `df.drop(df.index[df['var1'].isin(drop_idx.index)])` right?\n\n@jseabold doesn't seem like a problem in drop (which just calls reindex). I'm checking it out now.\n\nyeah, looks like it should have raised Exception for non-unique here.\n",
      "@changhiskhan indeed!\nProbably got confused, by the fact that membership testing on Series is dict like (so the keys matter) but for isin testing on a series, the values matter. (And not checking the result thoroughly.)\n\n``` python\nIn [55]: drop_idx\nOut[55]: \nvar1\ny-a     1\nz-b     1\nz-c     1\n\nIn [56]: s = pd.Series(['y-a', 1])\n\nIn [57]: s.isin(drop_idx)\nOut[57]: \n0    False\n1     True\n\nIn [59]: for x in ['y-a', 1]:\n   ....:     print x, x in drop_idx\n   ....:     \ny-a True\n1 False\n```\n",
      "I think Skipper's code should work. I'll have a look\n",
      "Maybe you can just call take instead of reindex and get around the non-uniqueness problem.\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "assigned",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 40,
    "deletions": 20,
    "changed_files_list": [
      "pandas/core/generic.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2113,
    "reporter": "leonbaum",
    "created_at": "2012-10-24T17:41:03+00:00",
    "closed_at": "2012-10-24T18:16:29+00:00",
    "resolver": "wesm",
    "resolved_in": "8cd93d3fe80caa1dbdf17f646563c1d337c180fe",
    "resolver_commit_num": 2478,
    "title": "Random seg fault with groupby",
    "body": "I came across this seg fault when I was trying to figure out the syntax to get column means.  The below example triggers the seg fault, but strangely only about 30% of the time.  About 10% of the time I get a reasonable error.  The randomness of the result is very strange to me because the same command was run every time!\n\nI am using the latest pandas and numpy from git.  I have ECC memory and have never seen any hardware triggered issues.\n\n\n",
    "labels": [],
    "comments": [
      "I'm able to reproduce; will investigate, thanks.\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 2,
    "additions": 10,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2114,
    "reporter": "mrjbq7",
    "created_at": "2012-10-24T18:53:20+00:00",
    "closed_at": "2012-11-04T22:04:13+00:00",
    "resolver": "wesm",
    "resolved_in": "86ab2e9631faf83f9e9bb3174273249ade598f45",
    "resolver_commit_num": 2529,
    "title": "rolling_mean and rolling_sum produce negative output from positive input",
    "body": "I have an array of non-negative numbers, that when used with `rolling_sum` or `rolling_mean` produce an output array that has a small negative number in it.\n\nThe test looks like this: \n\n\n\nIt requires a small binary array to reproduce, because of the floating point numbers (so I created a gist: ).\n\nYou can run the test case:\n\n\n\nI made sure this bug affects the most current version of Pandas:\n\n\n",
    "labels": [],
    "comments": [
      "It looks like `bottleneck` doesn't have this problem...\n",
      "I used a similar approach to `bottleneck` where the new value is added first before the previous value is subtracted and it fixes this particular bug...\n\n``` diff\ndiff --git a/pandas/src/moments.pyx b/pandas/src/moments.pyx\nindex 503a63c..9b5b621 100644\n--- a/pandas/src/moments.pyx\n+++ b/pandas/src/moments.pyx\n@@ -175,16 +175,16 @@ def roll_sum(ndarray[double_t] input, int win, int minp):\n     for i from minp - 1 <= i < N:\n         val = input[i]\n\n+        if val == val:\n+            nobs += 1\n+            sum_x += val\n+\n         if i > win - 1:\n             prev = input[i - win]\n             if prev == prev:\n                 sum_x -= prev\n                 nobs -= 1\n\n-        if val == val:\n-            nobs += 1\n-            sum_x += val\n-\n         if nobs >= minp:\n             output[i] = sum_x\n         else:\n@@ -218,16 +218,16 @@ def roll_mean(ndarray[double_t] input,\n     for i from minp - 1 <= i < N:\n         val = input[i]\n\n+        if val == val:\n+            nobs += 1\n+            sum_x += val\n+\n         if i > win - 1:\n             prev = input[i - win]\n             if prev == prev:\n                 sum_x -= prev\n                 nobs -= 1\n\n-        if val == val:\n-            nobs += 1\n-            sum_x += val\n-\n         if nobs >= minp:\n             output[i] = sum_x / nobs\n         else:\n```\n"
    ],
    "events": [
      "commented",
      "commented",
      "assigned"
    ],
    "changed_files": 4,
    "additions": 54,
    "deletions": 28,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/src/moments.pyx",
      "pandas/stats/tests/test_moments.py",
      "pandas/tseries/resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2117,
    "reporter": "lodagro",
    "created_at": "2012-10-25T13:06:57+00:00",
    "closed_at": "2012-11-03T19:11:45+00:00",
    "resolver": "wesm",
    "resolved_in": "189d04cc4ae69c5afb71426942a0362e29f30518",
    "resolver_commit_num": 2525,
    "title": "Case where xs(copy=False) does not return view.",
    "body": "from [stackoverflow](-slice-on-hierarchical-index-without-a-copy/)\n\n\n",
    "labels": [],
    "comments": [
      "It seems that xs(copy=False) does not work with MultiIndex.\n",
      "This can't be fixed. Would an exception be preferable?\n"
    ],
    "events": [
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 12,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2119,
    "reporter": "wesm",
    "created_at": "2012-10-25T13:49:27+00:00",
    "closed_at": "2012-11-01T00:42:14+00:00",
    "resolver": "wesm",
    "resolved_in": "fe5cc26a05ce4a44cc7512ace4d1def16b0c1e93",
    "resolver_commit_num": 2499,
    "title": "str.split('|') failure",
    "body": "Look at Series with strings like 'A|B|C'\n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 3,
    "additions": 36,
    "deletions": 14,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/strings.py",
      "pandas/tests/test_strings.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2120,
    "reporter": "nicktp",
    "created_at": "2012-10-25T14:00:58+00:00",
    "closed_at": "2012-10-31T20:23:03+00:00",
    "resolver": "lodagro",
    "resolved_in": "b3b6076156c3a92707eff027159927a24a7820dd",
    "resolver_commit_num": 59,
    "title": "Series 'print' output truncation",
    "body": "When printing a Series (I haven't checked if the same applies to a DataFrame), the output is truncated in such a way as to change the meaning.\n\n\n\nOddly, changing a single power to 9 rather than 10 makes the whole thing display properly:\n\n\n",
    "labels": [],
    "comments": [
      "The problem appears to be in pandas/core/format.py in the function _trim_zeros().  This function, as I understand it, checks whether it can safely remove a zero from the end of all the non-na elements and if it can, does so.  That process is repeated until it can no longer safely remove a zero from the end of any string.\n\nIn order to make it work correctly with numbers such as '123.45e+100' we can just check fro the presence of an 'e' in any of the strings.  This can be done with:\n\n``` python\nany(['e' in x for x in non_na])\n```\n\nie it can just be added to the function _cond(values):\n\n``` python\ndef _cond(values):\n    non_na = [x for x in values if x != na_rep]\n    return len(non_na) > 0 and all([x.endswith('0') for x in non_na]) and \\\n         not(any(['e' in x for x in non_na]))\n```\n"
    ],
    "events": [
      "commented",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 2,
    "additions": 18,
    "deletions": 3,
    "changed_files_list": [
      "pandas/core/format.py",
      "pandas/tests/test_format.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2128,
    "reporter": "Lamarth",
    "created_at": "2012-10-26T02:22:14+00:00",
    "closed_at": "2012-11-04T22:47:05+00:00",
    "resolver": "wesm",
    "resolved_in": "18788f4239f977147d80620352c97ba9f99fdb13",
    "resolver_commit_num": 2530,
    "title": "ewma adjust incorrectly ticked for NaNs",
    "body": "series = pandas.Series([1., numpy.NaN, numpy.NaN, 1.], index=pandas.DatetimeIndex(start='2000-01-01', periods=4, freq='T'))\npandas.ewma(series, com=5)\n\n2000-01-01 00:00:00    1.000000\n2000-01-01 00:01:00    0.545455\n2000-01-01 00:02:00    0.545455\n2000-01-01 00:03:00    1.000000\n\nAlmost right - but clearly the adjust factor is ticked in the wrong spot.\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 15,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/src/moments.pyx",
      "pandas/stats/tests/test_moments.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2133,
    "reporter": "gerigk",
    "created_at": "2012-10-26T15:56:21+00:00",
    "closed_at": "2012-11-24T23:06:35+00:00",
    "resolver": "wesm",
    "resolved_in": "adc923895e6935a7ca017887cc74840e004ec2b1",
    "resolver_commit_num": 2526,
    "title": "groupby.first() casts datetime64[ns] series to 'object' with 'long\" elements",
    "body": "\n",
    "labels": [],
    "comments": [
      "There's a couple issues here. looking into it\n",
      "Dear lord don't look at what I had to do to fix this in a simple/dirty way\n",
      "@wesm I don't know whether this reopens the issue:\n\nI received the following error after updating pandas:\n\n```\nind_part = df[time_dim].groupby(level=group_dims).first()\n  File \"/usr/local/lib/python2.7/dist-packages/pandas-0.9.2.dev_4519310-py2.7-linux-x86_64.egg/pandas/core/groupby.py\", line 38, in f\n    result = result.convert_objects()\nAttributeError: 'Series' object has no attribute 'convert_objects'\n```\n\nI fixed it by using \n\n```\nDataFrame(df[time_dim]).groupby(level=group_dims).first()\n```\n\nwhich kind of lets me think that your fix only works for DataFrames but not for Series.\n",
      "I'll have a look\n",
      "Please don't look at this one either :)\n",
      "closed via a28a5cc\n"
    ],
    "events": [
      "commented",
      "closed",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "reopened",
      "commented",
      "referenced",
      "commented"
    ],
    "changed_files": 6,
    "additions": 69,
    "deletions": 28,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/internals.py",
      "pandas/src/inference.pyx",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2139,
    "reporter": "changhiskhan",
    "created_at": "2012-10-27T15:30:35+00:00",
    "closed_at": "2012-12-09T22:41:49+00:00",
    "resolver": "wesm",
    "resolved_in": "36ef22710c2660532eca8f55f385c652b022a26e",
    "resolver_commit_num": 2718,
    "title": "explicitly setting Series.index should convert to TimeSeries as necessary",
    "body": "",
    "labels": [],
    "comments": [
      "Oof. But probably I guess\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 4,
    "additions": 21,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/series.py",
      "pandas/sparse/series.py",
      "pandas/src/properties.pyx",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2140,
    "reporter": "wesm",
    "created_at": "2012-10-27T15:31:13+00:00",
    "closed_at": "2012-11-03T16:19:38+00:00",
    "resolver": "wesm",
    "resolved_in": "ca40b48150cbe45db23281c3fd0bdad39073cace",
    "resolver_commit_num": 2521,
    "title": "Time rule inference regression with WOM- frequencies",
    "body": "This needs to be implemented (hopefully) in the frequency inference code\n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 3,
    "additions": 27,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/frequencies.py",
      "pandas/tseries/tests/test_frequencies.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2144,
    "reporter": "wesm",
    "created_at": "2012-10-29T16:16:42+00:00",
    "closed_at": "2012-11-01T00:56:44+00:00",
    "resolver": "wesm",
    "resolved_in": "5ed523c22e4447e89614ad1311334d1ab69adcc7",
    "resolver_commit_num": 2500,
    "title": "__repr__ display of dict elements misleading",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 22,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/common.py",
      "pandas/tests/test_format.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2148,
    "reporter": "wesm",
    "created_at": "2012-11-01T00:09:21+00:00",
    "closed_at": "2012-11-02T21:27:55+00:00",
    "resolver": "wesm",
    "resolved_in": "fbb5c5e9fa506ee9379b72aae2f67db8063d1a92",
    "resolver_commit_num": 2513,
    "title": "Series.iget / DataFrame.iget_value doesn't box Timestamps",
    "body": "Unclear how important this is\n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 4,
    "additions": 17,
    "deletions": 7,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/series.py",
      "pandas/src/engines.pyx",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2156,
    "reporter": "zachcp",
    "created_at": "2012-11-01T19:39:08+00:00",
    "closed_at": "2012-11-01T20:53:26+00:00",
    "resolver": "lodagro",
    "resolved_in": "88f4e53e8288fd3238aba53ceaf273fb0a219305",
    "resolver_commit_num": 60,
    "title": "Typo in the Documentation",
    "body": "-docs/dev/io.html?highlight=to_csv\n\nin the To_csv section:\n\n\n\nshould be\n\n\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 1,
    "additions": 1,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/io.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2163,
    "reporter": "wesm",
    "created_at": "2012-11-02T15:08:16+00:00",
    "closed_at": "2012-11-02T22:03:26+00:00",
    "resolver": "wesm",
    "resolved_in": "171633b67079cee101b22a9956e9138b2d3e47f0",
    "resolver_commit_num": 2515,
    "title": "DataFrame.to_panel discards minor index name",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 4,
    "additions": 26,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/core/panel.py",
      "pandas/tests/test_panel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2164,
    "reporter": "wesm",
    "created_at": "2012-11-02T15:10:01+00:00",
    "closed_at": "2012-11-02T22:47:26+00:00",
    "resolver": "wesm",
    "resolved_in": "d90d41afa39436352249157ee3d4d790e2581f49",
    "resolver_commit_num": 2517,
    "title": "Panel.shift with negative numbers broken",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 5,
    "additions": 36,
    "deletions": 20,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/common.py",
      "pandas/core/frame.py",
      "pandas/core/panel.py",
      "pandas/tests/test_panel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2179,
    "reporter": "ghost",
    "created_at": "2012-11-05T12:58:16+00:00",
    "closed_at": "2012-11-24T18:49:32+00:00",
    "resolver": "wesm",
    "resolved_in": "f64ccf0860b75d6c8e58b442c96bae62de41080f",
    "resolver_commit_num": 2601,
    "title": "BUG: df.from_records() is all kinds of broken",
    "body": "- the `columns` argument gets clobbered:\n  In [13]: pd.DataFrame.from_records({1:[\"foo\"],2:[\"bar\"]},columns=['a','b']).columns\n  Out[13]: Index([1, 2], dtype=int64)\n- if `index` is specified, and `result_index` computed, it will get clobbered\n  later on.\n- if `index` is specified as a list of labels, with the first few matching columns names\n  and the others not, then sdict will get mutilated, because the removal of columns\n  occurs within the try clause rather then after success.\n  **EDIT** - ignore, I misread the code.\n- There's duplication against the main Dataframe ctor which also accepts dicts and arrays, \n  and The exclusion  and float coercsion which are unique to `from_records()` would be useful \n  to have generally available in the main ctor anyway.\n- It's unclear if the `columns` argument should be specified relative to the original\n  data, or relative to the data modulo excluded columns\n- Doesn't support duplicate column names (Although that's checked with\n  a warning)\n- the docstring specifies the datatypes for `data` , which do not include a dict.\n  But the code specifically checks and handles dict input, This is a minor thing, but\n  using `columns` along with dict is not well defined because of key (non-)ordering,\n  so the original docstring spec seems more sane.\n",
    "labels": [],
    "comments": [
      "Indeed this code has been fairly neglected. I'll see if I can easily fix some of the issues you raised\n",
      "A major issue here is that the index argument is overloaded to take either index field names or an array _to be used_ as the index. This was a pretty bone-headed decision made a long time ago\n",
      "agreed, that was a bit too flexible. But if the loop for removing the columns\ngets put in an else caluse to the try/except that's taken care of.\n\nI couldn't figure out a way to get the main ctor to plug data in as columns rather \nthen rows after sorting out the rest.\n",
      "I'm adding a docs clarification about what `columns` means (i.e: renaming columns is not possible with this function at the moment) and leaving this issue open for more cleanup at a later time (and fixing the bug from the first bullet point). \n",
      "I'm pretty sure if you move the if/else setting result_index above \nthe check on `index` , you can close the 2rd as well.\n",
      "Test case?\n",
      "no need, I see you've removed the if entirely.\n",
      "What still needs to get fixed here?\n",
      "support for duplicate columns is still missing. other then that I think it's good.\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "referenced",
      "commented",
      "commented",
      "commented",
      "referenced",
      "commented",
      "commented",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 65,
    "deletions": 32,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2190,
    "reporter": "lbeltrame",
    "created_at": "2012-11-07T16:16:20+00:00",
    "closed_at": "2012-11-09T18:05:36+00:00",
    "resolver": "wesm",
    "resolved_in": "6b0939009c2d469fbdd785d861447a70edfb0ac9",
    "resolver_commit_num": 2555,
    "title": "Add exponentiation function similar to add() or sub()",
    "body": "Currently if I exponentiate a DataFrame with a Series, the indexing joining logic is performed on the columns. This means the following:\n\n\n\nIOW, the joining happened on columns and not on rows. The only way I know to do this is to do `(data.T ** series).T`, which is kind of ugly. The addition of a pow() or exponentiate() function would allow me to use a specific axis to exponentiate and avoid this problem.\n",
    "labels": [],
    "comments": [
      "Done deal:\n\n```\nIn [6]: data.pow(series, axis=0)\nOut[6]: \n     a    b\nax   1  529\nbx   4   16\ncx  27  125\ndx NaN  NaN\n```\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 3,
    "additions": 23,
    "deletions": 10,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2199,
    "reporter": "ghost",
    "created_at": "2012-11-08T22:38:32+00:00",
    "closed_at": "2012-11-09T17:00:35+00:00",
    "resolver": "wesm",
    "resolved_in": "d5d31f19fa3831cce263ad804c4718f0052900d2",
    "resolver_commit_num": 2552,
    "title": "Curious: df.ix[False] returns the first row",
    "body": "\n\nDon't know if this is a bug or intentiional.\n",
    "labels": [],
    "comments": [
      "Getting coerced to 0 I guess. Buglet\n",
      "In [9]: com.is_integer(True)\nOut[9]: True\n",
      "In the Python source (`Include/boolobject.h` I believe) you have,\n\n``` c\ntypedef PyIntObject PyBoolObject;\n```\n\nthus it is a subclass of `int` and will be auto-coerced to 0.\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 4,
    "additions": 21,
    "deletions": 6,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/src/engines.pyx",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2200,
    "reporter": "ghost",
    "created_at": "2012-11-08T23:13:36+00:00",
    "closed_at": "2012-11-09T17:40:06+00:00",
    "resolver": "wesm",
    "resolved_in": "27e34a4daab82e51312714794db78d2b860327f8",
    "resolver_commit_num": 2554,
    "title": "Breakage when setting MultiIndex with a mutable sequence",
    "body": "\n\nIf this acceptable usage and index entries must be immutable, failing early would be good.\n\n**Note:** If you get an `UnboundLocalError` Exception, That's an un-related issue, PR fix as soon\nas travis is green.\n",
    "labels": [],
    "comments": [],
    "events": [
      "cross-referenced"
    ],
    "changed_files": 4,
    "additions": 30,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/index.py",
      "pandas/core/internals.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2208,
    "reporter": "wesm",
    "created_at": "2012-11-09T20:01:58+00:00",
    "closed_at": "2012-11-17T21:20:59+00:00",
    "resolver": "wesm",
    "resolved_in": "54c81e2261dda7bd142884328e5825c9ee8a2ef1",
    "resolver_commit_num": 2581,
    "title": "Test case for skipinitialspace",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 1,
    "additions": 18,
    "deletions": 2,
    "changed_files_list": [
      "pandas/io/tests/test_parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2209,
    "reporter": "wesm",
    "created_at": "2012-11-09T20:19:31+00:00",
    "closed_at": "2012-11-30T23:58:49+00:00",
    "resolver": "wesm",
    "resolved_in": "7306b65ccc2ff5abef1af70d5e0cc054eb1b62ed",
    "resolver_commit_num": 2657,
    "title": "Ability to handle fractional seconds in date_converters",
    "body": "cc @michaelaye\n\n\n",
    "labels": [],
    "comments": [
      "You said in your tutorial that you want to support up to nanosecond time precision. But as I see, you are using the standard `datetime.datetime` class for parsing times, where it does not seem possible, as microseconds has to be an int as well. How should I fix it? String split the seconds string in 2 entities by the '.' and provide it to datetime?\n",
      "What about that? Of course string parsing must be too slow, so I tried to avoid it. Tell me if I should do something different.\n",
      "FYI: I confirmed that this does not fail with integer seconds that do not have microseconds data.\n"
    ],
    "events": [
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "commented",
      "referenced",
      "commented"
    ],
    "changed_files": 2,
    "additions": 32,
    "deletions": 4,
    "changed_files_list": [
      "pandas/io/tests/test_date_converters.py",
      "pandas/src/inference.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2220,
    "reporter": "ghost",
    "created_at": "2012-11-11T12:26:32+00:00",
    "closed_at": "2012-11-13T23:51:13+00:00",
    "resolver": "wesm",
    "resolved_in": "cda2084b50b91413fc96c6a2749561ead284f474",
    "resolver_commit_num": 2561,
    "title": "1 ** Sparse df with nans makes sp_values() produce strange results under df[] vs df.take([])",
    "body": "\n- I'm not familiar with the block management code so I can't figure this out\n  with reasonable effort right now.\n- as a side issue, it looks like 1 *\\* nan == 1 in this context, seems suspect.\n",
    "labels": [],
    "comments": [],
    "events": [
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 22,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_sparse.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2226,
    "reporter": "quintusdias",
    "created_at": "2012-11-11T19:42:34+00:00",
    "closed_at": "2012-11-12T04:37:38+00:00",
    "resolver": "justincjohnson",
    "resolved_in": "a72d886ed1d9b715844c2742a7b06b47abdf9e21",
    "resolver_commit_num": 0,
    "title": "Better error message in pandas.core.index",
    "body": "If sortlevel is called with an out-of-range level, the error message is a bit confusing, particularly if the level is equal to the maximum number of levels.\n\n\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 1,
    "additions": 2,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/index.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2227,
    "reporter": "ghost",
    "created_at": "2012-11-11T19:45:04+00:00",
    "closed_at": "2012-11-14T16:50:29+00:00",
    "resolver": "wesm",
    "resolved_in": "a97b9b55805725d9716270e52f05fab65990246d",
    "resolver_commit_num": 2565,
    "title": "SparseDataFrame.icol(j) returns Series rather then SparseSeries",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "cross-referenced",
      "assigned"
    ],
    "changed_files": 5,
    "additions": 18,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/sparse/frame.py",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_sparse.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2228,
    "reporter": "ghost",
    "created_at": "2012-11-11T19:52:49+00:00",
    "closed_at": "2012-11-14T00:10:35+00:00",
    "resolver": "wesm",
    "resolved_in": "bf538a960379358615f64c67ddf02544b86148e9",
    "resolver_commit_num": 2563,
    "title": "df.icol([0]) fails when df has non-unique column labels",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "cross-referenced",
      "assigned"
    ],
    "changed_files": 2,
    "additions": 9,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2234,
    "reporter": "grsr",
    "created_at": "2012-11-12T13:56:11+00:00",
    "closed_at": "2012-11-14T17:44:28+00:00",
    "resolver": "wesm",
    "resolved_in": "60e69a3f2baed2b664bdc93d7d1673412f31d5dd",
    "resolver_commit_num": 2566,
    "title": "Can't create a DataFrame from an empty Series",
    "body": "For a project I am working on it would be convenient if you could create a DataFrame from an empty Series. I have some library code that will create a Series object constructed from a dict, most of the time this dict will have some entries, but occasionally it does not. Currently you can create an empty DataFrame by not supplying anything to the constructor, and you can create a DataFrame or a Series from an empty dict, but if you try to create a DataFrame from an Series constructed from an empty dict pandas throws an AssertionError. I can code around this easily enough, but it would be preferable just to return an empty DataFrame. Example code below:\n\n\n\nCheers,\n\nGraham\n",
    "labels": [],
    "comments": [
      "Actually, on further inspection, it seems the problem is creating a DataFrame from an empty Series that has been given a name, a Series constructed only from an empty dict can be used to construct a DataFrame. I guess the issue is something to do with constructing the list of column names in DataFrame, but I can successfully create a DataFrame from an empty dict and explicitly include some columns, and I get what I want, namely a DataFrame with no rows but with entries in the column index. It would be handy if I could get the same behaviour from my existing code, i.e. creating a DataFrame from an empty Series which has a name. \n\nThis isn't a major issue now though as I didn't realise that you can join a Series with a DataFrame, which is why I was converting my Series into DataFrame in the first place, and if you join a (populated) DataFrame with an empty Series with a name then pandas does exactly what I want in that it creates a column in the resulting DataFrame with NA entries for all the rows.\n\n``` python\nIn [19]: pandas.__version__\nOut[19]: '0.9.0'\n\nIn [20]: DataFrame(Series({})) # works\nOut[20]: \nEmpty DataFrame\nColumns: array([], dtype=int64)\nIndex: array([], dtype=object)\n\nIn [21]: DataFrame({}, columns = ['foo']) # works, and adds the 'foo' column\nOut[21]: \nEmpty DataFrame\nColumns: array([foo], dtype=object)\nIndex: array([], dtype=object)\n\nIn [31]: df = DataFrame({'foo': {'item1': 1, 'item2': 2}, 'bar': {'item2': 3}}) # create a populated df\n\nIn [32]: s = Series({},name = 'baz') # and an empty Series with a name\n\nIn [33]: df.join(s)\nOut[33]: \n       bar  foo  baz\nitem1  NaN    1  NaN\nitem2    3    2  NaN\n```\n",
      "OK, I understand what's going on now. I can indeed create a DataFrame from an empty Series, but I have to do so by passing a dict with the name of the Series as the key and the Series as the corresponding value. Otherwise the Series is being interpreted as a numpy ndarray rather than a pandas Series object in the DataFrame constructor. This isn't a bug, so I'm going to close this issue. Thanks for reading anyway!\n\n``` python\nIn [56]: s = Series({}, name = 'foo')\n\nIn [57]: DataFrame({s.name: s})\nOut[57]: \nEmpty DataFrame\nColumns: array([foo], dtype=object)\nIndex: array([], dtype=object)\n```\n",
      "I actually might have expected what you typed to work. I'll reopen until someone can have a look. \n"
    ],
    "events": [
      "commented",
      "commented",
      "closed",
      "commented",
      "reopened"
    ],
    "changed_files": 3,
    "additions": 12,
    "deletions": 6,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2236,
    "reporter": "jpindi",
    "created_at": "2012-11-12T20:34:27+00:00",
    "closed_at": "2012-11-13T21:54:20+00:00",
    "resolver": "wesm",
    "resolved_in": "d56d0e691de8e3123575ebe80c2ad4d63328b7cb",
    "resolver_commit_num": 2557,
    "title": "Error converting DataFrame with duplicate columns to ndarray",
    "body": "Installed latest version of pandas 0.9.0 in case this was an error\nTrying to read Excel file. That part seems ok.\nOriginally, I was trying iteritems() for each row of the pandas dataframe, as the id_company had to be verified against a mysql database (code not included). Same/similar error message to putting it into a tuple (code is below). Error message follows.\n\nNote there is a .reindex() but it didn't work before, either. The reindex() was kind of a hail-mary.\n\nAs a work-around, I'm probably going to simply import from my target sql and do a join. I'm concerned because of the size of the datasets.\n\n\n",
    "labels": [],
    "comments": [
      "Any chance you could e-mail me the excel file? wesmckinn at gmail\n",
      "Done! Just a note that I am using Python 2.7 and the Excel file was created using Excel 2010 on Windows. Thx.\n",
      "The reindexing error is a red herring. I updated the title to reflect the underlying bug and I'll take a look now.\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 27,
    "deletions": 5,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/internals.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2237,
    "reporter": "khughitt",
    "created_at": "2012-11-12T22:43:41+00:00",
    "closed_at": "2012-11-14T19:36:35+00:00",
    "resolver": "wesm",
    "resolved_in": "3f4f2855652cb1454f37f2a6f27104e7968d8fe5",
    "resolver_commit_num": 2567,
    "title": "parallel_coordinates incorrect auto legend",
    "body": "Just tried the latest version of the parallel coordinates plot, but the default legend appears to be incorrect: instead of just choosing the four expected groups, one legend entry is included per row, many of them set to \"None\".\n\nTo reproduce, follow steps in the [pandas guide](-docs/stable/visualization.html#parallel-coordinates).\n## System info:\n\n Arch Linux 64-bit\n\n pandas 0.9.1rc1 (git)\n numpy 1.7.0b2\n matplotlib 1.2.0\n## Screenshot\n\n![pandas screenshot]()\n",
    "labels": [],
    "comments": [
      "I can't reproduce your issue on my current setup:\nosx 10.8.2\npandas - github master\nnumpy - 1.6.1\nmatplotlib - 1.1.0\n\nThe dev docs are fine as well so it's not an OS issue. I'll try installing mpl 1.2.0 and see what's going on. You sure you have the latest version of pandas from github master right?\n\nAlso what mpl backend are you using?\n",
      "Yep -- I tried initially with 0.9.0 and got the error so I pulled the latest version from git and tried that; same error (I checked pandas.**version** to make sure right version was being loaded.)\n\nFor back-end I'm currently using Qt4Agg, but I tried with Gtk3Agg just now and got the same result.\n\nMatplotlib made a lot of changes between 0.9.x, 1.1, and 1.2 so it wouldn't surprise me if that is the culprit.\n"
    ],
    "events": [
      "commented",
      "assigned",
      "commented"
    ],
    "changed_files": 2,
    "additions": 6,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tools/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2245,
    "reporter": "craustin",
    "created_at": "2012-11-14T15:03:51+00:00",
    "closed_at": "2012-11-14T16:34:16+00:00",
    "resolver": "wesm",
    "resolved_in": "bd34795c3a5767971ada339684de2ef7500f1045",
    "resolver_commit_num": 2564,
    "title": "Exception during resample w/ tz",
    "body": "from pandas import date_range, Series\nidx = date_range('2001-09-20 15:59','2001-09-20 16:00', freq='T', tz='Australia/Sydney')\ns = Series([1,2], index=idx)\ns.resample('D')\n## In [4]: s.resample('D')\n\nValueError                                Traceback (most recent call last)\n<ipython-input-4-5ce76d689c21> in <module>()\n----> 1 s.resample('D')\n\nQ:\\GAARD\\Prod\\Apps\\Pandas64\\4.36\\pandas\\core\\generic.pyc in resample(self, rule, how, axis, fill_method, closed,\nlabel, convention, kind, loffset, limit, base)\n    187                               fill_method=fill_method, convention=convention,\n    188                               limit=limit, base=base)\n--> 189         return sampler.resample(self)\n    190\n    191     def first(self, offset):\n\nQ:\\GAARD\\Prod\\Apps\\Pandas64\\4.36\\pandas\\tseries\\resample.pyc in resample(self, obj)\n     66\n     67         if isinstance(axis, DatetimeIndex):\n---> 68             rs = self._resample_timestamps(obj)\n     69         elif isinstance(axis, PeriodIndex):\n     70             offset = to_offset(self.freq)\n\nQ:\\GAARD\\Prod\\Apps\\Pandas64\\4.36\\pandas\\tseries\\resample.pyc in _resample_timestamps(self, obj)\n    180         axlabels = obj._get_axis(self.axis)\n    181\n--> 182         binner, grouper = self._get_time_grouper(obj)\n    183\n    184         # Determine if we're downsampling\n\nQ:\\GAARD\\Prod\\Apps\\Pandas64\\4.36\\pandas\\tseries\\resample.pyc in _get_time_grouper(self, obj)\n     95\n     96         if self.kind is None or self.kind == 'timestamp':\n---> 97             binner, bins, binlabels = self._get_time_bins(axis)\n     98         else:\n     99             binner, bins, binlabels = self._get_time_period_bins(axis)\n\nQ:\\GAARD\\Prod\\Apps\\Pandas64\\4.36\\pandas\\tseries\\resample.pyc in _get_time_bins(self, axis)\n    125\n    126         # general version, knowing nothing about relative frequencies\n--> 127         bins = lib.generate_bins_dt64(ax_values, bin_edges, self.closed)\n    128\n    129         if self.closed == 'right':\n\nQ:\\GAARD\\Prod\\Apps\\Pandas64\\4.36\\pandas\\lib.pyd in pandas.lib.generate_bins_dt64 (pandas\\src\\tseries.c:61444)()\n\nValueError: Values falls after last bin\n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 3,
    "additions": 24,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2247,
    "reporter": "wesm",
    "created_at": "2012-11-14T17:58:47+00:00",
    "closed_at": "2012-11-28T04:09:40+00:00",
    "resolver": "wesm",
    "resolved_in": "8316a1e52f3680c51011347bea2332dc382979b5",
    "resolver_commit_num": 2637,
    "title": "Detection of integer overflows in parsers",
    "body": "xref: -pandas-dataframe-column-as-string-not-int\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 38,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/parser.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2248,
    "reporter": "craustin",
    "created_at": "2012-11-14T18:01:54+00:00",
    "closed_at": "2012-11-14T20:22:25+00:00",
    "resolver": "wesm",
    "resolved_in": "261a0c2ed9f44c7a6f1a74c84190c14b7d96a1dc",
    "resolver_commit_num": 2568,
    "title": "Exception during tz_localize w/ empty Series",
    "body": "from pandas import Series\ns = Series()\ns.tz_localize('UTC')\n\nQ:\\GAARD\\Prod\\Apps\\Pandas64\\4.36\\pandas\\core\\series.pyc in tz_localize(self, tz, copy)\n   2730         localized : TimeSeries\n   2731         \"\"\"\n-> 2732         new_index = self.index.tz_localize(tz)\n   2733\n   2734         new_values = self.values\n\nAttributeError: 'Index' object has no attribute 'tz_localize'\n",
    "labels": [],
    "comments": [
      "Got to love the ol' corner cases\n"
    ],
    "events": [
      "commented",
      "assigned"
    ],
    "changed_files": 4,
    "additions": 25,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/series.py",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2251,
    "reporter": "ghost",
    "created_at": "2012-11-14T20:48:59+00:00",
    "closed_at": "2012-11-15T00:35:25+00:00",
    "resolver": "wesm",
    "resolved_in": "aed58967d566ab2e810adc37a7b43b9c9f76d7ee",
    "resolver_commit_num": 2569,
    "title": "BUG: icol() doesn't preserve index type on sparse DataFrames",
    "body": "\n",
    "labels": [],
    "comments": [
      "Haven't got a fix for this.\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 2,
    "additions": 37,
    "deletions": 0,
    "changed_files_list": [
      "pandas/sparse/frame.py",
      "pandas/sparse/tests/test_sparse.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2259,
    "reporter": "ghost",
    "created_at": "2012-11-15T18:02:29+00:00",
    "closed_at": "2012-12-14T02:41:34+00:00",
    "resolver": "wesm",
    "resolved_in": "25d522c1d426b1005fed5b9ddbde7c4b13d2af70",
    "resolver_commit_num": 2746,
    "title": "icol([0]) fails with non-unique, integer columns",
    "body": "\n",
    "labels": [],
    "comments": [
      "bump\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 3,
    "additions": 10,
    "deletions": 5,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2263,
    "reporter": "jassinm",
    "created_at": "2012-11-15T19:20:13+00:00",
    "closed_at": "2012-11-28T00:43:03+00:00",
    "resolver": "wesm",
    "resolved_in": "808c30c7cc6586f36aae1183c66ca19b09d7f75d",
    "resolver_commit_num": 2631,
    "title": "read_csv dateparser empty date's are assigned today's date",
    "body": "Hi, \n\n\n\nsecond row should be None.\n\nis this the default behaviour? \nthis is a quick fix for me\n\n\n",
    "labels": [],
    "comments": [
      "I believe this is fixed now on master:\n\n```\nIn [2]: import pandas as pd\n\nIn [3]: paste\nimport StringIO\ns = StringIO.StringIO(\"Date, test\\n2012-01-01, 1\\n,2\")\npd.read_csv(s, parse_dates=[\"Date\"])\n## -- End pasted text --\nOut[3]: \n         Date   test\n0  2012-01-01      1\n1         nan      2\n```\n\nCan you try updating?\n",
      "Note however in a pathological case:\n\n```\nIn [12]: pd.read_csv(s, parse_dates=[\"Date\"], na_filter=False)\nOut[12]: \n                  Date   test\n0  2012-01-01 00:00:00      1\n1  2012-11-18 00:00:00      2\n```\n\nI'm as yet unconvinced about being beholden to dateutil's wonkiness, but maybe not so bad.\n"
    ],
    "events": [
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 16,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/inference.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2270,
    "reporter": "wesm",
    "created_at": "2012-11-16T17:04:34+00:00",
    "closed_at": "2012-11-18T18:53:34+00:00",
    "resolver": "wesm",
    "resolved_in": "66415ec2a1b253cdad804d7de3455d154cdafec4",
    "resolver_commit_num": 2583,
    "title": "usecols conflicts with leading columns handling in read_csv",
    "body": "E.g. 3-column file, `usecols=[1, 2]`\n",
    "labels": [],
    "comments": [
      "This will require an API change in 0.10\n",
      "F it. We're going to 0.10\n"
    ],
    "events": [
      "commented",
      "commented"
    ],
    "changed_files": 6,
    "additions": 198,
    "deletions": 322,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/parser.pyx",
      "pandas/src/parser/tokenizer.c",
      "pandas/tests/test_format.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2272,
    "reporter": "wesm",
    "created_at": "2012-11-16T18:41:09+00:00",
    "closed_at": "2012-11-18T22:04:18+00:00",
    "resolver": "wesm",
    "resolved_in": "3c6f7b93b2d0c1a86600fd3dfc1bb470d88d59e4",
    "resolver_commit_num": 2584,
    "title": "datetime64 ns representation error in Series repr",
    "body": "Will post example\n",
    "labels": [],
    "comments": [
      "Is this related to PR #2265?\n",
      "No, but i'm about to push a fix for it\n"
    ],
    "events": [
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 27,
    "deletions": 11,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/internals.py",
      "pandas/tests/test_format.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2273,
    "reporter": "bluefir",
    "created_at": "2012-11-16T19:32:08+00:00",
    "closed_at": "2012-11-17T02:53:26+00:00",
    "resolver": "wesm",
    "resolved_in": "bdbca8e3dcc89bad5678e7d8a0f555c2157db07b",
    "resolver_commit_num": 2577,
    "title": "Significant performance degradation in 0.9.1 for SparseDataFrame methods like to_dense() and save() and for arithmetic operations",
    "body": "This is what I have in version 0.9.0:\n\n\n\n> '0.9.0'\n\n\n\n> <class 'pandas.core.frame.DataFrame'>\n> MultiIndex: 253738 entries, (20061229, '00036110') to (20120928, 'Y8564W10')\n> Data columns:\n> MINING_METALS                  253738  non-null values\n> GOLD                           253738  non-null values\n> FORESTRY_PAPER                 253738  non-null values\n> CHEMICAL                       253738  non-null values\n> ENERGY_RESERVES                253738  non-null values\n> OIL_REFINING                   253738  non-null values\n> OIL_SERVICES                   253738  non-null values\n> FOOD_BEVERAGES                 253738  non-null values\n> ALCOHOL                        253738  non-null values\n> TOBACCO                        253738  non-null values\n> HOME_PRODUCTS                  253738  non-null values\n> GROCERY_STORES                 253738  non-null values\n> CONSUMER_DURABLES              253738  non-null values\n> MOTOR_VEHICLES                 253738  non-null values\n> APPAREL_TEXTILES               253738  non-null values\n> CLOTHING_STORES                253738  non-null values\n> SPECIALTY_RETAIL               253738  non-null values\n> DEPARTMENT_STORES              253738  non-null values\n> CONSTRUCTION                   253738  non-null values\n> PUBLISHING                     253738  non-null values\n> MEDIA                          253738  non-null values\n> HOTELS                         253738  non-null values\n> RESTAURANTS                    253738  non-null values\n> ENTERTAINMENT                  253738  non-null values\n> LEISURE                        253738  non-null values\n> ENVIRONMENTAL_SERVICES         253738  non-null values\n> HEAVY_ELECTRICAL_EQUIPMENT     253738  non-null values\n> HEAVY_MACHINERY                253738  non-null values\n> INDUSTRIAL_PARTS               253738  non-null values\n> ELECTRICAL_UTILITY             253738  non-null values\n> GAS_WATER_UTILITY              253738  non-null values\n> RAILROADS                      253738  non-null values\n> AIRLINES                       253738  non-null values\n> FREIGHT                        253738  non-null values\n> MEDICAL_SERVICES               253738  non-null values\n> MEDICAL_PRODUCTS               253738  non-null values\n> DRUGS                          253738  non-null values\n> ELECTRONIC_EQUIPMENT           253738  non-null values\n> SEMICONDUCTORS                 253738  non-null values\n> COMPUTER_HARDWARE              253738  non-null values\n> COMPUTER_SOFTWARE              253738  non-null values\n> DEFENCE_AEROSPACE              253738  non-null values\n> TELEPHONE                      253738  non-null values\n> WIRELESS                       253738  non-null values\n> INFORMATION_SERVICES           253738  non-null values\n> INDUSTRIAL_SERVICES            253738  non-null values\n> LIFE_HEALTH_INSURANCE          253738  non-null values\n> PROPERTY_CASUALTY_INSURANCE    253738  non-null values\n> BANKS                          253738  non-null values\n> THRIFTS                        253738  non-null values\n> ASSET_MANAGEMENT               253738  non-null values\n> FINANCIAL_SERVICES             253738  non-null values\n> INTERNET                       253738  non-null values\n> REITS                          253738  non-null values\n> BIOTECH                        253738  non-null values\n> dtypes: int64(55)\n\n\n\n> <class 'pandas.sparse.frame.SparseDataFrame'>\n> MultiIndex: 253738 entries, (20061229, '00036110') to (20120928, 'Y8564W10')\n> Columns: 55 entries, AIRLINES to WIRELESS\n> dtypes: float64(55)\n\n\n\n> 100 loops, best of 3: 6.64 ms per loop\n\n\n\n> 10 loops, best of 3: 127 ms per loop\n\n\n\n> 1 loops, best of 3: 16.9 ms per loop\n\nNow this is what I get in 0.9.1:\n\n\n\n> '0.9.1'\n\n\n\n> 1 loops, best of 3: 92.2 s per loop\n\n\n\n> 1 loops, best of 3: 99.8 s per loop\n\n\n\n> 1 loops, best of 3: 100 s per loop\n\nSo, in the new version SparseDataFrame methods that used to run in less than 7-130 ms now run in more than 90 s. Ouch! What happened?\n",
    "labels": [],
    "comments": [
      "We need more performance benchmarks in the vbench suite. Thanks for the feedback. We'll investigate.\n",
      "This looks like 4a5b75b44b0048, though I'm not sure why take is so expensive.\nthe pending #2253 (3688e53)  fixes the problem for me.\n\nTestcase:\n\n``` python\nimport pandas as pd\nnum=250000\nl1=[randint(0,1000) for x in range(num)]\nl2=[randint(0,20000) for x in range(num)]\nl3=[randint(0,20000) for x in range(num)]\nl4=[randint(0,20000) for x in range(num)]\na=pd.DataFrame(dict(zip([0,1,2,3],[l1,l2,l3,l4]))).set_index([0,1])\nb=a.to_sparse()\n%timeit b/100\n%timeit b.to_dense()\n%timeit b.save('test.pk1')\n```\n\n**Edit: but perhaps there's another issue at play. I can't reproduce anything like 90s runtime on this data**\n",
      "Doh, this will teach me to review PRs more carefully; this is theoretically what vbench is for. I will fix\n",
      "Ugh, `iteritems` for all DataFrames has borked performance. Guess we're going to see 0.9.2 sooner rather than later\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 2,
    "additions": 2,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2277,
    "reporter": "dvic",
    "created_at": "2012-11-17T01:31:18+00:00",
    "closed_at": "2012-11-23T05:53:31+00:00",
    "resolver": "wesm",
    "resolved_in": "7a766564bc964f7a37674fad080627f0227592a3",
    "resolver_commit_num": 2596,
    "title": "Series.reset_index not working with \"inplace=True\"",
    "body": "When I call reset_index on a Series object with arguments inplace=True, it does not work. See the code example below. Am I missing something here? (I am using \"0.9.0\", checked with pandas.**version**)\n\n\n",
    "labels": [],
    "comments": [
      "Here df is a Series but df.reset_index('B') is a DataFrame.\nThere's no way to turn df from a Series into a DataFrame so inplace for Series only takes effect if \"drop=True\" and the result stays a Series.\n",
      "Alright, I understand now. So I guess it is a choice not to convert a Series object to a DataFrame object.\n\nBut why does a DataFrame object get converted to a Series object when I just select 1 column? For example if `df` is a DataFrame object, then `df[\"A\"]` would give a Series object, right? This made me think that reset_index would convert a Series object to a DataFrame object.\n",
      "`inplace` modifies the internals of `self`. \n\ndf[\"A\"] returns a Series but it doesn't convert df into a DataFrame.\n\nreset_index can indeed return a DataFrame object, but it doesn't make sense to change its type from Series to DataFrame.\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 2,
    "additions": 5,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2278,
    "reporter": "wesm",
    "created_at": "2012-11-17T02:32:14+00:00",
    "closed_at": "2012-11-22T20:34:04+00:00",
    "resolver": "wesm",
    "resolved_in": "bc271d8d2f227ca3bafd224ecd5d3b0205dffdb6",
    "resolver_commit_num": 2591,
    "title": "Unstack is not careful about potential memory use problems",
    "body": "Cartesian product problem; unstacks relative to hypothetical possibilities instead of observed combinations. I'm claiming this unless someone else wants to look inside the reshape code\n\n\n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 4,
    "additions": 69,
    "deletions": 30,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/reshape.py",
      "pandas/tests/test_multilevel.py",
      "vb_suite/reshape.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2280,
    "reporter": "juliantaylor",
    "created_at": "2012-11-18T17:35:32+00:00",
    "closed_at": "2012-11-19T01:41:28+00:00",
    "resolver": "wesm",
    "resolved_in": "2b36acd25a8fd7199c86b753fdc533b63d64952b",
    "resolver_commit_num": 2585,
    "title": "test_transform_broadcast failure on ubuntu 13.04",
    "body": "on ubuntu 13.04 test_transform_broadcast fails due to rounding issues.\nI'm not sure if this is a bug in the test or if some underlying component may have changed to cause rounding errors when there were non before.\na possible reason might be numpy 1.7 because the test works fine in ubuntu 12.10 with numpy 1.6\n\nsee also -raring-i386.pandas_0.8.1-1_FAILEDTOBUILD.txt.gz\nit affects git HEAD too.\n\n\n",
    "labels": [],
    "comments": [
      "it apparently only occurs on i386, not amd64\nnumpy seems to have nothing to do with the issue.\n",
      "The test case should be doing a fuzzy FP comparison rather than exact\n"
    ],
    "events": [
      "commented",
      "commented"
    ],
    "changed_files": 1,
    "additions": 12,
    "deletions": 6,
    "changed_files_list": [
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2295,
    "reporter": "bluefir",
    "created_at": "2012-11-19T22:41:40+00:00",
    "closed_at": "2012-11-24T00:14:41+00:00",
    "resolver": "wesm",
    "resolved_in": "5d6e7c8a901fa86b8bc89d361c6a2a542292251d",
    "resolver_commit_num": 2598,
    "title": "Error setting multiple columns via hierarchical indexing in DataFrame",
    "body": "I encountered the following:\n\n\n\n> ---\n> \n> AssertionError                            Traceback (most recent call last)\n> <ipython-input-26-86f8dc5d0623> in <module>()\n>       5     print(betas[name].shape)\n>       6     print(reg_result._beta_raw.shape)\n> ----> 7     betas[name] = reg_result._beta_raw\n> \n> C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in **setitem**(self, key, value)\n>    1802         else:\n>    1803             # set column\n> -> 1804             self._set_item(key, value)\n>    1805 \n>    1806     def _boolean_set(self, key, value):\n> \n> C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _set_item(self, key, value)\n>    1842         \"\"\"\n>    1843         value = self._sanitize_column(key, value)\n> -> 1844         NDFrame._set_item(self, key, value)\n>    1845 \n>    1846     def insert(self, loc, column, value):\n> \n> C:\\Python27\\lib\\site-packages\\pandas\\core\\generic.pyc in _set_item(self, key, value)\n>     491             if len(key) != self.columns.nlevels:\n>     492                 key += ('',)*(self.columns.nlevels - len(key))\n> --> 493         self._data.set(key, value)\n>     494 \n>     495         try:\n> \n> C:\\Python27\\lib\\site-packages\\pandas\\core\\internals.pyc in set(self, item, value)\n>     882         if value.ndim == self.ndim - 1:\n>     883             value = value.reshape((1,) + value.shape)\n> --> 884         assert(value.shape[1:] == self.shape[1:])\n>     885         if item in self.items:\n>     886             i, block = self._find_block(item)\n> \n> AssertionError: \n\nData shapes seem to be equal:\n\n\n\n> (1257, 3)\n> (1257L, 3L)\n> True\n\nThe walkaround that works is the following:\n\n\n\nIs this the way to go or did I encounter a bug?\n",
    "labels": [],
    "comments": [
      "It might be a bug; can you post a standalone reproduction of the issue?\n",
      "```\nimport numpy as np\nimport pandas as pd\nfrom pandas import Index, MultiIndex, Series, DataFrame\n\nn_dates = 1000\nn_securities = 2000\nn_factors = 3\nn_versions = 3\n\ndates = pd.date_range('1997-12-31', periods=n_dates, freq='B')\ndates = Index(map(lambda x: x.year * 10000 + x.month * 100 + x.day, dates))\n\nsecid_min = int('10000000', 16)\nsecid_max = int('F0000000', 16)\nstep = (secid_max - secid_min) // (n_securities - 1)\nsecurity_ids = map(lambda x: hex(x)[2:10].upper(), range(secid_min, secid_max + 1, step))\n\ndata_index = MultiIndex(levels=[dates.values, security_ids],\n    labels=[[i for i in xrange(n_dates) for _ in xrange(n_securities)], range(n_securities) * n_dates],\n    names=['date', 'security_id'])\nn_data = len(data_index)\n\nfactors = Index(['factor{}'.format(i) for i in xrange(1, n_factors + 1)])\nversions = ['version{}'.format(i) for i in xrange(1, n_versions + 1)]\nbeta_columns = MultiIndex(levels=[versions, factors.values],\n    labels=[[i for i in xrange(n_versions) for _ in xrange(n_factors)], range(n_factors) * n_versions])\nbetas = DataFrame(index=dates, columns=beta_columns)\n\nfor version in versions:\n    y = Series(np.random.randn(n_data), index=data_index)\n    x = DataFrame(np.random.randn(n_data, n_factors), index=data_index, columns=factors)\n    reg_result = pd.fama_macbeth(y=y, x=x, intercept=False)\n    betas[version] = reg_result._beta_raw\n```\n\n> Traceback (most recent call last):\n>   File \"C:\\Python27\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2431, in safe_execfile\n>     py3compat.execfile(fname,*where)\n>   File \"C:\\Python27\\lib\\site-packages\\IPython\\utils\\py3compat.py\", line 171, in execfile\n>     exec compile(scripttext, filename, 'exec') in glob, loc\n>   File \"D:\\BlueFir\\develop\\python\\AlphaModel\\TestShapes.py\", line 35, in <module>\n>     betas[version] = reg_result._beta_raw\n>   File \"C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py\", line 1804, in __setitem__\n>     self._set_item(key, value)\n>   File \"C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py\", line 1844, in _set_item\n>     NDFrame._set_item(self, key, value)\n>   File \"C:\\Python27\\lib\\site-packages\\pandas\\core\\generic.py\", line 493, in _set_item\n>     self._data.set(key, value)\n>   File \"C:\\Python27\\lib\\site-packages\\pandas\\core\\internals.py\", line 884, in set\n>     assert(value.shape[1:] == self.shape[1:])\n> AssertionError\n",
      "Assigning multiple columns like that doesn't work yet unfortunately. Will see what I can do\n",
      "As long as it works for one column, it's not a big deal since you can always iterate through columns. The AssertionError message was confusing though. Thanks for the explanation.\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 7,
    "additions": 46,
    "deletions": 21,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/index.py",
      "pandas/core/internals.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2296,
    "reporter": "cossatot",
    "created_at": "2012-11-19T23:17:27+00:00",
    "closed_at": "2012-11-20T04:53:42+00:00",
    "resolver": "wesm",
    "resolved_in": "b8dae948311d0f07f46a6dfb1b39dd98ae27934f",
    "resolver_commit_num": 2588,
    "title": "bug in csv_reader?",
    "body": "Some previously-functioning code of mine broke this morning after upgrading to the newest Ubuntu package of Pandas v. 0.9.2.  It seems to be a bug in the upgraded read_csv parser.  Beyond that, the error message (below) is fairly unhelpful to the uninitiated.  Maybe this is a bug, or maybe the new version uses different syntax (although some of the .csv files are able to be imported...).\n\nI am sending a copy of the offending file to Wes, as I've got no quick place to put it.\n\nThanks,\nRichard\n\naHe_df = pd.readcsv('aHe_aliquots.csv')\n\n---\n\nCParserError                              Traceback (most recent call last)\n/home/itchy/ecopetrol/ec-working/data/<ipython-input-29-9ec771184960> in <module>()\n----> 1 aHe_df =pd.read_csv('aHe_aliquots.csv')\n\n/usr/lib/pymodules/python2.7/pandas/io/parsers.pyc in parser_f(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, header, index_col, names, skiprows, skipfooter, skip_footer, na_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze)\n    361                     buffer_lines=buffer_lines)\n    362 \n--> 363         return _read(filepath_or_buffer, kwds)\n    364 \n    365     parser_f.__name__ = name\n\n/usr/lib/pymodules/python2.7/pandas/io/parsers.pyc in _read(filepath_or_buffer, kwds)\n    185 \n    186     # Create the parser.\n\n--> 187     parser = TextFileReader(filepath_or_buffer, **kwds)\n    188 \n    189     if nrows is not None:\n\n/usr/lib/pymodules/python2.7/pandas/io/parsers.pyc in **init**(self, f, engine, **kwds)\n    465         self.options, self.engine = self._clean_options(options, engine)\n    466 \n--> 467         self._make_engine(self.engine)\n    468 \n    469     def _get_options_with_defaults(self, engine):\n\n/usr/lib/pymodules/python2.7/pandas/io/parsers.pyc in _make_engine(self, engine)\n    567     def _make_engine(self, engine='c'):\n    568         if engine == 'c':\n--> 569             self._engine = CParserWrapper(self.f, **self.options)\n    570         else:\n    571             if engine == 'python':\n\n/usr/lib/pymodules/python2.7/pandas/io/parsers.pyc in **init**(self, src, *_kwds)\n    787         ParserBase.**init**(self, kwds)\n    788 \n--> 789         self._reader = _parser.TextReader(src, *_kwds)\n    790 \n    791         # XXX\n\n/usr/lib/pymodules/python2.7/pandas/_parser.so in pandas._parser.TextReader.__cinit__ (pandas/src/parser.c:3357)()\n\n/usr/lib/pymodules/python2.7/pandas/_parser.so in pandas._parser.TextReader._get_header (pandas/src/parser.c:4283)()\n\n/usr/lib/pymodules/python2.7/pandas/_parser.so in pandas._parser.TextReader._tokenize_rows (pandas/src/parser.c:5731)()\n\n/usr/lib/pymodules/python2.7/pandas/_parser.so in pandas._parser.raise_parser_error (pandas/src/parser.c:13774)()\n\nCParserError: Error tokenizing data. C error: no error message set\n",
    "labels": [],
    "comments": [
      "The file has `\\r` line breaks. Universal newline mode handled this in the past, but this case must be addressed directly in the new C tokenizer\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 4,
    "additions": 98,
    "deletions": 22,
    "changed_files_list": [
      "pandas/io/tests/test_cparser.py",
      "pandas/src/parse_helper.h",
      "pandas/src/parser/tokenizer.c",
      "pandas/src/parser/tokenizer.h"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2299,
    "reporter": "Vankisa",
    "created_at": "2012-11-20T10:30:45+00:00",
    "closed_at": "2012-11-26T01:40:39+00:00",
    "resolver": "wesm",
    "resolved_in": "3bf6a468da8de3feb3d829f7e63c9b64d4d5e3dd",
    "resolver_commit_num": 2613,
    "title": "Retrieving large frames with sparse data from hdf5 - 'NoneType' object is not iterable error",
    "body": "I have been using pandas within my scripts for some time now, especially to store large data sets in an easily accessible way. I have stumbled upon this problem a couple of days ago and have not been able to solve it so far.\n\nThe problem is that after I store a huge data frame into an hdf5 file, when I later load it back, it sometimes has one or more columns (only from the object type columns) completely inaccessible and returning the 'NoneType object is not iterable' error.\n\nWhile I use the frame in memory there are no problems, even with moderately larger data sets than the example below. It is worth mentioning that the frame contains either multiple datetime columns or multiple VMS timestamps (), as well as string and char and integer columns. All non-object columns can and do have missing values.\n\nAt first I thought I was saving 'NA' values in one of the 'object type' columns. Then I tried to update to latest pandas version (0.9.1). I was using 0.9.0 when this problem first occurred. Neither seem to be the solution.\n\nI have been able to reproduce the error with the following code:\n\n<pre lang=\"python\"><code>\nimport pandas as pd\nimport numpy as np\nimport datetime\n\n# Get VMS timestamps for today\ntime_now = datetime.datetime.today()\nstart_vms = datetime.datetime(1858, 11, 17)\nt_delta = (time_now - start_vms)\nvms_time = t_delta.total_seconds() * 10000000\n\n# Generate Test Frame (dense)\ntest_records = []\nvms_time1 = vms_time\nvms_time2 = vms_time\nfor i in range(2000000):\n    vms_time1 += 15 * np.random.randn()\n    vms_time2 += 25 * np.random.randn()\n    vms_time_diff = vms_time2 - vms_time1\n    string1 = 'XXXXXXXXXX'\n    string2 = 'XXXXXXXXXX'\n    string3 = 'XXXXX'\n    string4 = 'XXXXX'\n    char1 = 'A'\n    char2 = 'B'\n    char3 = 'C'\n    char4 = 'D'\n    number1 = np.random.randint(1,10)\n    number2 = np.random.randint(1,100)\n    number3 = np.random.randint(1,1000)\n    test_records.append((char1, string1, vms_time1, number1, char2, string2, vms_time2, number2, char3, string3, vms_time_diff, number3, char4, string4))\n\ndf = pd.DataFrame(test_records, columns = [\"column_1\", \"column_2\", \"column_3\", \"column_4\", \"column_5\", \"column_6\", \"column_7\", \"column_8\", \"column_9\", \"column_10\", \"column_11\", \"column_12\", \"column_13\", \"column_14\"])\n\n# Generate Test Frame (sparse)\ntest_records = []\nvms_time1 = vms_time\nvms_time2 = vms_time\ncount = 0\nfor i in range(2000000):\n    if (count%23 == 0):\n        vms_time1 += 15 * np.random.randn()\n        string1 = 'XXXXXXXXXX'\n        string2 = ' '\n        string3 = 'XXXXX'\n        string4 = 'XXXXX'\n        char1 = 'A'\n        char2 = 'B'\n        char3 = 'C'\n        char4 = 'D'\n        number1 = None\n        number2 = np.random.randint(1,100)\n        number3 = np.random.randint(1,1000)\n        test_records.append((char1, string1, vms_time1, number1, char2, None, None, number2, char3, string3, None, number3, None, string4))\n    else:\n        vms_time1 += 15 * np.random.randn()\n        vms_time2 += 25 * np.random.randn()\n        vms_time_diff = vms_time2 - vms_time1\n        string1 = 'XXXXXXXXXX'\n        string2 = 'XXXXXXXXXX'\n        string3 = 'XXXXX'\n        string4 = 'XXXXX'\n        char1 = 'A'\n        char2 = 'B'\n        char3 = 'C'\n        char4 = 'D'\n        number1 = np.random.randint(1,10)\n        number2 = np.random.randint(1,100)\n        number3 = np.random.randint(1,1000)\n        test_records.append((char1, string1, vms_time1, number1, char2, string2, vms_time2, number2, char3, string3, vms_time_diff, number3, char4, string4))\n    count += 1\n\ndf1 = pd.DataFrame(test_records, columns = [\"column_1\", \"column_2\", \"column_3\", \"column_4\", \"column_5\", \"column_6\", \"column_7\", \"column_8\", \"column_9\", \"column_10\", \"column_11\", \"column_12\", \"column_13\", \"column_14\"])\n\nstore_loc = \"foo.h5\"\nh5_store = pd.HDFStore(store_loc )\nh5_store['df1'] = df\nh5_store['df2'] = df1\nh5_store.close()\n</code></pre>\n\n\nWhen I try to load from this store now the 'df1' is behaving normally, but the 'df2' is producing the following error:\n<code>\nTypeError: 'NoneType' object is not iterable\n</code>\n\nAdditionally I just tried to reproduce this error on pandas version 0.8.1. It does not seem to be present there. So it is probably connected with the I/O changes introduced in 0.9.0?\n",
    "labels": [],
    "comments": [
      "There's a pending PR https://github.com/pydata/pandas/pull/2346, can you reproduce with that applied?\n",
      "git bisect turned up the commit (80b5082) that caused this breakage-- I think it's a bug in PyTables, I'm still digging. \n"
    ],
    "events": [
      "commented",
      "commented"
    ],
    "changed_files": 1,
    "additions": 3,
    "deletions": 1,
    "changed_files_list": [
      "pandas/io/pytables.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2300,
    "reporter": "dalejung",
    "created_at": "2012-11-20T14:09:25+00:00",
    "closed_at": "2012-12-07T20:48:03+00:00",
    "resolver": "wesm",
    "resolved_in": "ec864974d10ac99a3e254b93dae96cbb767dedaa",
    "resolver_commit_num": 2695,
    "title": "BinGrouper AttributeError when returning a Series from DataFrame.apply",
    "body": "\n\n\n\nError does not occur for regular Grouper. \n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 23,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2316,
    "reporter": "jreback",
    "created_at": "2012-11-21T16:56:42+00:00",
    "closed_at": "2012-11-26T18:56:49+00:00",
    "resolver": "wesm",
    "resolved_in": "d4810f075a07a30a4674c7367ea035667f5cb103",
    "resolver_commit_num": 2614,
    "title": "Series upconversion in apply",
    "body": "See the below examples. My use case started out as trying to use apply on a Series (and having the applied function return a Series) in order to get a DataFrame - but seeing some inconsistencies of how the returned function is accumulated (obviously can do this by iterating over the series in a list comprehension and using concat too).\n\n\n\nbase case\n\n\n\nthis looks like it is doing some sort of conversion\ne.g. using the Series values to construct the applied series\n\n\n\nThis is a bit inconsistent with the prior example;\nShould this be upconverting to a DataFrame?\n\n\n",
    "labels": [],
    "comments": [
      "What does s[0] print for you? Series output will abbreviated. \n",
      "updated....s[0] just a float\n",
      "I'm getting something different, what commit of pandas are you on?\n\n```\nIn [14]: s\nOut[14]: \n0    0.432431\n1    0.511677\n2    0.187196\n3    0.108227\n4    0.987615\n\nIn [15]: s.apply(lambda x: pd.Series(x, index = ['foo']))\nOut[15]: \n0   NaN\n1   NaN\n2   NaN\n3   NaN\n4   NaN\n```\n",
      "The issue is that `apply` tries to infer whether a function passed is a ufunc by calling it on the Series itself and seeing whether the result is an ndarray. I'm not sure whether this is right, though. Perhaps if the result is an array that you should get back a DataFrame?\n",
      "using 0.9.1 release...seems to work fine....\n\nmy original motivation was to do an apply operation and have it (possibily) upconverted...\n\nshould these do essentially the same thing? (or is this just asking for trouble?)\n\nessentially apply is a concat operation anyhow.\n\n```\npd.concat([ pd.Series(y, index = ['foo']) for x, y in s.iteritems() ], axis=1)\n\ns.apply(lambda x: pd.Series(x, index = ['foo']))\n\n```\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 19,
    "deletions": 16,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/series.py",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2317,
    "reporter": "lterry-t",
    "created_at": "2012-11-21T20:11:38+00:00",
    "closed_at": "2012-11-24T00:50:56+00:00",
    "resolver": "wesm",
    "resolved_in": "f29106daa73a0e25d37dd531e61d8f5d5e9b04a8",
    "resolver_commit_num": 2600,
    "title": "Error in Timezone handling of DataFrames when joined",
    "body": "When joining two DataFrames with localized indexes, the resulting DataFrame has erroneous Timestamps. (It appears that at some point the timestamps are interpreted at UTC without a correct timezone correction?)\n\nSample code:\n\n\n",
    "labels": [],
    "comments": [
      "Yeah looks like a bug. marked as such\n"
    ],
    "events": [
      "commented",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 18,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2341,
    "reporter": "bad",
    "created_at": "2012-11-24T11:51:39+00:00",
    "closed_at": "2012-11-25T05:29:09+00:00",
    "resolver": "wesm",
    "resolved_in": "92c489e8dc9f68e60433aff9263bef1b049be30b",
    "resolver_commit_num": 2607,
    "title": "integer constant overflow in generated tseries.c",
    "body": "The generated tseries.c included in the 0.9.0 and 0.9.1 distributions contains two integer constants that are to large for 32bit values and need an LL suffix.  Note that the LL suffix is present in the cython source.\n\n\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 1,
    "additions": 2,
    "deletions": 2,
    "changed_files_list": [
      "pandas/src/datetime.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2345,
    "reporter": "Redoute",
    "created_at": "2012-11-24T16:22:15+00:00",
    "closed_at": "2012-12-02T16:51:13+00:00",
    "resolver": "wesm",
    "resolved_in": "b46c33e26c51ba65d9ab7075cd276b2a1cc4bd70",
    "resolver_commit_num": 2670,
    "title": "Windows installer doesnt ask for permissions, then crashes",
    "body": "I just installed pandas on my \"new\" Windows 8 system using pandas-0.9.1.win32-py2.7.exe and had some trouble doing it: Python is installed to Program Files\\Python27 (nothing unusual I think), so the installer needs admin permissions to write.\n\nThe installer didn't ask for that permission (no UAC), instead it crashed without useful explanation:\n\n\n\nStarting the installer as administrator solves the problem.\n",
    "labels": [],
    "comments": [
      "Sounds like a distutils problem. \n",
      "Maybe. From the packages I installed, [RTree](http://pypi.python.org/packages/any/R/Rtree/Rtree-0.7.0.win32.exe#md5=35cf437617bc6cc0548a72c77d06020b) crashed the same way.\nOn the other side, [Numpy](http://ignum.dl.sourceforge.net/project/numpy/NumPy/1.6.2/numpy-1.6.2-win32-superpack-python2.7.exe), [PyWin32](http://switch.dl.sourceforge.net/project/pywin32/pywin32/Build%20218/pywin32-218.win32-py2.7.exe), [Shapely](http://www.lfd.uci.edu/%7Egohlke/pythonlibs/eh49duz9/Shapely-1.2.16.win32-py2.7.exe) and [Pyproj](http://pyproj.googlecode.com/files/pyproj-1.9.2.win32-py2.7.exe) installers behaved well and used UAC (IIRC):\n\nShould I report this on bugs.python.org ?\n",
      "That'd be great. In the meantime you might consider a (free) complete distribution of the scientific stack with Anaconda CE, or EPD Academic.\n",
      "I googled \"distutils UAC\" and found there is an [option](http://docs.python.org/2/distutils/builtdist.html#vista-user-access-control-uac) in bdist_wininst. I think this should be 'auto' if applicable.\n\nThanks for your support!\n",
      "I added the option to our windows build scripts. We're having some server issues atm, but please check back tomorrow for new dev windows binaries here: http://pandas.pydata.org/pandas-build/dev/  (new ones will be named 0.10.xxx)\n",
      "@changhiskhan is this an option that could go in setup.py or...?\n",
      "Wouldn't be a bad idea, just intercept the command class.\nRight now I just modified the build scripts on the Jenkins machine\n\nOn Dec 1, 2012, at 10:35 PM, Wes McKinney notifications@github.com wrote:\n\n> @changhiskhan is this an option that should go in setup.py or...?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 3,
    "deletions": 0,
    "changed_files_list": [
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2348,
    "reporter": "jseabold",
    "created_at": "2012-11-24T21:48:01+00:00",
    "closed_at": "2012-11-25T03:38:08+00:00",
    "resolver": "wesm",
    "resolved_in": "0982ee6b155c3ab331e68afa92544678557bb54c",
    "resolver_commit_num": 2602,
    "title": "read_csv segfault",
    "body": "Works fine on v0.9.0. With master\n\n\n",
    "labels": [],
    "comments": [
      "Looks like a bug in the c parser. We'll fix\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 4,
    "additions": 36,
    "deletions": 6,
    "changed_files_list": [
      "pandas/io/tests/test_parsers.py",
      "pandas/src/inference.pyx",
      "pandas/src/parser.pyx",
      "pandas/src/tseries.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2350,
    "reporter": "wesm",
    "created_at": "2012-11-25T04:28:10+00:00",
    "closed_at": "2012-11-25T05:19:30+00:00",
    "resolver": "wesm",
    "resolved_in": "2600432d85364e196bec37889cf6939b5f796243",
    "resolver_commit_num": 2606,
    "title": "Debug sparse memory usage",
    "body": "I would expect this to use almost no memory. Instead, > 2GB on my box. \n\n`SparseDataFrame(columns=np.arange(100), index=np.arange(1e6))`\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 4,
    "additions": 11,
    "deletions": 4,
    "changed_files_list": [
      "pandas/sparse/frame.py",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_sparse.py",
      "vb_suite/sparse.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2355,
    "reporter": "wesm",
    "created_at": "2012-11-25T18:16:16+00:00",
    "closed_at": "2012-11-28T02:47:33+00:00",
    "resolver": "wesm",
    "resolved_in": "6c6dae7fb0025654e12f4dc279157af0094b8a30",
    "resolver_commit_num": 2633,
    "title": "Raise exception from overflowing uint64 data",
    "body": "ref: -pandas-insert-long-integer\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 2,
    "additions": 23,
    "deletions": 0,
    "changed_files_list": [
      "pandas/core/internals.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2360,
    "reporter": "wesm",
    "created_at": "2012-11-26T16:39:11+00:00",
    "closed_at": "2012-11-28T05:42:33+00:00",
    "resolver": "wesm",
    "resolved_in": "e28903c248b2a4f71fd04163ea5b083bbbcc369a",
    "resolver_commit_num": 2639,
    "title": "Specify values to be recognized as boolean in parsers",
    "body": "And yes/no/YES/NO should not be recognized as boolean by default. Revert default behavior from #1691, #1295\n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 6,
    "additions": 204,
    "deletions": 57,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/inference.pyx",
      "pandas/src/parser.pyx",
      "pandas/src/parser/tokenizer.c"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2361,
    "reporter": "etyurin",
    "created_at": "2012-11-26T21:02:40+00:00",
    "closed_at": "2012-11-26T22:31:02+00:00",
    "resolver": "wesm",
    "resolved_in": "5568670ff1ecc23577825ae3c56d1a4ac40f93f1",
    "resolver_commit_num": 2618,
    "title": "Stdout/stderr not handled properly in pandas.read_table()",
    "body": "pandas.read_table() does not handle stderr consistently.\n\nerror_bad_lines=True writes to stderr (which is correct), but warn_bad_lines=True writes to stdout (which is wrong).\n\nThis is with commit ae1b8a888149f6474c13141f8194da322dccc466\n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 5,
    "additions": 55,
    "deletions": 7,
    "changed_files_list": [
      "pandas/core/groupby.py",
      "pandas/io/tests/test_cparser.py",
      "pandas/src/parser.pyx",
      "pandas/src/parser/tokenizer.c",
      "pandas/src/parser/tokenizer.h"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2367,
    "reporter": "wesm",
    "created_at": "2012-11-27T16:33:03+00:00",
    "closed_at": "2012-11-28T03:36:24+00:00",
    "resolver": "wesm",
    "resolved_in": "6b5be058abe03fbf36404fdbbd87926c7ae9d806",
    "resolver_commit_num": 2636,
    "title": "Setting timezone in DatetimeIndex.union looks fishy",
    "body": "Noted during code review\n",
    "labels": [],
    "comments": [
      "shift also sets self.tz\nNeed to make self.tz readonly for cached properties like is_normalized etc\n",
      "Well the issue is that the result might need to be UTC but that assignment would make the result the time zone of the calling time series\n",
      "Ah, this is a separate issue then. I'll create a new one.\n\nOn Nov 27, 2012, at 3:45 PM, Wes McKinney notifications@github.com wrote:\n\n> Well the issue is that the result might need to be UTC but that assignment would make the result the time zone of the calling time series\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 13,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2374,
    "reporter": "craustin",
    "created_at": "2012-11-28T14:55:22+00:00",
    "closed_at": "2012-12-07T21:32:06+00:00",
    "resolver": "wesm",
    "resolved_in": "b0a19c8a9bdcf096fc857691a966af821ea626f5",
    "resolver_commit_num": 2697,
    "title": "DataFrame.applymap with identity function changes dtype",
    "body": "from datetime import datetime\nfrom pandas import DataFrame\ndf = DataFrame({'x1': [datetime(1996,1,1)]})\ndf2 = df.applymap(lambda x: x)\n\nIssue:\ntype(df['x1'][0]) == datetime.datetime\ntype(df2['x1'][0]) == pandas.lib.Timestamp\n\nThis repros in the 11/27/2012 cut of 0.10.0 - but not in 0.9.0 release.\n",
    "labels": [],
    "comments": [
      "Is there somewhere where Timestamp is causing you problems? It's a subclass of datetime.datetime and should be interchangeable \n",
      "Here's an unexpected crash:\nfrom datetime import datetime\nfrom pandas import DataFrame\nfrom pandas.core.datetools import bday\ndf = DataFrame({'x1': [datetime(1996,1,1)]})\ndf = df.applymap(lambda x: x+bday)\ndf = df.applymap(lambda x: x+bday)\n\nThe first applymap succeeds (but alters the dtypes as in the above post). The second applymap raises this exception:\n\npandas\\core\\frame.pyc in applymap(self, func)\n   4114         applied : DataFrame\n   4115         \"\"\"\n-> 4116         return self.apply(lambda x: lib.map_infer(x, func))\n   4117\n   4118     #----------------------------------------------------------------------\n\npandas\\core\\frame.pyc in apply(self, func, axis, broadcast, raw, args, **kwds)\n   3981                     return self._apply_raw(f, axis)\n   3982                 else:\n-> 3983                     return self._apply_standard(f, axis)\n   3984             else:\n   3985                 return self._apply_broadcast(f, axis)\n\npandas\\core\\frame.pyc in _apply_standard(self,\nfunc, axis, ignore_failures)\n   4056                     # no k defined yet\n   4057                     pass\n-> 4058                 raise e\n   4059\n   4060         if len(results) > 0 and _is_sequence(results[0]):\n\nTypeError: (\"ufunc 'add' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule 'safe'\", u'occurred at index x1')\n",
      "I see the issue. Will get a fix in for 0.10\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "assigned"
    ],
    "changed_files": 5,
    "additions": 19,
    "deletions": 7,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/core/series.py",
      "pandas/sparse/frame.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2399,
    "reporter": "wesm",
    "created_at": "2012-11-30T22:55:01+00:00",
    "closed_at": "2012-11-30T23:16:36+00:00",
    "resolver": "wesm",
    "resolved_in": "747039b2c84268d587784f05cc919e13141cd34f",
    "resolver_commit_num": 2654,
    "title": "DataFrame.to_string bad performance",
    "body": "I'll add a vbench so this lights up next time\n\n\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 4,
    "additions": 51,
    "deletions": 16,
    "changed_files_list": [
      "pandas/core/format.py",
      "pandas/core/frame.py",
      "pandas/tests/test_format.py",
      "vb_suite/frame_methods.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2406,
    "reporter": "wesm",
    "created_at": "2012-12-03T00:44:59+00:00",
    "closed_at": "2012-12-07T17:20:57+00:00",
    "resolver": "wesm",
    "resolved_in": "eb64630c9c62677979f7e0157344533a1a2c40f6",
    "resolver_commit_num": 2693,
    "title": "Update dependency lists",
    "body": "Notably, newer openpyxl and newer Cython required to build pandas from source and for all tests to pass\n\nBoth README.rst and the docs\n",
    "labels": [],
    "comments": [
      "also...matplotlib is now required to run the tests (was not as of 0.9.1)..\n",
      "That's an oversight, should skip mpl-depending tests. \n"
    ],
    "events": [
      "commented",
      "commented"
    ],
    "changed_files": 2,
    "additions": 13,
    "deletions": 5,
    "changed_files_list": [
      "README.rst",
      "doc/source/install.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2424,
    "reporter": "jreback",
    "created_at": "2012-12-04T13:30:15+00:00",
    "closed_at": "2012-12-09T22:41:49+00:00",
    "resolver": "wesm",
    "resolved_in": "50f52771ebdac9bf94fded30e0f5d7d8f2a20198",
    "resolver_commit_num": 2719,
    "title": "tests_parsers should skip bz2 tests if not installed",
    "body": "",
    "labels": [],
    "comments": [
      "bz2 has been part of the python stdlib since 2.3, what kind of trouble are you having?\n",
      "test_fast.sh executes test_decompression in test_parsers.py which tests using gzip and bz2\nbz2 is technically optionally (though i suppose most major distributions have it compiled into python).\njust thought that this test should be skipped if bz2 is not installed...no biggie\n"
    ],
    "events": [
      "commented",
      "commented"
    ],
    "changed_files": 1,
    "additions": 5,
    "deletions": 3,
    "changed_files_list": [
      "pandas/io/tests/test_parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2428,
    "reporter": "ghost",
    "created_at": "2012-12-04T19:48:49+00:00",
    "closed_at": "2012-12-05T05:57:54+00:00",
    "resolver": "wesm",
    "resolved_in": "100fb8f965b44100c2500cf7cbd95b6ed20354d3",
    "resolver_commit_num": 2683,
    "title": "read_csv  segfaults when given a path to a non-existent file",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 2,
    "additions": 11,
    "deletions": 0,
    "changed_files_list": [
      "pandas/io/tests/test_parsers.py",
      "pandas/src/parser/io.c"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2430,
    "reporter": "wesm",
    "created_at": "2012-12-04T23:22:47+00:00",
    "closed_at": "2013-01-20T01:44:59+00:00",
    "resolver": "wesm",
    "resolved_in": "7859063fa5653ab7647a6b59e0fc7363eb611a77",
    "resolver_commit_num": 2698,
    "title": "Missing trailing fields in CSV file",
    "body": "\n",
    "labels": [],
    "comments": [
      "I'm still having problems with this:\n\ndata = ('a:b\\n'\n        'c:d:e\\n'\n        'f:g:h:i\\n'\n        'j:k\\n'\n        'l:m:n\\n')\n\n##if first row does not match longest line length, you error out doing this\nprint pandas.read_csv(StringIO(data),header=None,sep=':')\n##even if u specify names..\nprint pandas.read_csv(StringIO(data),header=None,sep=':',names=list(np.arange(4)))\n\nIt would be really awesome if there was a way to force the parser to expect a fixed number of columns. \n",
      "I'm converting this into a separate issue since the case you describe there is different than what was fixed before\n"
    ],
    "events": [
      "assigned",
      "closed",
      "commented",
      "reopened"
    ],
    "changed_files": 3,
    "additions": 28,
    "deletions": 7,
    "changed_files_list": [
      "pandas/io/tests/test_cparser.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/parser/tokenizer.c"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2441,
    "reporter": "rafaljozefowicz",
    "created_at": "2012-12-06T21:00:21+00:00",
    "closed_at": "2012-12-07T22:34:25+00:00",
    "resolver": "wesm",
    "resolved_in": "24133e8554764e23483faa3ea68263d06697f24a",
    "resolver_commit_num": 2699,
    "title": "Corrupted values in Panel when index is not unique",
    "body": "import pandas as pd\nfrom StringIO import StringIO\ntxt = \"\"\"time,id,value\n2012-10-20,1,1.5\n2012-10-20,1,2.5\n2012-10-21,1,1.5\n\"\"\"\nxxx = pd.read_csv(StringIO(txt), parse_dates=0)\nprint(xxx.set_index([\"time\"])[xxx.id == 1])\nprint(xxx.set_index([\"time\", \"id\"]).to_panel().value)\n\nIt prints out:\n            id  value\ntime  \n2012-10-20   1    1.5\n2012-10-20   1    2.5\n2012-10-21   1    1.5\n\nid            1\ntime  \n2012-10-20  1.5\n2012-10-21  2.5\n\nThe value for panel on 2012-10-21 is incorrect.\n\nI am using pandas 0.9.0\n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 3,
    "additions": 11,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tests/test_panel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2442,
    "reporter": "wesm",
    "created_at": "2012-12-06T22:15:57+00:00",
    "closed_at": "2012-12-10T21:22:21+00:00",
    "resolver": "wesm",
    "resolved_in": "648d581fefea7505ca1aa1b65fd26e550c79f2ba",
    "resolver_commit_num": 2725,
    "title": "Handling of trailing delimiters in read_csv",
    "body": "xref -pandas-trailing-delimiter-confuses-read-csv\n",
    "labels": [],
    "comments": [
      "To reproduce the bug, just create a two-line csv file. The first line is the header, without trailing delimiter. The second line is data, with trailing delimiter. Then `read_csv` will create a `DataFrame` in which headers and columns offset by one.\n",
      "This is very annoying because the index/row name inference is very useful in most cases, but breaks down in the case where you have a malformed file. I'll think about it some\n",
      "Hmmm...custom dialect option?\n",
      "Probably should have an option like index_col=False and deal with an empty column. I have the latest FEC file (which has ballooned--!!-- to 900+MB) to try it out\n",
      "While we are on it, may I suggest a feature about `read_csv`? The FEC file I used (it was 700+MB a week ago) was too large for 4GB memory I have on my macbook. If I try to read the file in one run, it would take forever because of page fault. But since not all 20 or so columns were used, I read file in 4 chunks, ditched half unused columns, appended it to a accumulator and ended up with a big `DataFrame`, which held all rows but only columns I was interested in.\n\nSo having an option to tell `pandas.read_csv` to only read certain columns could be very useful.\n",
      "Already done in the development version of pandas-- you should install it. `usecols` option\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 46,
    "deletions": 11,
    "changed_files_list": [
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/parser.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2443,
    "reporter": "wesm",
    "created_at": "2012-12-06T22:37:14+00:00",
    "closed_at": "2012-12-07T16:53:27+00:00",
    "resolver": "wesm",
    "resolved_in": "7461b45d129c93ae1de4c575ae24638d45d15012",
    "resolver_commit_num": 2692,
    "title": "Timestamp repr error with tzoffset tz",
    "body": "e.g.\n\n`result = Timestamp('2002-06-28T01:00:00.000000000+0100')`\n\nfrom -between-datetime-timestamp-and-datetime64\n",
    "labels": [],
    "comments": [
      "What's the error? That it prints out both +0100 and the tzoffset?\n",
      "dateutil 2.1:\n\n```\nIn [1]: result = Timestamp('2002-06-28T01:00:00.000000000+0100')\n\nIn [2]: result\nOut[2]: ---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-2-a5b1e83cd027> in <module>()\n----> 1 result\n\n/home/wesm/epd/lib/python2.7/site-packages/IPython/core/displayhook.pyc in __call__(self, result)\n    237             self.start_displayhook()\n    238             self.write_output_prompt()\n--> 239             format_dict = self.compute_format_data(result)\n    240             self.write_format_data(format_dict)\n    241             self.update_user_ns(result)\n\n/home/wesm/epd/lib/python2.7/site-packages/IPython/core/displayhook.pyc in compute_format_data(self, result)\n    149             MIME type representation of the object.\n    150         \"\"\"\n--> 151         return self.shell.display_formatter.format(result)\n    152 \n    153     def write_format_data(self, format_dict):\n\n/home/wesm/epd/lib/python2.7/site-packages/IPython/core/formatters.pyc in format(self, obj, include, exclude)\n    124                     continue\n    125             try:\n--> 126                 data = formatter(obj)\n    127             except:\n    128                 # FIXME: log the exception\n\n/home/wesm/epd/lib/python2.7/site-packages/IPython/core/formatters.pyc in __call__(self, obj)\n    445                 type_pprinters=self.type_printers,\n    446                 deferred_pprinters=self.deferred_printers)\n--> 447             printer.pretty(obj)\n    448             printer.flush()\n    449             return stream.getvalue()\n\n/home/wesm/epd/lib/python2.7/site-packages/IPython/lib/pretty.pyc in pretty(self, obj)\n    343                 if cls in self.type_pprinters:\n    344                     # printer registered in self.type_pprinters\n--> 345                     return self.type_pprinters[cls](obj, self, cycle)\n    346                 else:\n    347                     # deferred printer\n\n/home/wesm/epd/lib/python2.7/site-packages/IPython/lib/pretty.pyc in _repr_pprint(obj, p, cycle)\n    612 def _repr_pprint(obj, p, cycle):\n    613     \"\"\"A pprint that just redirects to the normal repr function.\"\"\"\n--> 614     p.text(repr(obj))\n    615 \n    616 \n\n/home/wesm/code/pandas/pandas/tslib.so in pandas.tslib.Timestamp.__repr__ (pandas/tslib.c:3899)()\n\n/home/wesm/epd/lib/python2.7/site-packages/python_dateutil-2.1-py2.7.egg/dateutil/tz.pyc in inner_func(*args, **kwargs)\n     37             return myfunc(*args, **kwargs)\n     38         else:\n---> 39             return myfunc(*args, **kwargs).encode()\n     40     return inner_func\n     41 \n\nAttributeError: 'NoneType' object has no attribute 'encode'\n```\n",
      "oh ok, i'm still on 1.5\n",
      "Gotcha. I'm about to push fix\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 15,
    "deletions": 4,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/tests/test_timezones.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2449,
    "reporter": "aldanor",
    "created_at": "2012-12-07T17:46:50+00:00",
    "closed_at": "2012-12-07T21:05:26+00:00",
    "resolver": "wesm",
    "resolved_in": "d0108990030dc4bba5794d699e1f7fd1b929dc43",
    "resolver_commit_num": 2696,
    "title": "get_level_values() method for MultiIndex containing dates",
    "body": "I was wondering if it was possible to elegantly extract the values of a MultiIndex which contains dates as datetime (and not an ndarray of datetime64[ns] which screws things up)?\n\n\n\nOf course, we can always use `to_datetime` and then `to_pydatetime`, is that the preferred method? Am I missing something?\n",
    "labels": [],
    "comments": [
      "Yeah, this should be reasonably simple to work around\n",
      "Yea. It just looks a bit inconsistent, I guess.\n"
    ],
    "events": [
      "commented",
      "commented",
      "assigned"
    ],
    "changed_files": 2,
    "additions": 14,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2453,
    "reporter": "rafaljozefowicz",
    "created_at": "2012-12-07T23:37:59+00:00",
    "closed_at": "2012-12-08T00:16:32+00:00",
    "resolver": "wesm",
    "resolved_in": "7e53266626b0345c5c372d8125c717db78a3c9e8",
    "resolver_commit_num": 2703,
    "title": "Data corruption in DataFrame when creating new columns while iterating over the index",
    "body": "That's the shortest example I could find. Data gets somehow corrupted when creating new columns while iterating over the index. Pre-generating initial values for columns solved my problem (commented out part)\nI am using pandas 0.9.0\n\n\n\nOutputs:\n\n\n",
    "labels": [],
    "comments": [
      "Looks fine on the development version. I'll see if I can figure out what was going wrong so I can add a unit test\n",
      "Well, I can verify this was fixed in 5d6e7c8-- the bug is still present in v0.9.1. I would suggest upgrading to 0.10.0 or the development version as soon as practical. I'll add a unit test for your example\n"
    ],
    "events": [
      "commented",
      "commented"
    ],
    "changed_files": 1,
    "additions": 22,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2457,
    "reporter": "wesm",
    "created_at": "2012-12-08T18:20:21+00:00",
    "closed_at": "2012-12-09T19:42:15+00:00",
    "resolver": "wesm",
    "resolved_in": "6a5b1f92933c5f986151cabd73acea4d9bc9b662",
    "resolver_commit_num": 2715,
    "title": "Easy specification of different line terminators in parsers",
    "body": "like the `lineterminator` dialect option\n\nref: -to-parse-delimited-data-in-python-pandas-to-create-dataframe\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 6,
    "additions": 256,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/parser.pyx",
      "pandas/src/parser/tokenizer.c",
      "pandas/src/parser/tokenizer.h"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2459,
    "reporter": "kieranholland",
    "created_at": "2012-12-09T02:21:15+00:00",
    "closed_at": "2012-12-09T18:04:08+00:00",
    "resolver": "wesm",
    "resolved_in": "921c754568d8e69495a76705936586362b9616e7",
    "resolver_commit_num": 2712,
    "title": "read_csv with names arg not implicitly setting header=None",
    "body": "\n",
    "labels": [],
    "comments": [
      "`names` is overloaded so that if header is not False, names means \"choose these columns\", rather like\nthe Dataframe constructor behaves.\n",
      "sorry, I misread the docstring and expected it to be consistent with the dataframe ctor.\n\nfrom the docstring:\n\n``` python\nnames : array-like\n    List of column names to use. If header=True, replace existing header,\n    otherwise this ignores the header=0 default\n```\n\nwhen header=0 by default, the names argument is used to override the column names.\nNot as consistent with the dataframe constructor, especially since header=False and header=0\nare currently treated the same, so there's room to specify that behaviour.\n",
      "Thanks for the explanation\n\nMy expectation came from the current development docs for read_table -\n\n```\nnames: List of column names to use. If passed, header will be implicitly set to None.\n```\n\nI suspect overloading names is likely to cause confusion.\n",
      "I see. The docs [here](http://pandas.pydata.org/pandas-docs/dev/generated/pandas.io.parsers.read_table.html#pandas.io.parsers.read_table) match the docstring i quoted.\nCan you provide a link to the docs you're refering to?\n",
      "http://pandas.pydata.org/pandas-docs/dev/io.html\n",
      "thanks!\n",
      "looks like weirdness related to bool coercion, similar to https://github.com/pydata/pandas/issues/2199\n\n``` python\nIn [36]: pd.read_csv(StringIO('1,2'), names=['a', 'b'],header=True)\n---------------------------------------------------------------------------\nCParserError                              Traceback (most recent call last)\n<ipython-input-36-7af61abcf59b> in <module>()\n----> 1 pd.read_csv(StringIO('1,2'), names=['a', 'b'],header=True)\n/home/user1/src/pandas/pandas/_parser.so in pandas._parser.TextReader.__cinit__ (pandas/src/parser.c:3801)()\n\n/home/user1/src/pandas/pandas/_parser.so in pandas._parser.TextReader._get_header (pandas/src/parser.c:4884)()\n\nCParserError: Passed header=1 but only 1 lines in file\n```\n\n``` ipython\npd.read_csv(StringIO('1,2'), names=['a', 'b'],header=False)\nOut[37]: \nEmpty DataFrame\nColumns: [a, b]\nIndex: []\n\npd.read_csv(StringIO('1,2'), names=['a', 'b'],header=0)\nOut[38]: \nEmpty DataFrame\nColumns: [a, b]\nIndex: []\n```\n",
      "Note the API changes in the 0.10 release notes:\n\n```\n  - ``names`` handling in file parsing: if explicit column `names` passed,\n    `header` argument will be respected. If there is an existing header column,\n    this can rename the columns. To fix legacy code, put ``header=None`` when\n    passing ``names``\n```\n\nIt's unfortunate but automatically forcing `header=None` was making handling some files (in which renaming columns was necessary) impossible.\n\nWe need to make a sweep through the docs to reflect the API change.\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "assigned",
      "commented",
      "commented"
    ],
    "changed_files": 2,
    "additions": 12,
    "deletions": 12,
    "changed_files_list": [
      "doc/source/io.rst",
      "pandas/io/parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2462,
    "reporter": "bmu",
    "created_at": "2012-12-09T09:36:21+00:00",
    "closed_at": "2012-12-09T16:43:42+00:00",
    "resolver": "wesm",
    "resolved_in": "fb560b8207cf0dd76eaba8cca8c19c7fe2906f6b",
    "resolver_commit_num": 2710,
    "title": "New parser is not mentioned in release notes for 0.10",
    "body": "The c-parser branch was merged in, however I don't see anything about this in the release notes.\n\nDid I miss it? \n",
    "labels": [],
    "comments": [
      "I haven't written the release notes for it yet. I will do so\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 1,
    "additions": 24,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2465,
    "reporter": "wesm",
    "created_at": "2012-12-09T16:22:39+00:00",
    "closed_at": "2012-12-09T21:27:05+00:00",
    "resolver": "wesm",
    "resolved_in": "7679eeee509449e39ad2e544b9fa4260f01900a7",
    "resolver_commit_num": 2717,
    "title": "Usecols fails when actual column names are used, but passed in names argument",
    "body": "e.g. usecols=['b', 'd', 'e'] vs. usecols=[1, 2, 4]\n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 2,
    "additions": 11,
    "deletions": 2,
    "changed_files_list": [
      "pandas/io/tests/test_parsers.py",
      "pandas/src/parser.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2466,
    "reporter": "wesm",
    "created_at": "2012-12-09T16:28:49+00:00",
    "closed_at": "2012-12-09T17:53:10+00:00",
    "resolver": "wesm",
    "resolved_in": "be7490ad77972f00e9aac0d6cbc28df842ae91f3",
    "resolver_commit_num": 2711,
    "title": "Expose decimal option in C parser",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 4,
    "additions": 161,
    "deletions": 137,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/parser.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2474,
    "reporter": "wesm",
    "created_at": "2012-12-10T15:52:51+00:00",
    "closed_at": "2012-12-10T19:33:20+00:00",
    "resolver": "wesm",
    "resolved_in": "dd7218cab88d0310b2efc67b48b3024366ce6766",
    "resolver_commit_num": 2724,
    "title": "Unrecognized compression mode causes segfault in read_csv",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 10,
    "deletions": 0,
    "changed_files_list": [
      "pandas/io/tests/test_parsers.py",
      "pandas/src/parser.pyx",
      "pandas/src/parser/tokenizer.c"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2480,
    "reporter": "wesm",
    "created_at": "2012-12-10T19:11:30+00:00",
    "closed_at": "2012-12-11T00:51:50+00:00",
    "resolver": "wesm",
    "resolved_in": "d1a1fa27e520c0d196981df58edb1bd5d3cc3161",
    "resolver_commit_num": 2727,
    "title": "Improve value_counts performance on object data types",
    "body": "Will do this sometime today\n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 5,
    "additions": 55,
    "deletions": 8,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/algorithms.py",
      "pandas/hashtable.pyx",
      "pandas/tests/test_series.py",
      "vb_suite/groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2488,
    "reporter": "gerigk",
    "created_at": "2012-12-11T10:11:16+00:00",
    "closed_at": "2012-12-11T21:00:41+00:00",
    "resolver": "wesm",
    "resolved_in": "6826609c94ad9b3551a26b9b7547862de6acbe34",
    "resolver_commit_num": 2736,
    "title": "df.sort(col) fails if col is not unique.",
    "body": "one more trouble case with duplicate column names\nthe exception could be nicer (sort ambiguous because of duplicate column or similar) or it could sort by the first? column with this name (although this would not be nice in case the column is not duplicate but has in fact different content with the same name and the user not being aware of this).\n\n\n",
    "labels": [],
    "comments": [
      "A better error message would be nice\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 3,
    "additions": 28,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2489,
    "reporter": "gbakalian",
    "created_at": "2012-12-11T10:35:11+00:00",
    "closed_at": "2012-12-14T04:27:26+00:00",
    "resolver": "wesm",
    "resolved_in": "90cddeecef813e591dfb15abea7691aca3f997e2",
    "resolver_commit_num": 2747,
    "title": "Loosing series names, when concatenating into DataFrame",
    "body": "In the version 9.1, when we are concatenating pandas.Series, we would expect the columns name of the created DataFrame to be the ones of the Series which were concatenated together. Nevertheless the series' names are lost, and the columns are simply 0, 1, 2 ...\n\nError shown below:\n\n\n",
    "labels": [],
    "comments": [
      "I thought this worked. Marked as bug to investigate\n"
    ],
    "events": [
      "commented",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 21,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2520,
    "reporter": "craustin",
    "created_at": "2012-12-13T16:47:27+00:00",
    "closed_at": "2012-12-13T17:33:34+00:00",
    "resolver": "wesm",
    "resolved_in": "f198721f91d095d4365a6fd2263a5eb4ea285f6e",
    "resolver_commit_num": 2740,
    "title": "DOC: formatters in DataFrame.to_html cannot be list",
    "body": "\n",
    "labels": [],
    "comments": [
      "Are you expecting to pass a list of same length as the columns?\n",
      "The doc says I can pass a list.  It's unclear, but maybe it should take a list as long as the number of cells to be printed. This would be the (optional) index + the number of columns.\n",
      "My read is that it should be of list having equal length to the DataFrame's columns. The code is designed (right now, at least) to use at most one formatter function per column\n",
      "Sounds good. Only issue then is that right now that causes the exception above.  Additionally, might want to have a way to format the index/\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 35,
    "deletions": 14,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/format.py",
      "pandas/tests/test_format.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2527,
    "reporter": "craustin",
    "created_at": "2012-12-13T21:46:27+00:00",
    "closed_at": "2012-12-13T22:21:16+00:00",
    "resolver": "wesm",
    "resolved_in": "3700b70f91a427fe3b465fab6f89036f246af0be",
    "resolver_commit_num": 2742,
    "title": "rolling_mean returns slightly-negative value on Series with only non-negative values",
    "body": "Seeing this in a few cases.  This is the simplest repro I can find:\n\n\n\nReturns 0 in 0.9.0.\nReturns -5E-20 in 0.10.0.\n\nWe found this issue because we do a sqrt on the result and expected 0s when the input was a 0.\n",
    "labels": [],
    "comments": [
      "This is related to #2114. I'll have a look\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 5,
    "additions": 44,
    "deletions": 7,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/algos.pyx",
      "pandas/stats/moments.py",
      "pandas/stats/tests/test_moments.py",
      "vb_suite/stat_ops.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2535,
    "reporter": "TomAugspurger",
    "created_at": "2012-12-14T15:15:16+00:00",
    "closed_at": "2012-12-14T16:41:14+00:00",
    "resolver": "wesm",
    "resolved_in": "9a98fb9c0a605ac6a728956b3fbe42bb64026534",
    "resolver_commit_num": 2749,
    "title": "read_csv, compression, and bad column names.",
    "body": "Hi.  I've got a data set that I'm working with from [here](?dir=data&sort=1&sort=2&start=b), the `bop_its_det.tsv.gz` file.  [Direct Link](?sort=1&file=data%2Fbop_its_det.tsv.gz). It's about 80 MB unzipped.  I tried to create a csv of just the first 10 rows [here]() but I think I messed up some of the newlines.  [Here's]() a gist of the plain text.\n\nI want to say up front that I'm using pandas version `0.10.0.dev-16774b3`, which I think is from a couple days before the official 10.0 beta was released, so maybe this isn't an issue anymore.\n\nSomeone in the EU's stats department decided it would be a good idea to separate some column names by columns and others by tabs so the first line looks like\n\n\n\nread_csv can handle this fine with:\n\n\n\nwhen I try with the compressed file `BulkDownloadListing.gz`:\n\n\n\nI get \n\n\n",
    "labels": [],
    "comments": [
      "Thanks for raising this-- the compression parameter isn't getting picked up by the regular expression code path. i'll look into it\n",
      "Got it working. Note you are missing a `\\` in that regular expression to slurp the trailing space:\n\n```\nIn [5]: df = pd.read_csv('/home/wesm/Downloads/bop_its_det.tsv.gz', compression='gzip', sep=',|\\s*\\t', na_values=[':', ' :', ': '], nrows=10)\n\nIn [6]: df\nOut[6]: \n  currency  post    flow partner geo\\time       2011      2010      2009       2008  \\\n0  MIO_EUR   200  CREDIT     ACP       AT    268.000   190.000   195.000    221.000   \n1  MIO_EUR   200  CREDIT     ACP       BE   1719.000  1524.000  1335.000   1226.000   \n2  MIO_EUR   200  CREDIT     ACP       BG     20.800    15.630     5.338      8.436   \n3  MIO_EUR   200  CREDIT     ACP       CY  112.000 p   100.060    40.900     35.440   \n4  MIO_EUR   200  CREDIT     ACP       CZ     40.426    31.435    22.831     25.142   \n5  MIO_EUR   200  CREDIT     ACP       DE   2943.000  2845.000  2489.000   2962.000   \n6  MIO_EUR   200  CREDIT     ACP       DK    947.709   960.348   774.489    992.355   \n7  MIO_EUR   200  CREDIT     ACP     EA12        NaN       NaN       NaN        NaN   \n8  MIO_EUR   200  CREDIT     ACP     EA13        NaN       NaN       NaN        NaN   \n9  MIO_EUR   200  CREDIT     ACP     EA15        NaN       NaN       NaN  18536.904   \n\n        2007       2006       2005       2004  \n0    176.000    143.000    112.000     49.000  \n1    975.000    422.000    348.000    282.000  \n2     11.249      5.624      3.988      3.686  \n3     22.320     20.220     22.450     62.628  \n4     19.235      5.791     12.810      2.844  \n5   2759.000   2754.000   2168.000   1865.000  \n6    955.762    944.752    816.715    915.738  \n7        NaN  13868.604  11944.563  10195.702  \n8  17463.867  13879.280  11955.662  10208.101  \n9  17594.348  14001.239  11989.206  10281.182  \n```\n"
    ],
    "events": [
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 60,
    "deletions": 10,
    "changed_files_list": [
      "pandas/core/common.py",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2537,
    "reporter": "dalejung",
    "created_at": "2012-12-14T18:01:02+00:00",
    "closed_at": "2012-12-16T04:43:19+00:00",
    "resolver": "wesm",
    "resolved_in": "24394e18bc098b853f9daea829961c1cba83a20c",
    "resolver_commit_num": 2755,
    "title": "TimeGrouper error on PanelGroupBy.agg with custom functions.",
    "body": "\n\nWhen using a regular groupby, custom agg functions will receive a Panel as expected. If you groupby with TimeGrouper, the agg function will receive a Series. The regular cython aggregates likes mean, sum work fine. \n",
    "labels": [],
    "comments": [
      "I see. Shouldn't be too hard to fix\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 6,
    "additions": 72,
    "deletions": 22,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/core/internals.py",
      "pandas/core/panel.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2538,
    "reporter": "hugadams",
    "created_at": "2012-12-14T19:31:37+00:00",
    "closed_at": "2012-12-14T23:21:36+00:00",
    "resolver": "wesm",
    "resolved_in": "7f8faf878b18dc323735b8f47402a66c006f7a0d",
    "resolver_commit_num": 2750,
    "title": "date_range() alert user when using periods and end args?",
    "body": "Hi all,\n\nI am using date_range() and noticed that  if the arguments start, end and period are all passed, the _period_ argument essentially overrides _end_. \n\nFor example, consider the following index created using start and end:\n\n\n\nAn alternative is to provide a start time,  and then a number of periods.\n\n\n\nThe weird part is when I pass all three arguments (start, end and periods).  It actually defers to the output of tperiods.\n\n\n\nThus, there is no difference between the following calls:\n\n\n\nIMO, I think this should at least yield an error or a flag.  My preferred behavior would actually yield a DatetimeIndex that runs from start to end, sampling 10 times over the entire array of second-spaced data.   One would be effectively defining a step size for sampling of length (start-end)/period, where start-end is an array spaced at second intervals.\n\nWhat do you guys think?  Here's an example of how the function could work:\n\n\n",
    "labels": [],
    "comments": [
      "Yeah an error should be raised here\n"
    ],
    "events": [
      "commented",
      "assigned"
    ],
    "changed_files": 5,
    "additions": 27,
    "deletions": 11,
    "changed_files_list": [
      "RELEASE.rst",
      "doc/source/io.rst",
      "pandas/io/parsers.py",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2539,
    "reporter": "gdraps",
    "created_at": "2012-12-15T00:17:30+00:00",
    "closed_at": "2012-12-15T23:12:48+00:00",
    "resolver": "wesm",
    "resolved_in": "e559e870602bd664c2cc02fba6612856afc36cc1",
    "resolver_commit_num": 2754,
    "title": "read_csv backwards compatibility: set header to None when names is passed",
    "body": "Given the awesome work on read_csv() pending for 0.10, this may be a necessary change, however, for backwards compatibility, can the `names` parameter change to implicitly set `header` to None again?  The new behavior of `names` is clearly documented, so feel free to reject as the new default.\n\nUp to and including 0.9.1, specifying the names parameter implied the first row is data, not header.\n\n\n\nIt came as a surprise that this no longer worked -- easily worked around though.\n\n\n",
    "labels": [],
    "comments": [
      "I was thinking about this myself. Maybe some kind of default option for `header` that's backwards compatible but doesn't lead to the inability to replace an existing header (which is why the API change was introduced). I'll have a look before the release\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 5,
    "additions": 34,
    "deletions": 33,
    "changed_files_list": [
      "RELEASE.rst",
      "doc/source/io.rst",
      "doc/source/v0.10.0.txt",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2548,
    "reporter": "ghost",
    "created_at": "2012-12-16T12:52:34+00:00",
    "closed_at": "2013-01-20T19:01:51+00:00",
    "resolver": "wesm",
    "resolved_in": "ee9b2b6eaa629b5197ae4cfff0503fbe38185b28",
    "resolver_commit_num": 2803,
    "title": "failing vbenches",
    "body": "-ci.org/pydata/pandas/jobs/3687471\n",
    "labels": [],
    "comments": [
      "These work fine locally for me\n",
      "I get this when I run locally: python vb_suite/perf_HEAD.py\n\n`append` breaks in timeseries_slice_mintely and timeseries_asof_nan\n\npretty sure I am on latest master, and I rebuilt clean....maybe I not updated somehow? (e.g. code that I dont' touch, but get via pulling)\n"
    ],
    "events": [
      "commented",
      "commented",
      "assigned"
    ],
    "changed_files": 1,
    "additions": 10,
    "deletions": 8,
    "changed_files_list": [
      "vb_suite/timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2549,
    "reporter": "jreback",
    "created_at": "2012-12-16T18:17:49+00:00",
    "closed_at": "2012-12-17T15:15:50+00:00",
    "resolver": "wesm",
    "resolved_in": "2a56d4112fffcd5fc2043296d326511b302b7f4a",
    "resolver_commit_num": 2757,
    "title": "BUG: string output on PeriodIndex fails",
    "body": "used to work in 0.9.1.\n\nthis is on current master\n\n\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 2,
    "additions": 8,
    "deletions": 1,
    "changed_files_list": [
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2563,
    "reporter": "aldanor",
    "created_at": "2012-12-19T08:19:01+00:00",
    "closed_at": "2012-12-28T14:10:16+00:00",
    "resolver": "wesm",
    "resolved_in": "81bf29fb5fea7b0fc94ef410cfbf5228eb633116",
    "resolver_commit_num": 2763,
    "title": "DatetimeIndex.unique() returns datetime64",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 29,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py",
      "vb_suite/timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2564,
    "reporter": "richbwood",
    "created_at": "2012-12-19T10:40:41+00:00",
    "closed_at": "2012-12-29T21:54:50+00:00",
    "resolver": "wesm",
    "resolved_in": "dcd9df7b985d261d2ad6dd23879d61553aa92461",
    "resolver_commit_num": 2774,
    "title": "read_csv should accept unicode objects as urls",
    "body": "Hi,\n\nI have tried the following with\npandas-f014b01af7bd2d03266697e672c2d41daded3fca.zip\nand\npandas-0.10.0.tar.gz\n\nThe unit tests on my Pandas build are failing with:\n# \n## FAIL: test_read_csv (pandas.io.tests.test_parsers.TestCParserHighMemory)\n\nTraceback (most recent call last):\n  File \"/home/woodri/build/out/lib/python2.7/site-packages/pandas-0.10.0-py2.7-linux-x86_64.egg/pandas/io/tests/test_parsers.py\", line 109, in test_read_csv\n    assert(False), \"read_csv should accept unicode objects as urls\"\nAssertionError: read_csv should accept unicode objects as urls\n# \n## FAIL: test_read_csv (pandas.io.tests.test_parsers.TestCParserLowMemory)\n\nTraceback (most recent call last):\n  File \"/home/woodri/build/out/lib/python2.7/site-packages/pandas-0.10.0-py2.7-linux-x86_64.egg/pandas/io/tests/test_parsers.py\", line 109, in test_read_csv\n    assert(False), \"read_csv should accept unicode objects as urls\"\nAssertionError: read_csv should accept unicode objects as urls\n# \n## FAIL: test_read_csv (pandas.io.tests.test_parsers.TestPythonParser)\n\nTraceback (most recent call last):\n  File \"/home/woodri/build/out/lib/python2.7/site-packages/pandas-0.10.0-py2.7-linux-x86_64.egg/pandas/io/tests/test_parsers.py\", line 109, in test_read_csv\n    assert(False), \"read_csv should accept unicode objects as urls\"\nAssertionError: read_csv should accept unicode objects as urls\n\n---\n\nRan 2934 tests in 193.600s\n\nFAILED (SKIP=44, failures=3)\n\nI have compiled python, cython, numpy, etc. myself. Are there any dependencies or compile options that I am likely to be missing?\n\nPlease let me know if there is any other information I can provide to help identify the problem.\n\nThank you\n",
    "labels": [],
    "comments": [
      "It looks like a red herring, something specific to your system is causing this function to not work:\n\n```\n    def test_read_csv(self):\n        if not py3compat.PY3:\n            fname=u\"file:///\"+unicode(self.csv1)\n            try:\n                df1 = read_csv(fname, index_col=0, parse_dates=True)\n            except IOError:\n                assert(False), \"read_csv should accept unicode objects as urls\"\n```\n\nTry loading a CSV from from a unicode HTTP link, e.g.:\n\n```\nIn [34]: read_csv(u'https://raw.github.com/pydata/pandas/master/pandas/io/tests/test1.csv')\nOut[34]: \n                 index         A         B         C         D\n0  2000-01-03 00:00:00  0.980269  3.685731 -0.364217 -1.159738\n1  2000-01-04 00:00:00  1.047916 -0.041232 -0.161812  0.212549\n2  2000-01-05 00:00:00  0.498581  0.731168 -0.537677  1.346270\n3  2000-01-06 00:00:00  1.120202  1.567621  0.003641  0.675253\n4  2000-01-07 00:00:00 -0.487094  0.571455 -1.611639  0.103469\n5  2000-01-10 00:00:00  0.836649  0.246462  0.588543  1.062782\n6  2000-01-11 00:00:00 -0.157161  1.340307  1.195778 -1.097007\n```\n\nPlease try that and report back here if it does not work. We'll have to figure out how to make the unit test more portable to other platforms (this is the first failure I've seen)\n",
      "I can't use the unicode HTTP link because my machine doesn't have internet access.\n\nHowever, the following works:\n\n```\nIn [109]: read_csv(u'file:///home/woodri/python-work/xy.csv')\nOut[109]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 536267 entries, 0 to 536266\nData columns:\ntimeStamp    536267  non-null values\nx            536267  non-null values\ny            536267  non-null values\ndtypes: float64(2), object(1)\n```\n\nThank you\n",
      "The unit test is trying to get\n\n```\n fname=u'file:////sbclocal/fmat_ir_local/64-bit/lib/python2.7/site-packages/pandas-0.10.0-py2.7-linux-x86_64.egg/pandas/io/tests/test1.csv'\n```\n\n(note the 4 forward slashes)\nand throws the exception:\n\n```\n <urlopen error ftp error: no host given>\n```\n\nRemoving one of the slashes fixes the unit test for me, but probably breaks it for windows.\n",
      "that's helpful, thanks. will fix the test\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 15,
    "deletions": 7,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/tests/test_parsers.py",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2576,
    "reporter": "invisibleroads",
    "created_at": "2012-12-21T16:09:50+00:00",
    "closed_at": "2012-12-28T15:49:38+00:00",
    "resolver": "wesm",
    "resolved_in": "ed0e39942875157ba7ffdfcde04d4b62da4db462",
    "resolver_commit_num": 2771,
    "title": "UnicodeDecodeError in DataFrame.__repr__() for non-ascii column names",
    "body": "Here is the traceback using Python 2.7.3 and Pandas trunk.\n\n\n",
    "labels": [],
    "comments": [
      "Can you give example data?\n\nSent from my mobile device\n\nOn Dec 21, 2012, at 11:10 AM, Roy Hyunjin Han notifications@github.com\nwrote:\n\nHere is the traceback using Python 2.7.3 and Pandas trunk.\n\n/pandas/core/frame.pyc in **repr**(self)\n    678         Yields Bytestring in Py2, Unicode String in py3.\n    679         \"\"\"\n--> 680         return str(self)\n    681\n    682     def _repr_html_(self):\n\n/pandas/core/frame.pyc in **str**(self)\n    634         if py3compat.PY3:\n    635             return self.**unicode**()\n--> 636         return self.**bytes**()\n    637\n    638     def **bytes**(self):\n\n/pandas/core/frame.pyc in **bytes**(self)\n    644         \"\"\"\n    645         encoding = com.get_option(\"display.encoding\")\n--> 646         return self.**unicode**().encode(encoding , 'replace')\n    647\n    648     def **unicode**(self):\n\n/pandas/core/frame.pyc in **unicode**(self)\n    655         buf = StringIO(u\"\")\n    656         if self._need_info_repr_():\n--> 657             self.info(buf=buf, verbose=self._verbose_info)\n    658         else:\n    659             is_wide = self._need_wide_repr()\n\n/pandas/core/frame.pyc in info(self, verbose, buf, max_cols)\n1624         dtypes = ['%s(%d)' % k for k in sorted(counts.iteritems())]\n1625         lines.append('dtypes: %s' % ', '.join(dtypes))\n-> 1626         _put_lines(buf, lines)\n1627\n1628     @property\n\n/pandas/core/format.pyc in _put_lines(buf, lines)\n1443 def _put_lines(buf, lines):\n1444     if any(isinstance(x, unicode) for x in lines):\n-> 1445         lines = [unicode(x) for x in lines]\n1446     buf.write('\\n'.join(lines))\n1447\n\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xd5 in position\n8: ordinal not in range(128)\n\n \u2014\nReply to this email directly or view it on\nGitHubhttps://github.com/pydata/pandas/issues/2576.\n",
      "``` python\nn=u\"\\u05d0\".encode('utf-8')\npd.options.display.max_rows=1\ndf=pd.DataFrame([1,2],columns=[n])\n```\n\nCalling `str` [here](https://github.com/pydata/pandas/blob/61176dbe9e321dd171ed63836334ea4fa490f5be/pandas/core/frame.py#L1617) should be replaced by `com.pprint_thing`.\n\nIf there is another code path that triggers this, I'd be interested to know.\n"
    ],
    "events": [
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented"
    ],
    "changed_files": 3,
    "additions": 10,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tests/test_format.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2593,
    "reporter": "vishnu2kmohan",
    "created_at": "2012-12-24T16:40:28+00:00",
    "closed_at": "2012-12-28T14:47:55+00:00",
    "resolver": "wesm",
    "resolved_in": "f2651b94ab425288f424cc7c7b9a0376e51df158",
    "resolver_commit_num": 2766,
    "title": "read_csv, compression, CParser",
    "body": "The CParser doesn't appear to handle the compression flag in 0.10.0\n\n\n\nI've shown my results with Python 3.2.3 and IPython 0.13.1 but it also manifests in Python 2.7.3\n\n\n\nThe raw csv file reads just fine:\n\n\n\nAttempting to read the compressed version, while passing compression='gzip' fails:\n\n\n\nHowever if I pass a GzipFile handle instead, we are able to read the csv;\n\n\n\nAlso, opening a bzip2 version while passing in compression='bz2' fails:\n\n\n\nPassing a BZ2File handle allows the parser to read the csv:\n\n\n\nVersion:\n\n\n\nThis issue also manifests in the current git master.\n\nThanks,\nVishnu\n",
    "labels": [],
    "comments": [
      "I will look more closely but I see 2 major problems with the above:\n\n1) you should always open file handles for compressed data in `'rb'` mode\n2) if you are opening a `BZ2File` or `GZipFile` by hand you should NOT pass any compression flag (since these are equivalent to passing a StringIO or normal file handle in binary mode)\n",
      "Wes, thanks for the real-time response.\n\n1) I've tried opening the files with both `'rb'` and defaults, doesn't appear to make a difference.\n2) I tried this both ways too, with and without the compression flag, doesn't appear to make a difference either.\n",
      "Confirmed that decompression only works if you pass the file name, not an open handle:\n\n``````\nIn [1]: read_csv('/home/wesm/tmp/sfoo.csv.gz', compression='gzip')\nOut[1]: \n                  Time  A  B\n0  12/24/2012 12:00:00  1  2\n\nIn [2]: read_csv(open('/home/wesm/tmp/sfoo.csv.gz', 'rb'), compression='gzip')\nOut[2]: \n                            \ufffd\ufffd\ufffdP\u0003sfoo.csvS\n0  \ufffd\ufffdMU\ufffdQr\u0004b'%.%C#}#\u0013}#\u0003C#C#+\u0003\u0003 \u0002\ufffd\u0018\ufffd)#\u0010\ufffd\ufffdn\u07d73```\n\nwill fix in the next week or two.\n``````\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 28,
    "deletions": 4,
    "changed_files_list": [
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/parser.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2601,
    "reporter": "dsm054",
    "created_at": "2012-12-27T20:13:30+00:00",
    "closed_at": "2012-12-28T14:35:37+00:00",
    "resolver": "wesm",
    "resolved_in": "7399f636946fdc702d6877ddf034819436bf6ebd",
    "resolver_commit_num": 2765,
    "title": "read_table/csv unexpected type dependence on delimiter",
    "body": "While answering a question on SO I came across something which puzzled me:\n\n\n\nChanging the delimiter from `\" \"` to `r\"\\s+\"` somehow triggered the interpretation of the first column as integers instead of floats.\n",
    "labels": [],
    "comments": [
      "Yeah the other code path does not detect overflows. I haven't gotten around to adding regular expression capability to the new parser engine (it won't be toooo hard)\n",
      "Actually no that's weird. I'll take a close look now\n"
    ],
    "events": [
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 17,
    "deletions": 1,
    "changed_files_list": [
      "pandas/io/tests/test_parsers.py",
      "pandas/lib.pyx",
      "pandas/src/inference.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2602,
    "reporter": "jim22k",
    "created_at": "2012-12-27T20:27:24+00:00",
    "closed_at": "2012-12-28T14:26:20+00:00",
    "resolver": "wesm",
    "resolved_in": "016b3200691fce632befa3f96c5d7e6229d5e4f3",
    "resolver_commit_num": 2764,
    "title": "Series StringMethods very slow",
    "body": "I understand the benefit of Series.str methods which automatically handle NA, but the implementation seems really slow.\n\n\n\nLooking in the code the difference seems to be that Series.map with na_action='ignore' uses some vectorized code to filter out the NA values while Series.str uses the _na_map function with a try/except for each item in the Series (non-vectorized).\n\nCan I make a request to eliminate the _na_map in favor of something more like Series.map(na_action='ignore')?\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 4,
    "additions": 44,
    "deletions": 23,
    "changed_files_list": [
      "pandas/core/strings.py",
      "pandas/io/pytables.py",
      "pandas/tests/test_strings.py",
      "vb_suite/frame_methods.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2605,
    "reporter": "tlmaloney",
    "created_at": "2012-12-28T05:01:32+00:00",
    "closed_at": "2012-12-28T13:48:05+00:00",
    "resolver": "wesm",
    "resolved_in": "25de0281e599d5452ec9b43d5fb0baa56e6cbef1",
    "resolver_commit_num": 2762,
    "title": "AssertionError when using apply after GroupBy",
    "body": "The following code raises an AssertionError with pandas 0.10.0, but works fine in 0.9.1. The error still exists in the latest dev version here. The code comes from Wes' book, pages 33-36. The data files are from -book\n\n\n\nThe last line results in the following error trace:\n\n\n",
    "labels": [],
    "comments": [
      "Shoot. A reminder that I should add all the book examples as a smoke test to be run with the rest of the pandas tests. I will sort out a fix soon. \n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 2,
    "additions": 10,
    "deletions": 1,
    "changed_files_list": [
      "pandas/src/reduce.pyx",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2608,
    "reporter": "mkleehammer",
    "created_at": "2012-12-28T20:10:10+00:00",
    "closed_at": "2013-01-21T19:30:16+00:00",
    "resolver": "wesm",
    "resolved_in": "a05ee0e85ec7fc6e4115f8a33b2ef31d8c001873",
    "resolver_commit_num": 2811,
    "title": "read_csv crashes when run on multiple threads",
    "body": "Running multiple threads each calling read_csv crashes on OS/X.  I've seen two traps, which I'll put into an attachment.  Sometimes it says \"Fatal Python error: GC object already tracked\\nAbort trap: 6\"\" and sometimes \"Segmentation fault: 11\".\n\nI've put together a small example to reproduce it: \n\nI've also added some of the OS/X crash report in case it isn't crashing on your install.\n\nI'm using the built-in Python 2.7.2 on OS/X.  Pandas 0.10.0 was built locally using pip install -U pandas.  I don't know if this is new to 0.10.0 since I wasn't threading 0.9 yet.\n\nThis might be relevant for the GC already tracked: \n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 2,
    "additions": 6,
    "deletions": 6,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/src/parser.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2613,
    "reporter": "gerigk",
    "created_at": "2012-12-29T16:19:36+00:00",
    "closed_at": "2013-01-20T03:39:26+00:00",
    "resolver": "wesm",
    "resolved_in": "61f73200b35fb3a6458b506495e7fc4db1f7c137",
    "resolver_commit_num": 2799,
    "title": "ExcelFile has non existent argument \"kind\"  in the docstring",
    "body": "\n",
    "labels": [],
    "comments": [
      "fixed. thanks!\n",
      "We might want to be able to explicitly specify the file format? Right now you have to have the extension right\n",
      "I found out also, that if it is a buffer it tries to read it as an \".xls\" and then silently falls back to \".xlsx\" in case of an ImportError.\nSince I had only openpyxl installed this resulted in a unicode error when opening a xls buffer. \n\n```\ndata = path_or_buf.read()\n\n            try:\n                import xlrd\n                self.book = xlrd.open_workbook(file_contents=data)\n                self.use_xlsx = False\n            except Exception:\n                from openpyxl.reader.excel import load_workbook\n                buf = py3compat.BytesIO(data)\n                self.book = load_workbook(buf, use_iterators=True)\n```\n\nThe 'kind' argument would help here. \n"
    ],
    "events": [
      "referenced",
      "commented",
      "closed",
      "commented",
      "reopened",
      "commented",
      "assigned"
    ],
    "changed_files": 4,
    "additions": 40,
    "deletions": 6,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/indexing.py",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_excel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2616,
    "reporter": "gerigk",
    "created_at": "2012-12-29T23:32:29+00:00",
    "closed_at": "2012-12-30T16:11:04+00:00",
    "resolver": "wesm",
    "resolved_in": "4f3472d2e6462a290de4af7370f1ee1dee4e0892",
    "resolver_commit_num": 2775,
    "title": "Weird IndexError /segfault for df.groupby(level=\"levelname\") if na in level",
    "body": "I know that there are na issues for indices but I have never encountered this behavior before (pandas master):\n(fully functional code depending on the requests  and xlrd library and a berlin open data set)\n\n\n\nnow calling\n\n\n\nworks nicely whereas \n\n\n\nresults in\n\n\n\nand\n\n\n\nsegfaults\n",
    "labels": [],
    "comments": [
      "I want to add that also  \n\n```\ndf.unstack()\n```\n\nfails with a duplicates in index error which is weird (the index before and after the unstack is unique).\nI tried to debug it but didn't get very far as of know. Will update if I find anything useful (I assume this is also related to the na value).\n\n```\n---------------------------------------------------------------------------\nReshapeError                              Traceback (most recent call last)\n<ipython-input-5-1aea4c4bae4b> in <module>()\n----> 1 df.unstack()\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.10.1.dev_dcd9df7-py2.7-linux-x86_64.egg/pandas/core/frame.pyc in unstack(self, level)\n   3944         \"\"\"\n   3945         from pandas.core.reshape import unstack\n-> 3946         return unstack(self, level)\n   3947 \n   3948     #----------------------------------------------------------------------\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.10.1.dev_dcd9df7-py2.7-linux-x86_64.egg/pandas/core/reshape.pyc in unstack(obj, level)\n    351     if isinstance(obj, DataFrame):\n    352         if isinstance(obj.index, MultiIndex):\n--> 353             return _unstack_frame(obj, level)\n    354         else:\n    355             return obj.T.stack(dropna=False)\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.10.1.dev_dcd9df7-py2.7-linux-x86_64.egg/pandas/core/reshape.pyc in _unstack_frame(obj, level)\n    389     else:\n    390         unstacker = _Unstacker(obj.values, obj.index, level=level,\n--> 391                                value_columns=obj.columns)\n    392         return unstacker.get_result()\n    393 \n\n/usr/local/lib/python2.7/dist-packages/pandas-0.10.1.dev_dcd9df7-py2.7-linux-x86_64.egg/pandas/core/reshape.pyc in __init__(self, values, index, level, value_columns)\n     74 \n     75         self._make_sorted_values_labels()\n---> 76         self._make_selectors()\n     77 \n     78     def _make_sorted_values_labels(self):\n\n/usr/local/lib/python2.7/dist-packages/pandas-0.10.1.dev_dcd9df7-py2.7-linux-x86_64.egg/pandas/core/reshape.pyc in _make_selectors(self)\n    113 \n    114         if mask.sum() < len(self.index):\n--> 115             raise ReshapeError('Index contains duplicate entries, '\n    116                                'cannot reshape')\n    117 \n\nReshapeError: Index contains duplicate entries, cannot reshape\n```\n"
    ],
    "events": [
      "commented",
      "closed",
      "reopened"
    ],
    "changed_files": 5,
    "additions": 43,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/core/index.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2619,
    "reporter": "jankatins",
    "created_at": "2012-12-31T10:28:29+00:00",
    "closed_at": "2013-01-20T18:56:34+00:00",
    "resolver": "wesm",
    "resolved_in": "3803a5a3affb43fcecc7baa5bfa9cb2e49ebc64e",
    "resolver_commit_num": 2801,
    "title": "dtype parameter of read_csv is not documented",
    "body": "The docs at -docs/stable/generated/pandas.io.parsers.read_csv.html do not explain how the dtype parameter is to be used.\n\nUsing google, it seems to be a dict with keys=columnnames, but what are the possible values in that dict?. \n",
    "labels": [],
    "comments": [
      "`na_filter` seems also not documented\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 1,
    "additions": 2,
    "deletions": 0,
    "changed_files_list": [
      "pandas/io/parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2621,
    "reporter": "rafaljozefowicz",
    "created_at": "2012-12-31T22:47:32+00:00",
    "closed_at": "2013-01-20T02:45:41+00:00",
    "resolver": "wesm",
    "resolved_in": "f3a02a4f7c1132888fba24c9940915dbf1a17a7d",
    "resolver_commit_num": 2796,
    "title": "DatetimeIndex.drop is losing timezone information",
    "body": "\n\nThat seems counter-intuitive.\nFreq information is also dropped, but that is IMHO less important\n\nThis is on pandas 0.10\n",
    "labels": [],
    "comments": [
      "Confirmed bug\n"
    ],
    "events": [
      "commented",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 19,
    "deletions": 0,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2624,
    "reporter": "dalejung",
    "created_at": "2013-01-01T19:18:45+00:00",
    "closed_at": "2013-01-20T19:51:00+00:00",
    "resolver": "wesm",
    "resolved_in": "ec0e61a20fbef0fddede0458274c325fdb66d0d6",
    "resolver_commit_num": 2804,
    "title": "pd.concat merges object-datetime and np.datetime columns incorrectly.",
    "body": "\n\nThe rows where date was a np.datetime are converted to ints, while the object dates are left as datetimes. \n\nI'm not sure if this is a bug. I would expect the np.datetime column to upconvert into Timestamp objects? \n\nI ran into this from a test failure. Thing is, it's a fairly old test and it used to pass. I checked it against 0.9.1 and 0.10.0 which failed as well. So I'm not sure if I was on a previous commit that worked differently. it's weird. \n\nEither way, just wanted to get a check on what the expected behavior is. \n\nEDIT: \n",
    "labels": [],
    "comments": [
      "Just a note. This isn't an exact replica of my unit test. In my code, df1.date is created with a list of Timestamps. Which is why it was working before. Prior to https://github.com/pydata/pandas/commit/e06334aa37028e265dedb3d1ab11046fd6e7cab9\nall of the date columns would've remained dtype(object). \n"
    ],
    "events": [
      "commented",
      "cross-referenced",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 34,
    "deletions": 4,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/common.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2627,
    "reporter": "dalejung",
    "created_at": "2013-01-02T20:47:31+00:00",
    "closed_at": "2013-01-20T18:51:53+00:00",
    "resolver": "wesm",
    "resolved_in": "6ffef1691c8cf103e31f8deab4f50b3c087b8396",
    "resolver_commit_num": 2800,
    "title": "apply on np.datetime col doesn't upconvert to Timestamp",
    "body": "\n\nThis is an error from some unit tests. Did something major change with how datetimes are handled? I recall that np.datetime64 was autoboxed into Timestamps. \n\nI'll update after poking around. \n",
    "labels": [],
    "comments": [
      "Ah, I'm a liar. The unit test was more like\n\n``` python\nind = pd.date_range(start=\"2000/1/1\", freq=\"D\", periods=10)\nvals = [pd.Timestamp(str(d)) for d in ind]\ndf = pd.DataFrame(index=ind)\ndf['dates'] = vals\ndf.dates.apply(lambda x: x.date())\n```\n\nWhich worked on 0.10.0 since it kept the objects around. \n",
      "https://github.com/pydata/pandas/commit/e06334aa37028e265dedb3d1ab11046fd6e7cab9\n\nFound the commit. Makes sense, I'll change stuff on my end. Perhaps this should be an enhancement? \n",
      "yes should be a mention in release notes \n(it's more of a partial bug fix actually)\nnot sure that it always converts to time stamps, just more often\neg see applymap in frame.py (had to force conversion here, not sure of the promises that pandas makes in regards to whether the object is a time stamp (or a np.datetime64)\nprob a number cases that are not well tested\n",
      "Yeah. It broke a couple things on my end. There should probably be more emphasis on the change since it breaks things in indirect ways. \n",
      "do u have an example of something that works on 0.10.0 and breaks in master?\n",
      "see my first comment. Anything that assumes the DataFrame columns will stay objects and thus have object methods available. \n",
      "your example fails in 0.10.0 as well, did this work in a previous version?\n\n(as a side note, your example is a series; frames I believe work correctly)\n\ninterestingly this works:\n\n```\nfor x in s:\n    s.date()\n```\n\nas s is the correct type 'M8[ns]', and the iteration does on-the-fly conversion to Timestamp...simple to fix apply to do that (as was done with applymap in Frame)....but bigger question is that this conversion _could_ be done in the constructor. maybe performance related? and/or since we are talking about a column of data (and not the index), its very hard to explicty infer the correct type (and you also want to allow somewhat arbitrary types in the container as well). The user I think should force the conversion (which is the way it is now)\n\n@wesm any thoughts here?\n",
      "I think you misread my comment. The original post never worked. At the time, I hadn't realized that the issue was the object -> datetime64 conversion. Without the conversion, auto-boxing is not necessary. \n\nI was referring to the DataFrame example which does work in 0.10.0. Also, creating a Series out of a list of object doesn't convert to datetime64 in dev. So there's a lack of consistency there. \n\nI'm ambivalent to whether automatic object -> datetime64 is correct. It's more of a backwards compatibility issue. \nhttps://github.com/pydata/pandas/issues/2624 also came up for me due to the change. \n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented"
    ],
    "changed_files": 3,
    "additions": 21,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/series.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2631,
    "reporter": "jankatins",
    "created_at": "2013-01-03T22:26:01+00:00",
    "closed_at": "2013-01-20T00:10:56+00:00",
    "resolver": "wesm",
    "resolved_in": "5da8df72ff4aec801413be9ffec982ad02fbdc2d",
    "resolver_commit_num": 2792,
    "title": "read_csv, integer dtype and empty cells",
    "body": "Reading in a csv file with an integer column which has empty cells will cast that column to float (which in the end will resulted in problems with merging this dataframe on that column with a dataframe where the corresponding column is int).\n\nIt would be nice if a warning could be printed when such conversation (maybe only when an explicit dtype={\"col\":np.int64} setting is passed to read_csv) takes place and optional let me specify that such rows should be droped (isn't there a NA value for int columns...?)\n\n\n\n\n",
    "labels": [],
    "comments": [
      "There is no integer NA values unfortunately. I plan to fix this (a big project-- requires circumventing NumPy probably) one of these days\n",
      "I don't mind that it is not possible (yet) but that read_csv changed the datatype even as I specified it and didn't say anything (throw exception or print warning). \n\npandas/src/pasrer.pyx has commented out [exception throwing in line 900](https://github.com/pydata/pandas/blob/master/pandas/src/parser.pyx#L900), which seems to do what I expected...? \n\nWould it be posible to add a param to specify a strategy (drop row, throw exception, cast to float) what should happen with such cases? I tried to understand the code and it seems that it operates on columns, so dropping rows if an int is NA seems not an easy option :-(\n"
    ],
    "events": [
      "commented",
      "commented",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 21,
    "deletions": 6,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/parser.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2633,
    "reporter": "pikeas",
    "created_at": "2013-01-04T01:28:46+00:00",
    "closed_at": "2013-01-19T23:55:20+00:00",
    "resolver": "wesm",
    "resolved_in": "dd439c942405fd49a2123c41e883287560439014",
    "resolver_commit_num": 2791,
    "title": "DataFrame.from_records() KeyError when passed empty list",
    "body": "What it says on the tin. from_records() fails with a KeyError if passed an empty list. If the list is empty, an empty DataFrame should be created, conforming to the other arguments (columns, etc).\n",
    "labels": [],
    "comments": [
      "Agreed. Thanks\n",
      "Which version are you on? This seems to work ok on master:\n\n```\nIn [7]: DataFrame.from_records([], index=Index([1,2]))\nOut[7]: \nEmpty DataFrame\nColumns: []\nIndex: [1, 2]\n```\n",
      "```\npd.DataFrame.from_records([], index='foo', columns=['foo', 'bar'])\n```\n\nLooks like I was inaccurate about the error message thrown. The above throws an IndexError. Version 0.10.0.\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 20,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/frame.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2643,
    "reporter": "wesm",
    "created_at": "2013-01-05T17:02:58+00:00",
    "closed_at": "2013-01-05T19:52:41+00:00",
    "resolver": "wesm",
    "resolved_in": "5f42b953192febca773ebe45f2cf669993cfdfd1",
    "resolver_commit_num": 2778,
    "title": "Improve pivot_table for more complex aggregation function specs",
    "body": "Currently hampered by multiple unstacking not being implemented for existing hierarchical indexes. This was just a function of my running out of time when I was last working on this\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 4,
    "additions": 33,
    "deletions": 2,
    "changed_files_list": [
      "pandas/core/index.py",
      "pandas/core/reshape.py",
      "pandas/tests/test_index.py",
      "pandas/tools/tests/test_pivot.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2645,
    "reporter": "wesm",
    "created_at": "2013-01-05T21:24:27+00:00",
    "closed_at": "2013-01-05T22:43:06+00:00",
    "resolver": "wesm",
    "resolved_in": "f469a915e61480932bebc595affda36b04a3a328",
    "resolver_commit_num": 2784,
    "title": "test_pytables failures on Windows 64-bit, PyTables 2.4.0",
    "body": "\n",
    "labels": [],
    "comments": [
      "@wesm is this using current master?\n",
      "@wesm it looks like the test file is from a previous run - sometimes when I debug I end up and the tests don't clean up the file is not deleted, next run can have weird errors...\n",
      "I randomized the hdf5 file name between individual tests. it appears that test_append_with_data_columns somehow is creating 2 file handles to the same file\n",
      "Current status with that commit: \n\n```\nC:\\code\\pandas>c:\\python27\\scripts\\nosetests.exe pandas\n..............................................................SS.........................................................................................................................................................................................................................................................................S....E.SSS.................S..S.....................................................SS...........................................................................................................................................................................S.....................SSSSSSS....................................c:\\python27\\lib\\site-packages\\scikits\\statsmodels\\tools\\tools.py:256: FutureWarning: The default of `prepend` will be changed to True in the\nnext release, use explicit prepend\n  \"next release, use explicit prepend\", FutureWarning)\n...S............................................S...........................................................................................................................S.......................................................................................................................................................................................................................................................S............................................................................................................................................................................................................................................................................................................................................................................S.......................................................................................................................................................................................................................................................SSS........................................................................................................................................................................................................................................S........................SSS...................................................................................................................................................................................................................................................................................................................................................................................S..........................................................................................................................................................................S...S............SSS.........S............SS...........SS.....SSSS.................................................................................................S.................................................................................................................................S.........................................................................................................\n======================================================================\nERROR: test_append_with_data_columns (pandas.io.tests.test_pytables.TestHDFStore)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"C:\\code\\pandas\\pandas\\io\\tests\\test_pytables.py\", line 40, in tearDown\n    os.remove(self.path)\nWindowsError: [Error 32] The process cannot access the file because it is being used by another process: '__3sSGxBdLbs__.h5'\n\n----------------------------------------------------------------------\nRan 2884 tests in 80.092s\n\nFAILED (SKIP=47, errors=1)\n```\n",
      "@wesm pushed an updated....still don't know where it failed, this will allow the tests to complete...can you post: ptdump -av of the file .h5 that was last created (so I can see where it failed)...thx\n",
      "```\nC:\\code\\pandas>c:\\python27\\python c:\\python27\\Scripts\\ptdump -av __3sSGxBdLbs__.h5\n/ (RootGroup) ''\n  /._v_attrs (AttributeSet), 4 attributes:\n   [CLASS := 'GROUP',\n    PYTABLES_FORMAT_VERSION := '2.0',\n    TITLE := '',\n    VERSION := '1.0']\n/df (Group) ''\n  /df._v_attrs (AttributeSet), 12 attributes:\n   [CLASS := 'GROUP',\n    TITLE := '',\n    VERSION := '1.0',\n    data_columns := ['A', 'B', 'string', 'string2'],\n    index_cols := [(0, 'index')],\n    levels := 1,\n    nan_rep := 'nan',\n    non_index_axes := [(1, ['A', 'B', 'C', 'D', 'string', 'string2'])],\n    pandas_type := 'frame_table',\n    pandas_version := '0.10.1',\n    table_type := 'appendable_frame',\n    values_cols := ['values_block_0', 'A', 'B', 'string', 'string2']]\n/df/table (Table(30,)) ''\n  description := {\n  \"index\": Int64Col(shape=(), dflt=0, pos=0),\n  \"values_block_0\": Float64Col(shape=(2,), dflt=0.0, pos=1),\n  \"A\": Float64Col(shape=(), dflt=0.0, pos=2),\n  \"B\": Float64Col(shape=(), dflt=0.0, pos=3),\n  \"string\": StringCol(itemsize=3, shape=(), dflt='', pos=4),\n  \"string2\": StringCol(itemsize=3, shape=(), dflt='', pos=5)}\n  byteorder := 'little'\n  chunkshape := (1424,)\n  autoIndex := True\n  colindexes := {\n    \"A\": Index(6, medium, shuffle, zlib(1)).is_CSI=False,\n    \"index\": Index(6, medium, shuffle, zlib(1)).is_CSI=False,\n    \"B\": Index(6, medium, shuffle, zlib(1)).is_CSI=False,\n    \"string2\": Index(6, medium, shuffle, zlib(1)).is_CSI=False,\n    \"string\": Index(6, medium, shuffle, zlib(1)).is_CSI=False}\n  /df/table._v_attrs (AttributeSet), 27 attributes:\n   [A_dtype := 'float64',\n    A_kind := ['A'],\n    B_dtype := 'float64',\n    B_kind := ['B'],\n    CLASS := 'TABLE',\n    FIELD_0_FILL := 0,\n    FIELD_0_NAME := 'index',\n    FIELD_1_FILL := 0.0,\n    FIELD_1_NAME := 'values_block_0',\n    FIELD_2_FILL := 0.0,\n    FIELD_2_NAME := 'A',\n    FIELD_3_FILL := 0.0,\n    FIELD_3_NAME := 'B',\n    FIELD_4_FILL := '',\n    FIELD_4_NAME := 'string',\n    FIELD_5_FILL := '',\n    FIELD_5_NAME := 'string2',\n    NROWS := 30,\n    TITLE := '',\n    VERSION := '2.6',\n    index_kind := 'datetime64',\n    string2_dtype := 'string24',\n    string2_kind := ['string2'],\n    string_dtype := 'string24',\n    string_kind := ['string'],\n    values_block_0_dtype := 'float64',\n    values_block_0_kind := ['C', 'D']]\n/df_dc (Group) ''\n  /df_dc._v_attrs (AttributeSet), 12 attributes:\n   [CLASS := 'GROUP',\n    TITLE := '',\n    VERSION := '1.0',\n    data_columns := ['B', 'C', 'string', 'string2', 'datetime'],\n    index_cols := [(0, 'index')],\n    levels := 1,\n    nan_rep := 'nan',\n    non_index_axes := [(1, ['A', 'B', 'C', 'D', 'string', 'string2', 'datetime'])],\n    pandas_type := 'frame_table',\n    pandas_version := '0.10.1',\n    table_type := 'appendable_frame',\n    values_cols := ['values_block_0', 'B', 'C', 'string', 'string2', 'datetime']]\n/df_dc/table (Table(30,)) ''\n  description := {\n  \"index\": Int64Col(shape=(), dflt=0, pos=0),\n  \"values_block_0\": Float64Col(shape=(2,), dflt=0.0, pos=1),\n  \"B\": Float64Col(shape=(), dflt=0.0, pos=2),\n  \"C\": Float64Col(shape=(), dflt=0.0, pos=3),\n  \"string\": StringCol(itemsize=3, shape=(), dflt='', pos=4),\n  \"string2\": StringCol(itemsize=4, shape=(), dflt='', pos=5),\n  \"datetime\": Int64Col(shape=(), dflt=0, pos=6)}\n  byteorder := 'little'\n  chunkshape := (1191,)\n  autoIndex := True\n  colindexes := {\n    \"index\": Index(6, medium, shuffle, zlib(1)).is_CSI=False,\n    \"C\": Index(6, medium, shuffle, zlib(1)).is_CSI=False,\n    \"B\": Index(6, medium, shuffle, zlib(1)).is_CSI=False,\n    \"string\": Index(6, medium, shuffle, zlib(1)).is_CSI=False,\n    \"string2\": Index(6, medium, shuffle, zlib(1)).is_CSI=False,\n    \"datetime\": Index(6, medium, shuffle, zlib(1)).is_CSI=False}\n  /df_dc/table._v_attrs (AttributeSet), 31 attributes:\n   [B_dtype := 'float64',\n    B_kind := ['B'],\n    CLASS := 'TABLE',\n    C_dtype := 'float64',\n    C_kind := ['C'],\n    FIELD_0_FILL := 0,\n    FIELD_0_NAME := 'index',\n    FIELD_1_FILL := 0.0,\n    FIELD_1_NAME := 'values_block_0',\n    FIELD_2_FILL := 0.0,\n    FIELD_2_NAME := 'B',\n    FIELD_3_FILL := 0.0,\n    FIELD_3_NAME := 'C',\n    FIELD_4_FILL := '',\n    FIELD_4_NAME := 'string',\n    FIELD_5_FILL := '',\n    FIELD_5_NAME := 'string2',\n    FIELD_6_FILL := 0,\n    FIELD_6_NAME := 'datetime',\n    NROWS := 30,\n    TITLE := '',\n    VERSION := '2.6',\n    datetime_dtype := 'datetime64',\n    datetime_kind := ['datetime'],\n    index_kind := 'datetime64',\n    string2_dtype := 'string32',\n    string2_kind := ['string2'],\n    string_dtype := 'string24',\n    string_kind := ['string'],\n    values_block_0_dtype := 'float64',\n    values_block_0_kind := ['A', 'D']]\n```\n",
      "looks like it fully completed that test (the df_dc node is created at the end)....so weird that it would think there were 2 handles open....\n",
      "I inserted `return` statements into the test and it starts failing after the final `self.store.append` at the end of the function-- if you end the test sooner it still works. So something bad going on inside there\n",
      "to be perfectly clear:\n\n```\n        self.store.remove('df_dc')\n        self.store.append('df_dc', df_dc, data_columns=['B', 'C',\n                          'string', 'string2', 'datetime'])\n        result = self.store.select('df_dc', [Term('B>0')])\n```\n\nIf I put `return` after the first remove statement, everything is OK. Somehow another file handle is being created inside that append. weird\n",
      "Or there is a reference to the handle that is not being garbage collected\n",
      "weird...i have no easy way to test this....runs fine on linux (which generally is better about file handles anyhow)\n",
      "Must be a bug of some kind in pytables. i don't have the time for a bisection exercise to find out where the dangling reference is created. skipping failure to os.remove file and the test passes now\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 2,
    "additions": 11,
    "deletions": 4,
    "changed_files_list": [
      "pandas/io/pytables.py",
      "pandas/io/tests/test_pytables.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2649,
    "reporter": "jankatins",
    "created_at": "2013-01-06T21:34:29+00:00",
    "closed_at": "2013-01-20T20:15:38+00:00",
    "resolver": "wesm",
    "resolved_in": "e68e7b89eed3c05d7811ed7abcca343ac0b488e1",
    "resolver_commit_num": 2805,
    "title": "Better error message when merging fails because of non unique index",
    "body": "I've seen this error lot's of times in the last few days and usually it usuall took me a long time to get it working (and mostly I had no idea why it worked when I did something differently :-( ). The below error is actually from \n\nWould it be possibleto let the merge method catch this error and let it produce a more informative error message which says what has to be done differently? Or the original Exception say what index (value) is the problem?\n\n\n",
    "labels": [],
    "comments": [
      "Somehow this feels like a regression, as I would be surprised that it would work in statsmodels/statsmodels#582 be simple adding different column names for each merged dataframe.\n"
    ],
    "events": [
      "commented",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 25,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2654,
    "reporter": "floux",
    "created_at": "2013-01-07T17:39:54+00:00",
    "closed_at": "2013-01-21T19:07:46+00:00",
    "resolver": "wesm",
    "resolved_in": "bbfb95d5d14e2c6541e544eb1c725c3b42399c74",
    "resolver_commit_num": 2810,
    "title": "read_csv in combination with index_col and usecols",
    "body": "Starting point:\n\n-docs/stable/io.html#index-columns-and-trailing-delimiters\n\nIf there is one more column of data than there are colum names, usecols exhibits some (at least for me) unintuitive behavior:\n\n\n\nI was expecting it to be equal to\n\n\n\nI am not sure if my expectation is unfounded, though, and that this behavior is indeed intentional?\n",
    "labels": [],
    "comments": [
      "This feels buggy or at minimum not intuitive to me. I think it's just an edge case that's not addressed in the test suite. I'll have a look\n"
    ],
    "events": [
      "commented",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 36,
    "deletions": 16,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/parser.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2668,
    "reporter": "hayd",
    "created_at": "2013-01-09T15:25:55+00:00",
    "closed_at": "2013-01-20T01:29:42+00:00",
    "resolver": "wesm",
    "resolved_in": "e05a3d064164e6357511254c7f7ff43dc821761a",
    "resolver_commit_num": 2793,
    "title": "Odd behaviour in read_csv",
    "body": "This is a strange example from [this StackOverflow question](-read-csv-strange-behavior-for-empty-default-values):\n\n\n\n\n",
    "labels": [],
    "comments": [
      "Not cool. Marked for 0.10.1\n"
    ],
    "events": [
      "commented",
      "assigned"
    ],
    "changed_files": 4,
    "additions": 21,
    "deletions": 4,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/parser/tokenizer.c",
      "pandas/src/parser/tokenizer.h"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2684,
    "reporter": "gerigk",
    "created_at": "2013-01-11T11:41:40+00:00",
    "closed_at": "2013-01-21T16:54:33+00:00",
    "resolver": "wesm",
    "resolved_in": "2842ad12f2bd63e617f6aeeed8a9ff7f8767061f",
    "resolver_commit_num": 2808,
    "title": "MemoryError on df.sortlevel(k)",
    "body": "see -does-pandas-pandas-pydata-org-throw-a-memory-error-on-df-sortlevelk/14267566#14267566\nUnfortunately I can't provide the data set.\n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 3,
    "additions": 18,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2690,
    "reporter": "gdraps",
    "created_at": "2013-01-14T09:32:28+00:00",
    "closed_at": "2013-01-19T23:39:12+00:00",
    "resolver": "wesm",
    "resolved_in": "78d090af266049bad913ac344f38a796686560a5",
    "resolver_commit_num": 2789,
    "title": "combinatorial explosion when merging dataframes",
    "body": "Hi Wes, Not sure if this is real, but opening to follow up on a comment in -explosion-when-merging-dataframes-in-pandas.  Here's a fabricated example to reproduce `MergeError: Combinatorial explosion! (boom)` in tools/merge.py:\n\n\n\nThe value of group_sizes at the time of exception is:\n\n\n\nThe exception does not occur if the common columns are set as the index, though.\n\n\n",
    "labels": [],
    "comments": [
      "Thanks. I'll have a look when I can (later this week)\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 3,
    "additions": 21,
    "deletions": 6,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2692,
    "reporter": "lselector",
    "created_at": "2013-01-14T18:52:07+00:00",
    "closed_at": "2013-01-19T23:23:56+00:00",
    "resolver": "wesm",
    "resolved_in": "b5b04e0d7f342d7396612422ae421a104aa96b30",
    "resolver_commit_num": 2788,
    "title": "groupby().sum() very slow when applied to boolean columns",
    "body": "While upgrading pandas from 0.7.2 to 0.9.1 we have bumped into slowness of certain groupby().sum() operations. Here is a simple example:\n\nN=10000\naa=DataFrame({'ii':range(N),'bb':[True for x in range(N)]})\ntimeit aa.sum()  # fast\ntimeit aa.groupby('bb').sum() #fast\ntimeit aa.groupby('ii').sum()  # very slow (~ 1000 times slower)\n",
    "labels": [],
    "comments": [
      "Strange. Thanks for letting me know-- I will have a look\n"
    ],
    "events": [
      "commented",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 13,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/common.py",
      "vb_suite/groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2698,
    "reporter": "wesm",
    "created_at": "2013-01-15T15:21:26+00:00",
    "closed_at": "2013-01-20T02:24:56+00:00",
    "resolver": "wesm",
    "resolved_in": "923d31195ec05021acda9399e18f92a2e3a4fb97",
    "resolver_commit_num": 2795,
    "title": "Parsers should attempt ISO8601 date parsing before using slow code path",
    "body": "",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 3,
    "additions": 28,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/parsers.py",
      "vb_suite/io_bench.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2699,
    "reporter": "wesm",
    "created_at": "2013-01-15T15:31:18+00:00",
    "closed_at": "2013-01-19T23:45:48+00:00",
    "resolver": "wesm",
    "resolved_in": "03bee8d22728fadd6e97bc2b508c9977a236bb8a",
    "resolver_commit_num": 2790,
    "title": "pandas.to_datetime called on existing datetime64 Series results in bad data",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 3,
    "additions": 12,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tools.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2700,
    "reporter": "lselector",
    "created_at": "2013-01-15T17:24:16+00:00",
    "closed_at": "2013-01-20T01:41:22+00:00",
    "resolver": "wesm",
    "resolved_in": "dda23636354136f753561ead07bdbe88b21fa5ae",
    "resolver_commit_num": 2794,
    "title": "groupby().max() operation removes string columns",
    "body": "While upgrading pandas from 0.7.2 to 0.9.1 we have found that groupby().max() operation now removes non-numeric columns. This broke our code in several places. Workaround is to use  groupby().aggregate(np.max).\n\nHere is an example demonstrating the problem:\n\naa=DataFrame({'nn':[11,11,22,22],'ii':[1,2,3,4],'ss':4*['mama']})\naa.groupby('nn').max()\n\noutput on pandas 0.7.2\n   ii  nn    ss\nnn  \n11  2  11  mama\n22  4  22  mama\n\noutput on pandas 0.9.1\n    ii\nnn  \n11   2\n22   4\n\nAs you see, object column 'ss' is dropped in new version !!!\nThis was very un-intuitive.\n",
    "labels": [],
    "comments": [
      "I'll have a look. It's not obvious that this should work by default on non-numeric data\n",
      "strings have __lt__() defined so the built in min() and max() work on them. If the non-numeric object supports the proper comparison methods, min() and max() aggregate functions should be non-ambiguous.\n"
    ],
    "events": [
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 14,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2729,
    "reporter": "bshanks",
    "created_at": "2013-01-23T00:07:19+00:00",
    "closed_at": "2013-04-08T04:59:30+00:00",
    "resolver": "wesm",
    "resolved_in": "e5e21af6b1e2f232000284e791ed1fc8c34b9973",
    "resolver_commit_num": 2856,
    "title": "http://pandas.pydata.org/pandas-docs/stable/gotchas.html should note that DataFrame objects are not threadsafe",
    "body": "-docs/stable/gotchas.html should note that DataFrame objects are not threadsafe: -pandas-dataframe-thread-safe\n\n(as of 0.7.3, they still are not threadsafe afaict)\n",
    "labels": [],
    "comments": [
      "want to do a PR for this?, prob not going to actually be thread safe for a while\n\nsee #2728, #2440\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 1,
    "additions": 11,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/gotchas.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2775,
    "reporter": "gtakacs",
    "created_at": "2013-01-30T14:25:05+00:00",
    "closed_at": "2013-02-10T22:37:40+00:00",
    "resolver": "wesm",
    "resolved_in": "9faec8293aa7fda64b85f3aa05bff5600c7b28d1",
    "resolver_commit_num": 2834,
    "title": "segmentation fault in fillna",
    "body": "The following code causes segmentation fault (tried with pandas 0.10.1):\npandas.DataFrame(columns=[\"x\"]).x.fillna(method=\"pad\", inplace=1)\n",
    "labels": [],
    "comments": [
      "fixed in PR #2708\n"
    ],
    "events": [
      "cross-referenced",
      "commented"
    ],
    "changed_files": 2,
    "additions": 4,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2797,
    "reporter": "dhirschfeld",
    "created_at": "2013-02-04T19:06:25+00:00",
    "closed_at": "2013-03-28T05:51:14+00:00",
    "resolver": "wesm",
    "resolved_in": "250c8e08a1a7b9686658f0e737560d9b587f7567",
    "resolver_commit_num": 2852,
    "title": "BUG: `concat` throwing TypeError when `ignore_index` is True",
    "body": "\n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 3,
    "additions": 18,
    "deletions": 6,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2856,
    "reporter": "lodagro",
    "created_at": "2013-02-12T21:23:42+00:00",
    "closed_at": "2013-04-12T13:21:06+00:00",
    "resolver": "lodagro",
    "resolved_in": "745408ff63e83bfc73aa18ee4b972af20f40e515",
    "resolver_commit_num": 63,
    "title": "max_columns=0 no longer switches over to summary view",
    "body": "\n\ndf wraps it columns, but no switch over to summary view\n\n\n\nsee also [mailing list](?fromgroups=#!topic/pydata/FNcnHz-PskI)\n",
    "labels": [],
    "comments": [
      "Just making sure you're aware of the shiny `pd.options` shortcut with tab completion.\n",
      "@y-p thanks for the tip!\n",
      "@lodagro wants to put finishing touches on #2856. Note the 0.11 issue \nqueue is empty now, so better be quick...\n",
      "done, i pushed directly to master\n"
    ],
    "events": [
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "referenced",
      "closed",
      "reopened",
      "commented",
      "mentioned",
      "subscribed",
      "referenced"
    ],
    "changed_files": 1,
    "additions": 5,
    "deletions": 6,
    "changed_files_list": [
      "pandas/core/frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2932,
    "reporter": "hayd",
    "created_at": "2013-02-26T00:55:57+00:00",
    "closed_at": "2013-03-28T05:28:02+00:00",
    "resolver": "wesm",
    "resolved_in": "94893ff7a87e2aa89fd385ebaec711df8df6f753",
    "resolver_commit_num": 2848,
    "title": "BUG date_parser not accurate with Timestamp",
    "body": "When reading in this DataFrame (with `parse_dates=[[0,1]]`) we see the datetimes have (29000) microseconds but when using `date_parser=pd.Timestamp` we lose this information:\n\n\n\n\n\n_Note: This works accurately individually:_\n\n\n\nMigrated from [StackOverflow](-to-efficiently-handle-pandas-read-csv-with-datetime-index).\n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 3,
    "additions": 15,
    "deletions": 4,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 2981,
    "reporter": "wesm",
    "created_at": "2013-03-06T19:53:49+00:00",
    "closed_at": "2013-04-08T07:36:14+00:00",
    "resolver": "wesm",
    "resolved_in": "900a552f92aaabf3d13e72670a3959c8a396c1df",
    "resolver_commit_num": 2854,
    "title": "Issues reading ragged CSV files",
    "body": "-variable-number-of-columns-with-pandas-python\n",
    "labels": [],
    "comments": [
      "I just came across another case where because the input data is very ragged, it's surprisingly difficult even to get the data into a DataFrame for processing.  Entirely independent of performance, it'd be nice to have a canonical way which Just Worked(tm) for this.\n",
      "And we're in business (\"just works\" now):\n\n```\nIn [5]: paste\ndata = \"\"\"1,2,3\n1,2,3,4\n1,2,3,4,5\n1,2\n1,2,3,4\"\"\"\n\n## -- End pasted text --\n\nIn [6]: pd.read_csv(StringIO(data), names=['a', 'b', 'c', 'd', 'e'])\nOut[6]: \n   a  b   c   d   e\n0  1  2   3 NaN NaN\n1  1  2   3   4 NaN\n2  1  2   3   4   5\n3  1  2 NaN NaN NaN\n4  1  2   3   4 NaN\n```\n",
      "I think the fix may have unintended consequences:\n\n```\n>>> import pandas as pd\n>>> from StringIO import StringIO\n>>> \n>>> data = \"\"\"\n... 1,2\n... 3,4,5\n... \"\"\".strip()\n>>> \n>>> pd.read_csv(StringIO(data), header=None, names=range(3))\n   0  1   2\n0  1  2 NaN\n1  3  4   5\n>>> pd.read_csv(StringIO(data), header=None, names=range(20))\n*** glibc detected *** python: realloc(): invalid next size: 0x0a58f158 ***\n```\n\nNot sure if it's worth opening a separate issue or not.  \n",
      "Thanks. I will have a look at the C code\n"
    ],
    "events": [
      "closed",
      "reopened",
      "commented",
      "closed",
      "commented",
      "cross-referenced",
      "commented",
      "reopened",
      "commented",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 53,
    "deletions": 11,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/parser.pyx",
      "pandas/src/parser/tokenizer.c",
      "pandas/src/parser/tokenizer.h"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 3020,
    "reporter": "dsimpson1980",
    "created_at": "2013-03-12T13:46:53+00:00",
    "closed_at": "2013-03-18T17:25:38+00:00",
    "resolver": "wesm",
    "resolved_in": "0098856366c2368fe67f2ab2d75999242df7fe78",
    "resolver_commit_num": 2843,
    "title": "resample up skips first day",
    "body": "When resampling from a TimeSeries with freq='MS' to freq='D' the TimeSeries is shifted forward by one day, skipping the first element:\n\n\n",
    "labels": [],
    "comments": [
      "A unit-test for this bug:\n\n``` python\ndef test_resample_doesnt_truncate():\n    \"\"\"Test for issue #3020\"\"\"\n    import pandas as pd\n    dates = pd.date_range('01-Jan-2014','05-Jan-2014', freq='D')\n    series = pd.TimeSeries(1, index=dates)\n    series = series.resample('D')\n    assert series.index[0] == dates[0]\n```\n"
    ],
    "events": [
      "commented",
      "cross-referenced",
      "assigned"
    ],
    "changed_files": 3,
    "additions": 19,
    "deletions": 3,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 3033,
    "reporter": "stephenwlin",
    "created_at": "2013-03-13T00:13:41+00:00",
    "closed_at": "2016-08-17T10:21:25+00:00",
    "resolver": "chris-b1",
    "resolved_in": "07804437d3a7e10d7a5472a7edc95e3dcc31bc6d",
    "resolver_commit_num": 43,
    "title": "TODO: convert from `ndarray.take` to internal takes, remove platform int <-> int64 conversions?",
    "body": "just throwing this out there, spliting off from discussion at #2892... right now index arrays are constantly being converted back and forth between platform int and int64 on 32-bit platforms, since `ndarray.take` requires the former but the cython routines use the latter\n\nthis can probably be removed if all `ndarray.take` usages can be converted to use cython routines, simplifying things considerably and probably improving performance (due to avoiding the conversions, and also since `ndarray.take` is generic and isn't specialized to handle 1-d and 2-d arrays or particular types efficiently)\n\nthe only caveats are that `take_nd` doesn't allow normal negative indices and uses promotion/fill behavior by default, the latter can be turned off with `allow_fill=False` but that might get pretty redundant if it's used everywhere; probably best to write a helper which does it. as for the former, there might not be any more cases of places where normal negative indicies are required anymore after #3027...not really sure \n",
    "labels": [],
    "comments": [
      "I'm strongly +1 on getting rid of all the platform int conversions. Fused types (fuse at minimum int32_t, int64_t) can alleviate some of the hurting. Was the bane of my existence for a long period of time\n",
      "@stephenwlin any interest still in this?\n"
    ],
    "events": [
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 22,
    "additions": 200,
    "deletions": 146,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/algorithms.py",
      "pandas/hashtable.pyx",
      "pandas/indexes/base.py",
      "pandas/src/algos_common_helper.pxi",
      "pandas/src/algos_common_helper.pxi.in",
      "pandas/src/join.pyx",
      "pandas/tests/frame/test_operators.py",
      "pandas/tests/indexes/common.py",
      "pandas/tests/indexes/test_base.py",
      "pandas/tests/indexes/test_category.py",
      "pandas/tests/indexes/test_datetimelike.py",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tests/indexes/test_numeric.py",
      "pandas/tests/indexes/test_range.py",
      "pandas/tests/test_algos.py",
      "pandas/tests/test_base.py",
      "pandas/tests/test_groupby.py",
      "pandas/tools/merge.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_timedeltas.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 3071,
    "reporter": "taavi",
    "created_at": "2013-03-17T02:11:47+00:00",
    "closed_at": "2013-03-17T03:11:33+00:00",
    "resolver": "wesm",
    "resolved_in": "9e99a5e2bf9ba6bb54216700fdf3c15a5648f337",
    "resolver_commit_num": 2842,
    "title": "read_csv(date_parser=x) fails because datetime.datetime has no dtype",
    "body": "\n\nThe crash looks like this (on 0.11.0.dev-6e7b37b, OSX 10.6 if it matters):\n\n\n\nThanks!\n",
    "labels": [],
    "comments": [],
    "events": [],
    "changed_files": 3,
    "additions": 20,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 3223,
    "reporter": "wesm",
    "created_at": "2013-03-30T19:18:42+00:00",
    "closed_at": "2013-04-09T00:32:41+00:00",
    "resolver": "wesm",
    "resolved_in": "2c81846a35175a43a75c29dfe1b7c3792ec23c67",
    "resolver_commit_num": 2867,
    "title": "Series.str[...] should maybe default to NA if some list/tuple does not contain enough values",
    "body": "",
    "labels": [],
    "comments": [
      "Anyone have an opinion on this? Here is an example:\n\n```\nIn [8]: s = Series([(1, 2), (1,), (3,4,5)])\n\nIn [9]: s.str[0]\nOut[9]: \n0    1\n1    1\n2    3\ndtype: int64\n\nIn [10]: s.str[1]\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n<ipython-input-10-946f1b211e37> in <module>()\n----> 1 s.str[1]\n\n/Users/wesm/code/pandas/pandas/core/strings.pyc in __getitem__(self, key)\n    660                               step=key.step)\n    661         else:\n--> 662             return self.get(key)\n    663 \n    664     def _wrap_result(self, result):\n\n/Users/wesm/code/pandas/pandas/core/strings.pyc in get(self, i)\n    678     @copy(str_get)\n    679     def get(self, i):\n--> 680         result = str_get(self.series, i)\n    681         return self._wrap_result(result)\n    682 \n\n/Users/wesm/code/pandas/pandas/core/strings.pyc in str_get(arr, i)\n    561     \"\"\"\n    562     f = lambda x: x[i]\n--> 563     return _na_map(f, arr)\n    564 \n    565 \n\n/Users/wesm/code/pandas/pandas/core/strings.pyc in _na_map(f, arr, na_result)\n     87 def _na_map(f, arr, na_result=np.nan):\n     88     # should really _check_ for NA\n---> 89     return _map(f, arr, na_mask=True, na_value=na_result)\n     90 \n     91 \n\n/Users/wesm/code/pandas/pandas/core/strings.pyc in _map(f, arr, na_mask, na_value)\n     96         mask = isnull(arr)\n     97         try:\n---> 98             result = lib.map_infer_mask(arr, f, mask.view(np.uint8))\n     99         except (TypeError, AttributeError):\n    100             def g(x):\n\n/Users/wesm/code/pandas/pandas/lib.so in pandas.lib.map_infer_mask (pandas/lib.c:41222)()\n\n/Users/wesm/code/pandas/pandas/core/strings.pyc in <lambda>(x)\n    560     items : array\n    561     \"\"\"\n--> 562     f = lambda x: x[i]\n    563     return _na_map(f, arr)\n    564 \n\nIndexError: tuple index out of range\n```\n",
      "I agree, should default to return na, as you are viewing the Series effectiviely like a Frame here\n\nalso note that this is another case of the above (just slightly different exception)\n\n```\nIn [6]: s = pd.Series([\"foo\",\"b\",\"bar\"])\n\nIn [7]: s\nOut[7]: \n0    foo\n1      b\n2    bar\n\nIn [8]: s.str[0]\nOut[8]: \n0    f\n1    b\n2    b\n\nIn [9]: s.str[1]\n\nIndexError: string index out of range\n```\n"
    ],
    "events": [
      "assigned",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 17,
    "deletions": 1,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/strings.py",
      "pandas/tests/test_strings.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 3258,
    "reporter": "nisaggarwal",
    "created_at": "2013-04-04T20:44:55+00:00",
    "closed_at": "2013-04-08T19:51:29+00:00",
    "resolver": "wesm",
    "resolved_in": "6b5ee264598c523ccaf6866758de8a3daa657001",
    "resolver_commit_num": 2865,
    "title": "read_csv, read_table in version 0.9.0 are parsing integers as double but reporting type as int64",
    "body": "For example a file containing the text below, read using:\n\n\n\nis 2.0 for this range of numbers\n\nbut the type reported for the value is int64 not double/float64\n\nfile.log contains:\n\n\n",
    "labels": [],
    "comments": [
      "This works in 0.10.1 and git master, recommend you update\n\n```\nIn [3]: read_clipboard()\nOut[3]: \n             Numbers\n0  17007000002000191\n1  17007000002000191\n2  17007000002000191\n3  17007000002000191\n4  17007000002000192\n5  17007000002000192\n6  17007000002000192\n7  17007000002000192\n8  17007000002000192\n9  17007000002000194\n```\n\nHowever, it's broken with the \"python parser\" (`engine='python'`). Let me look into this quickly.\n"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 3,
    "additions": 35,
    "deletions": 2,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/inference.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 3308,
    "reporter": "TomAugspurger",
    "created_at": "2013-04-10T02:56:47+00:00",
    "closed_at": "2013-04-12T04:44:35+00:00",
    "resolver": "wesm",
    "resolved_in": "859d2605d5a2bcbbe963a8f800b8be1061f55b7d",
    "resolver_commit_num": 2885,
    "title": "segfault on set_index",
    "body": "I probably shouldn't have been doing this in the first place, but I'm able to consistently generate a segfault with:\n\n\n\n is probably relevant.  I have epsilon experience with Cython so I won't really be able to help out with this one.  Happy to do more debugging if you need any though.\n",
    "labels": [],
    "comments": [],
    "events": [
      "assigned"
    ],
    "changed_files": 4,
    "additions": 18,
    "deletions": 5,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/core/index.py",
      "pandas/core/indexing.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 3406,
    "reporter": "vshkolyar",
    "created_at": "2013-04-20T16:26:42+00:00",
    "closed_at": "2013-04-22T23:52:41+00:00",
    "resolver": "wesm",
    "resolved_in": "f24b923099a543eeefb2e4894215bd356410d9d4",
    "resolver_commit_num": 2891,
    "title": "TextFileReader.get_chunk returns full file DataFram despite of chunksize specified in read_csv",
    "body": "TextFileReader.get_chunk returns full file DataFram despite of chunksize specified in read_csv.\n",
    "labels": [],
    "comments": [
      "can you show exactly what you are doing? you need to specify the number of rows that you want in `get_chunk`, otherwise it returns the full frame. normally you would just iterator over the returned object anyhow.\n\nhttp://pandas.pydata.org/pandas-docs/dev/io.html#iterating-through-files-chunk-by-chunk\n",
      "read_csv method called with some specific chunk size.\nWhen I call afterwards get_chunk method there is two cases:\n1. chunksize parameter passed to the method - I expect to receive chunk with size specified as argument in the method that overrides the instance chunk size the TextFileReader initialized with (passed to read_csv as parameter), and this OK.\n2. But, in case no such parameter passed to the get_chunk, I would expect to receive DataFrame with chunk size specified in read_csv, that TextFileReader instance initialized with and stored as instance variable (property). Again, that because get_chunk is type's instance method (not static type method, not some global function), and this instance of this type holds the chunksize member inside.\n\nP.S. So I can call my_text_file_reader.get_chunk(my_text_file_reader.chunksize), but this is ugly and not object oriented.\n",
      "@wes this is expected behavior yes?\nIOW get_chunk allows u to get variable sizes chunks by calling with a parameter, or everything if no parameter? or should default to passed chunk size ?\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 24,
    "deletions": 5,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 3453,
    "reporter": "sandbox",
    "created_at": "2013-04-25T05:22:37+00:00",
    "closed_at": "2013-06-02T23:26:45+00:00",
    "resolver": "wesm",
    "resolved_in": "c549299c1ed66d94680fed798d4c628a1c460a16",
    "resolver_commit_num": 2902,
    "title": "read_csv parse issues with \\r line ending and quoted items",
    "body": "There seems to be an issue with quotes containing the separator in read_csv\n\n\n##### EXPECTED BEHAVIOR:\n\n\n\nThis should have the same behavior as when the line ending is `\\n`\n\n---\n\nMaybe this should be in a separate bug report, but a possibly related issue occurs when you don't say `header=None`\n\n\n\nThe above shows the first quoted-delimited item set as the `index_col`.  The following shows what happens when we tell pandas to use `index_col=False`\n\n\n##### EXPECTED BEHAVIOR:\n\n\n\nand with index_col=False\n\n\n\n---\n\nHere is my system information if that is necessary\n\n\n",
    "labels": [],
    "comments": [
      "And testing this out with the latest from github gives me the same issues\n\n``` python\n>>> pd.__version__\n'0.12.0.dev-1e2b447'\n>>> import StringIO\n>>> pd.read_csv(StringIO.StringIO(' a,b,c\\r\"a,b\",\"e,d\",\"f,f\"'), header=None)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/john/app/venv/lib/python2.7/site-packages/pandas/io/parsers.py\", line 401, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/john/app/venv/lib/python2.7/site-packages/pandas/io/parsers.py\", line 216, in _read\n    return parser.read()\n  File \"/home/john/app/venv/lib/python2.7/site-packages/pandas/io/parsers.py\", line 634, in read\n    ret = self._engine.read(nrows)\n  File \"/home/john/app/venv/lib/python2.7/site-packages/pandas/io/parsers.py\", line 958, in read\n    data = self._reader.read(nrows)\n  File \"parser.pyx\", line 654, in pandas._parser.TextReader.read (pandas/src/parser.c:6014)\n  File \"parser.pyx\", line 676, in pandas._parser.TextReader._read_low_memory (pandas/src/parser.c:6231)\n  File \"parser.pyx\", line 729, in pandas._parser.TextReader._read_rows (pandas/src/parser.c:6833)\n  File \"parser.pyx\", line 716, in pandas._parser.TextReader._tokenize_rows (pandas/src/parser.c:6718)\n  File \"parser.pyx\", line 1582, in pandas._parser.raise_parser_error (pandas/src/parser.c:17131)\npandas._parser.CParserError: Error tokenizing data. C error: Expected 3 fields in line 2, saw 4\n>>> pd.read_csv(StringIO.StringIO(' a,b,c\\r\"a,b\",\"e,d\",\"f,f\"'))\n     a    b    c\n\"a  b\"  e,d  f,f\n>>> pd.read_csv(StringIO.StringIO(' a,b,c\\r\"a,b\",\"e,d\",\"f,f\"'), index_col=False)\n    a   b    c\n0  \"a  b\"  e,d\n```\n",
      "And also confirming that this error occurs in `0.11.0`\n\n``` python\n>>> pd.__version__\n'0.11.0'\n>>> pd.read_csv(StringIO.StringIO(' a,b,c\\r\"a,b\",\"e,d\",\"f,f\"'), header=None)\npd.read_csv(StringIO.StringIO(' a,b,c\\r\"a,b\",\"e,d\",\"f,f\"'), header=None)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/john/app/venv/lib/python2.7/site-packages/pandas/io/parsers.py\", line 401, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/john/app/venv/lib/python2.7/site-packages/pandas/io/parsers.py\", line 216, in _read\n    return parser.read()\n  File \"/home/john/app/venv/lib/python2.7/site-packages/pandas/io/parsers.py\", line 633, in read\n    ret = self._engine.read(nrows)\n  File \"/home/john/app/venv/lib/python2.7/site-packages/pandas/io/parsers.py\", line 957, in read\n    data = self._reader.read(nrows)\n  File \"parser.pyx\", line 654, in pandas._parser.TextReader.read (pandas/src/parser.c:5921)\n  File \"parser.pyx\", line 676, in pandas._parser.TextReader._read_low_memory (pandas/src/parser.c:6138)\n  File \"parser.pyx\", line 729, in pandas._parser.TextReader._read_rows (pandas/src/parser.c:6740)\n  File \"parser.pyx\", line 716, in pandas._parser.TextReader._tokenize_rows (pandas/src/parser.c:6625)\n  File \"parser.pyx\", line 1582, in pandas._parser.raise_parser_error (pandas/src/parser.c:17029)\npandas._parser.CParserError: Error tokenizing data. C error: Expected 3 fields in line 2, saw 4\n```\n",
      "Looking\n"
    ],
    "events": [
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "assigned",
      "commented"
    ],
    "changed_files": 4,
    "additions": 43,
    "deletions": 10,
    "changed_files_list": [
      "RELEASE.rst",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/parser/tokenizer.c",
      "pandas/src/parser/tokenizer.h"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 3714,
    "reporter": "gerigk",
    "created_at": "2013-05-30T12:21:08+00:00",
    "closed_at": "2013-08-11T20:01:03+00:00",
    "resolver": "jtratner",
    "resolved_in": "a19ab47cc008d77217f4c3f1d72e4f1e945c5312",
    "resolver_commit_num": 31,
    "title": "Weird MultiIndex bug",
    "body": "Bug: Assigning levels/labels to a multiindex (or really any fields in Index) should raise (if done externally)\n\nThis is (for me) one of the weirdest things I have found so far (0.10.1 still)\ntl;dr\nit seems like df.copy() creates a shallow copy of the MultiIndex w.r.t levels\nAlso, the setting of the index.levels does not seem to have effect on the index itself\n\nRunning the same code twice results in different results during the second time \nalthough I do not alter the original object in any place.\nalso note the line \"print data.index.levels[1]\" which changes after changing data2\n\nmy original goal (concatenating) always contains the original date in all rows\n\nCode\n\n\n\nOutput:\n\n\n",
    "labels": [],
    "comments": [
      "@gerigk \n\nWhen you copy the data, you get the same identical index, do\n\n`id(data.index) == id(data2.index)`\n\nthe problem is that you are changing the levels on a MultiIndex, which is an immutable object \nthis should not be allowed by a user of the index; if you need to change it, you need to create a new object\n\nThis is a bug in that assigning to levels should raise \n",
      "@wesm we do allow assigning to the `names`, which is ok (though in theory could run into the same problem), but clearly levels/labels assignment should be disallowed....?\n",
      "it definitely is confusing the way pandas behaves right now.\n\nso I would have to do something like\n\n```\ndef set_index_level(multi_index, level, k):\n    levels = multi_index.levels\n    levels[k] = level\n    new_index = pd.MultiIndex(levels=levels, labels=data2.index.labels, names=data2.index.names)\n    return new_index\ndata2.index = set_index_level(data2.index, pd.Index([date.today() + timedelta(1)]), 1)\n```\n\nor is there a more convenient way to do this?\n",
      "use set/reset index\n\n```\nIn [16]: data\nOut[16]: \n                       orders_ga  revenues_ga\n2013-04-30 2013-05-30         10            5\n2013-05-01 2013-05-30         15            7\n\nIn [17]: df = data.reset_index()\n\nIn [18]: df\nOut[18]: \n              level_0     level_1  orders_ga  revenues_ga\n0 2013-04-30 00:00:00  2013-05-30         10            5\n1 2013-05-01 00:00:00  2013-05-30         15            7\n\nIn [19]: df['new_level'] = df['level_1']+timedelta(1)\n\nIn [20]: df.set_index(['level_0','new_level'])\nOut[20]: \n                          level_1  orders_ga  revenues_ga\nlevel_0    new_level                                     \n2013-04-30 2013-05-31  2013-05-30         10            5\n2013-05-01 2013-05-31  2013-05-30         15            7\n```\n",
      "I don't know the internals but this sounds pretty expensive given that I\nactually want to change only one value inside one index level.\n\nDoesn't this first construct new columns, then I operate on a whole column\nand then I construct a complete new index?\n\nOn Mon, Jun 3, 2013 at 4:35 PM, jreback notifications@github.com wrote:\n\n> use set/reset index\n> \n> In [16]: data\n> Out[16]:\n>                        orders_ga  revenues_ga\n> 2013-04-30 2013-05-30         10            5\n> 2013-05-01 2013-05-30         15            7\n> \n> In [17]: df = data.reset_index()\n> \n> In [18]: df\n> Out[18]:\n>               level_0     level_1  orders_ga  revenues_ga\n> 0 2013-04-30 00:00:00  2013-05-30         10            5\n> 1 2013-05-01 00:00:00  2013-05-30         15            7\n> \n> In [19]: df['new_level'] = df['level_1']+timedelta(1)\n> \n> In [20]: df.set_index(['level_0','new_level'])\n> Out[20]:\n>                           level_1  orders_ga  revenues_ga\n> level_0    new_level\n> 2013-04-30 2013-05-31  2013-05-30         10            5\n> 2013-05-01 2013-05-31  2013-05-30         15            7\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/pydata/pandas/issues/3714#issuecomment-18845251\n> .\n",
      "this is pretty cheap to do; see if this actually a bottleneck for you\n\nwhat is your end goal?\n",
      "these ops can also be done inplace, fyi (pass `inplace=True`)\n",
      "`reorder_levels` might also be useful to you\n",
      "I have some situations where I want to change some values in the levels and in the labels.\nespecially the latter is interesting.\nassume I did a group by and I want to output the data (millions of entries).\nIf I want to change the labels according to a mapping then your reset_index() method would have to be followed by a series.apply() to map the labels to different labels which is slow.\nif there are only 10 distinct labels it is (I think) much faster to replace the .labels entry with my new mapping\n",
      "you can try to copy the index, then assign to the levels, then set the index\n\ne.g.\n\n```\nindex = index.copy()\nindex.levels[2] = 'foo'\ndf.index = index\n```\n\nto avoid the aliasing issue (this way you make sure that the index you have is ONLY attached to where you want it\n",
      "much better, thanks!\n",
      "great!\n\nthis is still a 'bug' in any event, so will keep this issue open\n",
      "ok.\n\njust for me to understand:\nis your solution\n\n```\n\nindex = index.copy()\nindex.levels[2] = 'foo'\ndf.index = index\n\n```\n\nsupposed to fail in the future since we still access the index.levels ?\n\nin this case my previously suggested (clumsy) method would be safer to use\n\nOn Mon, Jun 3, 2013 at 5:47 PM, jreback notifications@github.com wrote:\n\n> great!\n> \n> this is still a 'bug' in any event, so will keep this issue open\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/pydata/pandas/issues/3714#issuecomment-18850505\n> .\n",
      "At some point I think we might raise if you try to set level directly, so your solution bypasses that, but in the meantime go ahead and use it\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 21,
    "additions": 493,
    "deletions": 177,
    "changed_files_list": [
      "doc/source/indexing.rst",
      "doc/source/release.rst",
      "doc/source/v0.13.0.txt",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/index.py",
      "pandas/core/reshape.py",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_excel.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/io/tests/test_pytables.py",
      "pandas/tests/test_frame.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_index.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_panel4d.py",
      "pandas/tools/pivot.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tools/tests/test_pivot.py",
      "pandas/util/decorators.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 3715,
    "reporter": "gerdemb",
    "created_at": "2013-05-30T12:52:37+00:00",
    "closed_at": "2017-01-21T22:50:21+00:00",
    "resolver": "nchmura4",
    "resolved_in": "2540d5ac7dd772daa307395be982dc5ce86c3c3f",
    "resolver_commit_num": 0,
    "title": "ENH: Add fill_value option to resample()",
    "body": "see also #3707\n\nAdd a `fill_value` option to `resample()` so that it is possible to resample a `TimeSeries` without creating `NaN` values. If the series is an int dtype and an int is passed to `fill_value`, it should be possible to resample the series without casting the values to floats.\n\n\n",
    "labels": [
      "API Design"
    ],
    "comments": [
      "wouldn't this be the right way to do this?\ns.resample('M').sum().fillna(0)\n",
      "@nchmura4 that is a way, but this issue is about filling _before_ reindexing, like `.reindex` provides.\n\ne.g. compare this\n\n```\nIn [9]: s.reindex(pd.date_range(s.index.min(), s.index.max(), freq='D'), fill_value=0).resample('M').sum()\nOut[9]:\n2013-01-31    3\n2013-02-28    0\n2013-03-31    4\nFreq: M, dtype: int64\n\nIn [10]: s.reindex(pd.date_range(s.index.min(), s.index.max(), freq='D'), fill_value=1).resample('M').sum()\nOut[10]:\n2013-01-31    32\n2013-02-28    28\n2013-03-31     4\nFreq: M, dtype: int64\n```\n",
      "got it. thanks for the clarification!\n"
    ],
    "events": [
      "labeled",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 6,
    "additions": 164,
    "deletions": 21,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/generic.py",
      "pandas/tests/frame/test_timeseries.py",
      "pandas/tests/series/test_timeseries.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 3788,
    "reporter": "hayd",
    "created_at": "2013-06-07T03:29:37+00:00",
    "closed_at": "2016-11-22T11:39:11+00:00",
    "resolver": "mroeschke",
    "resolved_in": "3443de7801188dd07da65eafcc2792acd241f99f",
    "resolver_commit_num": 4,
    "title": "Aggregating over arrays",
    "body": "This _is_ frowned upon behaviour (storing arrays inside DataFrames) but is there a reason for this raise? \n\nDeleting the raising lines seems to only break tests to check that they're raising... \n\n\n\n\n\n-aggregate-when-column-contains-numpy-arrays\n",
    "labels": [
      "Difficulty Novice",
      "Effort Low",
      "Testing"
    ],
    "comments": [
      "Instead of the long traceback, let's be more explicit. Why does `_aggregate_named` in `pandas.core.groupby` check for `ndarray`?  Is there a reason behind it?\n\n``` python\ndef _aggregate_named(self, func, *args, **kwargs):\n    result = {}\n\n    for name, group in self:\n        group.name = name\n        output = func(group, *args, **kwargs)\n        if isinstance(output, np.ndarray):\n            raise Exception('Must produce aggregated value')\n        result[name] = self._try_cast(output, group)\n\n    return result\n```\n\nMy impression is that this happens because the result is unclear / not well-defined. Take the test case that expects an error from `test_groupby.TestGroupBy.test_basic`:\n\n``` python\ndata = Series(np.arange(9) // 3, index=np.arange(9))\ngrouped = data.groupby(lambda x: x // 3)\ngrouped.aggregate(lambda x: x * 3)\n```\n\nShould this return a dataframe with 3 columns? transform the column to an array of `ndarray`? If it should return an `ndarray`, you lose all of the ability to do fast operations on it, since it changes its dtype to `object`, etc.\n",
      "It seems no less well-defined than passing lists.\n\nAgreed that you lose all the fast operations, but if that's the only reason shouldn't this be left up to the user...?\n",
      "the `ndarray` test is testing whether this is a reduction; you are right it is not needed\n\nIf the function produces a Series, then the result of the groupby operation would be a DataFrame\n\nyou can't do this with an ndarray wbecause it can't align; I agree its sort of a degenerate case,\nprob should just create a Series and thus return a Frame...\n",
      "@hayd do you have an example where this would be a better choice than a different setup (Panel, etc.)? SO question user realized that they were going down the wrong path.\n\nHow should we handle this case, where you end up with a DataFrame with arrays in it? Create stacked columns?\n\n``` python\ndata = DataFrame({\"A\": np.arange(9) // 3, \"B\": np.arange(9)}, index=np.arange(9))\ngrouped = data.groupby(lambda x: x.A // 3)\ngrouped.aggregate({\"A\": lambda x: x * 3, \"B\": lambda x: x[0] * 3})\n```\n",
      "Could we just give a performance warning ? This seems easiest.\n\n_Then we don't need to worry about... lots of things in these weird corner cases._\n",
      "That makes a lot of sense. I'll write up a PR tonight.\n\nOn Fri, Jun 7, 2013 at 9:58 AM, Andy Hayden notifications@github.comwrote:\n\n> The easiest solution is to just giving a performance warning ?\n> \n> _Then we don't need to worry about... lots of things in these weird\n> corner cases._\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/pydata/pandas/issues/3788#issuecomment-19108370\n> .\n",
      "@jtratner \n\nfyi...there is a `PerformanceWarning` Warning I defined in io/pytables.py\nwhy don't you move that to core/common.py ?\n(need import then in io/pytables.py and io/tests/test_pytables.py)\n",
      "let's revisit in 0.14\n",
      "This seems to \"just work\" in master. Could add a test case...\n",
      "Hi, I still have this problem using pandas 0.18.0, when I'm generating a ndarray as a vector.\n\nwhy not allow ndarray as aggregated value?\n",
      "@Earthson as indicated if you are trying to return ndarrays then you are using pandas in a non-performant / non-supported way.\n",
      "this works in current master. anyone want to add a test?\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "labeled",
      "milestoned",
      "demilestoned",
      "commented",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "demilestoned",
      "unlabeled",
      "unlabeled",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 1,
    "additions": 17,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 3800,
    "reporter": "cpcloud",
    "created_at": "2013-06-07T22:27:22+00:00",
    "closed_at": "2016-01-24T23:04:08+00:00",
    "resolver": "jorisvandenbossche",
    "resolved_in": "1330b9fe40a2e31295ecec1fd0e00fb3712f7549",
    "resolver_commit_num": 560,
    "title": "add travis doc build",
    "body": "not sure if this is worth the trouble but it may catch version incompatibility issues and other bugs not caught by the test suite.\n",
    "labels": [
      "CI"
    ],
    "comments": [
      "Check out jtratner/pandas@2c285ea9d44545921b4def34d9c4f1244f80fc6b . I think that covers what you're looking for. It really depends how long the job takes - https://travis-ci.org/jtratner/pandas/jobs/7895295\n",
      "looks ok, but i'm thinking about the possible numpy dependency issue here. there's a few examples in the docs that throw an error for slightly older versions of numpy...i wouldn't want this to always fail because of that. i was actually thinking _only_ a doc build and no other tests so another row in the build matrix...\n",
      "@cpcloud I updated it a bit - jtratner/pandas@bcae377c493f97902b50a7f48e5f8d01dd8bcf12 . It does mean that the test now has to wait for one of the other tests to finish (since it looks like Travis will only run 5 tests concurrently), which means it adds about 20 minutes to testing time. Would it make more sense to incorporate with one of the other  version tests?\n",
      "An extra 20 minutes is unacceptable IMHO. Travis is already slow enough. Probably better to put it in one of the 2.7 like u had. Just need to make sure that no exceptions are generated (which is the point really...) but need to see if there are any benign exceptions generated by numpy... I get these when I run the doc build and I believe it's because I use dev numpy. \n",
      "i'm going to test a doc build on `pip install <dependency>`-backed pandas to see if any warnings are generated...if not this might be a go pending someone else's review...\n",
      "can we shove this in the 2.7 non-us build?\n",
      "probably would be ok, i just want to check those deps first...\n",
      "So, my branch wasn't actually testing doc building, because it didn't have `make doc` in the Makefile, but this wasn't causing an error. This is fixed now, but it required pulling in the Makefile commit too... (jtratner/pandas@8994c2629179f8391165ffa9510d7216710f4b02)\n",
      "@jreback can assume that the doc-building machine has `scikits.timeseries` installed so that it won't raise?\n",
      "yep has all deps AFAIN\n",
      "Okay, this doesn't work right now, because make docs is set to be silent. Also, it looks like sphinx doesn't get installed, so I'll have to fix again.\n",
      "also there are some doc build issues that i'm fixing right now...for some reason when branch depending on numpy versions in `timeseries.rst` i get a few errors in `visualisation.rst`. probably some variable aliasing somewhere...\n",
      "Looks like it needs IPython too ... check out the failing build: https://travis-ci.org/jtratner/pandas/jobs/7913726\n\n(at least we now know that it errors out here)\n",
      "yep just add a `pip install ipython`\n",
      "if u do that you might want to add ipython to `ci/print_versions.py`\n",
      "@cpcloud yeah, added that and also added sphinx to versions too\n",
      "@cpcloud now it finds a weird unicode issue - do you get this too? https://travis-ci.org/jtratner/pandas/jobs/7915482\n",
      "might be because of the locale...see if u can repro it on ur machine...i'll try in a [vagrant box](http://www.vagrantup.com)\n",
      "actually before we do that can u try running it in the non zh_CN locale version\n",
      "yeah, though I have 2 builds pending so it might take a while.\n\nOn Sat, Jun 8, 2013 at 10:12 PM, Phillip Cloud notifications@github.comwrote:\n\n> actually before we do that can u try running it in the non zh_CN locale\n> version\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/pydata/pandas/issues/3800#issuecomment-19159467\n> .\n",
      "@jtratner any luck/progress/decisions to abandon this? i would happily close if this is going to waste time\n",
      "@cpcloud I have to look back, but I think it was just fixing the `Makefile` to not squelch errors. Ultimately depends on whether you want to accept #3803 , because it can then just run `make doc`. It doesn't add that much time to the build (I don't think) - greatest amount of time is compiling and installing the build itself.\n",
      "yes everything on travis is completely dwarfed by build time although u can put `PTF` in ur commit message and it will try to use a cached build. i haven't used it a lot but maybe i should start\n",
      "That's interesting.... Does it try to cache between commits?\nOn Jun 13, 2013 5:16 PM, \"Phillip Cloud\" notifications@github.com wrote:\n\n> yes everything on travis is completely dwarfed by build time although u\n> can put PTF in ur commit message and it will try to use a cached build. i\n> haven't used it a lot but maybe i should start\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/pydata/pandas/issues/3800#issuecomment-19425599\n> .\n",
      "@cpcloud building docs works now, it adds very little time to the overall build time, plus this build shows that it actually fails on errors :P https://travis-ci.org/jtratner/pandas/builds/8261960 [turns out there was a `print cmd` instead of `print(cmd)` in `make.py`]\n",
      "probably not necessary to build on python 3 _and_ python 2 unless the docs are built on that. @changhiskhan what python version are you running on the doc-building box?\n",
      "@cpcloud yeah, that's not what I'd put in the merge to pandas if we included at all...I mostly wanted to see if it would fail (which, of course, it did :P)\n",
      "btw makefile stuff is nice :+1: \n",
      "@jreback @y-p any opinions here? this could save some minor headaches but also cause a few since another thing is added to travis...\n",
      "It probably should be added to a full build, else it will skip things that\ncan't import.\nOn Jun 21, 2013 9:19 PM, \"Phillip Cloud\" notifications@github.com wrote:\n\n> @jreback https://github.com/jreback @y-p https://github.com/y-p any\n> opinions here. this could save some minor headaches but also cause a few\n> since another thing is added to travis...\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/pydata/pandas/issues/3800#issuecomment-19848378\n> .\n",
      "agreed. just wondering if it's worth it...docs can sometimes spot bugs that don't show up in tests and also of course sphinx will report doc formatting errors, albeit not in the most useful way\n",
      "are we shelving this for now?\n",
      "closing as stale\n",
      "@cpcloud In what sense this is stale? I thought the docs are build by travis now?\n",
      "At the time we were considering having Travis check that doc builds\nsucceeded (ie everything built successfully and unit tests passed), but we\ncouldn't figure it out (or there may not be a simple way at all) and now\nthis has not gotten any attention for a while.\n",
      "docs ARE built by travis now in the 2.7 build (and then master uploads), thanks @y-p\n\nthat said, this doesn't 'catch' errors especially since it only tests with 2.7\n",
      "@jtratner @jreback I didn't realize they were being built by travis, my mistake. Should I reopen?\n",
      "It is actually not fully clear to me what docs are exactly built:\n- the docs are built for every PR (for every travis build) on build 3, where you can see the log output, but not the docs itself. Would that also be possible?\n- the docs are also uploaded to http://pandas-docs.github.io/pandas-docs-travis, but which exactly? Not every build on travis is uploaded? How is this scheduled?\n- I think ideally we would also have a dev doc build including the api docs, but I assume this was left out as this takes to much time on travis?\n",
      "Docs are built only on the 2.7 build for all travis runs, but the upload ONLY happens on master (which makes sense), otherwise it fails (but the doc log is always their).\n\nThe api docs are NOT built as they take a lot of time. The build is actually in the background while network tests are running. @y-p did a nice job on this!\n\nA py3 doc build could be done (just add it in travis).\n\nYou could modify `build_docs.sh` to do just about anything, but the problem is where would they go? \nin theory you _could_ have a site somewhere where they are put referenced by travis build number. Anyone want to do this? (it could actually be on the users github site which allows hosting!)\n",
      "Thanks for the clarification!\n",
      "I was thinking, would it be an option to have a seperate travis build for only the docs (and then maybe the api can be included), as a build that is not included in the fast-finish matrix so it is not delaying the tests? \n(but not that I have much knowledge about this)\n",
      "its not that hard to add another build (as we have an optional part of the matrix, e.g. the numpy 1.9 build is their).\n\nyou could certainly build the docs, then write the log file, then parse the log file and fail it appropriately, e.g. say you fail on certain types of warnings.\n\nok..let's reopen if you want me to add a build I can easily, then you could tweak the build script and test for it.\n",
      "Hmm, in principle we could search for something like\n\n```\n>>>-------------------------------------------------------------------------\nException in\n```\n\nin the doc logs and let travis fail.\n\nSomething else, Read The Docs also provides automatic updating of your docs for every commit to master (and also some other maybe interesting features: http://read-the-docs.readthedocs.org/en/latest/features.html).\n",
      "read the docs doesn't build extensions - so won't work for us iirc\n",
      "@jreback ah yes, indeed, that is the case\n\nSomething like this https://github.com/jorisvandenbossche/pandas/commit/b086afd6c54d862ca452dea07bef191ff45dd4ac would let travis fail if there is an exception in the code examples of the doc build (with the doc build in travis build 3 as is the case now, so without adding an extra build. That could be a start). Result of travis you can see here: https://travis-ci.org/jorisvandenbossche/pandas/jobs/22765601 (the third failed because of the docs).\n",
      "Something else, the scipy version in the 2.7 build (where the docs are built) is too old (scipy 0.10) for the docs (you get an exception in the interpolation section, `ImportError: Your version of scipy does not support PCHIP interpolation.`). Is this a problem to use a more recent scipy? Or is it because we want to test against the oldest supported scipy version?\n",
      "building a wheel for scipy 0.13.3 now for the 2.7 build. this is all done in the `pandas/ci/speedback/build.sh` script. then upload to the pandas.pydata.org site.\n\ni'll post when its done.\n",
      "how much extra time do you figure build the full docs is to versus the with `--no-api`\n",
      "Well, I think a full doc build is about 10 mins, while --no-api is something like 2-3 minutes (rough estimates). So a lot longer as the test suite I think? (as most time on travis is spent installing all packages?)\n\nBut maybe this are two not necessarily intertwined issues:\n- checking the doc builds for exceptions (https://github.com/jorisvandenbossche/pandas/commit/b086afd6c54d862ca452dea07bef191ff45dd4ac) and letting travis fail. For this, it is not necessary that the api docs are included, as only exceptions occur in the written docs (with ipython directives). At least, in the commit, I am only checking for that. I think that is the most important to check (we could also check for sphinx warnings, but then we should first have it completely warning free, which is not the case)\n- It would be nice to also have the dev version of the api docs on the online dev docs, but this does not need to happen for every PR, so it shouldn't be necessarily included in the travis build (although this would be a handy way to have it)\n",
      "Ok, maybe easiest way to proceed is to add a validation step in `build_docs.sh`. It can be a python script! that you can print to stdout for whatever; if you have an error, then just return a non-zero exit code and the build will fail (otherwise return 0). You can download the /tmp/doc.log to test from a build. Then the 2.7 build will fail if their is an exception (or whatever in the doc script).\n\njust put it at the end of the  `build_docs.sh`, scan the `/tmp/doc.log` and exit -1 if you want \n\nfyi....scipy is now updated and docs building correctly http://pandas-docs.github.io/pandas-docs-travis/missing_data.html#interpolation\n",
      "@jreback Thanks!\n\nWell, I first put it in `build_docs.sh`, but then it complained because the `/tmp/doc.log` is actually the log of `build_docs.sh`, so I copied it to script.sh (https://github.com/jorisvandenbossche/pandas/commit/b086afd6c54d862ca452dea07bef191ff45dd4ac).\nIt worked, but it complained (but maybe I can silence that)\n",
      "you may have to wrap that in something like\n\n`if [x\"$DOC_BUILD\" != \"x\"]; then`\n\nto only do this on the doc builds.....\n",
      "you can test for a nonempty string with\n\n``` sh\n[ -n \"$DOC_BUILD\" ]\n```\n",
      "@jorisvandenbossche - that looks like a nice and simple way to check\nwhether the docs failed. Does that get tripped up by expected Exceptions?\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "closed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "reopened",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 9,
    "additions": 61,
    "deletions": 23,
    "changed_files_list": [
      ".travis.yml",
      "ci/build_docs.sh",
      "ci/requirements-2.7_DOC_BUILD.build",
      "ci/requirements-2.7_DOC_BUILD.run",
      "ci/run_build_docs.sh",
      "ci/script.sh",
      "pandas/core/frame.py",
      "pandas/core/window.py",
      "pandas/util/nosetester.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 4052,
    "reporter": "benjello",
    "created_at": "2013-06-27T10:39:19+00:00",
    "closed_at": "2016-02-20T18:40:31+00:00",
    "resolver": "kshedden",
    "resolved_in": "23810e583b81cfca4b0f51fa8a46635c83099567",
    "resolver_commit_num": 12,
    "title": "ENH: read_sas, to_sas",
    "body": "It would be really convenient to be able to at least import SAS tables into pandas dataframe. Is this planned ? Are they insurmountable issues ?\n\nThanks\n",
    "labels": [
      "IO SAS",
      "Prio-high"
    ],
    "comments": [
      "can you post a link to the format? and see if any converters have been writtenin python?\n\nobviously the idea would be to read the native format file\n",
      "http://support.sas.com/techsup/technote/ts140.html\n",
      "so looks like simple binary read/write stuff....could be done....only other question is there any license issue with doing this?\n",
      "not sure. @benjello want to contact SAS and ask them?\n",
      "That technote is describing the XPORT format, which isn't the native binary format. I've never seen a published layout of the native format. Some people have partially reverse engineered it, but I've never seen a solution that could handle data sets with compression.\n\nhttps://github.com/BioStatMatt/sas7bdat\n",
      "@mtkni thanks had no idea.\n",
      "aside from using SAS to actually export (e.g. csv or whatever), is there aformat that one could save that provides some interoperbility (and is openish)?\n",
      "a 10 minute search suggests no but maybe someone else knows more.\n",
      "I am not a specialist much more a potential user in heavy need of such a tool.\nFor now I have used alternatively [StatTransfer](http://stattransfer.com/) which is not a free software \nor when importing to R I used one of the method exposed [here](http://rconvert.com/sas-vs-r-code-compare/5-ways-to-convert-sas-data-to-r/) but you need to have sas installed.\nI am sorry for not being able to provide ypu for more information than above.\n",
      "probably could do\n1. call to sas if exists\n2. if doesn't exist can only read xport so try to read that (someone would need to implement the xport reader)\n3. if not 1 or 2 then fail saying u need sas to read sas binary files.\n",
      "I don't think 1) is a good idea\n\nexport data in stata to xport format or csv format \n",
      "ok. just throwing it out there. i don't like calling out to other programs either, but this seems like it's going to be tough. i can implement the R code above...if u think that's a good idea...but it basically forces users to use that particular version of the format and if it ever changes we won't know until it breaks.\n",
      "i wouldn't be able to test to_sas though since i don't have sas\n",
      "I would be glad to test everything that would do the job. I have sas.  \n",
      "I've spent some time on this in the past. These are my thoughts:\n\n1) Reverse engineering the binary data set format is a difficult task and not a good priority for this project. Any solution, like the R solution, that doesn't deal with compressed data sets won't do me any good. I can't speak for others on that. I also worry about the licensing issues.\n2) Given that, the only option for read_sas may be to use the sas executable to first export SAS to a format that can be read by Python.\n3) The SAS transport format, as described in the tech note, is an option but it has some quirks. In particular, the native XPORT engine doesn't handle long variable names. It really hasn't evolved since SAS 6 (we're on SAS 9 now). The SAS workaround for long variable names is a set of macros that not everyone has installed. \n4) I'm not convinced the XPORT format is a better option than just CSV. If it's dramatically faster it might be. I'll study that next week.\n5) If the mechanism for reading SAS data sets turns out to be export to CSV and then using read_csv, then there's not much code to write that isn't SAS environment dependent. It just doesn't make for a good shared module.\n\nAs much as I wish there was a good solution to this, and as much as I'd be willing to help build it, I just don't think there is. I've built read_sas using CSV as an intermediate format. Obviously, this requires a SAS license. It takes very few lines to implement, but most of those lines are specific to our SAS environment and are not well-portable.\n\nI will study the performance of XPORT vs CSV next week. If it's dramatically faster, then it may be worth the effort to implement. Even then, I'm not sure it's worth taking that on as part of the Pandas project. I would be interested in comments from other SAS users on that.\n\nJust my two cents.\n",
      "nice to hear from someone who tried to do this. FWIW i think it might be tough to beat CSV for speed, most of it is written C/Cython.\n",
      "Agreed. The new, fast CSV was a game changer.\n",
      "can sas export to HDF5?\n",
      "No, it can't.\n",
      "export in STATA format?\n",
      "No, and if it did it would be an expensive add-on module. SAS is pretty good at reading from databases (http://www.sas.com/resources/factsheet/sas-access-factsheet.pdf), although each database platform is a separate license. I haven't found it good at all at writing to databases (it can, but it's slow). Other than that, interoperability doesn't seem to be part of their business model.\n",
      "Oh wait, I may have spoken to soon. Apparently I can export to a stata file: http://support.sas.com/documentation/cdl/en/acpcref/63184/HTML/default/viewer.htm#a003102702.htm\n\nIs STATA supported in Pandas? It would still requires a SAS license, but I can benchmark that versus CSV. \n",
      "There is a read_stata  that will be available in te coming version but already available on github\n",
      "BTW, @mtkni I would be happy to look at the read_sas you implemented if you would share it ...\n",
      "Just to close the loop on this, exporting to STATA requires an add-on for which I'm not licensed, so I can't benchmark it. \n",
      "FYI, the XPT or transport format is a non-proprietary format that has no licensing issues and is the only format currently accepted by the Food and Drug Administration (FDA) for clinical trial data. Most pharmaceutical companies submit XPT format to the FDA. It would be nice to have a way to read these files in just like a csv file.\n",
      "@dramage1 Is \"XPT\" the same as \"XPORT\" above?\n",
      "Heyo - there's at least one Python package for reading XPT files - https://pypi.python.org/pypi/xport/0.1.0\n",
      "Just what I needed, much appreciated.\n",
      "@dramage1 if you use this enough to want to write up a pandas wrapper for it, that could be a useful addition to pandas (depending on the stability of xport)\n",
      "@jtratner I worked on the `xport` library before. I can refactor `xport` to give a better API for use in a `pandas` `read_xpt` or borrow some code to include directly in `pandas`.\n",
      "@dramage1 let me know if the `xport` library is confusing or broken. I'll try to improve the docs and/or code.\n",
      "As I said over email - I'm glad that you're interested in working on this.\nFeel free to ping me if you have any pandas-related issues.\n",
      "@benjello @selik any action on this?\n",
      "Not yet... check back in a couple weeks :-0\n",
      "I am sorry but I won't be qualified enough but I am willing to test any code\n",
      "@benjello Could you give me a few test cases? I don't have SAS available to me. I'd like to get just a few tiny test files to make some unit tests.\n",
      "@selik you are going to use the `xport` soln?\n",
      "@jseabold you have thoughts on this?\n",
      "Not really anything to add beyond what's here. I'm sure it will be useful if XPT format is used places (yikes that's a terrible data policy re: FDA). I've been lucky enough to avoid SAS beyond coursework which required it.\n",
      "@jreback It makes sense to refactor the `xport` library to make it friendly as a dependency for a `pandas.read_xpt()`.\n",
      "@selik yes....prob best to simply incorporate it directly (with the licensening references / included) - see what we did with msgpack. Then you can modify and not introduce a dep.\n\nI am not a license expert...but I think that the MIT license is compat with pandas BDS 3 clause\n\n(you basically just copy the LICENSE to the LICENSES dir) and are good 2 go\n",
      "@jseabold Regarding the FDA data format. They are beginning to realize that it is time to move forwar dand have a XML format pilot project proposal http://goo.gl/1xNiv8. The SAS transport (XPT) format is not going away anytime soon - the FDA moves at a snails pace implementing changes, so I glad to see you are working on this. Unfortunately, I am a complete newbie at python and can't help much. I could provide some sample data in XPT format if someone can explain how to upload it to GIT.\n",
      "@dramage1 You can email me the files if you'd like. I'm looking for a Rosetta stone for XPT and CSV. Or XPT and some other plain-text format. I think my email address is in my profile.\n",
      "@selik Mike, I tried mike@selik.org and got an undeliverable message.\n",
      "@dramage1 That's not good. I wonder who else is having trouble emailing me. Mind sharing your email in your profile?\n",
      "I updated my profile\n",
      "Even though it would be slower, would it be worthwhile to add pyodbc-based support for SAS?\n",
      "@spearsem It wouldn't necessarily be slower if SAS has some secret awesome algorithm for reading XPT files. That's how R reads XPT. But if you already have SAS, the best thing to do is read the file in SAS and save as CSV, not to read it directly from Python.\n",
      "BTW, I'm slowly moving along with `xport`. I think I'll have code ready for inclusion in `pandas` by end of April.\n",
      "I'm thinking specifically when you don't already have SAS, just someone's old data files. It would be interesting to be able to connect to the data via some within-pandas wrapper on pyodbc for the SAS drivers and then perform some queries on it into pandas.\n",
      "I don't follow. How would you have SAS drivers without SAS?\n",
      "Well, you may be a person who knows absolutely nothing about SAS, but who can solve the problem very quickly in Pandas. This happened to me before with Stata. My company had plenty of Stata licenses, but no one who knew Stata had time to help explain to me what was going on with the script that generated some data. In the interim, I found a statsmodels function that would read .dta files (pandas didn't have that ability yet) and solved the problem with the data very quickly. It would have taken much longer for me to do it with Stata.\n",
      "Ah, I see. Unfortunately, I can't help with that, as I don't have SAS.\n",
      "Some progress on the front of reading SAS sas7bdat format in R: http://cran.r-project.org/web/packages/sas7bdat/index.html and it seems that a pythonista got inspired by it http://git.pyhacker.com/sas7bdat\n",
      "Can anyone vouch for the R solution?\n",
      "(I'm here by accident, and have no idea about anything.)\n\nsas7bdat is on pypi https://pypi.python.org/pypi/sas7bdat   MIT licensed\nand sas2pd in \nhttp://www.sasanalysis.com/2014/08/python-extension-functions-to-translate.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+SasAnalysis+%28SAS+ANALYSIS%29\n",
      "FWIW, there is apparently also support for reading _compressed_ files in the Java-based \"parso\" open-source library, http://search.maven.org/remotecontent?filepath=com/ggasoftware/parso/1.2.1/parso-1.2.1-sources.jar\n",
      "the sas7bdat Python library now supports compressed files too: https://pypi.python.org/pypi/sas7bdat\n",
      "@jaredhobbs yeah! Thanks a lot for this! If only it supported Python3, it would be even more awesome :)\n",
      "@gdementen After a few long nights, Python3 support has landed: https://pypi.python.org/pypi/sas7bdat/2.0.1\n\nI also fixed some bugs I came across.\n",
      "@jaredhobbs Thanks!\n",
      "Many thanks, this expands my options for working with Clinical Trial data sets.\n\nDate: Sun, 4 Jan 2015 01:11:26 -0800\nFrom: notifications@github.com\nTo: pandas@noreply.github.com\nCC: dramage1@gmail.com\nSubject: Re: [pandas] ENH: read_sas, to_sas (#4052)\n\n@gdementen After a few long nights, Python3 support has landed: https://pypi.python.org/pypi/sas7bdat/2.0.1\n\nI also fixed some bugs I came across.\n\n\u2014\nReply to this email directly or view it on GitHub.\n\n```\n                  =\n```\n",
      "Hadley just release [haven](https://github.com/hadley/haven) for reading SAS, Spss, and Stata files. It wraps a C library (ReadStat)[https://github.com/WizardMac/ReadStat]. It has an MIT license.\n",
      "Did anyone ever solve the read_sas issue?  Or is reading fixed width ascii files with accompanying dictionary still an issue?\n",
      "see #9711 - going to be merged shortly - \n",
      "But does that only deal with '.xpt' file types?  Glancing at the code it doesn't seem to deal with the other two part SAS file format.\n",
      "@tyler-abbot yes that is for xport type files. It would be straightforward to wrap the library mentioned above to extend this to the sas binary format. just need a volunteer - interested?\n",
      "@jreback I'm actually working on transcribing the SAScii (http://cran.r-project.org/web/packages/SAScii/index.html) package from R to Python.  I would be happy to share the results if it is ok with the author of that package.  I haven't done much development, though, so don't know much about sop's.  I'm also not sure how compatible it would be with the library you mentioned.  Perhaps just an add on with the option of  ...format=\"sas\"... or something along those lines.\n",
      "just saw this. https://pypi.python.org/pypi/sas7bdat/2.0.1. Even if this is pure-python (slower), that is ok to start. Better to have it able to read than not.\n\ncc @kshedden\n",
      "@jreback @kshedden I use extensively https://pypi.python.org/pypi/sas7bdat/2.0.1 It is slow but works well.\nSince usually I transfer my data to HDF format once at the beginning of a study, it was worth it.\n",
      "So, I don't think the `sas7bdat` package can read the type of sas files I'm talking about.  I have finished writing a function to do it, but am going out of town for a few months.  It is all contained in this package:\n\nhttps://pypi.python.org/pypi/psid_py\n\nI have a few days during which I could work on incorporating the `read_sas()` function into pandas.  I'm going to read through the documentation and do some more testing, but if anyone has suggestions that will help me move more quickly it would be greatly appreciated.\n",
      "I am going to mark this for 0.17.1. The implementation for using [sas7bdat](https://pypi.python.org/pypi/sas7bdat/2.0.1) is quite trivial. So should start with that.\n",
      "I used `sas7bdat` the other day... here is what it took:\n\n```\nfrom sas7bdat import SAS7BDAT\nwith SAS7BDAT('/homes/abie/projects/2015/TICS/tics_07.sas7bdat') as f:\n    df = f.to_data_frame()\n```\n\nThere is some useful information hidden in the sas file that does not make it into the dataframe, though, such as the column labels.\n",
      "agreed. all that is really needed are:\n- docs (e.g. `sas7bdat` will now be an optional import)\n- integration into the `pd.read_sas` method via the `format` kw\n- tests (and add to the CI so we can actually tests this), and some sample files\n- verify the dtypes are returned correctly (and if now fix / post fixes upstream)\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "labeled",
      "commented",
      "labeled",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "cross-referenced",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 39,
    "additions": 1328,
    "deletions": 91,
    "changed_files_list": [
      "LICENSES/SAS7BDAT_LICENSE",
      "asv_bench/benchmarks/packers.py",
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/io/api.py",
      "pandas/io/sas/__init__.py",
      "pandas/io/sas/sas7bdat.py",
      "pandas/io/sas/sas_xport.py",
      "pandas/io/sas/saslib.pyx",
      "pandas/io/sas/sasreader.py",
      "pandas/io/tests/sas/data/DEMO_G.csv",
      "pandas/io/tests/sas/data/DEMO_G.xpt",
      "pandas/io/tests/sas/data/DRXFCD_G.csv",
      "pandas/io/tests/sas/data/DRXFCD_G.xpt",
      "pandas/io/tests/sas/data/SSHSV1_A.csv",
      "pandas/io/tests/sas/data/SSHSV1_A.xpt",
      "pandas/io/tests/sas/data/paxraw_d_short.csv",
      "pandas/io/tests/sas/data/paxraw_d_short.xpt",
      "pandas/io/tests/sas/data/test1.sas7bdat",
      "pandas/io/tests/sas/data/test10.sas7bdat",
      "pandas/io/tests/sas/data/test11.sas7bdat",
      "pandas/io/tests/sas/data/test12.sas7bdat",
      "pandas/io/tests/sas/data/test13.sas7bdat",
      "pandas/io/tests/sas/data/test14.sas7bdat",
      "pandas/io/tests/sas/data/test15.sas7bdat",
      "pandas/io/tests/sas/data/test16.sas7bdat",
      "pandas/io/tests/sas/data/test2.sas7bdat",
      "pandas/io/tests/sas/data/test3.sas7bdat",
      "pandas/io/tests/sas/data/test4.sas7bdat",
      "pandas/io/tests/sas/data/test5.sas7bdat",
      "pandas/io/tests/sas/data/test6.sas7bdat",
      "pandas/io/tests/sas/data/test7.sas7bdat",
      "pandas/io/tests/sas/data/test8.sas7bdat",
      "pandas/io/tests/sas/data/test9.sas7bdat",
      "pandas/io/tests/sas/data/test_sas7bdat_1.csv",
      "pandas/io/tests/sas/data/test_sas7bdat_2.csv",
      "pandas/io/tests/sas/test_sas7bdat.py",
      "pandas/io/tests/sas/test_xport.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 4131,
    "reporter": "davidshinn",
    "created_at": "2013-07-05T13:17:04+00:00",
    "closed_at": "2013-07-06T04:17:34+00:00",
    "resolver": "davidshinn",
    "resolved_in": "1c6440e1fa01b0d637ddc69cfbb55a6d85ddb483",
    "resolver_commit_num": 1,
    "title": "Excel parser cannot change *keep_default_na* value",
    "body": "The parse method in the ExcelFile class does not pass the argument _keep_default_na_ to the TextParser from pandas.io.parsers.  There is no clean way to override the default na values when parsing excel files with the current code.\n\nMy specific problem involves \"NA\" as a typical value for \"North America\" in excel files.\n",
    "labels": [],
    "comments": [
      "@davidshinn I think #4139 should fix this - just changed `parse` and `_parse_excel` to pass keyword arguments through\n",
      "@jtratner This fixes the problem perfectly and other potential kwds missing from upstream functions. I'm extremely impressed with the quick commits.  On another note, I noticed that because this doesn't specifically add the keyword argument, the feature is not as visible through ipython [tab completion](http://ipython.org/ipython-doc/stable/interactive/tutorial.html#tab-completion) and [introspection](http://ipython.org/ipython-doc/stable/interactive/tutorial.html#exploring-your-objects).  However from my perspective, the main issue is closed.  Maybe I'll tackle those niceties myself.  Thanks.\n"
    ],
    "events": [
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 10,
    "deletions": 0,
    "changed_files_list": [
      "pandas/io/excel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 4343,
    "reporter": "jreback",
    "created_at": "2013-07-24T14:15:27+00:00",
    "closed_at": "2017-03-10T14:36:33+00:00",
    "resolver": "kernc",
    "resolved_in": "1be66ac975d89be9c5b695ce34a4a18ffed355ec",
    "resolver_commit_num": 4,
    "title": "ENH: native sparse conversion from csc/csr scipy sparse matrix to DataFrame",
    "body": "-a-pandas-sparsedataframe-from-a-scipy-sparse-matrix\n",
    "labels": [],
    "comments": [
      "* https://stackoverflow.com/questions/31084942/pandas-sparse-dataframe-to-sparse-matrix-without-generating-a-dense-matrix-in-m\r\n* https://stackoverflow.com/questions/33457626/huge-sparse-dataframe-to-scipy-sparse-matrix-without-dense-transform\r\n* https://stackoverflow.com/questions/41152264/convert-pandas-sparsedataframe-to-scipy-sparse-csc-matrix\r\n* https://stackoverflow.com/questions/37084032/how-do-i-create-a-scipy-sparse-matrix-from-a-pandas-dataframe\r\n...",
      "@kernc there *might* be some other sparse issues which are covered / dupes by this. If you find any pls ping on them.",
      "Surprisingly, [no duplicates](https://github.com/pandas-dev/pandas/issues?utf8=%E2%9C%93&q=is%3Aissue%20is%3Aopen%20scipy%20label%3ASparse)."
    ],
    "events": [
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented"
    ],
    "changed_files": 10,
    "additions": 266,
    "deletions": 22,
    "changed_files_list": [
      "doc/source/api.rst",
      "doc/source/sparse.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/sparse/array.py",
      "pandas/sparse/frame.py",
      "pandas/tests/sparse/common.py",
      "pandas/tests/sparse/test_frame.py",
      "pandas/tests/types/test_inference.py",
      "pandas/types/common.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 4386,
    "reporter": "davidshinn",
    "created_at": "2013-07-28T04:56:00+00:00",
    "closed_at": "2017-04-18T22:37:17+00:00",
    "resolver": "yui-knk",
    "resolved_in": "e082eb2c95a22a16d67e533cbf581a304cf5e70e",
    "resolver_commit_num": 0,
    "title": "DOC/BUG: pivot_table returns Series in specific circumstance",
    "body": "The docstrings and other documentation say that the `pivot_table` function **returns a DataFrame**.   However, this likely leads to confusion like #4371, because under narrow circumstances, passing a certain set of argument dtypes results in the function **returning a Series** (see ipython examples at end):\n1. values is single string (not a list, not even a single valued list)\n2. cols=None\n3. aggfunc is single string/function (not a list, not even a single valued list)\n\nUnfortunately, this is not clear from the docs or from normal use (except for condition 1).\n\nShould this:\n1. eventually be fixed to only return a DataFrame no matter the circumstances to be less confusing\n2. be documented correctly (seems a little difficult to convey in the docstring and other docs without a lot of bulk).\n\nMy thoughts are changing the function to return only a DataFrame in future versions (> 0.13) and providing some deprecation warning in the meantime is better than trying to explain this in the docs.\n\nI would be happy to provide the deprecation warning and document notes as a pull request.\n\nThanks.\n\n\n",
    "labels": [
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "@davidshinn want to do a PR for this?\n",
      "Hi\ni want to ask about comment above:\n_passing a certain set of argument dtypes results in the function returning a Series_\n\ncalling `df.pivot_table` with empty index, also returns Series object instead of DataFrame\nis there reason for this behaviour?\nthanks\n",
      "if u would like to implement as indicated above that would be great\n",
      "@youlyst, feel free to do a PR to fix this.  Sorry I've been out of touch since I originally posted. If no one gets to this by February, I'll submit a PR so that pivot_table always returns a DataFrame for consistency.\n",
      "I do not feel for it yet, i am working with pandas very short time. but it could change after month...\nmy question was mainly to make sure that i am not doing something wrong.\n",
      "contributing docs are: http://pandas.pydata.org/pandas-docs/stable/contributing.html\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 72,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/reshape/pivot.py",
      "pandas/tests/reshape/test_pivot.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 4400,
    "reporter": "njheyu",
    "created_at": "2013-07-29T16:07:06+00:00",
    "closed_at": "2016-04-11T12:47:31+00:00",
    "resolver": "sinhrks",
    "resolved_in": "da0371009e9f5635656783a0cd13f2d6f2394585",
    "resolver_commit_num": 286,
    "title": "TST: more testing of indexing with sparse",
    "body": "related is #6076 (`loc` not working either)\n\nIt would be nice if common operations like indexing/slicing is available for sparse dataframes as for their dense counterparts.\n\nThanks!\n\n\n",
    "labels": [
      "Testing",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "I think that this was fixed at some point and can be closed.\n\n``` python\nIn [4]: data = np.zeros((2,2))\n\nIn [5]: data[:] = np.nan\n\nIn [6]: data[1,1] = 1\n\nIn [7]: df = pd.DataFrame(data)\n\nIn [9]: df.ix[:,1]\nOut[9]: \n0   NaN\n1     1\nName: 1, dtype: float64\n\nIn [10]: df.to_sparse().ix[:,1]\nOut[10]: \n0   NaN\n1     1\nName: 1, dtype: float64\nBlockIndex\nBlock locations: array([1], dtype=int32)\nBlock lengths: array([1], dtype=int32)\n\nIn [24]: df.to_sparse().iloc[1,1]\nOut[24]: 1.0\n\nIn [25]: df.to_sparse().iloc[1,0]\nOut[25]: nan\n```\n\n``` python\nIn [3]: pd.show_versions()\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.4.1.final.0\npython-bits: 64\nOS: Linux\nOS-release: 2.6.32-431.17.1.el6.x86_64\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.15.2\nnose: 1.3.4\nCython: 0.20.2\nnumpy: 1.9.1\nscipy: 0.14.1\n```\n",
      "ok. I am not 100% sure that this is actually tested though. Can you confirm? If it is we want to add this issue reference, and if not put some tests in place. thxs.\n",
      "I searched [`test_sparse`](https://github.com/pydata/pandas/blob/master/pandas/sparse/tests/test_sparse.py) for indexers. I was surprised at how few cases I found, but maybe I should be looking elsewhere. Here's a summary:\n\n`.iloc` only [applied](https://github.com/pydata/pandas/blob/master/pandas/sparse/tests/test_sparse.py#L164) to a DataFrame, not a SparseDataFrame\n\n`.loc` does not occur.\n\n`.ix` is used on a SparseDataFrame in a few spots e.g. [L270](https://github.com/pydata/pandas/blob/master/pandas/sparse/tests/test_sparse.py#L270), [L1051](https://github.com/pydata/pandas/blob/master/pandas/sparse/tests/test_sparse.py#L1051), and [1112](https://github.com/pydata/pandas/blob/master/pandas/sparse/tests/test_sparse.py#L1112)\n\n`.isin` is [tested](https://github.com/pydata/pandas/blob/master/pandas/sparse/tests/test_sparse.py#L1504) on SparseDataFrame.\n\n`.iat` and `.at` do not occur.\n\nMaybe inheriting from `NDFrame` makes this good enough? If needed, I can make a PR with some additional tests.\n",
      "you should run the tests with -v and see\n\nas since it inhersts tests from test_frame.py as well (and test_series).\n",
      "It appears to me that the tests inherited from `test_frame.SafeForSparse` only execute `.ix`. `TestSparseSeries` only inherits `test_series.CheckNameIntegration`\n\nFrom what I can gather, it seems that the \"do it right\" solution might be to reuse some of the `test_indexing` machinery in `test_libsparse`. I don't think I'm the right person for that job at this point in time.\n",
      "after #12779, this issue is adding more tests for `.iat/.at`\n"
    ],
    "events": [
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "renamed",
      "unlabeled",
      "commented",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 2,
    "additions": 235,
    "deletions": 1,
    "changed_files_list": [
      "pandas/sparse/tests/test_indexing.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 4471,
    "reporter": "wesm",
    "created_at": "2013-08-05T22:58:49+00:00",
    "closed_at": "2016-12-20T13:49:05+00:00",
    "resolver": "gfyoung",
    "resolved_in": "0c5281305baa203151d1b19cd0268a85f37c028d",
    "resolver_commit_num": 115,
    "title": "lib.maybe_convert_objects will fail on uint64 values that exceed int64 max",
    "body": "xref #11440 for addtl tests\n\nObserved in the wild. cc @blais\n",
    "labels": [
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "@jreback @wesm objects that would pass this wouldn't be compatible with anything else but integers that are all > 0 right? Float, float128, complex, and longdouble all lose precision.\n",
      "I'm wrong, float128 (which I think is the same as longdouble) can work...\n",
      "float128 doesn't have be long double... long double could be 64 bits... it's an implementation detail but it ends up being what you expect most of the time...\n",
      "same applies to int64 etc ... e.g., `long` is 32 bits on 32 bit arch and 64 on 64-bit arch\n",
      "@cpcloud so what's the right dtype to contain something that's uint64 and greater than what int64 can handle? - this SO answer claims float128 is 'a mess'. http://stackoverflow.com/questions/9062562/what-is-the-internal-precision-of-numpy-float128\n",
      "i.e., just allow uint64_t size and go from there? and then disallow with anything that's not an actual integer > 0?\n",
      "I'm not sure why this is happening ... `uint64` should hold values up to `2 * INT_MAX`... i think probably allowing `uint64` is the way 2 go...not sure i follow the second question.\n",
      "@cpcloud in convert_objects, if you can't fit everything into the same container, then it doesn't work. This is why uint64 doesn't work:\n\n``` python\n        elif util.is_integer_object(val):\n            seen_int = 1\n            floats[i] = <float64_t> val\n            complexes[i] = <double complex> val\n            if not seen_null:\n                try:\n                    ints[i] = val\n                except OverflowError:\n                    seen_object = 1\n                    break\n```\n",
      "it's not hard to set this up, I just wanted to clarify I had the right idea...going to fix it now.\n",
      "@cpcloud what I mean by the second question is what should be returned from this:\n\n``` python\nimport sys\narr = np.array([-5, sys.maxint + 5, 3], dtype=object)\nlib.maybe_convert_objects(arr)\n```\n\nIt should be object right? Otherwise the -5 becomes gobbdledygook.\n",
      "Well, this is mostly useless anyways, because BlockManager converts uint64 to object internally in form_block:\n\n``` python\n        elif issubclass(v.dtype.type, np.integer):\n            if v.dtype == np.uint64:\n                # HACK #2355 definite overflow\n                if (v > 2 ** 63 - 1).any():\n                    object_items.append((i, k, v))\n                    continue\n            int_items.append((i, k, v))\n```\n\nSo need a unsigned int type or something in block manager\n",
      "Anyways, working version of lib.maybe_convert_objects here: https://github.com/jtratner/pandas/tree/GH4471_fix_uint64_maybe_convert_objects\n",
      "[I keep hitting this](https://github.com/pydata/pandas/issues/11846#issuecomment-219501714) while importing a dataset which has uint64's in it. Is there anything I can do to help it along, given that someone already made a patch but it didn't get in?\n",
      "where's the patch?\n",
      "@jreback see @jtratner's [comment above](https://github.com/pydata/pandas/issues/4471#issuecomment-24473554). Is the patch unsuitable or is it just that it wasn't shepherded into master?\n",
      "that's 2 years old - if someone wants to cherry pick and present then can look\n",
      "also hitting this bug - just wondering if the fix is in progress, or if interest is simply too low\n",
      "well need someone motivated to push a fix\n"
    ],
    "events": [
      "mentioned",
      "subscribed",
      "assigned",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "labeled",
      "labeled",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "unassigned",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 43,
    "deletions": 6,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/src/inference.pyx",
      "pandas/tests/frame/test_block_internals.py",
      "pandas/tests/types/test_inference.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 4792,
    "reporter": "hayd",
    "created_at": "2013-09-09T23:28:48+00:00",
    "closed_at": "2017-02-10T00:19:23+00:00",
    "resolver": "mroeschke",
    "resolved_in": "3d6fcdcd356b2b1853346bc4e709baa3bf16ddad",
    "resolver_commit_num": 29,
    "title": "describe on a groupby",
    "body": "I think expected result of this is with multiindex column...\n\n\n\ncc #4740 @jreback \n\nI guess it just applies method/attribute from df:\n\n\n\nshould we just special case describe? :s\n",
    "labels": [
      "API Design"
    ],
    "comments": [
      "u could do this like median which is special cased as a python apply I thnk\n",
      "@hayd if you would like to address this at some point.....but pushing for now\n"
    ],
    "events": [
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 6,
    "additions": 150,
    "deletions": 59,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/groupby.py",
      "pandas/tests/formats/test_format.py",
      "pandas/tests/groupby/test_categorical.py",
      "pandas/tests/groupby/test_groupby.py",
      "pandas/tests/test_generic.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 4793,
    "reporter": "johnclinaa",
    "created_at": "2013-09-09T23:37:06+00:00",
    "closed_at": "2016-08-05T10:35:50+00:00",
    "resolver": "gfyoung",
    "resolved_in": "e5ee5d2e034c9a4be795596633c199d0ba23970b",
    "resolver_commit_num": 65,
    "title": "read_csv does not parse in header with BOM utf-8",
    "body": "I am using Pandas version 0.12.0 on a Mac.\n\nI noticed that when there is a BOM utf-8 file, and if the header row is in the first line, the read_csv() method will leave a leading quotation mark in the first column's name.  However, if the header row is further down the file and I use the \"header=\" option, then the whole header row gets parsed correctly.\n\nHere is an example code:\n\nbing_kw = pd.read_csv('../../data/sem/Bing-Keyword_daily.csv', header=9,   thousands=',', encoding='utf-8')\n\nParses the header correctly.\n\nbing_kw = pd.read_csv('../../data/sem/Bing-Keyword_daily.csv', thousands=',', encoding='utf-8')\n\nParses the first header column name incorrectly by leaving the leading quotation mark.\n",
    "labels": [],
    "comments": [
      "which version of Python are you using? (`python --version`)\n",
      "Python 2.7.5 :: Anaconda 1.6.1 (x86_64)\n\nAnd the code is written in a python notebook using ipython 1.0.0.\n",
      "@john-orange-aa can you provide a reproducible example (link to a file if you need to)\n",
      "@jreblack, here is the link to the folder: \nhttps://drive.google.com/folderview?id=0BwxOyJG828PySFFQVlBSUEdlcEk&usp=sharing\n\nBOM-temp.csv is the offending file.  BOM-temp2.csv is the same file with headers removed.  The \"pandas BOM utf-8 bug.ipynb\" is the ipython notebook that illustrates the bug.\n\nOn Sep 28, 2013, at 12:50 PM, jreback notifications@github.com wrote:\n\n> @john-orange-aa can you provide a reproducible example (link to a file if you need to)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n",
      "This isn't exactly the same issue, but I'm also having trouble with BOMs. File with a utf-8 BOM [here](https://www.dropbox.com/s/m3eoszxl9s3kuj6/bom_file.csv).\n\n```\n    [~/work/]\n    [1]: pd.version.version\n    [1]: '0.12.0-1149-g141e93a'\n\n    [~/work/]\n    [2]: dta = pd.read_csv(\"bom_file.csv\", encoding='utf-8')\n\n    [~/work/]\n    [3]: dta.columns[0]\n    [3]: u'\\ufeffDate (MMM-YY)'\n```\n",
      "It looks like you should use 'utf-8-sig' as the encoding for utf-8 files with a BOM, so my comment is likely invalid.\n",
      "Is it possible for Pandas to infer the encoding from the BOM automatically - or is it really required to pass this information through in the encoding? I think the purpose of the BOM was to provide this kind of capability?\n",
      "'utf-8-sig' does not resolve the issue.  I faced the same issue but using 'utf-8-sig' just got me another decoding problem\nUnicodeDecodeError: 'utf8' codec can't decode byte 0xae in position 14: invalid start byte\n",
      "I ran into this problem as well (version 0.15.2). I tried 'utf-8-sig' encoding, and though I didn't see an error, the result was not quite right as the first key is quoted and none of the other keys are, though all column headers/values are quoted throughout the file.\n\n```\nf = open('data/bomex.csv')\nheader = f.read(10)\nf.close()\nheader\n\nout: '\\xef\\xbb\\xbf\"Name\",'\n\nf = codecs.open('data/bomex.csv', encoding='utf-8-sig')\nheader = f.read(10)\nf.close()\nheader\n\nout: u'\"Name\",\"Team\",\"G\"'\n\ndf = pd.read_csv('data/bomex.csv', encoding='utf-8-sig')\ndf.keys()[0]\n\nout: u'\"Name\"'\n\ndf.keys()[1]\n\nout: u'Team'\n```\n\nNote the extra set of quotes on the first key\n",
      "With 0.15.2, I am able to use `encoding=\"utf-8-sig\"` and the BOM disappears from the first column header.\n",
      "In 0.15.2, I find that the BOM disappears, but the quotes around the first column header are erroneously preserved, while quotes around all other column headers (and all other values) are stripped.\n\nSo the problem with utf-8-sig seems to only affect quoted column headers. Here's an example file to try\n\nhttps://dl.dropboxusercontent.com/u/27287953/bom.csv\n\n...\n",
      "Hi,\n\nI'm using pandas 0.16 in python 3.4 (anaconda distro).\nI was having a problem with some files, and it seems to be related with the BOM of the file.\nI\u00a0have this file: https://www.dropbox.com/s/nced7whmt2rr0c8/sample_file2.txt\n\nI'm reading the file with:\n\n```\nimport pandas as pd\ntest = pd.read_table(\"sample_file2.txt\", decimal = \",\", \n                     parse_dates = True)\n```\n\nWhen I\u00a0print the column names:\n\n```\n>>> print(test.columns)\nIndex(['\ufeffTimeStamp', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14',\n 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29',\n 'V30', 'V31'], dtype='object')\n```\n\nSo, there seems to be a 'TimeStamp' column. Let me check what is in there:\n\n```\n>>> test[\"TimeStamp\"] \nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nFile \"C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\", line 1787, in __getitem__\n[truncated]\nFile \"pandas\\hashtable.pyx\", line 705, in pandas.hashtable.PyObjectHashTable.get_item\n(pandas\\hashtable.c:12300)\nKeyError: 'TimeStamp'\n```\n\nAnd, I\u00a0don't seem to be able to find out what is the real name of the TimeStamp column =/ I had this in a longer script and it took me forever to understand where the problem came from. Adding the `encoding = \"utf-8-sig\"` to the read_table solves the issue. Can pandas figure out the enconding on its own, to avoid such problems?\n",
      "@zelite In this example, your column was actually spelled `'\ufeffTimeStamp'` (yes, with the proceeding space). I don't know if it's possible to detect encodings like this automatically... in general, that can be a hard problem.\n",
      "@shoyer I don't think that was a space thing; it was definitely the BOM. If you do something like df.columns[0] you will see the utf-8 characters.\n",
      "OK, I'm clearly lost. Just looked up exactly what \"BOM\" is :).\n",
      "I can reproduce the issue with Pandas 0.16.0. I can get the same error on first column name with read_csv , or get the first row of the first column erroneous too with a file without columns on row 1 and with names= argument in the read_csv call.\n\nIt sounds like read_csv interprets correctly encoding='utf-8-sig' by skipping the 3 first characters of the file, then interpreting the file as UTF8. However, the bug experienced makes me think that Pandas \"forgets\" to skip the first 3 characters when it starts to parse the file and create the dataframe. Something like the offset of the beginning of the effective data in the file didn't get +len(UTF8_BOM), thus leading to have the BOM included in the first column name or in the first cell of the dataframe. The most misleading part is that the characters do not print naturally when the dataframe of the column names are displayed in ipython, but the BOM is clearly kept and behind that cell string as pointed out in a previous comment above.\n\nHTH\n",
      "With encoding 'utf-8-sig', the BOM is correctly skipped rather than prepending it to the first column label. However, as described by others, the quotes around the first label remain. \n\nThe zip archive at\nhttps://www.dropbox.com/s/kcbh7fbsj9fwh13/sample.zip?dl=0\n\ncontains a script for demonstration as well as 2 csv files that differ only by having / not having a BOM. The one without BOM is parsed correctly, with BOM the first label remains quoted.\n\nPython 3.5.1_x86, PD 18.1, Win7x64\n",
      "A minimal example for future reference:\n\n``` python\n>>> from pandas.compat import BytesIO\n>>> from pandas import read_csv\n>>> import codecs\n>>>\n>>> BOM = codecs.BOM_UTF8\n>>> data = '\"name\"\\n\"foo\"'.encode('utf-8')\n>>>\n>>> read_csv(BytesIO(data), encoding='utf-8', engine='c')\n# same result if engine='python'\n  name\n0  foo\n>>>\n>>> read_csv(BytesIO(BOM + data), encoding='utf-8', engine='c')\n# same result if engine='python'\n  \ufeff\"name\"\n0     foo\n```\n\nWhile I agree that there is a bug in the C engine, I don't believe the same can be said with the Python engine, as `csv.reader` (the foundation of the Python engine) cannot parse the BOM correctly (cannot run this if using Python 2.x):\n\n``` python\n>>> from io import TextIOWrapper\n>>> from csv import reader\n>>> for row in reader(BytesIO(BOM + data), encoding='utf-8'): print(row)\n['\\ufeff\"name\"']\n['foo']\n```\n\nSince the Python engine failure is beyond our control, the question is then can this issue be closed if we can patch the C engine?\n",
      "this is mainly an issue on windows, where these BOM markers can easily be put in files, so if possible to patch would be good. \n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 186,
    "deletions": 51,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/io/parsers.py",
      "pandas/io/tests/parser/common.py",
      "pandas/src/parser/tokenizer.c"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 5636,
    "reporter": "cancan101",
    "created_at": "2013-12-03T20:03:36+00:00",
    "closed_at": "2016-04-19T18:36:05+00:00",
    "resolver": "gfyoung",
    "resolved_in": "fe8f8f496f4774150740596ff9dc0f6720312a63",
    "resolver_commit_num": 10,
    "title": "read_csv not correctly parsing dates when parse_dates is string and index_col not set",
    "body": "With:\n\n\n\nThese all work as expected:\n\n\n\nbut this does not parse the string:\n\n\n",
    "labels": [
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "I don't think `parse_dates` can be a string, so not sure this should work at all (I think your first example works because 'C' is True). will mark as a bug though.\n",
      "@jreback : I think this issue can be closed.  The \"buggy\" example fails as expected because `parse_dates=\"C\"` is the same as `parse_dates=True`, which will try to convert the `index` attribute of the resulting `DataFrame` as `datetime` objects per the `0.18.0` documentation.  By not specifying `index` to be the `C` column, it correctly does not parse it as a `datetime`.\n",
      "```\nparse_dates : boolean or list of ints or names or list of lists or dict, default False\n\n    * boolean. If True -> try parsing the index.\n    * list of ints or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n      each as a separate date column.\n    * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n        a single date column.\n    * dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call result\n      'foo'\n```\n\nSo I think this should then have a nice meesage if a non-boolean scalar is passed as its not valid\n"
    ],
    "events": [
      "commented",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "labeled",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 473,
    "deletions": 420,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 5724,
    "reporter": "hsharrison",
    "created_at": "2013-12-17T22:22:32+00:00",
    "closed_at": "2017-01-18T16:14:35+00:00",
    "resolver": "jreback",
    "resolved_in": "8e13da24818dffaec88321a75a1e728b50a642f2",
    "resolver_commit_num": 4169,
    "title": "groupby.mean, etc, doesn't recognize timedelta64",
    "body": "See -apply-combine-on-pandas-timedelta-column\nrelated as well: -pandas-dataframe-1st-line-issue-with-datetime-timedelta/20802902#20802902\n\nI have a DataFrame with a column of timedeltas (actually upon inspection the dtype is `timedelta64[ns]` or `'<m8[ns]'`), and I'd like to do a split-combine-apply, but the timedelta column is being dropped:\n\n\n\nOr, forcing pandas to try the operation on the 'td' column:\n\n\n\nHowever, taking the mean of the column works fine, so numeric operations should be possible:\n\n\n",
    "labels": [
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "Worth mentioning that DataFrame's mean doesn't do this either, I think it should (convert those originally datelike to date):\n\n```\nIn [11]: data.mean()\nOut[11]: \nf1          0.528609\nf2          0.583264\ntd    5975598.700000\ndtype: float64\n```\n\nCompare to `data.mean(1)` which ignores date columns (correctly imo as you're going across dtypes).\n",
      "Interestingly, `mean` works on `Series` but not `DataFrame` (this is using the new timedelta formatting code #5701):\n\n```\nIn [10]: pd.to_timedelta(list(range(5)), unit='D')\nOut[10]: \n0   0 days\n1   1 days\n2   2 days\n3   3 days\n4   4 days\ndtype: timedelta64[ns]\n```\n\n```\nIn [9]: pd.to_timedelta(list(range(5)), unit='D').mean()\nOut[9]: \n0   2 days\ndtype: timedelta64[ns]\n```\n\n```\nIn [6]: pd.DataFrame(pd.to_timedelta(list(range(5)), unit='D')).mean()\nOut[6]: \n0   55785 days, 14:53:21.659060\ndtype: timedelta64[ns]\n```\n",
      "@hayd actually the results of `data.mean()` are correct. The result for _td_ is in nanoseconds. It IS however possible to return an object array that is correct, .eg.\n\n```\nIn [34]: Series([0.1,0.2,timedelta(10)])\nOut[34]: \n0                 0.1\n1                 0.2\n2    10 days, 0:00:00\ndtype: object\n```\n\nso @hayd maybe create a separate issue for this type of inference (Its just a bit of inference detection in nanops.py/_reduce)\n",
      "@hsharrison as far as the groupby; this is just not implemented ATM in groupby.py; its not that difficult, just needs to follow basically what datetime64 stuff does\n",
      "See #6884.\n",
      "Workaround: Use `.describe()` (which includes the mean) rather than `.mean()`\n\nSee https://gist.github.com/tomfitzhenry/d36ebba697a1f3eeefcb for demo.\n",
      "Wow. Any insight into why that works? I would not have expected a convenience method to take a different code path.\n",
      "Also has the same problem. @tomfitzhenry 's solution works.\n",
      "Seeing as this keeps getting kicked to the next release, you can also perform `sum` and `count`. Referencing @hsharrison's original dataframe from the bug report:\n\n```\ndata = pd.DataFrame(np.random.rand(10, 3), columns=['f1', 'f2', 'td'])\n\ndata['td'] *= 10000000\n\ndata['td'] = pd.Series(data['td'], dtype='<m8[ns]')\n\ngrouped_df = data.groupby(data.index < 5)\n\nmean_df = grouped_df.mean()\n\nIn [14]: mean_df\nOut[14]: \n             f1        f2\nFalse  0.271488  0.614299\nTrue   0.535522  0.476918\n\nmean_df['td'] = grouped_df['td'].sum() / grouped_df['td'].count()\n\nIn [16]: mean_df\nOut[16]: \n             f1        f2              td\nFalse  0.271488  0.614299 00:00:00.004187\nTrue   0.535522  0.476918 00:00:00.003278\n```\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "labeled",
      "labeled",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 240,
    "deletions": 34,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/compat/numpy/function.py",
      "pandas/core/groupby.py",
      "pandas/tests/groupby/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 5888,
    "reporter": "cancan101",
    "created_at": "2014-01-09T05:55:57+00:00",
    "closed_at": "2016-05-26T23:56:09+00:00",
    "resolver": "chris-b1",
    "resolved_in": "4b050552faec3b6cf8a82de1e7b2df2515765f55",
    "resolver_commit_num": 33,
    "title": "API/DOC: status of low_memory kwarg of read_csv/table",
    "body": "I am getting the following warning:\n\n\n\nbut I can find no documentation for `low_memory`\n",
    "labels": [
      "Bug",
      "CSV"
    ],
    "comments": [
      "its a kind of deprecated option (but still works)\n",
      "If the `low_memory` parameter is deprecated, it should be marked as such. Also the warning message should be removed or re-worded.\n",
      "I said kind of deprecated in that I don't think it's necessary anymore\n",
      "As another data point, I got this warning about mixed types. Setting to `low_memory=False` as suggested actually crashed Python (Win7 64-bit, through IPython Notebook). I'm nowhere near my memory limits, so removing the argument and keeping the warning was no big deal for me.\n",
      "There is also an example in the book \"Python for Data Analysis\" that leads to this warning (p278):\n\n```\nIn [13]: fec = pd.read_csv('ch09/P00000001-ALL.csv')\n\nC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py:1130: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n  data = self._reader.read(nrows)\n```\n\nSo I agree with above:\n- or the option is deprecated, and then the mention in the warning should be removed\n- or the option is not deprecated and should be documented (and should not crash your python)\n\nFrom the code, it does not seem like deprecated (it is still used: https://github.com/pydata/pandas/blob/master/pandas/parser.pyx#L727), but it seems that it is given a default value of True in `read_csv` regardless of what you specify (https://github.com/pydata/pandas/blob/master/pandas/io/parsers.py#L354). There are also still tests specifically for high memery: https://github.com/pydata/pandas/blob/master/pandas/io/tests/test_parsers.py#L2897\n",
      "@mdmueller @selasley As csv parser experts, somebody interested in looking into this? (What does `low_memory` do exactly? Do we still need it (should it be deprecated or not)? And depending on that, document it or really deprecate it (and remove as suggestion in one of the warnings).\n\nThis came up again at SO: http://stackoverflow.com/questions/28697501/how-to-know-line-and-col-when-the-read-csv-method-of-pandas-thows-exception/28702078#28702078\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "demilestoned",
      "commented",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "renamed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 16,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/io.rst",
      "pandas/io/parsers.py",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 6194,
    "reporter": "michaelaye",
    "created_at": "2014-01-30T21:54:32+00:00",
    "closed_at": "2016-07-29T00:08:33+00:00",
    "resolver": "sinhrks",
    "resolved_in": "dcb7bf722f619ec39a8766fc570ba82c4197fe14",
    "resolver_commit_num": 362,
    "title": "Enhancement: Add dropna to Float64Index",
    "body": "\n\nAny reason why it could not have a dropna()?\n",
    "labels": [],
    "comments": [
      "it could\n\nkeep in mind that the indexing code for Nan's is really hairy - so maybe a good idea!\n",
      "yeah, this was the result of prematurely making something the index while I should have dropped the NaNs before assigning it as an index. So maybe it's better to teach the user to prevent NaNs in the index? Possibly would avoid a lot of other issues.\n",
      "the problem with nan in an index is that selection basically goes out the window and indexes are almost useless because since nan != nan and so ordering is shot\n\ncan you post a usecase?\n\nwhat we maybe could do is have an index that has a fill value so that ordering / selection is maintained, but treat it as missing  - just a thought\n",
      "i'm not sure i CAN defend a use case, now that I remember how bad NaNs are in the index?\nAs I said I had the following happening: dataframe with some columns having NaNs at the tail end. Creating a new dataframe with 2 columns of this dataframe without dropping NaNs. Assigning one of the new columns as an index to the new dataframe, ending up with NaNs in the index.\nMaybe the real 'issue' here could be that I didn't notice? Maybe a warning would be appropriate if an array containing NaNs is made an Index?\n",
      "no warning as setting an index possibly containing nan's is allowed. Its really the indexing of it that is the problem, which does raise an exception in some cases.\n\nTheir needs to be some more work in this area in any event. If you do find a case where nans are useful (or a nuiscance) in a Float64Index, pls post.\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 6,
    "additions": 141,
    "deletions": 16,
    "changed_files_list": [
      "doc/source/api.rst",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/indexes/base.py",
      "pandas/indexes/multi.py",
      "pandas/tests/indexes/test_base.py",
      "pandas/tests/indexes/test_multi.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 6322,
    "reporter": "wangshun98",
    "created_at": "2014-02-11T14:54:49+00:00",
    "closed_at": "2017-02-21T13:38:57+00:00",
    "resolver": "tzinckgraf",
    "resolved_in": "bb2144a32cb30bc7428b117389a280b2515e9cf1",
    "resolver_commit_num": 1,
    "title": "BUG: reset_index with NaN in MultiIndex",
    "body": "This is related to the following commit.\n\n\nWhen one level in the MultiIndex is all NaN, line 2819 in DataFrame.reset_index() and subfunction _maybe_cast, will have a problem, because values is empty.\n2819:    values = values.take(labels)\nIndexError: cannot do a non-empty take from an empty axes\n\nA possible fix is to add the following lines before 2819:\n                if mask.all():\n                    values = labels \\* np.nan\n                    return values\n",
    "labels": [
      "Bug",
      "Missing-data",
      "MultiIndex"
    ],
    "comments": [
      "can U give an example of your use case\nan all nan level in a multi index is really really hard to support properly \n",
      "This comes out of a database query of mixed fields of dates and values. And I want to use the combination of dates as index so I can easily join the values from multiple queries. however, sometimes all dates in one column are all null.  \n\nNaN in MultiIndex is already supported. That 3 lines I suggested would make it work with a level with all NaN.  We can test case other functionalities. But after I made that change in my local copy, I haven't experienced other issues with all NaN MultiIndex yet.\n",
      "go ahead and do a PR (with a test case) and i'll take a look\n",
      "I can confirm I am also experiencing precisely this issue with an all-NaN level in my multi-index. Looks to me like those three lines should work if included.\n",
      "@jreback\n\nThe code below returns the error: `IndexError: cannot do a non-empty take from an empty axes.`\n\n```\nimport pandas as pd\nimport numpy as np\nimport random\nimport string\n\n\n\n# Create 12 random strings 3 char long \nrndm_strgs = [''.join(random.SystemRandom().choice(string.ascii_uppercase + string.digits) for _ in range(3)) for i in range(12)]            \nrndm_strgs[0] = None\nrndm_strgs[5] = None\n\n# Make Dataframe\ndf = pd.DataFrame({'A' : list('pandasisgood'),\n                   'B' : np.nan,\n                   'C' : rndm_strgs,\n                   'D' : np.random.rand(12)})\n\n# Set an Index -> Columns have Nans\ndf_w_idx = df.set_index(['A','B','C'])\n\n# This is where it fails\ndf_w_idx.reset_index()\n```\n\nThis is the desired result:\n\n```\nfor clmNm in df_w_idx.index.names:\n    print(clmNm)\n\n    print(clmNm)\n\n    df_w_idx[clmNm] = df_w_idx.index.get_level_values(clmNm)\n\n\ndf_w_idx = df_w_idx.reset_index(drop=True).copy()\ndf_w_idx\n```\n",
      "@jreback I have the same issue and was playing around with a solution.\r\n\r\nThe `reset_index` function in `frame.py` function has the following code\r\n```\r\n            # if we have the labels, extract the values with a mask\r\n            if labels is not None:\r\n                mask = labels == -1\r\n                values = values.take(labels)\r\n```\r\nsee [here](https://github.com/pandas-dev/pandas/blob/5710947759e13f54a5bf896768fb7f9c214f245d/pandas/core/frame.py#L2938).\r\n\r\nThe mask variable is all `True` only if all the labels are -1. Assuming this means all the values were `np.nan`, one solution I was playing with is as follows\r\n```\r\n            # if we have the labels, extract the values with a mask\r\n            if labels is not None:\r\n                mask = labels == -1\r\n                if mask.all():\r\n                    values = (np.nan * mask).values()\r\n                else:\r\n                    values = values.take(labels)\r\n```\r\n\r\nPlease advise. More than happy to create a pull request and create all appropriate test cases.\r\n\r\n",
      "@tzinckgraf you can just do the take if ``len(values)``; ``.take()`` is picky :>\r\n\r\nsure why don't you put up a PR with a test (you can use the above example)."
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 39,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/frame.py",
      "pandas/tests/frame/test_alter_axes.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 6602,
    "reporter": "jseabold",
    "created_at": "2014-03-11T17:54:10+00:00",
    "closed_at": "2014-03-11T18:50:06+00:00",
    "resolver": "jseabold",
    "resolved_in": "53a206b4d3a1a63ed0721ceaedba43feb308645e",
    "resolver_commit_num": 81,
    "title": "makeMissingDataFrame is broken",
    "body": "Did something change in iloc...? I could have sworn this worked and we talked about it during the PR. I was told _not_ to use values but to use iloc instead IIRC.\n\n\n\nThis must be a copy because there's no assignment.\n\nThis still works\n\n\n",
    "labels": [],
    "comments": [
      "`i` and `j` need to be integers not float indexers (which `_create_missing_idx`) is returning\n\n```\nIn [12]: df.iloc[np.array(i).astype(int),np.array(j).astype(int)] = np.nan\n\nIn [13]: df\nOut[13]: \n                   A         B         C         D\nGKeS6xwGkH  0.076111 -0.192877  1.098092 -0.074225\nb5WFjcgcvp -0.502561 -0.498398 -1.230549 -1.543185\nvK5bWohkVI  0.780080  0.075142  0.980444  0.242915\nPAtwXIifAf       NaN       NaN       NaN       NaN\nTls586DpAI  0.012728  1.426050  0.102844  0.871050\nELEPklQ7yw       NaN       NaN       NaN       NaN\nEEGXwqnRlO       NaN       NaN       NaN       NaN\nsSTuc11xNN -0.162915 -0.909402 -0.619644 -1.207251\nevjm7MZ0qk -0.680087  0.887411  1.195531  1.237603\na5Z4It2V5H -2.073016 -0.788900 -0.083732  1.165306\nFGkBqukGOb  0.165393  1.794161  1.753696 -1.385686\nCRbt0EJpN2 -0.524696 -1.469448  0.134130  0.586626\niikzGFllAx  0.778601  1.434450 -1.176926 -1.753035\nTZ9t34OOdY       NaN       NaN       NaN       NaN\nqEOLe8ZdW2  1.117555  0.289419 -1.507414  0.632285\ndlxec9h9KB       NaN       NaN       NaN       NaN\nw1u7GXTZVs       NaN       NaN       NaN       NaN\nMXhbZLpw2b  0.199026  0.555814  0.691348 -1.141643\nNU9NOqHljV       NaN       NaN       NaN       NaN\n783raWid6e -0.445552  0.109983 -0.042305  0.340432\n1bcwNH4txV       NaN       NaN       NaN       NaN\n04J1LdP5xW       NaN       NaN       NaN       NaN\ndFwyxzhRoC -2.083956  0.105314 -1.350285  0.528947\npZPufraB0b  0.654603 -1.824764 -0.104437 -0.589233\n9IutFKQyT8  0.307688  1.495336 -0.250889 -1.067093\nSCWHxJuagm -0.426220  1.160561  1.126022  0.707325\n1X5q9CmOvN       NaN       NaN       NaN       NaN\n3a9oEAXL4i  0.623358  0.677829 -0.927471 -0.119501\nRW9vXQg0Dk  0.817401  1.911899 -0.475811  1.122407\npFyaA4zfqE       NaN       NaN       NaN       NaN\n\n[30 rows x 4 columns]\n\n```\n",
      "other problem is that these are treated not as a grid (which is what I think you want).\n\nso just do `df.values[i,j] = np.nan`\n\nwe don't handle this type of mesh indexing\n\n```\nIn [24]: zip(*[[ int(i_) for i_ in i ],[ int(j_) for j_ in j ]])\nOut[24]: \n[(21, 0),\n (3, 1),\n (15, 1),\n (16, 1),\n (20, 1),\n (13, 2),\n (26, 2),\n (5, 3),\n (6, 3),\n (15, 3),\n (18, 3),\n (29, 3)]\n```\n\ne.g. basically a list of cordinates to set\n",
      "This _happens_ to work because its a single dtype\n\n```\n\nIn [35]: df = pd.util.testing.makeDataFrame()\n\nIn [36]: df.unstack().iloc[[ int(i_*len(df.columns) + j_) for i_, j_ in zip(i, j) ]] = np.nan\n\nIn [37]: df\nOut[37]: \n                   A         B         C         D\nfhXEt3cb41 -2.542137  0.682847 -0.307920  0.267502\nmT2YVWqTbS  0.920000 -0.572048       NaN  0.643715\n4L9B4yUozs  0.470971 -1.750536 -0.264348  1.350301\nzVXa3OswAo -0.789714 -0.509468       NaN  0.159287\nIKKaXLIEwn -1.000156  1.091673  0.717248  0.433991\n4K8wcFV2vx -0.714839  0.731494       NaN -1.448743\nY9OImXzIOr -1.114815  0.992466 -0.566328 -0.810867\ngq5Rq6u0B0 -1.214338 -1.675467  1.714498 -1.336355\npn2G8ud7Su -1.369294 -0.115031 -0.021097  0.996644\nxxARn8LbH6 -0.105542 -1.192954  0.026511 -0.108402\n7dqRmDrRc7 -0.817408 -0.224906 -0.297257 -1.321547\nFnOjGpwi8u  0.202575  0.060629 -1.782457  1.203774\nkr9yi451RI -1.221792  0.805534 -1.607661  0.543769\nzaaPJGPIPD       NaN  1.481853  0.552138  1.193685\n41WfdGgzIu  0.632413  0.728245  2.119639 -0.777506\nKyIx2TK1VI -1.122135  0.329781       NaN -0.205368\ne3a4PS5v7I -0.871674  1.059267  2.825929       NaN\n5spuEC8EyY -0.013621  1.069023  1.057529 -0.063874\n9VD9a53o74 -0.619559  1.440201  0.254996  0.355991\nMgYGRNRhSW -0.719056  0.161600 -2.143606 -1.898759\nmfdOE8KlLt -0.309020  0.049107  0.289401  2.033655\nRHO9UFLqp8  1.307798  0.193412       NaN  1.407008\nqrYcSwhggM -1.538358  1.504896 -0.071199 -0.348272\nvuwXj43xJZ       NaN  0.740426  0.156312  1.405110\nHmCLyu0gay -1.371543       NaN       NaN -1.173484\nbwWhkISrz0  1.757823 -0.751076  1.480796 -0.142391\nepsoAg0kQ8 -0.020010 -1.651050 -0.951079  1.455640\nUJ0QAhUHRh       NaN  1.401063  0.181608 -0.375468\niYP7NFcbGQ  0.119397 -1.528577  2.022177 -0.370158\nUx3wTsUCl5  1.051203 -0.595926  1.248444       NaN\n\n[30 rows x 4 columns]\n\n```\n"
    ],
    "events": [
      "commented",
      "commented",
      "referenced",
      "commented"
    ],
    "changed_files": 1,
    "additions": 2,
    "deletions": 2,
    "changed_files_list": [
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 6608,
    "reporter": "rosnfeld",
    "created_at": "2014-03-11T23:10:06+00:00",
    "closed_at": "2017-05-24T12:12:12+00:00",
    "resolver": "huguesv",
    "resolved_in": "b0038ac72721058a3ae71f1dbcaa24d2f10f23c0",
    "resolver_commit_num": 1,
    "title": "BUG: strange timeseries plot behavior",
    "body": "After some discussion below, here's a simple repro case:\n\n\n\ncauses\n\n\n\n-- ORIGINAL MESSAGE --\n\nHere's a small dataset:\n\n\n\nIf I read this in using\n\n\n\nand then try and plot it using\n\n\n\nI get more or less what I expect (perhaps the xlim doesn't go up to 2010-12-31, but otherwise fine).\n\nIf I delete out the BRA rows and try this again, I get:\n\n\n\nIf I delete out both BRA and VEN rows, then there is no exception raised but I only see one series plotted and the x-axis is not formatted as a date.\n\nOne could also approach this whole exercise via something like\n\n\n\nbut this works even worse, I just get a truncated VEN series and nothing else.\n\nThis is with current master pandas (but also happens in 0.13.1) and matplotlib 1.3.1.\n\nAre there known issues with plotting sparse-yet-overlapping timeseries?\n",
    "labels": [
      "Visualization",
      "Timeseries",
      "Difficulty Novice",
      "Effort Low",
      "Testing"
    ],
    "comments": [
      "I guess if I do\n\n``` python\ndata.pivot('date', 'region', 'value').interpolate().plot()\n```\n\nor\n\n``` python\npivoted = data.pivot('date', 'region', 'value')\npivoted.index = pd.to_datetime(pivoted.index)\npivoted.interpolate(method='time').plot()\n```\n\nI get something like what I wanted as it cleans up the missing values. Interpolate's a new feature I hadn't seen before. (cool!)\n\nMaybe this is all user-error but I had been doing groupby plotting like this for a while, and had been getting what looked like correct results. I feel there may actually be a bug here someplace, that groupby behavior seems so bizarre.\n",
      "Actually, interpolation is not really what I want, as it makes it seem as if there is more data than is actually present. I basically just want to see the various region series all plotted as they would be if they were plotted individually, except all together on the same axes.\n\n(and a loop like\n\n``` python\nfigure = plt.figure()\nax = figure.gca()\ndata = pd.read_csv('./data.csv', parse_dates='date')\nfor region in data.region.unique():\n    subset = data[data.region == region]\n    subset = subset.set_index('date')\n    subset.value.plot(ax=ax, label=region)\n```\n\nseems to just over-write the axes)\n",
      "I think the groupby/plot issue seems certainly like a bug. I can't fully lay my hand on it, but I think it has something to do with combining regular/irregular timeseries.\n\nThe issue with the xlim not respecting the data is because it is updated by the last group (while this is a smaller group), and this is a seperate issue I think (you can open another issue for that).\n\nThe reason you get almost no points on the plot with `pivot` you seem to already figured out, this is indeed because of all the NaN values. You can also deal with this by plotting points instead of lines.\n",
      "Thanks @jorisvandenbossche, it looks like the \"separate issue\" is already filed as #2960 . So this one is just the groupby/plot weirdness.\n\nDid you mean to add to your comment? (it ends in what looks like the start to some code)\n",
      "@rosnfeld ah yes, I first wanted to add a code snippet how to do your last example easier, but this also had the same bug, but forgot to remove it. Removed it now \n",
      "@rosnfeld @jorisvandenbossche \n\nso this is the exception that's in the top section?\n\nwhat is causing this?\n",
      "Yeah, the exception is the most alarming thing, though changing the data slightly causes some other incorrect behavior (missing/incorrect plotting, which is harder to spot/diagnose).\n\nI don't know what's causing it without further investigation, but I can try to investigate and hopefully submit a fix. (it will be my first time digging into the plotting code, not sure how involved it is)\n",
      "The error is occurring in\n\n``` python\ndef _get_xlim(lines):\n    left, right = np.inf, -np.inf\n    for l in lines:\n        x = l.get_xdata()\n        left = min(x[0].ordinal, left)\n        right = max(x[-1].ordinal, right)\n    return left, right\n```\n\nThe line `left = min(to_ordinal(x[0]), left)` apparently expects a PeriodIndex.\nFor whatever reason, when you select `sub = data[data.region != 'BRA']` and plot that, you get an array of `datetime.datetime` objects at that point, instead of a PeriodIndex.\n\nI'm not too familiar with our Datetime code, but does anyone know why these aren't the same?\n\n``` python\nIn [40]: import datetime\nIn [41]: b = datetime.datetime(1997, 12, 31, 0, 0)\n\nIn [42]: y = pd.Period(year=1997, month=12, day=31, hour=0, minute=0, freq='S')\n\nIn [43]: b.toordinal()\nOut[43]: 729389\n\nIn [44]: y.ordinal\nOut[44]: 883526400\n```\n",
      "Some time ago I looked into this (for another issue), and then one of the fundamental problems was the design of pandas plotting for timeseries splitted in two ways: with datetimes when having irregular serieses, and with ordinals when having a regular timeseries (which was then converted to periodindex), and that those two types are incompatible with each other (so when you combine both types in a certain way it gives problems).\n\nBut I have to dig it up again to fully remember (I have some overview of the problem somewhere, but never finished it). I don't know i I find some time in the short term, but will try.\n",
      "And the `datetime.datetime.toordinal` is in days, the `pandas.Period.ordinal` int he frequency you specified (in this case seconds). \nPlus `datetime.datetime.toordinal` is since 01/01/0001, pandas base is 1970\n",
      "Yes, I looked at this a little last night, and agree with what you're saying - when you whittle the dataset down to just COL/PAN/VEN rows and then plot, COL and VEN get converted to have PeriodIndexes, but PAN stays with a DatetimeIndex for some reason, and then plotting them all on the same axes (via groupby) blows up somehow.\n",
      "Thanks.\n\n@rosnfeld you may want the `x_compat=True` keyword argument to plot. That seems to \"solve\" the problem\n",
      "Indeed, thanks! I actually hadn't seen that option before. It also fixes the \"missing series\" variant I mentioned in the original description. The bad xlim variant for the original dataset still remains, though.\n\nI'll try and dig into things a bit and see what can be done - I presume that not requiring x_compat is desirable.\n",
      "Yeah it would be desirable. May be tricky though. I'm guessing that argument was added for cases precisely like this one.\n",
      "For a bit more detail, I think this is what's going on: \n\npandas uses special timeseries plotting if it can infer a \"periodic\" frequency from a series. While it takes a bit of digging, this is part of the `_use_dynamic_x()` check in tools/plotting.py:\n\n``` python\n    def _make_plot(self):\n        # this is slightly deceptive\n        if not self.x_compat and self.use_index and self._use_dynamic_x():\n            data = self._maybe_convert_index(self.data)\n            self._make_ts_plot(data)\n        else:\n...  # regular plotting\n```\n\nThis special tseries logic converts plotted series to use a PeriodIndex, and sets a \"base\" version of  the frequency on the axes object for later reference. (Note that x_compat disables all of this and uses regular, non-tseries plotting)\n\nThe first series to be plotted in my dataset (COL) gets a frequency of '5A-DEC', which can be converted to a period. In the timeseries plotting code the \"base\" version of this frequency ('A-DEC') gets assigned to the axes object.\n\nThe 3-item DatetimeIndex of 1997-12-31, 2003-12-31, and 2008-12-31 for the next series (PAN) has a surprising inferred frequency of 'WOM-5WED' since 1997, 2003, and 2008 all ended on a Wednesday (the 5th Wednesday of December). pandas can't convert frequencies like that to periods, so it uses regular plotting rather than the special timeseries plotting for that series, and its index is not converted to PeriodIndex. It doesn't try and use the axes frequency since it has already inferred a frequency for this series.\n\nThe next and final series (VEN) does not have an inferred frequency, so it inherits the axes frequency, and tries to use tseries plotting again. tseries plotting tries to re-calculate x_lim's to include all data, so it looks at the lines already plotted, but it assumes all existing lines will have PeriodIndex data. It blows up when it tries to call 'ordinal' on the DatetimeIndex entries from the earlier (PAN) series. \n\nI'm not sure what the right fix is here. Frequency inference clearly makes some interesting choices, that are relied on in other parts of the codebase. I'm not sure if either the frequency inference or the usage of it should be modified. Timeseries plotting should maybe tolerate non-PeriodIndex data when calculating x_lim, though I don't yet understand much of that code yet, e.g. why PeriodIndex is desirable.\n",
      "@rosnfeld so this occurs when you have **multiple** series overlaid on the same plot and 1 is converted to PeriodIndex for display while 1 is not.\n\ncan you edit the top of the post to make it easily copy-pastable for the failing case?\n",
      "I added an even simpler example at the start of the post. No groupby or anything, just plot a timeseries with an inferred frequency that can be converted to a period, then one that can't, then the first one again, all on the same axes, and you get the same stack trace.\n\nHope that's along the lines of what you were looking for.\n",
      "ok...I guess the soln is in the plotting routines to check if their is a plot on the axis already that has a conflicting axis/index, then handle the current plotting better.\n\nI am not sure if this involves too much introspection or is even possible (e.g. you would have to get the index state from the axis and not sure if saved 'enough' to be able to figure out what is up)\n\n@rosnfeld give it a shot?\n",
      "moving to 0.15, but if you are able to figure out soon can move back\n",
      "Sure, I can take a shot at it. I'm optimistic something can be done.\n",
      "I think this one should be re-opened - my bad for having a comment in #7322 saying \"this does not fix #6608\".\n\nHowever, I think @sinhrks has some PRs that look to affect this behavior somewhat, changing this issue if not closing it.\n",
      "#7459 partially fixes this not to raise `AttributeError`.\n\nBut unable to set correct `xlim` and `formatter` yet. The result after #7459 is as below.\n![figure_1](https://cloud.githubusercontent.com/assets/1696302/3485762/7017b90e-03fe-11e4-93e1-957837347a2d.png)\n. \n",
      "Well, regular vs irregular series have pretty different ordinals, as in @TomAugspurger comment above, so I think the problem is unfortunately deeper than just xlim/representation. A solution might be to rework `_use_dynamic_x()` (in tools/plotting.py), to better catch cases that might mix these two together.\n",
      "@TomAugspurger push?\n",
      "@TomAugspurger status (pushing #7670) ok, so push this as well\n",
      "It looks like this issue is solved in the meantime. At least the simplified example at the top now works correctly for me.\n\n@rosnfeld Would you be able to test with your more complex example as well?\n",
      "Yes! I tested with the more complex example and everything works now. (as of 0.18.1)\n",
      "I see this is still open - should I close it?\r\n\r\nOr do people want some unit tests to explicitly try to protect against this happening again? Unfortunately given our (or at least my) incomplete understanding of why it was happening and how it has since been fixed, perhaps the best we could do would be writing a test that would have failed against code from a couple of years ago.\r\n\r\nNot sure what community practice is on things like this.",
      "Yes, I would first like to see a test added to confirm this (and keep it working!). A PR very welcome!"
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "commented",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "closed",
      "commented",
      "mentioned",
      "subscribed",
      "reopened",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "unlabeled",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 1,
    "additions": 16,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tests/plotting/test_datetimelike.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 6616,
    "reporter": "jseabold",
    "created_at": "2014-03-12T14:59:29+00:00",
    "closed_at": "2014-03-12T18:50:31+00:00",
    "resolver": "jseabold",
    "resolved_in": "98f51f53ecfa217a381938db84270e52ca59c9cb",
    "resolver_commit_num": 83,
    "title": "Add longtable keyword for to_latex",
    "body": "Allow specifying longtable environment format for to_latex. Might even make sense to have a to_longtable_latex, since there is going to be a bit to customize. May be able to get away with some sensible defaults though.\n",
    "labels": [
      "Enhancement",
      "Output-Formatting"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 79,
    "deletions": 14,
    "changed_files_list": [
      "doc/source/v0.14.0.txt",
      "pandas/core/format.py",
      "pandas/core/frame.py",
      "pandas/tests/test_format.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 6618,
    "reporter": "dsm054",
    "created_at": "2014-03-12T16:22:58+00:00",
    "closed_at": "2016-09-02T11:19:42+00:00",
    "resolver": "gfyoung",
    "resolved_in": "362a56153f4c9df4447c8407899237c9e6da6b70",
    "resolver_commit_num": 84,
    "title": "BUG: to_csv extra header line with multiindex columns",
    "body": "This seems strange to me, but I don't often use a MultiIndex so I might be missing something obvious.\n\n\n\nIs there supposed to be that empty line at the end of the header?  Compare\n\n\n",
    "labels": [
      "CSV",
      "Output-Formatting"
    ],
    "comments": [
      "yes, its for the row index-names (they are None here), in theory could could not print it as the `read_csv` will try both ways, but that's the 'original' format.\n",
      "Ah, okay.  \n",
      "@dsm054 I think its reasonable to do a PR which takes out the line and see if anything breaks....(obviously a tests which exactly is supposed to match won't), but I am talking about the read_csv _should_ still work correctly.\n\nand I guess its more in-line with what you'd except.\n",
      "hi i have the same issue, any workarround how to not have this empty line there?\n",
      "pandas will read this format \nwhat version?\n",
      "yes pandas will, but I need an output without this extra line (it's an input for other application)\nsee http://stackoverflow.com/questions/24372993/pandas-dataframe-with-2-rows-header-and-export-to-csv\n",
      "you can use tupleize_cols=True to make the header in a single line \n"
    ],
    "events": [
      "commented",
      "commented",
      "closed",
      "commented",
      "mentioned",
      "subscribed",
      "reopened",
      "labeled",
      "labeled",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 40,
    "deletions": 24,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/formats/format.py",
      "pandas/tests/formats/test_format.py",
      "pandas/tests/frame/test_to_csv.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 6645,
    "reporter": "jreback",
    "created_at": "2014-03-15T14:37:40+00:00",
    "closed_at": "2014-03-25T11:48:31+00:00",
    "resolver": "jsexauer",
    "resolved_in": "599156fe06ca7f4bc7f87fca982f561f0ec675a9",
    "resolver_commit_num": 1,
    "title": "DEPR: deprecate cols / change to columns in to_csv / to_excel",
    "body": "related is #6581\n",
    "labels": [
      "Deprecate"
    ],
    "comments": [
      "cc @jsexauer\n",
      "I'm trying to convert this issue into a PR using the API:\n\nURI:  https://api.github.com/repos/pydata/pandas/pulls\nPOST Payload:  `{\"head\": \"jsexauer:fix6645\", \"base\": \"master\", \"issue\":\"6645\"}`\nResponse: \n\n```\n{\n  \"message\": \"Validation Failed\",\n  \"documentation_url\": \"http://developer.github.com/v3/pulls/#create-a-pull-request\",\n  \"errors\": [\n    {\n      \"resource\": \"PullRequest\",\n      \"field\": \"issue\",\n      \"code\": \"unauthorized\"\n    }\n  ]\n}\n```\n\nI've done this before successfully but now it says I'm unauthorized.  Any idea why this may be?\n",
      "never tried using the API\nI just go to the branch an pull it on the website\n",
      "But won't that create a new issue number?\n",
      "yep\nyou want to do that\nthe pr will then close this issue \n"
    ],
    "events": [
      "milestoned",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "unlabeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 11,
    "additions": 206,
    "deletions": 106,
    "changed_files_list": [
      "doc/source/comparison_with_r.rst",
      "doc/source/release.rst",
      "doc/source/reshaping.rst",
      "doc/source/v0.14.0.txt",
      "pandas/core/frame.py",
      "pandas/io/tests/test_excel.py",
      "pandas/tests/test_format.py",
      "pandas/tests/test_frame.py",
      "pandas/tools/pivot.py",
      "pandas/tools/tests/test_pivot.py",
      "pandas/util/decorators.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 6724,
    "reporter": "dalejung",
    "created_at": "2014-03-28T01:05:07+00:00",
    "closed_at": "2014-03-28T03:39:43+00:00",
    "resolver": "dalejung",
    "resolved_in": "8b30816cc3f1501376d6d5d8f3b63ab0dfbb207d",
    "resolver_commit_num": 3,
    "title": "shift with named axis errors",
    "body": "\n\nnp.roll is being passed the original axis\n",
    "labels": [],
    "comments": [],
    "events": [
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 2,
    "additions": 11,
    "deletions": 4,
    "changed_files_list": [
      "pandas/core/generic.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 6749,
    "reporter": "bdanalytics",
    "created_at": "2014-03-31T13:50:22+00:00",
    "closed_at": "2016-04-29T17:17:59+00:00",
    "resolver": "sinhrks",
    "resolved_in": "8439d28f07fece62bc873f306cdd248ad22e665b",
    "resolver_commit_num": 298,
    "title": "SparseSeries.value_counts doesn't include fill_value counts",
    "body": "Based on a suggestion from an user at stackoverflow.com, I am reporting a bug / enhancement request for sparse data frames. Please let me know if you need any more information.\n\nThanks in advance.\n\nI am encountering a TypeError with a pandas sparse data frame when I use the value_counts method. I have listed the versions of the packages that I am using.\n\nPython 2.7.6 |Anaconda 1.9.1 (x86_64)| (default, Jan 10 2014, 11:23:15) \n[GCC 4.0.1 (Apple Inc. build 5493)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n> > > import pandas\n> > > print pandas.**version**\n> > > 0.13.1\n> > > import numpy\n> > > print numpy.**version**\n> > > 1.8.0\n> > > \n> > > dense_df = pandas.DataFrame(numpy.zeros((10, 10))\n> > >                                ,columns=['x%d' % ix for ix in range(10)])\n> > > dense_df['x5'] = [1.0, 0.0, 0.0, 1.0, 2.1, 3.0, 0.0, 0.0, 0.0, 0.0]\n> > > print dense_df['x5'].value_counts()\n> > > 0.0    6\n> > > 1.0    2\n> > > 3.0    1\n> > > 2.1    1\n> > > dtype: int64\n> > > \n> > > sparse_df = dense_df.to_sparse(fill_value=0) # Tried fill_value=0.0 also\n> > > print sparse_df.density\n> > > 0.04\n> > > \n> > > print sparse_df['x5'].value_counts()\n> > > Traceback (most recent call last):\n> > >  File \"<stdin>\", line 1, in <module>\n> > >  File \"//anaconda/lib/python2.7/site-packages/pandas/core/series.py\", line 1156, in     value_counts\n> > >     normalize=normalize, bins=bins)\n> > >  File \"//anaconda/lib/python2.7/site-packages/pandas/core/algorithms.py\", line 231, in value_counts\n> > >     values = com._ensure_object(values)\n> > >   File \"generated.pyx\", line 112, in pandas.algos.ensure_object (pandas/algos.c:38788)\n> > >   File \"generated.pyx\", line 117, in pandas.algos.ensure_object (pandas/algos.c:38695)\n> > >   File \"//anaconda/lib/python2.7/site-packages/pandas/sparse/array.py\", line 377, in astype\n> > >     raise TypeError('Can only support floating point data for now')\n> > > TypeError: Can only support floating point data for now\n",
    "labels": [
      "Enhancement",
      "Algos",
      "Sparse"
    ],
    "comments": [
      "ok...will mark this as an enhancement (fyi I am Jeff!)\n\nA pr would be welcome as sparse needs some TLC\n",
      "Thanks a lot for all your help, Jeff.\n\nPlz let me know if there is any way I can help. Am a bit of python newbie, so not very confident of getting the pull request right at this point.\n\nBalaji\n\nSent from my iPad\n\nOn Mar 31, 2014, at 10:10 AM, jreback notifications@github.com wrote:\n\n> ok...will mark this as an enhancement (fyi I am Jeff!)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n",
      "On current master, `fill_value` is not included result without `TypeError`. Changing the title.\n\n```\npd.SparseSeries([1, 2, 0, 0], fill_value=0).value_counts()\n# 2    1\n# 1    1\n# dtype: int64\n```\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "milestoned",
      "demilestoned",
      "renamed",
      "cross-referenced"
    ],
    "changed_files": 8,
    "additions": 413,
    "deletions": 80,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/algorithms.py",
      "pandas/core/base.py",
      "pandas/sparse/array.py",
      "pandas/sparse/tests/test_series.py",
      "pandas/tests/series/test_analytics.py",
      "pandas/tests/test_categorical.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 6852,
    "reporter": "jreback",
    "created_at": "2014-04-09T19:49:44+00:00",
    "closed_at": "2014-10-05T02:31:10+00:00",
    "resolver": "asobrien",
    "resolved_in": "ab97e0a3fa542416fc28db3c55cea4f3ce640f19",
    "resolver_commit_num": 0,
    "title": "ENH: showing memory used",
    "body": "maybe a function `nbytes()` to return memory usage\n\nmaybe as a tuple : the Series in [6] and then a series of the axis data?\n\n\n",
    "labels": [
      "Ideas",
      "Performance",
      "Low-Memory",
      "Good as first PR"
    ],
    "comments": [
      "my 2c. i think `meminfo` should return your series and a property `nbytes` should return `df.meminfo().sum()`\n",
      "Using the following DataFrame as an example:\n\n``` python\nIn [11]:  df = pd.DataFrame({ 'float' : np.random.randn(10000000), 'int' : np.random.randint(0,5,size=10000000), 'date' : Timestamp('20130101'), 'string' : 'foo', 'smallint' : np.random.randint(0,5,size=10000000).astype('int16') })\n```\n\nI have implemented `meminfo()` as as DataFrame method, resulting in a Series. Note that units can be specified:\n\n``` python\n\nIn [13]: # Default units = B\nIn [14]: df.meminfo()\nOut[14]: \ndate        80000000\nfloat       80000000\nint         80000000\nsmallint    20000000\nstring      80000000\ndtype: float64\n\nIn [15]: df.meminfo(units=\"MB\")\nOut[15]: \ndate        76.293945\nfloat       76.293945\nint         76.293945\nsmallint    19.073486\nstring      76.293945\ndtype: float64\n```\n\nTotal memory usage of the DataFrame has been implemented as part of `df.info()` where the units representation can be specified:\n\n``` python\nIn [16]: df.info()             \n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 10000000 entries, 0 to 9999999\nData columns (total 5 columns):\ndate        datetime64[ns]\nfloat       float64\nint         int64\nsmallint    int16\nstring      object\ndtypes: datetime64[ns](1), float64(1), int16(1), int64(1), object(1)\nmemory_usage: 340000000.00 B\n\nIn [18]: df.info(mem_unit='MB')\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 10000000 entries, 0 to 9999999\nData columns (total 5 columns):\ndate        datetime64[ns]\nfloat       float64\nint         int64\nsmallint    int16\nstring      object\ndtypes: datetime64[ns](1), float64(1), int16(1), int64(1), object(1)\nmemory_usage: 324.25 MB\n```\n\nAny more thoughts?\n",
      "I think that would be useful!\n\nwould need a couple of display options I think:\n- default unit (MB?)\n- whether to display this by default (in say df.info()), default False for now\n",
      "Let's use MB as the default unit and add a `memory_usage` argument to the DataFrame `info()` method to specify unit type (where default is None).\n\nTotal memory usage of the DataFrame (by default in MB) can always be determined by using:\n\n``` python\ndf.meminfo().sum()\n```\n",
      "Seems like it would be easy to infer the unit by taking the log10 of the number of bytes, dividing by 10 to that number and using the bisect module to lookup the corresponding name of the unit in an array. \n",
      "I have units mapped in a dictionary with unit options consisting of (B, KB, MB, GB, TB, PB)\n\n``` python\n        byte_conv = {\n                    \"B\": 2**0,\n                    \"KB\": 2**10,\n                    \"MB\": 2**20,\n                    \"GB\": 2**30,\n                    \"TB\": 2**40,\n                    \"PB\": 2**50}\n```\n\nYet, this requires an explicit indication of desired units (as implemented [here](https://github.com/asobrien/pandas/blob/df-mem-info/pandas/core/frame.py#L1489). Are you suggesting to use a method to automatically infer the appropriate unit based on DataFrame size?\n",
      "Yep. Just an intuition, haven't thought it out completely. \n",
      "Inferring units would be nice for `df.info()` but not desirable for the machine readable `df.meminfo()`.\n",
      "@shoyer `df.meminfo()` would have no arguments, so that the series would always return memory usage in columns in terms of bytes. In this manner, usage would be consistent.\n\n@cpcloud a [small function](http://stackoverflow.com/a/1094933) could be used to infer the memory usage in `df.info()` with the appropriate order of magnitude if a`memory_usage` argument is `True`.\n\n``` python\ndef sizeof_fmt(num):\n    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n        if num < 1024.0:\n            return \"%3.1f %s\" % (num, x)\n        num /= 1024.0\n    return \"%3.1f %s\" % (num, 'PB')\n```\n"
    ],
    "events": [
      "milestoned",
      "labeled",
      "labeled",
      "cross-referenced",
      "labeled",
      "labeled",
      "unlabeled",
      "labeled",
      "unlabeled",
      "commented",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 7,
    "additions": 188,
    "deletions": 7,
    "changed_files_list": [
      "doc/source/faq.rst",
      "doc/source/options.rst",
      "doc/source/v0.15.0.txt",
      "pandas/core/config_init.py",
      "pandas/core/frame.py",
      "pandas/tests/test_format.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 7097,
    "reporter": "puterleat",
    "created_at": "2014-05-11T00:04:39+00:00",
    "closed_at": "2014-07-16T00:35:09+00:00",
    "resolver": "bashtage",
    "resolved_in": "7745d726b0625c8a4ab017629be0c4eb6241a81a",
    "resolver_commit_num": 6,
    "title": "df.to_stata doesn't handle boolean columns",
    "body": "I'd expected this to generate a row of 1's and 0's in the dta file...\n\n\n\nBut it generates the following error:\n\nsbox/lib/python2.7/site-packages/pandas/io/stata.pyc in _dtype_to_stata_type(dtype)\n    884     else:  # pragma : no cover\n    885         raise ValueError(\"Data type %s not currently understood. \"\n--> 886                          \"Please report an error to the developers.\" % dtype)\n    887 \n    888 \n\nValueError: Data type bool not currently understood. Please report an error to the developers.\n",
    "labels": [
      "Dtypes",
      "Enhancement",
      "Stata"
    ],
    "comments": [
      "its straightforward to do this enhancement, would welcome a pull-request.\n",
      "you can always `df[[boolean_columns]] = df[[boolean_columns]].astype(int)` to get the desired effect (as a work-around)\n",
      "Ah  - many thanks for the workaround. Just spotted that datetime64[ns] is also not currently understood. \n",
      "datetimens64[ns] works just fine in master/0.14 and should in 0.13.1.\n",
      "EDIT:\nI've just realised that the convert_dates parameter solves the problem, although this isn't obvious from the error message thrown (which suggests contacting the developers...)\n\nPerhaps dates without a conversion specified could throw a specific error requesting the convert_dates mapping be set?\n",
      "I just got this same error (0.14.0) for a uint16 dtype. Not sure if it's better to make an independent issue thread or post here. I'm new at this, apologies.\n",
      "maybe post separate issues for the last 2 (error reporting for using `convert_dates` & the uint16 issue). and would appreciate pull-requests !\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "labeled",
      "unlabeled",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 80,
    "deletions": 5,
    "changed_files_list": [
      "doc/source/io.rst",
      "doc/source/v0.15.0.txt",
      "pandas/io/stata.py",
      "pandas/io/tests/test_stata.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 7160,
    "reporter": "rafaljozefowicz",
    "created_at": "2014-05-18T04:06:33+00:00",
    "closed_at": "2016-05-23T21:43:04+00:00",
    "resolver": "gfyoung",
    "resolved_in": "9a6ce07ce19adb6d8ded8af2ef66326d6750171e",
    "resolver_commit_num": 21,
    "title": "Bug in read_csv with duplicated column names",
    "body": "Tested on 0.13.0, 0.13.1 and 0.14.0rc1:\n\n\n\nThe last one returns:\n\n\n\nI would expect all 3 methods to return the same DataFrame. I noticed this when I wanted to read csv file that had a separate file with a header (and a duplicated column in it). BTW is there a better way to do it than to read the header file first and pass the output into 'names' parameter of read_csv?\n",
    "labels": [
      "Bug",
      "CSV",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "read_csv needs to interpret many formats so that's why it changes to not have duplicate columns\n(and some code that assumes they r unique)\n\nso this needs some work\n\nmarking as a bug for 0.15 - feel free to submit a pr\n",
      "@jreback : This bug still exists in `master`.  What sort of output should be expected from this?  Is the second one supposed to give the _exact_ same output as the first and third (when patched) ones?\n",
      "yeh I think the problem is that the names->columns are passed back as a dict and not as 2 lists, so it gets lots. Its in the post-processing code in python somewhere.\n\nI would expect as the OP suggests. Note tthis is different that if `usecols` has dupes. `names` is merely acting as the header here.\n",
      "FYI: for the second example, that output is correct because `mangle_dupe_cols` defaults to `True`, meaning that all columns become unique with `.{x}` labelling as expected.  The bug also surfaces if you set `mangle_dupe_cols=False`.\n",
      "@jreback : Question, what does the `as_recarray` option do exactly?  And, should we be raising a `ValueError` (as we do now in `master`) in the third example if we set `as_recarray=True`?\n",
      "`as_recarray` returns a numpy rec-array. But AFAIK its pretty much unused / not tested very well / somewhat buggy. Originally for numpy compat (and of course would be important if we eventually extracted the `read_csv` as a separate indepedent module (so that numpy could use it directly).\n",
      "Oh, okay.  How about my second question?\n\nEDIT: never mind - `numpy` says the answer is yes.\n",
      "@jreback : Question, what does `mangle_dupe_cols` mean exactly?  It seems to only apply when the header is in the file but has no effect if `names` has duplicates (as in this issue).\n",
      "it's a way to turn duplicates into things like\nA_1 A_2.  etc\n\nwe really don't need this anymore but it's there so leave I guess -main issue is supporting duplicates properly \n",
      "That is true...as of right now, there is ZERO support for duplicates AFAICT. :disappointed: \n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 7,
    "additions": 149,
    "deletions": 38,
    "changed_files_list": [
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/io/parsers.py",
      "pandas/io/tests/parser/c_parser_only.py",
      "pandas/io/tests/parser/common.py",
      "pandas/io/tests/parser/test_parsers.py",
      "pandas/io/tests/parser/test_unsupported.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 7222,
    "reporter": "seth-p",
    "created_at": "2014-05-23T16:40:37+00:00",
    "closed_at": "2016-02-12T14:30:24+00:00",
    "resolver": "scari",
    "resolved_in": "3e4c5722848c733b6b583f483534b38e550be723",
    "resolver_commit_num": 5,
    "title": "BUG: Series.plot() doesn't work with CustomBusinessDay frequency",
    "body": "I posted this on #!topic/pydata/FnHBkkdBIFY.\n\nIn IPython, trying to .plot() a Series with a CustomBusinessDay frquency produces \"ValueError: Unknown freqstr: C\". See below. I imagine DataFrame.plot() would have a similar issue.\n\nAm running -build/dev/pandas-0.14.0rc1-51-gccd593f.win-amd64-py3.4.exe and matplotlib 1.2.0.\n\n\n",
    "labels": [
      "Bug",
      "Visualization"
    ],
    "comments": [
      "post `pd.show_versions()` as well (e.g. which numpy are you on)\n",
      "its converting it to a `PeriodIndex` which is not supported by that freq....\n",
      "```\npd.show_versions()\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.4.0.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 45 Stepping 7, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.14.0rc1-51-gccd593f\nnose: None\nCython: 0.20.1\nnumpy: 1.8.1\nscipy: 0.14.0\nstatsmodels: 0.5.0\nIPython: 2.1.0\nsphinx: None\npatsy: 0.2.1\nscikits.timeseries: None\ndateutil: 2.2\npytz: 2014.3\nbottleneck: 0.8.0\ntables: 3.1.1\nnumexpr: 2.4\nmatplotlib: 1.3.1\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: 0.5.5\nlxml: None\nbs4: None\nhtml5lib: None\nbq: None\napiclient: None\nrpy2: None\nsqlalchemy: 0.9.4\npymysql: None\npsycopg2: None\n```\n",
      "Running into the same issue.\n",
      "Running into the same issue.\n",
      "Ran into the same issue.  Worked around it by doing `series.asfreq('D').interpolate(how='time').plot()`, which does a decent job visually, but is not the most elegant to write.\n",
      "Passing `x_compat=True` also appears to be a decent workaround.\n",
      "Ran into same problem and took a look a bit. I believe freqstr 'C' should be in `_period_code_map`. I'll send a PR after finish the test.\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "commented",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 11,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/tests/test_graphics.py",
      "pandas/tseries/frequencies.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 7271,
    "reporter": "TomAugspurger",
    "created_at": "2014-05-29T01:41:48+00:00",
    "closed_at": "2016-07-20T22:04:49+00:00",
    "resolver": "StephenKappel",
    "resolved_in": "63a1e5c58af8ddc8dec192f39a0999aad74acaf9",
    "resolver_commit_num": 0,
    "title": "ENH: `df.astype` could accept a dict of {col: type}",
    "body": "This would be consistent with other pandas methods.\n\nThe reason I'm running into it is having some NaNs scattered across some `int` and `bool` columns, which converts to float / objects. If I discard those NaNs, it would be nice to do\n\n`df = df.astype({'my_bool', 'bool', 'my_int': 'int'})`\n\ninstead of\n\n\n",
    "labels": [
      "Dtypes",
      "Enhancement",
      "Good as first PR"
    ],
    "comments": [
      "that's a nice idea!\n",
      ":+1:\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "labeled",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 5,
    "additions": 130,
    "deletions": 6,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/generic.py",
      "pandas/tests/frame/test_dtypes.py",
      "pandas/tests/series/test_dtypes.py",
      "pandas/tests/test_panel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 7286,
    "reporter": "bquistorff",
    "created_at": "2014-05-30T20:12:56+00:00",
    "closed_at": "2014-06-16T12:52:02+00:00",
    "resolver": "bashtage",
    "resolved_in": "904933aa9c4ca97aede4d3b88999ca5a4757405d",
    "resolver_commit_num": 5,
    "title": "Error when writing non-ascii allowed characters to Stata dta",
    "body": "When trying to write a Stata dataset with strings containing upper latin-1 characters (which are allowed by the Stata format), I get an encoding error.\n\n\n\nI get the following output:\n\n\n\nMachine info:\n\n\n",
    "labels": [
      "Bug",
      "Unicode",
      "Stata"
    ],
    "comments": [
      "cc @bashtage\n\nhmm. it seems it not being encoded to the passed encoding, odd (and is not tested).\n\nthe reading however works\n\nFYI, you can/should use:\n\n`df = pd.read_stata(.......,encoding='latin-1')`\n`df.to_stata(......,encoding='latin-1')`\n(thought this last seems to be a bug)\n",
      "A simple fix - the encoding was never used when writing a file.\n",
      "assume that stata itself can read the encoded file?\n\niIIRC they always encode in cp1252 or something like that\n",
      "The original files opens fine in stata, so latin1 seems to be OK.  Unicode isn't supported though.\n\nOn Jun 13, 2014 6:38 PM, jreback notifications@github.com wrote:\n\nassume that stata itself can read the encoded file?\n\niIIRC they always encode in cp1252 or something like that\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pydata/pandas/issues/7286#issuecomment-46067490.\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "unlabeled",
      "referenced",
      "commented",
      "commented",
      "commented",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 14,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/v0.14.1.txt",
      "pandas/io/stata.py",
      "pandas/io/tests/test_stata.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 7365,
    "reporter": "dmsul",
    "created_at": "2014-06-06T01:48:48+00:00",
    "closed_at": "2014-07-16T00:35:09+00:00",
    "resolver": "bashtage",
    "resolved_in": "7745d726b0625c8a4ab017629be0c4eb6241a81a",
    "resolver_commit_num": 6,
    "title": "df.to_stata does not support uint16",
    "body": "\n\nMentioned in #7097 \n",
    "labels": [
      "Dtypes",
      "Stata"
    ],
    "comments": [],
    "events": [
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "unlabeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 80,
    "deletions": 5,
    "changed_files_list": [
      "doc/source/io.rst",
      "doc/source/v0.15.0.txt",
      "pandas/io/stata.py",
      "pandas/io/tests/test_stata.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 7445,
    "reporter": "aizvorski",
    "created_at": "2014-06-12T21:05:10+00:00",
    "closed_at": "2016-12-16T11:22:01+00:00",
    "resolver": "mroeschke",
    "resolved_in": "256622339ae3d9137879f422ccce886920e6d688",
    "resolver_commit_num": 14,
    "title": "Column information lost in to_json() if data frame is empty",
    "body": "Create empty dataframe with a column and write it out with to_json(), then read it back in:\n\n\n\nI think the expected string is '{\"test\":{}}'\n\nThis situation arises when code filters data frames and writes them out, while not being very careful to make sure the filtered frames contain at least one row.\n",
    "labels": [
      "JSON",
      "Bug"
    ],
    "comments": [
      "cc @Komnomnomnom \n",
      "Thanks  @aizvorski I can confirm this bug, and I agree about the expected output. \n\nThe JSON handling is producing its output by looping over the dataframe values array, which is why in this case the labels end up being ignored during serialisation.\n\n@jreback I'm on vacation until mid July but I'll see if I can get this fixed for 0.15. There's also the other JSON issues / features I've been meaning to do since pre 0.14...I'll see if I can knock them out at the same time.\n",
      "closed by https://github.com/pydata/pandas/pull/9805\n\n``` python\n>>> pd.DataFrame({'test':[]}, index=[]).to_json(orient='columns')\n'{\"test\":{}}'\n```\n"
    ],
    "events": [
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 1,
    "additions": 4,
    "deletions": 0,
    "changed_files_list": [
      "pandas/io/tests/json/test_pandas.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 7453,
    "reporter": "sinhrks",
    "created_at": "2014-06-14T01:37:22+00:00",
    "closed_at": "2016-04-10T14:04:45+00:00",
    "resolver": "sinhrks",
    "resolved_in": "ea9a5a8555a455150ffc84018cbf992a07171dc6",
    "resolver_commit_num": 280,
    "title": "BUG: Unable to aggregate TimeGrouper ",
    "body": "Derived from #7373. There seems to be 3 issues related to `TimeGrouper` aggregation.\n##### 1. var, std, mean\n\nvar/std/mean raises `ValueError` when group key contains `NaT`. \n\n\n##### 2. size (#7600)\n\n`size` raises `AttributeError` regardless of `NaT` existence.\n\n\n##### 3. first, last, nth\n\nIt looks work, but `TimeGrouper` outputs different result from normal `groupby`. \n\n\n\nI assume the difference derived from `BinGrouper` sorts rows differently from normal groupby. Thus, result of normal groupby and `TimeGrouper` can differ. \n\n\n",
    "labels": [
      "Bug",
      "Groupby",
      "Missing-data",
      "Testing",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "first 2 look fixed, just need validation tests. Then can deal with 3rd issue separately.\n"
    ],
    "events": [
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 30,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/groupby.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 7588,
    "reporter": "dershow",
    "created_at": "2014-06-27T13:53:37+00:00",
    "closed_at": "2016-04-11T15:03:57+00:00",
    "resolver": "gliptak",
    "resolved_in": "1c8816f891c693d81b6e2593543eb310e069ff62",
    "resolver_commit_num": 20,
    "title": "Add Akima Interpolation to Pandas",
    "body": "Currently Akima is not a valid interpolation method in Pandas.  However scipy does have Akima1Dinterpolator.  As Pandas uses SciPy for doing interpolation, adding the Akima method to pandas should be easy. \n",
    "labels": [
      "Missing-data",
      "Enhancement",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "sure, requires `scipy` >= 0.14.0 though.\n",
      "It looks like for the pchip interpolate that pandas just does a check to verify that it is there, and otherwise reports that scipy doesn't support it. \n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "labeled",
      "labeled",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 7,
    "additions": 103,
    "deletions": 14,
    "changed_files_list": [
      "ci/requirements-3.5_OSX.run",
      "doc/source/conf.py",
      "doc/source/missing_data.rst",
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/generic.py",
      "pandas/core/missing.py",
      "pandas/tests/test_generic.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 7626,
    "reporter": "ChristopherShort",
    "created_at": "2014-07-01T02:55:23+00:00",
    "closed_at": "2016-12-06T01:48:05+00:00",
    "resolver": "jeffcarey",
    "resolved_in": "4378f82967f59097055eef17ede50aa515525551",
    "resolver_commit_num": 0,
    "title": "nrows limit fails reading well formed csv files from Australian electricity market data",
    "body": "Reading Australian electricity market data files, read_csv reads past the nrows limit for certain nrows values and consequently fails.\n\nThese market data files are 4 csv files combined into a single csv file and so the file has multiple headers and variable field size across the rows. \n\nThe first set of data is from rows 1-1442.\n\n Intent was to extract first set of data with nrows = 1442. \n\nTesting several arbitrary CSV files from this data source shows well formed CSV - 120 fields between rows 1 to 1442  (with a 10 field at row 0)\n\n\n\nreturns\n    120    1441\n    10        1\n    dtype: int64\n\nOther python examples of reading the market data using csv module [work fine](-json/blob/master/script/extract-historic-public-prices.py)\n\nIn the reproducible example below, code works for nrows< 824, but fails on any value above it. \n\nTesting on arbitrary files suggests the 824 limit is variable - sometimes a few more rows, sometimes a few less rows. \n\n\n",
    "labels": [
      "CSV",
      "Bug"
    ],
    "comments": [
      "I meant to say - the sort of error returned is:\n\n```\nCParserError: Error tokenizing data. C error: Expected 120 fields in line 1443, saw 130\n```\n\nWhich is expected - the field size changes past row 1442 - but for these files, the nrows limit reads past the 1442 (or 823+) value.\n\nI also tested nrows on arbitrarily created csv files via numpy arrays but couldn't reproduce the error from the real data I was working with.\n\n(And apologies for poorly formed markdown above - first time posting  :-)\n",
      "why don't you create a test. Pull the header and 2 rows from each section (then limit the number of fields). Then try this using nrows to skip. If this is a bug, would need to create a reproducible example.\n",
      "thanks - but I'm unclear on your request - that is, I thought I did what you asked already.\n\nI created a reproducible example with the code at the bottom of my post - admittedly with iPython rather than a straight python file.\n\nI'm trying to extract the first section (rows 1-1442 of a 3366 row file) - this is where my problem occurs.\n\nWas my code example unclear?\n\nFor reproducibility purposes, the bulk of the code deals with downloading a zip file,  but the test is in the five lines from 'with thezipfile.open(fname) as csvFile:' onwards\n\nI'm expecting it to be a subtle bug (or I'm doing something very wrong) - nrows parameter clearly works on the various examples I threw at it that were much larger in row length.\n\nBut at the same time, these electricity market files are well formed CSV files (they are part of the market data process in a live electricity market where auctions are run every 5 minutes for the past 15 years)  - and pandas is failing to parse the files I used in developing the code.\n",
      "no, what I mean is we need an example that you can simply copy and paste (and not use an external URL).\n",
      "thanks - you've given me a thought that I can test it by just breaking the relevant part of the CSV file out.\n\nBut if it turns out to be related to the file structure itself - not sure how to provide a test without a link to a sample file.  Would creating a github repo with some sample csv files and a few lines of code be suitable as the test?\n",
      "see what you come up with. this is either a bug, which can be reproduced via a generated type of test file (e.g. create a specific structure), a problem with the csv, or incorrect use of the options. \n\nWe need a self-contained test in order to narrow down the problem.\n\nlmk what you find.\n",
      "Thanks.\n\nIf the file is only the relevant section (or rows to skip at the front) - no error.\n\nImplies not a problem with use of options too I think.\n\nIf the file structure includes the very next line (no. 1443) - the 130 field header for the next section - it fails with any nrows>823.\n\nI also experimented with deleting arbitrary number of rows (but small number) at the end of the section before the next header row - to see if the issue related to that particular line ending.  Again fail.\n\nI'm not sure I can create a test file - other than the sample files I've been experimenting with.\n\nI'll go and figure out how to make a github repo and perhaps we can take it from there.\n\nFor info - the full error at the fail point is:\n\n/Users/ChristopherShort/anaconda/lib/python3.4/site-packages/pandas/io/parsers.py in read(self, nrows)\n   1128 \n   1129         try:\n-> 1130             data = self._reader.read(nrows)\n   1131         except StopIteration:\n   1132             if nrows is None:\n\n/Users/ChristopherShort/anaconda/lib/python3.4/site-packages/pandas/parser.so in pandas.parser.TextReader.read (pandas/parser.c:7146)()\n\n/Users/ChristopherShort/anaconda/lib/python3.4/site-packages/pandas/parser.so in pandas.parser.TextReader._read_low_memory (pandas/parser.c:7547)()\n\n/Users/ChristopherShort/anaconda/lib/python3.4/site-packages/pandas/parser.so in pandas.parser.TextReader._read_rows (pandas/parser.c:7979)()\n\n/Users/ChristopherShort/anaconda/lib/python3.4/site-packages/pandas/parser.so in pandas.parser.TextReader._tokenize_rows (pandas/parser.c:7853)()\n\n/Users/ChristopherShort/anaconda/lib/python3.4/site-packages/pandas/parser.so in pandas.parser.raise_parser_error (pandas/parser.c:19604)()\n\nCParserError: Error tokenizing data. C error: Expected 120 fields in line 1443, saw 130\n",
      "how about this for a test.  \n\nCreate two CSV files ( 1442 rows by 120 cols and 5 rows by 130 cols)\nConcatenate them\nRead the joined CSV file back into a dataframe with nrows option <= 1442\n\nIt fails in the same way - though the nrows parameter was much larger before failure occurred relative to the examples above (where the files contained more strings)\n\nIn the example below - it fails for nrow>1360.  Works fine for lower values.\n\n```\npd.DataFrame(np.random.uniform(size=1442*120).reshape(1442,120)).to_csv('test120.csv',index=False)\n\npd.DataFrame(np.random.uniform(size=5*130).reshape(5, 130)).to_csv('test130.csv',index=False)\n\n\nfilenames = ['test120.csv', 'test130.csv']\nwith open('testNrows.csv', 'w') as outfile:\n    for fname in filenames:\n        with open(fname) as infile:\n            for line in infile:\n                outfile.write(line)\n\ndf = pd.read_csv('testNrows.csv', nrows=1361)\n\n```\n",
      "ok great, must be a bug somewhere\n",
      "I have some time now to try and look at this bug, but not much experience.\n\nDo you have any recommendations on things I should know first?\n",
      "well it's going to be in parser.pyx\n\nIMHO not so easy to debug cython\n\nI would start by putting print statements to figure out what it is doing on this file\n",
      "OK - thanks \n",
      "This is a small update (and to see if any thoughts occur to you).\n\nBefore I went to look at parser.py, I tried to generalise the test file above in order to explore row/column variations to see if there was a boundary to the error.\n\nI didn't get far in exploring row parameters before realising the error appears to occur randomly. \n\nIn the code below, it loops over the 'test' 3 times, printing out the number of rows in the failed example, as well as the memory size of the dataframe in the failed run.\n\nDifferent number of errors across different runs (I've seen one where there was no error at all).\n\nThe dataframe memory size doesn't appear relevant - when i printed it for all tests, bigger ones passed, smaller ones failed, nothing obvious to look for.\n\nAnd indicating that it's not a memory thing - a typo that set the number of columns to 12, instead of 120, got that error each and every time read_csv was called.\n\nI'll go look at parser.py to see where I could put some print statements - but, as you say, probably in cython call somewhere - and I'm an economist (not a programer) who last used C sparingly 20 years ago.\n\n``` python\ndef test_RowCount (size_1=(1442,120), rowCount=1361):   #original parameters where failure occurred\n\n    df_1 = pd.DataFrame (np.random.uniform(size=size_1))\n    df_1.to_csv('test120.csv',index=False)\n\n    #create file of combined csv file ('testNrows.csv') of different record lengths\n    filenames = ['test120.csv', 'test130.csv']\n    with open('testNrows.csv', 'w') as outfile:\n        for fname in filenames:\n            with open(fname) as infile:\n                for line in infile:\n                    outfile.write(line)\n\n\n    try:\n        df = pd.read_csv('testNrows.csv', header=0, nrows=rowCount)\n\n    except (pd.parser.CParserError) as error:\n        print (error)\n        print ('Rows: ', size_1[0])\n        print ('Memory (MB): ', df_1.memory_usage(index=True).sum()/1024/1024, '\\n')\n\n#    except:\n#        print (\"Unexpected error: \", sys.exc_info()[0])\n\n\n\n### Write out 1 file of different record length for later use in test_RowCount function \nsize_2 = (1,130)\ndf_2 = pd.DataFrame(np.random.uniform(size=size_2))\ndf_2.to_csv('test130.csv',index=False)       \n\n\n### Loop for testing various row counts and record lengths\nfor j in range(3):\n    print ('Run ', j)\n    for i in range(1442, 1361, -1): \n        #print (i)\n        test_RowCount(size_1=(i,120), rowCount=1360)\n```\n",
      "@jreback This is a real problem.  It's still present in 0.19.  It can be worked around by `engine='python'` but this is not a real solution.  Stack Overflow has now discovered this problem at least twice:\r\n\r\n1. http://stackoverflow.com/questions/25985817/why-is-pandas-read-csv-not-reading-the-right-number-of-rows\r\n2. http://stackoverflow.com/questions/37040634/pandas-read-csv-with-engine-c-issue-bug-or-feature\r\n",
      "well if u have a reproducible example pls show it",
      "@jreback OK, the input file is 516 KB.  Where would you like me to put it?  I tried removing \"unnecessary\" rows from it but this bug doesn't reproduce if you shrink the file a lot.",
      "best to put this up on a separate repo or gist, and use a URL to access.",
      "@jreback I have uploaded a file which reproduces this error: https://gist.githubusercontent.com/jzwinck/838882fbc07f7c3a53992696ef364f66\r\n\r\nSimply download that file and run this:\r\n\r\n    import pandas as pd\r\n    pd.read_table('pandas_issue_7626.txt', skiprows=2195, nrows=100)\r\n\r\nIt fails, saying:\r\n\r\n    File \"pandas/parser.pyx\", line 846, in pandas.parser.TextReader.read (pandas/parser.c:9884)\r\n    File \"pandas/parser.pyx\", line 880, in pandas.parser.TextReader._read_low_memory (pandas/parser.c:10347)\r\n    File \"pandas/parser.pyx\", line 922, in pandas.parser.TextReader._read_rows (pandas/parser.c:10870)\r\n    File \"pandas/parser.pyx\", line 909, in pandas.parser.TextReader._tokenize_rows (pandas/parser.c:10741)\r\n    File \"pandas/parser.pyx\", line 2018, in pandas.parser.raise_parser_error (pandas/parser.c:25878)\r\n    pandas.io.common.CParserError: Error tokenizing data. C error: Expected 6 fields in line 2355, saw 14\r\n\r\nSince we told Pandas to read from line 2195 for 100 rows, it should never have seen line 2355."
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 21,
    "deletions": 5,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.2.txt",
      "pandas/io/tests/parser/c_parser_only.py",
      "pandas/src/parser/tokenizer.c"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 7630,
    "reporter": "jreback",
    "created_at": "2014-07-01T11:33:16+00:00",
    "closed_at": "2016-08-12T15:44:54+00:00",
    "resolver": "sinhrks",
    "resolved_in": "29d9e24f4c778b0c9ebe9288bfc217808d2c6edb",
    "resolver_commit_num": 377,
    "title": "BUG: tz lost on combine first/fillna",
    "body": "-tzinfo-lost-by-combine-first\n\n\n",
    "labels": [
      "Bug",
      "Timezones",
      "Missing-data"
    ],
    "comments": [
      "This also happened with `fillna`, so perhaps it's a more wide-ranging problem?\n\n```\n    utcdate = pd.to_datetime('20000101', utc=True)\n    df1 = pd.DataFrame(columns=['UTCdatetime'], data=utcdate, index=pd.date_range('20140627', periods=1))\n    print(df1)\n    df1 = df1.fillna(utcdate)\n    print(df1)\n\n                          UTCdatetime\n2014-06-27  2000-01-01 00:00:00+00:00\n\n           UTCdatetime\n2014-06-27  2000-01-01\n```\n",
      "this is a related issue, its marked as a bug. feel free to do a pull-request to fix.\n",
      "Thanks. As much as I'd love to fix it pandas is my first foray into really programming so I think I'd probably do more harm than good at this stage.. ;)  Later hopefully!\n"
    ],
    "events": [
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "renamed",
      "commented",
      "cross-referenced",
      "labeled",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 7,
    "additions": 410,
    "deletions": 227,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/frame.py",
      "pandas/tests/frame/test_combine_concat.py",
      "pandas/tests/frame/test_operators.py",
      "pandas/tests/types/test_cast.py",
      "pandas/types/cast.py",
      "pandas/types/common.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 7640,
    "reporter": "shoyer",
    "created_at": "2014-07-01T21:18:41+00:00",
    "closed_at": "2017-04-14T13:32:37+00:00",
    "resolver": "jreback",
    "resolved_in": "9991579c812e5a7c977e69f03b390adf7974445f",
    "resolver_commit_num": 4376,
    "title": "Proposal: New Index type for binned data (IntervalIndex)",
    "body": "### Design\n\nThe idea is to have a natural representation of the grids that ubiquitously appear in simulations and measurements of physical systems. Instead of referencing a single value, a grid cell references a _range_ of values, based on the chosen discretization. Typically, cells boundaries would be specified by floating point numbers. In one dimension, a grid cell corresponds to an _interval_, the name we use here.\n\nThe key feature of `IntervalIndex` is that looking up an indexer should return all intervals in which the indexer's values fall. `FloatIndex` is a poor substitute, because of [floating point precision issues](), and because I don't want to label values by a single point.\n\nA `IntervalIndex` is uniquely identified by its `intervals` and `closed` (`'left'` or `'right'`) properties, an ndarray of shape `(len(idx), 2)`, indicating each interval. Other useful properties for `IntervalIndex` would include `left`, `right` and `mid`, which should return arrays (indexes?) corresponding to the left, right or mid-points of each interval.\n\nThe constructor should allow the optional keyword argument `breaks` (an array of length `len(idx) + 1`) to specified instead of `intervals`.\n\nIt's not entirely obvious what `idx.values` should be (`idx.mid`? strings like `'(0, 1]'`? an array of tuples or `Interval` objects?). I think the most useful choice for cross compatibility would probably be to an ndarray like `idx.mid`.\n\n`IntervalIndex` _should_ support mathematical operations (e.g., `idx + 1`), which are calculated by vectorizing the operation over the breaks.\n### Examples\n\nAn example already in pandas that should be a `IntervalIndex` is the `levels` property of categorical returned by `cut`, which is currently an object array of strings:\n\n\n\nExample usage:\n\n\n### Implementation\n\nA `IntervalIndex` would be a monotonic and non-overlapping one-dimensional array of intervals. It is _not_ required to be contiguous. A scalar `Interval` would correspond to a contiguous interval between start and stop values (e.g., given by integers, floating point numbers or datetimes).\n\nFor index lookups, I propose to do a binary search (`np.searchsorted`) on `idx.left`. If we add the constraint that all intervals must have a fixed width, we could calculate the bin using a formula in constant time, but I'm not sure the loss in flexibility would be worth the speedup.\n\n`IntervalIndex` should play nicely when used as the levels for `Categorical` variable (#7217), but it is _not_ the same as a `CategoricalIndex` (#7629). For example, a `IntervalIndex` should not allow for redundant values. To represent redundant or non-continuous intervals, you would need to make in a `Categorical` or `CategoricalIndex` which uses a `IntervalIndex` for the levels. Calling `df.reset_index()` on an `DataFrame` with an `IntervalIndex` would create a new `Categorical` column.\n\n---\n\nNote: I'm not entirely sure if this design doc belongs here or on mailing list (I'm happy to post it there if requested).\n\nHere is the comment where I brought this up previously: #issuecomment-44474502\n\nCC @hugadams -- I expect `IntervalIndex` would be very handy for your [pyuvvis]().\n",
    "labels": [
      "API Design",
      "Internals",
      "Indexing",
      "Enhancement"
    ],
    "comments": [
      "FWIW, in our local in-house n-dim library we have something similar (an IntervalAxis), and it works quite well.\n",
      "@shoyer all for this!\n\nI know you are against this, but I would encorage you to inherit from `Index`. OR create a new base class that is ABC like which we can eventually use as a base class for `Index`.\n",
      "+1 here too. tho i think `IntervalIndex` might be a better name.\n\n@shoyer great idea and excellent write up :)\n",
      "Thanks for the support! I'm not sure when I'll get around to implementing this, but I will add it to my source open backlog :).\n\n@jreback Agreed, for an new index class _inside_ pandas, it is OK to subclass from `Index`. I haven't thought too much about the details of implementing this in pandas yet.\n\n@cpcloud Also agreed, `IntervalIndex` is a better name for the described functionality. I will update the first comment. `CellIndex` makes more sense for an index that is actually constrained to a grid. That would also be useful, but is less general.\n",
      "This would be very useful for me, too.  Currently I'm using a `DatetimeIndex` that's one longer than my data, which are padded with a row of `nan`s at the end, so that `df.index[i]:df.index[i+1]` is the \"index\" corresponding to `iloc[i]`.  It seemed clever when I started the project.\n\nThis also seems like it will help make contiguous groupby (#5494) easier, since it gives a natural choice of index for the groupings.\n",
      "@shoyer \n\nif u (or anyone else)\ncould post test pairs for this would really help it along\n\nessentially test cases for everything from construction to various indexing ops\nthat define as much behavior as possible\n\neg for Int64Index\n\nresult = Index([1,2,3])\nexpected = [1,2,3]\nassert_almost_equal(result,expected)\n",
      "@shoyer \n\nThanks for including me; sorry I didn't notice earlier (mail filter was throwing github alerts out).  Indeed, I think a general interval index is probably a great addition; although, I lack the breadth in vision to see a general solution.\n\nI did actually implement a hacky version of an interval index in pyuvvis that converts a datetime index to intervals of seconds, minutes etc...  The main lesson I learned is that your interval index should be able to map back to the original data.  To do this, I actually retain the original datetimeindex, and use metadata like \"_interval=True\" to navigate between all of the logic.  In my case, this mapping is stored on the TimeSpectra object (dataframe + metadata). \n\nI put a demo of this up in case seeing a hack in action might help in the design of a general solution.\n\nhttp://nbviewer.ipython.org/github/hugadams/pyuvvis/blob/master/examples/Notebooks/intervals.ipynb\n",
      "@hugadams Looking at your notebook, it appears you may be thinking of a [TimedeltaIndex](https://github.com/pydata/pandas/issues/3009)? \n\nThe idea behind IntervalIndex is somewhat distinct -- although I can imagine that an IntervalIndex wrapping a TimedeltaIndex could be useful in some cases.\n\n@jreback Sounds like a good idea, when I get the chance I will start writing some test cases and add them to this issue.\n",
      "Ha ya exactly!  Thanks, never even saw this thread.  I'll post my notebook there for reference as well.  I must not understand the intervalindex then.\n",
      "Here are a bunch of test cases: https://github.com/shoyer/pandas/commit/838a59767a0611ea7a07f80f4634cf39d2743046\n\nI can open a PR if that makes things easier.\n",
      "@shoyer that's a nice test suite...link is good for now. but of course expand to an actual impl!\n",
      "I have updated the first post with some revisions to implementation details (per by test-cases). Basically, I realized that there is no a strong need to require that intervals be contiguous, and dropping that requirement should add some nice flexibility (e.g., the ability to subsample intervals with `idx[::step]`).\n\n@jreback Haha, I thought that was your job? ;)\n\nIn all seriousness, I will probably get around to this at some point but the existing Index objects are pretty complex. #5080 would help -- I'm not looking forward to writing kludges around this also being an `ndarray`.\n",
      "well this is the removing of ndarray from Index!\n\nhttps://github.com/jreback/pandas/tree/index\n\nalmost done\n",
      "First draft PR is up in #8707. So far, this is actually much easier than I feared...\n",
      "For those of you not following along in #901 (which is honestly a dup of this issue), I am now thinking that the implementation here should probably use an actual interval-tree rather than relying on sortedness.\n\nAlso, for future reference: a suitable data-structure for an index of multi-dimensional intervals  (an `NDIntervalIndex`) is an [\"R-tree\"](http://en.wikipedia.org/wiki/R-tree). And in fact, this is quite a handy data-structure for GIS queries -- there is an R-tree now implemented in Geopandas: https://github.com/geopandas/geopandas/issues/140\n"
    ],
    "events": [
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "renamed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "labeled",
      "cross-referenced",
      "cross-referenced",
      "unlabeled",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files": 54,
    "additions": 4195,
    "deletions": 504,
    "changed_files_list": [
      "asv_bench/benchmarks/indexing.py",
      "doc/source/advanced.rst",
      "doc/source/api.rst",
      "doc/source/reshaping.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/_libs/hashtable.pyx",
      "pandas/_libs/interval.pyx",
      "pandas/_libs/intervaltree.pxi.in",
      "pandas/_libs/lib.pyx",
      "pandas/_libs/src/inference.pyx",
      "pandas/_libs/tslib.pyx",
      "pandas/core/algorithms.py",
      "pandas/core/api.py",
      "pandas/core/groupby.py",
      "pandas/core/indexing.py",
      "pandas/formats/format.py",
      "pandas/indexes/api.py",
      "pandas/indexes/base.py",
      "pandas/indexes/category.py",
      "pandas/indexes/interval.py",
      "pandas/indexes/multi.py",
      "pandas/tests/api/test_api.py",
      "pandas/tests/api/test_types.py",
      "pandas/tests/frame/test_alter_axes.py",
      "pandas/tests/frame/test_sorting.py",
      "pandas/tests/groupby/test_categorical.py",
      "pandas/tests/groupby/test_groupby.py",
      "pandas/tests/indexes/common.py",
      "pandas/tests/indexes/test_base.py",
      "pandas/tests/indexes/test_category.py",
      "pandas/tests/indexes/test_interval.py",
      "pandas/tests/indexing/test_interval.py",
      "pandas/tests/scalar/test_interval.py",
      "pandas/tests/series/test_constructors.py",
      "pandas/tests/series/test_missing.py",
      "pandas/tests/series/test_sorting.py",
      "pandas/tests/test_algos.py",
      "pandas/tests/test_base.py",
      "pandas/tests/test_categorical.py",
      "pandas/tests/tools/test_tile.py",
      "pandas/tests/types/test_dtypes.py",
      "pandas/tests/types/test_missing.py",
      "pandas/tools/tile.py",
      "pandas/tseries/base.py",
      "pandas/tseries/interval.py",
      "pandas/tseries/period.py",
      "pandas/types/api.py",
      "pandas/types/common.py",
      "pandas/types/dtypes.py",
      "pandas/types/generic.py",
      "pandas/types/inference.py",
      "pandas/types/missing.py",
      "pandas/util/testing.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 7751,
    "reporter": "yonil7",
    "created_at": "2014-07-14T18:47:54+00:00",
    "closed_at": "2016-12-30T19:02:01+00:00",
    "resolver": "ashishsingal1",
    "resolved_in": "8051d61223b10002a7b4a4f66373e7cfeb976095",
    "resolver_commit_num": 0,
    "title": "qcut() should make sure the bins bounderies are unique before passing them to _bins_to_cuts",
    "body": "xref #8309\n\nfor example:\n\n\n\nwill raise \"ValueError: Bin edges must be unique: array([ 1.,  1.])\" exception\n\nFix suggestion - add one new line:\n\n\n",
    "labels": [
      "Error Reporting",
      "Reshaping",
      "Good as first PR"
    ],
    "comments": [
      "why would you not just catch the `ValueError` ?  better to be informed of the issue, right?\n",
      "I think this should not throw exception as the is legitimate use.\nmaybe a better example:\n\n``` python\npd.qcut([1,2,3,4,4], [0, .25, .5, .75, 1.])\n```\n\nalso throw: ValueError: Bin edges must be unique: array([1, 2, 3, 4, 4], dtype=int64)\n",
      "## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.5.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.14.0\nnose: 1.3.0\nCython: 0.19.1\nnumpy: 1.8.1\nscipy: 0.14.0\nstatsmodels: 0.5.0\nIPython: 1.0.0\nsphinx: 1.1.3\npatsy: 0.2.1\nscikits.timeseries: None\ndateutil: 2.2\npytz: 2014.4\nbottleneck: 0.8.0\ntables: 3.1.1\nnumexpr: 2.3.1\nmatplotlib: 1.3.1\nopenpyxl: 1.8.5\nxlrd: 0.9.3\nxlwt: 0.7.5\nxlsxwriter: 0.5.5\nlxml: 3.3.5\nbs4: 4.3.1\nhtml5lib: None\nbq: None\napiclient: None\nrpy2: None\nsqlalchemy: 0.9.6\npymysql: None\npsycopg2: None\n",
      "maybe you misunderstand me. This is a user error (in that you generally pass unique bins, and you CAN make them unique before passing them in). Yes it could be done in the function, but wouldn't you as a user want to know that you are passing non-unique bins?\n",
      "The issue is regarding the **pd.qcut()** function (not pd.cut()). It should get array of samples and quantiles/ranks.\nIn the prev example, I passed array of samples [1,2,3,4,4] and would like it to calculate for each sample in that array to which of the 4 quantiles it belongs. (quantile 1,2,3 or 4)\n",
      "its the same issue. If you uniqfy in one, you would in the other. That's not the question though. _why_ should this happen silently? (automatically). maybe I am missing something.\n\n@jseabold ?\n",
      "I would expect this to raise an informative error to the user. \"The quantiles you selected do not result in unique bins. Try selecting fewer quantiles.\" I.e., if your 25th quantile is the same as your median, then you should be told this and you shouldn't ask for both. Trying to get fancy under the hood is asking for confusion IMO. \n",
      "@jseabold ok, I can buy that this could produce a better msg.\n\n@yonil7 want to take this on with a pull-request?\n",
      "In addition to just raising a more informative error, I think that there should be an option to automatically handle duplicates in the following way: \n\nSuppose we have a list with too many duplicates, say we want to split [1,2,3,3,3,3,3,3,4,5,6,7] into quartiles. Right now qcut fails, because the second-lowest quartile consists entirely of '3's, duplicating the bin edges. But there is a natural way to assign the quartiles if we allow duplicate values to be split among different bins: [1,2,3], [3,3,3], [3,3,4], [5,6,7]. \n\nNow this is not completely ideal -- the assignment is arbitrary, and there is no way of knowing what to do by looking at the bin edges alone. But in the real world it can be convenient to have an option that 'just works', and I think this warrants a 'split_dup=True' option that is false by default. \n\nWhat do people think? I'll add this if people support it.  \n",
      "@edjoesu start with the error msg. If you want to propose a new issue for discussion that is ok. I think if you show several test cases, some of which you CAN distinguish and some of which have to raise then prob ok, but needs discussion.\n",
      "Is there any work around for this until it's changed?\n",
      "this is not a bug, just a more informative error message.\n",
      "Got it, I thought it was being changed to a warning instead of an error.  I personally would like the option to allow non-unique bins, but I also understand the case against it.\n\nI just commented out those lines in tile.py and it seems to have allowed me to use duplicates.\n",
      "Does anyone own this change? Or has it already been done?\n",
      "See http://stackoverflow.com/questions/20158597/how-to-qcut-with-non-unique-bin-edges for a discussion about this. \n\n@edjoesu I agree with your proposal, but I foresee that the implementation won't be trivial, since the way the current code assigns values to bins I think some bins would become empty if duplicates are allowed.\n",
      "I think we could add a \"duplicate_edges\" parameter, with the following options:\n- 'raise' (default). Raise an error if duplicate bin edges are found.\n- 'drop'. Delete duplicate edges, adding the `bins = np.unique(bins)` line as in https://github.com/pydata/pandas/issues/7751#issue-37814702. This would result in less bins than specified, and some larger (with more elements) than others.\n- 'unique'. Perform the binning based on the unique values present in the input array. This is essentialy the first solution in http://stackoverflow.com/questions/20158597/how-to-qcut-with-non-unique-bin-edges and would imply unevenly sized bins, without altering the total number of bins.\n\nDoes it look reasonable? What do you think?\n",
      "dukebody's suggestion looks good to me.\n\nFor my use case, I would prefer for qcut to just return the (non-unique) bin list, and let me handle it. \n",
      "hi guys let me jump in and push for that to happen as well.  Solutions already exist in many other languages, its just a matter of choosing one of them.  See also https://stackoverflow.com/questions/38594277/how-to-compute-quantile-bins-in-pandas-when-there-are-ties-in-the-data/38596578?noredirect=1#comment64582414_38596578\n",
      "see for instance here for a possible solution adopted in SAS\n\nhttps://support.sas.com/documentation/cdl/en/proc/61895/HTML/default/viewer.htm#a000146840.htm\n",
      "@randomgambit its not about a soln. This is quite straightforward. Its about someone actually implementing it. There are many many many issues. If someone wants to submit a PR that moves things WAY faster.\n",
      "i get it jeff, no worries. unfortunately i dont have the python skills to help you and get the job done. I can only give hints for the good samaritan that wants to improve the current work in this direction\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "labeled",
      "milestoned",
      "labeled",
      "cross-referenced",
      "cross-referenced",
      "labeled",
      "referenced",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 33,
    "deletions": 6,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/tools/tests/test_tile.py",
      "pandas/tools/tile.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 7758,
    "reporter": "fulmicoton",
    "created_at": "2014-07-15T12:21:22+00:00",
    "closed_at": "2014-07-16T11:57:13+00:00",
    "resolver": "fulmicoton",
    "resolved_in": "a797b28c87d90a439dfa2c12b4a11e62bf0d6db2",
    "resolver_commit_num": 0,
    "title": "astype(unicode) does not work as expected",
    "body": "astype unicode seems to call str, so that the following code throws\n\n\n\nraises :\n\n\n",
    "labels": [
      "Unicode",
      "Dtypes",
      "Enhancement",
      "Bug"
    ],
    "comments": [
      "you can do: `df['somecol'].values.astype('unicode')`\n\nwhat are you doing with this?\n\npandas keeps all string-likes as `object` dtype so this is really only for external usage\n",
      "I have a method that detects whether a column should be considered as a category based on its type and cardinality. Columns that are considered as categories are casted into unicode object.\n\nI know how to workaround this issue, but I thought I should report what I thought was a bug.\n\nLet me know if you need more information.\n",
      "ok, this could be more informative, but its fundamentally an issue. This would return a numpy array (and NOT a series, and that would simply recast, and lose the cast to unicode).\n\nI think that is a bit odd though. What do you think should happen?\n",
      "Ideally, I would have either wanted the cast to work as python unicode() function.\nThat is : returned object are always of the \"unicode\" type.\n- Unicode objects are left unchanged.\n- Numbers are stringified into unicode strings.\n- str object are decoded using the default encoding and a unicode object is returned.\n\nDoes that make sense in Pandas?\n",
      "@fulmicoton Why do you need to convert to unicode? Do you have things that are convertible to unicode but aren't already converted? Can you give a more detailed example that illustrates why you need to do this. I think I'm just missing something. \n",
      "This could all be done I think (may need to allow an `encoding` argument for your 3rd bullet.\nKeep in mind that current pandas does not have a unicode type per-se (str and unicode are stored as `object` dtype), but its really not a big deal, as when a `unicode` dtype is presented it can simply be inferred.\n\nhere's a picture of the internal structure:\n\n```\nIn [16]: df\nOut[16]: \n  somecol\n0      \u9069\u5f53\n\nIn [17]: df._data\nOut[17]: \nBlockManager\nItems: Index([u'somecol'], dtype='object')\nAxis 1: Int64Index([0], dtype='int64')\nObjectBlock: slice(0, 1, 1), 1 x 1, dtype: object\n\nIn [18]: df._data.blocks[0]\nOut[18]: ObjectBlock: slice(0, 1, 1), 1 x 1, dtype: object\n\nIn [19]: df._data.blocks[0].values\nOut[19]: array([[u'\\u9069\\u5f53']], dtype=object)\n\nIn [20]: pd.lib.infer_dtype(df._data.blocks[0].values)\nOut[20]: 'unicode'\n```\n",
      "@fulmicoton interested in doing a pull-request for this?\n",
      "@cpcloud Just having a piece of code trying to coerce a bunch of columns marked as categorical into unicode strings. Some of them are already unicode, some of them have been detected as int but have such a low cardinality I want to handle them as categories.\nThey are getting dummified after... So it's important they all end up as unicode string at one point or another.\n",
      "@jreback I'll take a look at that tonight.\n",
      "@fulmicoton you might wasn to explore this as well (just merged in): http://pandas-docs.github.io/pandas-docs-travis/categorical.html. Prob not a lot of tests for unicode (but it should work)\n",
      "Here is the pull requests. I didn't have to use infer_dtype, so I hope I didn't do anything wrong.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "commented",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 44,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/v0.15.0.txt",
      "pandas/core/common.py",
      "pandas/lib.pyx",
      "pandas/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 7773,
    "reporter": "Nodd",
    "created_at": "2014-07-17T09:34:07+00:00",
    "closed_at": "2016-04-18T15:51:26+00:00",
    "resolver": "gfyoung",
    "resolved_in": "7f678ca3c62d591541d21c6d857dc52bda0fb72a",
    "resolver_commit_num": 9,
    "title": "BUG: sniffing a csv raises with only a header",
    "body": "I'm trying to read a bunch of csv files, but some of them have a header line without any data. I expect to get empty data but Pandas raises an exception on these files:\n\n\n\nI used this command: `pd.read_csv(fichier_data, sep=None, index_col=False, engine='python')`\nThe file content is a single line:\n\n\n\n\n",
    "labels": [
      "CSV",
      "Bug"
    ],
    "comments": [
      "you are specifying a separator of `None`, using `sep=';'` seems to work, no? (I have an extra column as I have a trailing ;). The engine doesn't matter.\n\n```\nIn [4]: pd.read_csv(StringIO(data), sep=';', index_col=False)\nOut[4]: \nEmpty DataFrame\nColumns: [TEMPS, MODE, STATUS, LATITUDE, LONGITUDE, ALTITUDE, VNORD, VEST, VHAUT, ROLL, PITCH, YAW, DELTA YAW, RATEX, RATEY, RATEZ, ACCX, ACCY, ACCZ, GISEMENT, SITE, CONAZ, CONEL, POSAZ, POSEL, LATCIBLE, LONGICIBLE, ALTICIBLE, DISTANCE, Unnamed: 29]\nIndex: []\n```\n",
      "I set the engine to remove a warning.\n\nI'll try to set the separator, but I expected pandas to guess it.\n",
      "well the warning is telling u that u are not doing the right thing\nu need to specify a sep\nnot trivial to sniff (though it can be done) just not recommended\n",
      "this might be a sniffing error (as it in theory _should_ work).\n",
      "Hello, we are facing the same bug with pandas 0.15.2, simple way to reproduce:\n\n```\n>pandas.io.parsers.read_csv(StringIO(\"a,b,c\"), index_col=False, engine=\"python\")\nTypeError: object of type 'bool' has no len()\n>pandas.io.parsers.read_csv(StringIO(\"a,b,c\"), index_col=False, engine=\"c\")\nEmpty DataFrame\nColumns: [a, b, c]\nIndex: []\n```\n\nnow we patched _get_empty_meta function in pandas.io.parsers to treat False and None the same way, but I wonder if this is the right solution.\n",
      "@jreback : I think this issue can be closed now.  I can't reproduce these bugs in `0.18.0`.\n",
      "ok, can you put up a validation test (if you can't find any in the codebase).\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "labeled",
      "milestoned",
      "labeled",
      "commented",
      "milestoned",
      "demilestoned",
      "renamed",
      "commented",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "milestoned",
      "demilestoned",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 1,
    "additions": 10,
    "deletions": 0,
    "changed_files_list": [
      "pandas/io/tests/test_parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 7872,
    "reporter": "hayd",
    "created_at": "2014-07-29T19:36:57+00:00",
    "closed_at": "2015-04-14T14:19:43+00:00",
    "resolver": "hoh",
    "resolved_in": "8fe1cf6d1931071c378fcc3170b82a145a037da0",
    "resolver_commit_num": 0,
    "title": "Give all NotImplementedErrors a description",
    "body": "Currently some NotImplementedErrors don't have a description (most do!) .\n\n\n\nI think this would be pretty useful, and makes it less confusing for users, and at least they would be pointed in the right direction to request said feature (and know which bit of specifically is NotImplemented)...\n\nYou can find them all with grep (probably more usefully the same regex in your text editor):\n\n\n",
    "labels": [
      "Good as first PR",
      "Error Reporting",
      "PEP8",
      "Docs",
      "Effort Medium"
    ],
    "comments": [
      "Fix in #7899 .\n\nI just noticed the PEP8 label. \n`pep8 | wc -l` gave me 17687 pep8 errors. \n",
      "no pepping\n",
      "Starting to work on this\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "cross-referenced",
      "commented",
      "commented",
      "milestoned",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "labeled",
      "commented",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 20,
    "additions": 108,
    "deletions": 72,
    "changed_files_list": [
      "pandas/core/base.py",
      "pandas/core/categorical.py",
      "pandas/core/common.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/internals.py",
      "pandas/core/panelnd.py",
      "pandas/core/series.py",
      "pandas/io/html.py",
      "pandas/io/json.py",
      "pandas/io/parsers.py",
      "pandas/io/sql.py",
      "pandas/sparse/frame.py",
      "pandas/sparse/panel.py",
      "pandas/src/generate_code.py",
      "pandas/tools/plotting.py",
      "pandas/tseries/base.py",
      "pandas/tseries/index.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tdi.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 8020,
    "reporter": "cpcloud",
    "created_at": "2014-08-13T16:10:22+00:00",
    "closed_at": "2016-02-27T14:46:28+00:00",
    "resolver": "jylin",
    "resolved_in": "c81d03b0bd98f0e946b4cc03b23da39b8949cedd",
    "resolver_commit_num": 0,
    "title": "Make empty data construction error message less dev-y",
    "body": "When one tries to construct a `DataFrame` where the data column count and the passed in column count differ the error message is completely unintuitive unless you know something about the internal representation of the data inside of `NDFrame`s.\n\nI think we should at the very least make the dimensions in the error message consistent with how the frame _looks_ rather than how it's _implemented_.\n\nthis code:\n\n\n\nraises this exception:\n\n\n\nsee here for another example: \n",
    "labels": [
      "Error Reporting",
      "Good as first PR"
    ],
    "comments": [
      "sure\n"
    ],
    "events": [
      "labeled",
      "milestoned",
      "commented",
      "assigned",
      "milestoned",
      "demilestoned",
      "labeled",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 9,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/frame.py",
      "pandas/core/internals.py",
      "pandas/tests/frame/test_constructors.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 8158,
    "reporter": "jorisvandenbossche",
    "created_at": "2014-09-02T10:00:44+00:00",
    "closed_at": "2016-04-26T15:17:03+00:00",
    "resolver": "jreback",
    "resolved_in": "59082e9b26414bfada9f05967b88f759639749ff",
    "resolver_commit_num": 3994,
    "title": "ENH: convert datetime components (year, month, day, ...) in different columns to datetime",
    "body": "from [SO](-construction-of-datetimeindex-in-pandas)\n\nI didn't find an issue about this, but it has come up some times at stackoverflow: having columns with integers for year, month, day, hour, ..., how do you convert this to a datetime column/index ?\n\n-to-convert-columns-into-one-datetime-column-in-pandas\n\nYou have the typical solution of adding the columns: `pd.to_datetime((df['Y']*10000 + df['M']*100 + df['D']).astype('int'), format='%Y%m%d')`, and @unutbu added now a faster solution using numpy's different datetime64 resolutions to that question on SO.\n\nI personally think this would be a nice addition to pandas to have a more native solution for this. But then we need to figure out a nice API. Or we keep it as is, but try to document it more (add as example to docs?)\n",
    "labels": [
      "API Design",
      "Timeseries",
      "Enhancement",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "I think we tried to doc this some where (or cookbook - don't remember offhand)\n\nhow about\n\npd.to_datetime(DataFrame, \n  format={'Y' : column_year,\n                'm' : column_month,\n                'd' : column_day})\n\nformat could also be list of these columns I guess as well (this is more powerful though; a list would have to be unambiguously Ymd)\n",
      "How about adding a helper function like\n\n```\ndef combine64(years, months=1, days=1, weeks=None, hours=None, minutes=None,\n              seconds=None, milliseconds=None, microseconds=None, nanoseconds=None):\n    years = np.asarray(years) - 1970\n    months = np.asarray(months) - 1\n    days = np.asarray(days) - 1\n    types = ('<M8[Y]', '<m8[M]', '<m8[D]', '<m8[W]', '<m8[h]',\n             '<m8[m]', '<m8[s]', '<m8[ms]', '<m8[us]', '<m8[ns]')\n    vals = (years, months, days, weeks, hours, minutes, seconds,\n            milliseconds, microseconds, nanoseconds)\n    return sum(np.asarray(v, dtype=t) for t, v in zip(types, vals)\n               if v is not None)\n```\n\nIt would be easier than adding the functionality to `pd.to_datetime`, and people could apply it wherever they wish. Note that `combine64` returns a NumPy array of dtype `datetime64[*]`. It can be passed to `pd.to_datetime` to create a `DatetimeIndex`:\n\n```\nIn [196]: combine64(df['Y'], df['M'])\nOut[196]: array(['1990-01-01', '1991-02-01', '1992-03-01', '1993-04-01'], dtype='datetime64[D]')\n\nIn [197]: pd.to_datetime(combine64(df['Y'], df['M']))\nOut[197]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[1990-01-01, ..., 1993-04-01]\nLength: 4, Freq: None, Timezone: None\n```\n",
      "I like @unutbu's `combine64` helper function. It seems cleaner than shoving support for format dictionaries into `to_datetime`.\n\nA couple of thoughts:\n1. The name `combine64` is too ambiguous. How about `combine_datetime` instead?\n2. I think it should return a `DatetimeIndex` directly, since this is the only fully functional datetime array we have access to in pandas.\n",
      "@unutbu want to do a PR for this?\n"
    ],
    "events": [
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 452,
    "deletions": 164,
    "changed_files_list": [
      "doc/source/timeseries.rst",
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tools.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 8287,
    "reporter": "TomAugspurger",
    "created_at": "2014-09-17T02:39:48+00:00",
    "closed_at": "2016-05-24T15:28:17+00:00",
    "resolver": "jreback",
    "resolved_in": "69ad08b0273ddc3d117690bd8746eecec6ab29ac",
    "resolver_commit_num": 4032,
    "title": "BUG: HDFStore.select ignores start and stop parameters",
    "body": "Or I'm doing this wrong\n\n\n\nDocstring says:\n\n\n\nI can look tomorrow (catching up on pandas stuff all day tomorrow hopefully)\n",
    "labels": [
      "HDF5",
      "Docs",
      "Bug"
    ],
    "comments": [
      "that's a fixed store\n\nonly table implements these \n\nthey could be for a fixed store (though inefficient I think)\n",
      "OK thanks. I'll add a note to the docstring.\n",
      "actually it should work - maybe raise NotImplemented error for now \n\nhttp://pytables.github.io/usersguide/libref/homogenous_storage.html#the-array-class\n",
      "here's partway to an implementation of this: http://stackoverflow.com/questions/29069592/read-n-rows-from-an-h5-file/29089216#29089216\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "commented",
      "commented",
      "unlabeled",
      "labeled",
      "commented",
      "cross-referenced",
      "milestoned",
      "labeled",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "cross-referenced",
      "referenced",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 132,
    "deletions": 44,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/io/pytables.py",
      "pandas/io/tests/test_pytables.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 8376,
    "reporter": "jankatins",
    "created_at": "2014-09-23T21:54:53+00:00",
    "closed_at": "2016-07-15T10:28:35+00:00",
    "resolver": "gfyoung",
    "resolved_in": "d7c028d4965932160fa3b69f56c716b1454c42a5",
    "resolver_commit_num": 46,
    "title": "Deprecations of levels in categorical in 2017/ after 0.18",
    "body": "`levels` was renamed to `categories`. Take out the deprecated property, the proerty in the Series.cat accessor, and the attribute in the constructor\n\nLook for `TODO: Remove after deprecation period in 2017/ after 0.18` and `TODO: remove levels after the deprecation period`.\n\nShould be added to #6581\n",
    "labels": [
      "Deprecate",
      "Categorical"
    ],
    "comments": [
      "@JanSchulz what's going to happen to functions like `reorder_levels` and `remove_unused_levels`? Are those going to be replaced as well?  I was inspired by this to go change all the references to `levels` in Categorical docs to `categories`, and then noticed these other functions.\n",
      "additionally, `reorder_levels` has a different meaning for `Series` and `DataFrame`\n",
      "this is already fixed (just not merged yet): https://github.com/pydata/pandas/pull/8153\n",
      "@jtratner `reorder_levels` is not the method anyhow, its `s.cat.reorder_levels()` (now `s.cat.reorder_categories()`) doesn't act on the Series OR DataFrame, rather on an embedded Categorical\n",
      "The `<whatever_levels>` methods in Categorical and `Series.cat.<whatever>_levels` were never public API in a stable release, so don't need a deprecation cycle.\n",
      "I was just behind the times here :)\n\nOn Wed, Sep 24, 2014 at 1:53 PM, Jan Schulz notifications@github.com\nwrote:\n\n> The <whatever_levels> methods in Categorical and\n> Series.cat.<whatever>_levels were never public API, so don't need a\n> deprecation cycle.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/pydata/pandas/issues/8376#issuecomment-56736920.\n"
    ],
    "events": [
      "labeled",
      "cross-referenced",
      "milestoned",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 7,
    "additions": 43,
    "deletions": 80,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/categorical.py",
      "pandas/io/tests/data/categorical_0_14_1.pickle",
      "pandas/io/tests/data/categorical_0_15_2.pickle",
      "pandas/io/tests/test_pickle.py",
      "pandas/tests/test_categorical.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 8397,
    "reporter": "TomAugspurger",
    "created_at": "2014-09-26T14:59:06+00:00",
    "closed_at": "2017-03-23T13:46:36+00:00",
    "resolver": "jreback",
    "resolved_in": "76cb401597f5b51916d143e3386f5aea04f8fd4a",
    "resolver_commit_num": 3888,
    "title": "TST: Catch Warnings in tests",
    "body": "If only to have nicer looking test output.\n\n\n",
    "labels": [
      "Testing"
    ],
    "comments": [
      "yep, these should be explicity tested/caught. Maybe add checkboxes to list them out ?\n",
      "I'm doing graphics right now. Will tick them off by module as I go.\n",
      "Isn't there some way to make all uncaught warnings failures?\n",
      "`warnings.simplefilter('error',Warning)` I think works (to sort of figure this out)\n\nSort of do this here: (this is for the experimental builds): https://github.com/pydata/pandas/blob/master/pandas/util/testing.py#L62\n",
      "@jreback I just found out that you can do that and came here to post it :)\n\nI'll get back to this soon. It'll be so much easier now that it raises.\n",
      "this doesn't fix the slow tests :<\n",
      "closing this, i think we are better at this."
    ],
    "events": [
      "commented",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "closed",
      "milestoned",
      "demilestoned",
      "reopened",
      "commented",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 25,
    "deletions": 7,
    "changed_files_list": [
      "pandas/tests/frame/test_indexing.py",
      "pandas/tests/series/test_operators.py",
      "pandas/tests/test_graphics_others.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 8542,
    "reporter": "jgbarah",
    "created_at": "2014-10-11T22:16:59+00:00",
    "closed_at": "2016-04-29T13:40:30+00:00",
    "resolver": "sinhrks",
    "resolved_in": "b56cea248f6f3cbcdd18d98c4a8614e6639a43ef",
    "resolver_commit_num": 294,
    "title": "groupby, TimeGrouper error",
    "body": "The following code seems to raise an error, since the result object does not make sense (well, at least to me):\n\n\n\nI would expect, for Out[68], two groups, one for each (date, event) pair.\n\nAm I wring, or this is a bug?\n",
    "labels": [
      "Bug",
      "Resample"
    ],
    "comments": [
      "show pd.show_versions()\n",
      "```\nIn [13]: pd.show_versions()\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.8.final.0\npython-bits: 32\nOS: Linux\nOS-release: 3.14-2-686-pae\nmachine: i686\nprocessor: \nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.14.1\nnose: 1.3.4\nCython: None\nnumpy: 1.9.0\nscipy: 0.14.0\nstatsmodels: 0.5.0\nIPython: 2.3.0\nsphinx: 1.2.3\npatsy: 0.3.0\nscikits.timeseries: None\ndateutil: 2.2\npytz: 2014.7\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.4.0\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: 0.999\nhttplib2: 0.9\napiclient: None\nrpy2: None\nsqlalchemy: 0.9.7\npymysql: None\npsycopg2: None\n'''\n```\n",
      "BTW, with three elements in the dataframe, it seems to work:\n\n```\nIn [8]: df = pd.DataFrame.from_records ( [[datetime.datetime(2014,9,10),1234,\"start\"], [datetime.datetime(2014,10,10),1238,\"start\"], [datetime.datetime(2014,12,10),1564,\"start\"]], columns = [\"date\", \"change\", \"event\"] )\n\nIn [9]: ts = df.set_index('date')\n\nIn [10]: byperiod = ts.groupby([pd.TimeGrouper(freq=\"M\"), \"event\"], as_index=False)\n\nIn [11]: byperiod.groups\nOut[11]: \n{(Timestamp('2014-09-30 00:00:00'),\n  'start'): [Timestamp('2014-09-10 00:00:00')],\n (Timestamp('2014-10-31 00:00:00'),\n  'start'): [Timestamp('2014-10-10 00:00:00')],\n (Timestamp('2014-12-31 00:00:00'),\n  'start'): [Timestamp('2014-12-10 00:00:00')]}\n'''\n```\n",
      "the prob with 2 elements is that the frequency is not defined for the timeseries \n\nso maybe not behaving properly\n\nwill mark as a bug\nfeel free to have a look !\n",
      "Thanks! I'm not familiar with the internals of pandas, but I'll try.\n",
      "hey @jreback I found that check on the code that drives that behavior: https://github.com/pydata/pandas/blob/master/pandas/core/groupby.py#L2077\n\nThe `match_axis_length` will be `True` because size of groups and keys are equal, thus this code will be executed:\n\n``` python\nif (not any_callable and not all_in_columns\n        and not any_arraylike and match_axis_length\n            and level is None):\n    keys = [com._asarray_tuplesafe(keys)]\n```\n\nThe thing is even if I remove `match_axis_length` from the above condition, all others checks will evaluate True on my case, and then `keys = [com._asarray_tuplesafe(keys)]` will execute anyway.\n\nDo you have any ideas on how to relax/remove the `match_axis_length` and still keep the other cases?\n",
      "I think a way to make this work is to relax the freqency inference engine to only require 2 dates (though this may break lots of other things, not sure).\n\n```\nIn [10]: pd.DatetimeIndex(df['date'])\nOut[10]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2014-09-10, 2013-10-10]\nLength: 2, Freq: None, Timezone: None\n```\n",
      "@victorpoluceno did you figure out a solution?\n",
      "This looks to be fixed by current master. Adding tests and close.\n\n```\nbyperiod.groups\n# {(Timestamp('2013-10-31 00:00:00'), 'start'): [Timestamp('2013-10-10 00:00:00')],\n#  (Timestamp('2014-09-30 00:00:00'), 'start'): [Timestamp('2014-09-10 00:00:00')]}\n```\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 1,
    "additions": 51,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 8573,
    "reporter": "ginzor",
    "created_at": "2014-10-17T13:33:26+00:00",
    "closed_at": "2016-04-19T12:03:05+00:00",
    "resolver": "jreback",
    "resolved_in": "2267bd32c3b4e6cdf50ff52306fc09d5f9a279a1",
    "resolver_commit_num": 3981,
    "title": "Using resample() with groupby on this DataFrame causes Segmentation Fault",
    "body": "When trying to resample timestamps into 5 minute time slots grouping on an id column (tried both counting and summing aggregation in 'how' parameter). In a DataFrame with TimeSeries data I get a memory crash, i.e. Segmentation Fault.\n\nI reduced the DataFrame as far as I could in reproducing the crash. Also noted that it will not cause a segfault if I sort the index (don't know if this is needed for resample() function could not find such documentation).\n\n\n\nTried on following setups of pandas.\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.8.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.16-2-amd64\nmachine: x86_64\nprocessor: \nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.14.1\nnose: 1.3.4\nCython: None\nnumpy: 1.9.0\nscipy: None\nstatsmodels: None\nIPython: 2.3.0\nsphinx: None\npatsy: None\nscikits.timeseries: None\ndateutil: 2.2\npytz: 2014.7\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.0.1\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: 0.999\n None\napiclient: None\nrpy2: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.8.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.16-2-amd64\nmachine: x86_64\nprocessor: \nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.13.1\nCython: None\nnumpy: 1.8.0\nscipy: None\nstatsmodels: None\nIPython: 2.3.0\nsphinx: None\npatsy: None\nscikits.timeseries: None\ndateutil: 1.5\npytz: 2012c\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.0.1\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nsqlalchemy: None\nlxml: None\nbs4: None\nhtml5lib: 0.999\nbq: None\napiclient: None\n",
    "labels": [
      "Resample",
      "Groupby",
      "Bug",
      "Testing",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "pretty sure this is fixed in 0.15rc1\n\nsee pandas.pydata.org\n",
      "The syntax you are using is not supported, and should raise and error, use the following.\n\n```\nIn [8]: df.groupby(['ID',pd.Grouper(freq='5min')]).sum()\nOut[8]: \n                        A  B\nID timestamp                \n1  2013-10-01 16:20:00  1  0\n2  2013-10-01 16:10:00  2  0\n   2013-10-01 18:15:00  1  0\n```\n",
      "hmm, I see it 'sort of' works if you sort, ok. will make this a bug then\n",
      "this appears good in master, just needs validation tests\n"
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "labeled",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "labeled",
      "commented",
      "labeled",
      "labeled",
      "unlabeled",
      "unlabeled",
      "referenced",
      "referenced"
    ],
    "changed_files": 1,
    "additions": 26,
    "deletions": 1,
    "changed_files_list": [
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 8596,
    "reporter": "miketkelly",
    "created_at": "2014-10-21T18:38:30+00:00",
    "closed_at": "2016-05-27T12:41:32+00:00",
    "resolver": "jreback",
    "resolved_in": "e8d9e79fc7d0a31e8c37c82f1e48d51cce59e9e0",
    "resolver_commit_num": 4037,
    "title": "BUG: DataFrame outer merge changes key columns from int64 to float64",
    "body": "\n\nWas expecting `key` to stay int64, since a merge can't introduce missing key values if none were present in the inputs.\n\n\n\nVersion 0.15.0-6-g403f38d\n",
    "labels": [
      "Bug",
      "Dtypes",
      "Reshaping",
      "Effort Medium",
      "Difficulty Advanced"
    ],
    "comments": [
      "hmm, this should work. dig in if you can.\n",
      "~~I remember paying special attention to these cases when doing blockmanager refactoring, may be related to categorical revamp in 0.15.~~\n\nEDIT: no, it's not. `concatenate_block_managers` gets the following:\n\n```\n[(BlockManager\n  Items: Index([u'key', u'val1'], dtype='object')\n  Axis 1: Int64Index([0, 1, 2, 3], dtype='int64')\n  IntBlock: slice(0, 2, 1), 2 x 4, dtype: int64,\n  {1: array([ 0,  1,  2,  3, -1])}),\n (BlockManager\n  Items: Index([u'val2'], dtype='object')\n  Axis 1: Int64Index([0, 1, 2, 3], dtype='int64')\n  IntBlock: slice(0, 1, 1), 1 x 4, dtype: int64,\n  {1: array([ 0,  1,  2, -1,  3])})]\n```\n\nwhich means that frames are merged along axis=1, but this operation requires adding rows to both frames which introduces NaNs and converts integral columns. We should probably concatenate  columns present in both frames along axis=0 (and reorder resulting rows as necessary) beforehand in `merge._MergeOperation`.\n",
      "I think it makes sense post-merge (and can do in `concatenate_join_unit`), something like this: https://github.com/pydata/pandas/blob/master/pandas/core/groupby.py#L1063 (which is cheap unless it actually changes the dtype), though maybe have to split out the results\n",
      "The missing values aren't filled in until `_maybe_add_join_keys`, so I made the change there: 90ef7dfc528fffd4d71a5c544864891a44c2d096. One test failed initially, but it just seemed to be working around the issue. For now I commented out the workaround.\n\nA quick look at performance didn't show anything bad:\n\n```\n-------------------------------------------------------------------------------\nTest name                                    | head[ms] | base[ms] |  ratio   |\n-------------------------------------------------------------------------------\njoin_non_unique_equal                        |   0.6567 |   0.6644 |   0.9884 |\njoin_dataframe_index_multi                   |  15.1957 |  15.3310 |   0.9912 |\njoin_dataframe_index_single_key_bigger_sort  |  11.3726 |  11.4484 |   0.9934 |\nstrings_join_split                           |  32.7196 |  32.6459 |   1.0023 |\nmerge_2intkey_sort                           |  28.4477 |  28.3287 |   1.0042 |\nmerge_2intkey_nosort                         |  11.7714 |  11.6763 |   1.0081 |\njoin_dataframe_integer_2key                  |   3.6847 |   3.6399 |   1.0123 |\nleft_outer_join_index                        | 2123.6037 | 2092.0046 |   1.0151 |\njoin_dataframe_index_single_key_small        |   9.6610 |   9.4910 |   1.0179 |\njoin_dataframe_index_single_key_bigger       |  10.1710 |   9.8880 |   1.0286 |\njoin_dataframe_integer_key                   |   1.4577 |   1.4137 |   1.0311 |\n-------------------------------------------------------------------------------\nTest name                                    | head[ms] | base[ms] |  ratio   |\n-------------------------------------------------------------------------------\n```\n\nIf you think the approach is sound, I'll add a few more tests (e.g. for multi-key joins), make sure we have a vbench test that exercises the new code, and tidy things up a bit\n",
      "You may lose lower digits of int64 numbers > 2**53 when converting int64 -> float64 -> int64:\n\n``` python\nIn [62]: np.float64((1<<53))\nOut[62]: 9007199254740992.0\n\nIn [63]: np.float64((1<<53)) + 1.\nOut[63]: 9007199254740992.0\n\nIn [64]: df2 = pd.DataFrame({'k': (1<<53) + np.delete(np.arange(5), -2), 'v2': np.arange(4)}); df2\nOut[64]: \n                  k  v2\n0  9007199254740992   0\n1  9007199254740993   1\n2  9007199254740994   2\n3  9007199254740996   3\n\nIn [65]: df1 = pd.DataFrame({'k': (1<<53) + np.delete(np.arange(5), -1), 'v1': np.arange(4)}); df1\nOut[65]: \n                  k  v1\n0  9007199254740992   0\n1  9007199254740993   1\n2  9007199254740994   2\n3  9007199254740995   3\n\nIn [66]: m = df1.merge(df2, how='outer'); m\nOut[66]: \n              k  v1  v2\n0  9.007199e+15   0   0\n1  9.007199e+15   1   1\n2  9.007199e+15   2   2\n3  9.007199e+15   3 NaN\n4  9.007199e+15 NaN   3\n\nIn [67]: m['k'].astype(np.int64)\nOut[67]: \n0    9007199254740992\n1    9007199254740992\n2    9007199254740994\n3    9007199254740996\n4    9007199254740996\nName: k, dtype: int64\n```\n\nIt _is_ a far-fetched example, but I think this should be fixed (at least someday).\n",
      "yeh I recall talking about on an issue a while back\nI think the problem with the int-float-int comversion was that it's not easy to figure out that it is going to fail because of precision - well it's easy just not performant for the very tiny amounts of times this would happen\n\nwhat we need is a quick test when this type of conversion is wrong (then maybe raise/warn) - maybe some sort of bit shift and sum or something\n",
      "Here's an implementation of `_maybe_add_join_keys` that also solves the int->float->int problems: https://github.com/mtkni/pandas/commit/e79b97815d3ec884261674e91af2c0a138debead. The performance seems comparable to the previous implementation (no material vbench differences), but that needs more study.\n",
      "@miketkelly want to do a PR on the branch above? see how it does for passing travis. (I haven't really looked thru it yet though). If you can do this in next few days can squeeze this into 0.15.2. Pls also post a perf summary (well show if their is any change).\n",
      "Bump.  I'm experiencing this when doing outer joins as well.\n\nDoes anyone have a work-around for the time being?  E.g., an easy way to either convert the data types back or to prevent it from happening in the first place?  I don't want to end up in a position where going from `int64`-> `float64` causes any issues.\n",
      "no easy way around this. you can simply convert the dtypes for the keys after.\n",
      "Presently having an issue with this. The loss of precision from int64->float64 is problematic for me as I'm working with data that is stored as an int64 type. \n\nI'm not sure if converting the dtypes after the merge operation will work if precision is lost..., if I'm understanding @jreback 's solution correctly.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "unlabeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 14,
    "additions": 464,
    "deletions": 75,
    "changed_files_list": [
      "doc/source/api.rst",
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/indexes/base.py",
      "pandas/indexes/category.py",
      "pandas/indexes/multi.py",
      "pandas/tests/indexes/common.py",
      "pandas/tests/indexes/test_category.py",
      "pandas/tests/indexes/test_datetimelike.py",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tests/types/test_types.py",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tseries/base.py",
      "pandas/types/api.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 8625,
    "reporter": "jreback",
    "created_at": "2014-10-24T18:42:12+00:00",
    "closed_at": "2017-04-14T13:32:37+00:00",
    "resolver": "jreback",
    "resolved_in": "9991579c812e5a7c977e69f03b390adf7974445f",
    "resolver_commit_num": 4376,
    "title": "API/ENH: create Interval class",
    "body": "xref #8595\n\nsomething like: `Interval(left=(-1.5,'open),right=(0,'closed'))`\n\nreprs to `(-1.5,0]`\n\ncan be used immediately in the index values for `pd.cut` and such (as an object index)\nevenutally can form the basis for `IntervalIndex`\n\ncc @shoyer\ncc @JanSchulz\ncc @rosnfeld\n",
    "labels": [
      "Enhancement",
      "API Design",
      "Categorical",
      "Interval"
    ],
    "comments": [
      "Sounds good to me. As a reminder: here is the `IntervalIndex` proposal: #7640\n\nLet's consider whether you should be able to control whether the level and right margins are closed/open separately or if you should only be able to say `closed='left'` or `closed='right'`. I am inclined to go for the later since it will be more complex for other cases in `IntervalIndex`.\n",
      "@shoyer\n\nyou could also do\n\n`Interval(left,right,closed='left|right|both|neither')` I guess for full compat (and could have both constructors I guess)\n",
      "Idea: `def __init__(leftside='[',leftnum=0,rightnum=0,rightside=']')`\n\nThen...\n\n`Interval('[',left,right,']') == Interval('closed',left,right,'closed') == Interval(leftnum=left,rightnum=right)`\n\ncould subclass and create `left(leftside,leftnum)` and `right(rightnum, rightside)`, so that `left('(',-1) + right(2,']') == Interval('(',-1,2,']')`\n\n...but individually they would repr to `(-1,inf]` and `[-inf,2]`\n",
      "One follow up thought: if we support `closed='both'` and `closed='neither'` we don't want the want the repr to look like `[-1.5, 0]` or `(-1.5, 0)`. Probably better to repr to the more verbose `Interval(-1.5, 0, closed='both')` for these cases.\n",
      "Why this verbose repr? In repr of a dataframe (as index or values), I would expect that I only get the \"short\" repr...\n",
      "The problem is ambiguity with list and tuple, at least as long as the array has dtype=object.\n\nOn Tue, Oct 28, 2014 at 2:56 AM, Jan Schulz notifications@github.com\nwrote:\n\n> ## Why this verbose repr? In repr of a dataframe (as index or values), I would expect that I only get the \"short\" repr...\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/pydata/pandas/issues/8625#issuecomment-60731926\n",
      "Could clear up the ambiguity with dash ( or even double dash ).  Nobody should confuse \"(0 - 2)\" with a tuple.  Nor would \"[0 - 2]\" be confused with a list.\n",
      "Rough draft implementation up for review in #8707.\n\nTurns out `closed='neither'` and `closed='both'` are probably not harder cases for `IntervalIndex` than `'left'`/`'right'`. But I still like the constructor `Interval(left, right, closed='right')`.\n\nWe could also have a `to_interval` function like `to_datetime` that parses strings, e.g., `to_interval('(0, 1]')` -> `Interval(0, 1, closed='right')` (lists of strings would turn into `IntervalIndex`).\n",
      "+1 for `to_interval('(0,1]')`\n",
      "Here's another design question: how should comparison operations work with intervals?\n\nMy initial thought was to support all comparisons operations, but when I attempted to write it, I realized that it's not obvious to me what the result of `0.5 < Interval(0, 1)` should be (it's even less clear when both values are intervals). \n\nSo, my current proposal is that we _do not_ actually want to support support most comparisons with intervals. Instead, we should encourage users to write things like `0.5 < interval.left`. The only comparison like operations we want to define are `__eq__`/`__ne__`, for comparing two intervals objects (e.g., `Interval(0, 1) == Interval(0, 1)`), and `__contains__`, for checking if a scalar value is contained in an interval (e.g., `0.5 in Interval(0, 1)`).\n",
      "sure\n\nthough you will need to provide an ordering in used for IntervalIndex\n\neg imagine spitting these though it's sort of trivial:\n\ni = IntervalIndex(...)\ni.take(...) equiv to IntervalIndex(_left=i._left.take(), _righti._right.take())\n",
      "So after testing out this alternative, I have now waffled back to defining comparison operations -- otherwise we can't get sorting to work, even in cases of non-overlapping intervals. I would like set operations like union to be able to sort, at least in unambiguous cases, and I don't see any other obvious way to do this.\n\n@jreback I'm not sure I understand your point about take.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "milestoned",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "unsubscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "labeled",
      "cross-referenced",
      "unlabeled",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files": 54,
    "additions": 4195,
    "deletions": 504,
    "changed_files_list": [
      "asv_bench/benchmarks/indexing.py",
      "doc/source/advanced.rst",
      "doc/source/api.rst",
      "doc/source/reshaping.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/_libs/hashtable.pyx",
      "pandas/_libs/interval.pyx",
      "pandas/_libs/intervaltree.pxi.in",
      "pandas/_libs/lib.pyx",
      "pandas/_libs/src/inference.pyx",
      "pandas/_libs/tslib.pyx",
      "pandas/core/algorithms.py",
      "pandas/core/api.py",
      "pandas/core/groupby.py",
      "pandas/core/indexing.py",
      "pandas/formats/format.py",
      "pandas/indexes/api.py",
      "pandas/indexes/base.py",
      "pandas/indexes/category.py",
      "pandas/indexes/interval.py",
      "pandas/indexes/multi.py",
      "pandas/tests/api/test_api.py",
      "pandas/tests/api/test_types.py",
      "pandas/tests/frame/test_alter_axes.py",
      "pandas/tests/frame/test_sorting.py",
      "pandas/tests/groupby/test_categorical.py",
      "pandas/tests/groupby/test_groupby.py",
      "pandas/tests/indexes/common.py",
      "pandas/tests/indexes/test_base.py",
      "pandas/tests/indexes/test_category.py",
      "pandas/tests/indexes/test_interval.py",
      "pandas/tests/indexing/test_interval.py",
      "pandas/tests/scalar/test_interval.py",
      "pandas/tests/series/test_constructors.py",
      "pandas/tests/series/test_missing.py",
      "pandas/tests/series/test_sorting.py",
      "pandas/tests/test_algos.py",
      "pandas/tests/test_base.py",
      "pandas/tests/test_categorical.py",
      "pandas/tests/tools/test_tile.py",
      "pandas/tests/types/test_dtypes.py",
      "pandas/tests/types/test_missing.py",
      "pandas/tools/tile.py",
      "pandas/tseries/base.py",
      "pandas/tseries/interval.py",
      "pandas/tseries/period.py",
      "pandas/types/api.py",
      "pandas/types/common.py",
      "pandas/types/dtypes.py",
      "pandas/types/generic.py",
      "pandas/types/inference.py",
      "pandas/types/missing.py",
      "pandas/util/testing.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 8633,
    "reporter": "fkaufer",
    "created_at": "2014-10-25T13:53:55+00:00",
    "closed_at": "2014-10-25T14:08:33+00:00",
    "resolver": "bashtage",
    "resolved_in": "204b50e5a30644179188ef06641d4c0e095c5bcf",
    "resolver_commit_num": 15,
    "title": "ENH: categorical dataexport - graceful degradation ",
    "body": "It would be great to generally apply graceful degradation for export of categorical data instead of raising exceptions.\n\nCurrently this is only the case for `to_sql` and `to_csv`, where the categories are exported, while `to_pickle` is the only option to persist categorical data\n\nFor Stata and HDF it is:\n- `to_hdf`: `NotImplementedError: cannot store a category dtype`\n- `to_stata`: `ValueError: Data type category not currently understood. Please report an error to the developers.`\n\nAs long as a backend does not support categoricals or the conversion is not yet implemented, why not generally export categories as a fallback? With the separately discussed decode method (#8628) this would be easy. If the same rigor (backend supports data type natively or fail) would be applied to CSV-IO we could only export string dtypes to CSV.\n\nThinking one step further, the `to_...` functions could have an optional parameter named something like `convert_cat` with options:\n- None: either try to export as a categorical (pickle, potentially HDF, Stata) or raise exception\n- 'category': only export categories (decode method)\n- 'code': export s.cat.codes\n- 'mapping' or 'emulate': export code:category mapping in one/two columns or separate table/frame/... with the code-category mapping. \n\nThe last option would probably need additional parameters to control the technical implementation (e.g. table name for mapping or suffixes as for join/merge, ...)\n",
    "labels": [],
    "comments": [
      "see #7621 for master issue\n\nof course appreciate user contributions to extend to these formats\nyou can simply convert to object if u really need to do this atm\n"
    ],
    "events": [
      "closed",
      "reopened",
      "cross-referenced"
    ],
    "changed_files": 4,
    "additions": 313,
    "deletions": 29,
    "changed_files_list": [
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.15.2.txt",
      "pandas/io/stata.py",
      "pandas/io/tests/test_stata.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 8711,
    "reporter": "jreback",
    "created_at": "2014-11-02T15:03:00+00:00",
    "closed_at": "2017-02-22T16:06:33+00:00",
    "resolver": "jgoppert",
    "resolved_in": "1400305899d55bee21253952de9f6f0cf245b089",
    "resolver_commit_num": 0,
    "title": "ENH/BUG: support TimedeltaIndex plotting",
    "body": "This raises\n\n\n\nThis will show the timedeltas with a formatted (albeit string index)\n\n\n\nwonder if we can just register a converter somehow? like #8614 \n",
    "labels": [
      "Bug",
      "Enhancement",
      "Visualization",
      "Timedelta"
    ],
    "comments": [
      "I don't think that matplotlib already has a converter for `datetime.timedelta`, so just registering our `Timedelta` type will not be enough. Eg `plt.plot(s.index.to_pytimedelta(), s)` also fails. \n\nBut writing a basic converter should not be that difficult I think (and if it also works for `datetime.timedelta` it could maybe also be pushed upstream to matplotlib)\n",
      "Timedelta is s. subclass of datetime.timedelta \n",
      "I just encountered a MemoryError when attempting to plot a TimedeltaIndex!\n\n```\npd.Series(range(15), pd.timedelta_range(0, freq='D', periods=15)).plot()\n```\n\n```\n---------------------------------------------------------------------------\nMemoryError                               Traceback (most recent call last)\n<ipython-input-113-e9a2d53dcace> in <module>()\n----> 1 pd.Series(range(15), pd.timedelta_range(0, freq='H', periods=15)).plot()\n\n/Users/shoyer/dev/pandas/pandas/tools/plotting.pyc in plot_series(data, kind, ax, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, label, secondary_y, **kwds)\n   2516                  yerr=yerr, xerr=xerr,\n   2517                  label=label, secondary_y=secondary_y,\n-> 2518                  **kwds)\n   2519 \n   2520 \n\n/Users/shoyer/dev/pandas/pandas/tools/plotting.pyc in _plot(data, x, y, subplots, ax, kind, **kwds)\n   2322         plot_obj = klass(data, subplots=subplots, ax=ax, kind=kind, **kwds)\n   2323 \n-> 2324     plot_obj.generate()\n   2325     plot_obj.draw()\n   2326     return plot_obj.result\n\n/Users/shoyer/dev/pandas/pandas/tools/plotting.pyc in generate(self)\n    925         self._make_legend()\n    926         self._post_plot_logic()\n--> 927         self._adorn_subplots()\n    928 \n    929     def _args_adjust(self):\n\n/Users/shoyer/dev/pandas/pandas/tools/plotting.pyc in _adorn_subplots(self)\n   1058                     ax.set_xticklabels(xticklabels)\n   1059                 self._apply_axis_properties(ax.xaxis, rot=self.rot,\n-> 1060                                             fontsize=self.fontsize)\n   1061                 self._apply_axis_properties(ax.yaxis, fontsize=self.fontsize)\n   1062             elif self.orientation == 'horizontal':\n\n/Users/shoyer/dev/pandas/pandas/tools/plotting.pyc in _apply_axis_properties(self, axis, rot, fontsize)\n   1069 \n   1070     def _apply_axis_properties(self, axis, rot=None, fontsize=None):\n-> 1071         labels = axis.get_majorticklabels() + axis.get_minorticklabels()\n   1072         for label in labels:\n   1073             if rot is not None:\n\n/Users/shoyer/miniconda/envs/rapid/lib/python2.7/site-packages/matplotlib/axis.pyc in get_majorticklabels(self)\n   1166     def get_majorticklabels(self):\n   1167         'Return a list of Text instances for the major ticklabels'\n-> 1168         ticks = self.get_major_ticks()\n   1169         labels1 = [tick.label1 for tick in ticks if tick.label1On]\n   1170         labels2 = [tick.label2 for tick in ticks if tick.label2On]\n\n/Users/shoyer/miniconda/envs/rapid/lib/python2.7/site-packages/matplotlib/axis.pyc in get_major_ticks(self, numticks)\n   1295         'get the tick instances; grow as necessary'\n   1296         if numticks is None:\n-> 1297             numticks = len(self.get_major_locator()())\n   1298         if len(self.majorTicks) < numticks:\n   1299             # update the new tick label properties from the old\n\n/Users/shoyer/dev/pandas/pandas/tseries/converter.pyc in __call__(self)\n    901             vmin, vmax = vmax, vmin\n    902         if self.isdynamic:\n--> 903             locs = self._get_default_locs(vmin, vmax)\n    904         else:  # pragma: no cover\n    905             base = self.base\n\n/Users/shoyer/dev/pandas/pandas/tseries/converter.pyc in _get_default_locs(self, vmin, vmax)\n    882 \n    883         if self.plot_obj.date_axis_info is None:\n--> 884             self.plot_obj.date_axis_info = self.finder(vmin, vmax, self.freq)\n    885 \n    886         locator = self.plot_obj.date_axis_info\n\n/Users/shoyer/dev/pandas/pandas/tseries/converter.pyc in _daily_finder(vmin, vmax, freq)\n    505                     Period(ordinal=int(vmax), freq=freq))\n    506     span = vmax.ordinal - vmin.ordinal + 1\n--> 507     dates_ = PeriodIndex(start=vmin, end=vmax, freq=freq)\n    508     # Initialize the output\n    509     info = np.zeros(span,\n\n/Users/shoyer/dev/pandas/pandas/tseries/period.pyc in __new__(cls, data, ordinal, freq, start, end, periods, copy, name, tz, **kwargs)\n    637             else:\n    638                 data, freq = cls._generate_range(start, end, periods,\n--> 639                                                  freq, kwargs)\n    640         else:\n    641             ordinal, freq = cls._from_arraylike(data, freq, tz)\n\n/Users/shoyer/dev/pandas/pandas/tseries/period.pyc in _generate_range(cls, start, end, periods, freq, fields)\n    651                 raise ValueError('Can either instantiate from fields '\n    652                                  'or endpoints, but not both')\n--> 653             subarr, freq = _get_ordinal_range(start, end, periods, freq)\n    654         elif field_count > 0:\n    655             subarr, freq = _range_from_fields(freq=freq, **fields)\n\n/Users/shoyer/dev/pandas/pandas/tseries/period.pyc in _get_ordinal_range(start, end, periods, freq)\n   1317                              dtype=np.int64)\n   1318     else:\n-> 1319         data = np.arange(start.ordinal, end.ordinal + 1, dtype=np.int64)\n   1320 \n   1321     return data, freq\n\nMemoryError: \n\n> /Users/shoyer/dev/pandas/pandas/tseries/period.py(1319)_get_ordinal_range()\n   1318     else:\n-> 1319         data = np.arange(start.ordinal, end.ordinal + 1, dtype=np.int64)\n   1320 \n```\n",
      "Working on this. Doesn't look too bad.\n",
      "As an update, it's a bit worse than I thought. I think it was @changhiskhan who put in a ton of heuristics for figuring out what to resolution to draw when plotting datetimes. I wasn't sure if we'd need that for timedeltas, and then I got busy with other thing. My branch is [here](https://github.com/TomAugspurger/pandas/commit/ddb52378c4c0f8b871b67dcf7b646d123aba032c)\n",
      "As a workaround, the following works with master:\n\n``` python\nplt.plot(s.index,s.values)\n```\n",
      "I don't think freq adjustment of different timedeltas is mandatory at initial version. If ok, I'll try.\n",
      "Coming here from #10650, and adding a little more info just in case it can help. In my case, the bug manifests in `_get_ordinal_range`'s `end` parameter having a huge `ordinal`. This means the [following line](https://github.com/pydata/pandas/blob/e244bdd/pandas/tseries/period.py#L990):\n\n```\ndata = np.arange(start.ordinal, end.ordinal + 1, mult, dtype=np.int64)\n```\n\nallocates a gigantic array. To be specific, when doing:\n\n```\npd.Series(np.random.randn(4), index=pd.timedelta_range('0:00:00', periods=4, freq='min')).plot()\n```\n\nthe values of `start.ordinal` and `end.ordinal` are 0 and 180000000000, respectively.\n",
      "@lucas-eyer is the `mult` parameter on that line appropriate, or is it some very small number? That might be the source of the issue...\n",
      "I don't know what appropriate would be, but it's `1` (one).\n\nEdit: `pip freeze | grep pandas` gives `pandas==0.17.0`.\n",
      "I also just ran into this issue on 0.17.1. I'm not very familiar with the code, but it appears the issue is in `pandas.tseries.converter`.\n\nThe issue is that `vmin` and `vmax` as specified in the call to `_get_default_locs` in the `get_major_locator` function are in nanoseconds as returned from `XAxis.get_view_interval`:\n\n```\ndef __call__(self):\n    'Return the locations of the ticks.'\n    # axis calls Locator.set_axis inside set_m<xxxx>_formatter\n    vi = tuple(self.axis.get_view_interval())             # THIS IS IN NANOS\n    if vi != self.plot_obj.view_interval:\n        self.plot_obj.date_axis_info = None\n    self.plot_obj.view_interval = vi\n    vmin, vmax = vi\n    if vmax < vmin:\n        vmin, vmax = vmax, vmin\n    if self.isdynamic:\n        locs = self._get_default_locs(vmin, vmax)     # VMIN AND VMAX ARE IN NANOS\n    else:  # pragma: no cover\n        base = self.base\n        (d, m) = divmod(vmin, base)\n        vmin = (d + 1) * base\n        locs = lrange(vmin, vmax + 1, base)\n    return locs\n```\n\nBut downstream in _daily_finder the `freq` parameter is used, which means that the system is interpreting the deltas in terms of minutes/hours/etc. rather than nanos:\n\n```\ndef _daily_finder(vmin, vmax, freq):\n    periodsperday = -1\n\n    if freq >= FreqGroup.FR_HR:\n        if freq == FreqGroup.FR_NS:\n            periodsperday = 24 * 60 * 60 * 1000000000\n       # ETC MAPPING periodsperday\n       # .....\n    # save this for later usage\n    vmin_orig = vmin\n\n    (vmin, vmax) = (Period(ordinal=int(vmin), freq=freq),    # NOW THESE ARE INTERPRETED AS MINUTES (or whatever freq)\n                    Period(ordinal=int(vmax), freq=freq))\n```\n\nReplacing the final line above with \n\n```\n (vmin, vmax) = (Period(ordinal=int(vmin), freq='N'), Period(ordinal=int(vmax), freq='N'))\n```\n\nappears to fix the issue. \n",
      "@Liam3851 glad you have tracked this down! Any chance you're interested in making a pull request with the fix? :)\n",
      "Sure, I just have to figure out how to do it lol. Longtime pandas user but kinda new on this github thingy. I'll head over to the FAQ.\n",
      "Great! Give it a try and let us know if you have any questions :).\n\nOn Wed, Jan 13, 2016 at 11:48 AM, Liam3851 notifications@github.com wrote:\n\n> Sure, I just have to figure out how to do it lol. Longtime pandas user but\n> kinda new on this github thingy. I'll head over to the FAQ.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/pydata/pandas/issues/8711#issuecomment-171412395.\n",
      "Lots of love from me too @Liam3851!\n",
      "Hmm, ok still slightly more complicated. Was testing the fix and the bounds are now right and the graphs themselves look correct but the axis labels don't always work properly (sometimes they disappear)-- probably something related to how the labels are interpreted. I'm busy these next few days but I'll try to get around to making the fix sound.\n",
      "Just guessing, but you could be hitting what [I ran into](https://github.com/pydata/pandas/issues/8711#issuecomment-110754442). I can't remember how much progress if any I made on that.\n",
      "@TomAugspurger Hmm.. I'll try your version to see what it does. From the diff it looks like we're taking slightly different paths. It looks like you were building a TimedeltaConverter that worked parallel to DatetimeConverter and TimeConverter; I've been trying to fix the codepath the timedeltas are currently taking (through DatetimeConverter). But it's entirely possible that getting it to look just right will require going down your path. \n",
      "I\u2019d say getting it somewhat functional is good enough for now. Hopefully you don\u2019t have to go down that rabbit hole.\n\n> On Jan 14, 2016, at 10:29 AM, Liam3851 notifications@github.com wrote:\n> \n> @TomAugspurger https://github.com/TomAugspurger Hmm.. I'll try your version to see what it does. From the diff it looks like we're taking slightly different paths. It looks like you were building a TimedeltaConverter that worked parallel to DatetimeConverter and TimeConverter; I've been trying to fix the codepath the timedeltas are currently taking (through DatetimeConverter). But it's entirely possible that getting it to look just right will require going down your path.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub https://github.com/pydata/pandas/issues/8711#issuecomment-171691022.\n",
      "Hello. I am using pandas version 0.19.0 and matplotlib version 1.5.3 with python 3 and this issue is still there: If I try to plot a Dataframe where the index is a timedelta I get `Memory Error`. I am working around this by calling plt.plot(df.index, df.values) but it would be nice if there was a proper fix for this...\n",
      "@sam-cohan As you can see, the issue is still open, so it's indeed not yet solved. But any help is certainly welcome! \n",
      "Sorry I was looking at the wrong \"Closed\" :)\n",
      "Really wish this was fixed. I'm using datetime as a work around but stringing along 1970-01-01 to do time deltas is not fun.",
      "@TomAugspurger does your branch with a first attempt still exist? (the link above is not working anymore)",
      "So the issue here is that we are trying to use the Int64Index as a base class for TimedeltaIndex but we are trying to use the plotting routines for the PeriodIndex which relies on DatetimeIndex (matplotlib.date) underneath.  Matplotlib.date scales the view interval to the selected frequency. Int64Index does not, so this explains the issues above.\r\n\r\n### Options:\r\n\r\n1. Rebase timedelta index on DatetimeIndex\r\n2. Write a another routine to plot time deltas like this: http://stackoverflow.com/questions/15240003/matplotlib-intelligent-axis-labels-for-timedelta. I think this is the easiest path forward, but I need help figuring out how to hook it in. With the time series mix-ins for plotting I'd have to override the plotting routines based on the type of index somewhere.",
      "@jgoppert you should take a look at `pandas/tseries/converter.py` and the `TimeConverter` and `DatetimeConverter` classes. A possible way forward is to make a new `TimedeltaConverter` similar to those. ",
      "@jorisvandenbossche I did consider that approach, but I think having a separate matplotlib plotting function  is cleaner and will require less maintenance. We also won't have to worry about ever seeing jan 1970 on the time delta plot like we do on the period index based plots now. It seems pretty robust and I have added nano-second level precision labels.",
      "> @TomAugspurger does your branch with a first attempt still exist? (the link above is not working anymore)\r\n\r\nSeems like I deleted that branch when I was cleaning up my fork. I didn't get far beyond the `TimedletaConverter`, which is pretty straightforward. IIRC the difficult part was getting the dynamic relabeling to work like datetimes do (which can be a separate fix from fixing the memory error).",
      "@TomAugspurger can you take a look at my PR. Totally different approach but seems to work for me."
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 6,
    "additions": 154,
    "deletions": 23,
    "changed_files_list": [
      "doc/source/visualization.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/tests/plotting/test_datetimelike.py",
      "pandas/tools/plotting.py",
      "pandas/tseries/converter.py",
      "pandas/tseries/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 8725,
    "reporter": "jreback",
    "created_at": "2014-11-03T23:20:15+00:00",
    "closed_at": "2016-09-02T11:30:54+00:00",
    "resolver": "TomAugspurger",
    "resolved_in": "ccec504e31ce74f8016952ac75add1cc4bec7080",
    "resolver_commit_num": 41,
    "title": "BUG: dtype compat with get_dummies",
    "body": "from [SO](-map-values-of-categorical-variable-to-a-predefined-list-of-dummy-columns)\n\nI think should not coerce to floats but be an int (or a categorical)\n\n\n",
    "labels": [
      "Bug",
      "Dtypes",
      "Categorical"
    ],
    "comments": [
      "You didn't call `get_dummies()` on the categorical in your example code, you called it on the original `int64` variable, so does this issue apply to `get_dummies()` in general? Same happens on the categorical:\n\n``` python\npd.get_dummies(df['hour_cat']).dtypes\nOut[12]: \n0     float64\n1     float64\n2     float64\n3     float64\n4     float64\n5     float64\n6     float64\n7     float64\n8     float64\n9     float64\n# etc.\ndtype: object\n```\n",
      "sorry..that was a typo....but it has the same problem either way.\nhave to think about this. I just fixed unstack with a categorical. This is almost the same. It has to reshape then convert at the end.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "labeled",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "referenced",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 179,
    "deletions": 94,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/reshape.py",
      "pandas/stats/tests/test_ols.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_reshape.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 8731,
    "reporter": "jreback",
    "created_at": "2014-11-04T21:54:40+00:00",
    "closed_at": "2016-12-06T19:10:40+00:00",
    "resolver": "mroeschke",
    "resolved_in": "1725d24639a7c350a48fd201ca71b4548ea7186b",
    "resolver_commit_num": 7,
    "title": "BUG: incorrectly output index ordering with an ordered Categorical and pivot",
    "body": "xref #8860, soln might be the same\n\nfrom [SO](-pivot-table-alphabetically-sorts-categorical-data-incorrectly-when-addi)\n\n\n",
    "labels": [
      "Reshaping",
      "Categorical"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 1,
    "additions": 69,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tools/tests/test_pivot.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 8735,
    "reporter": "onesandzeroes",
    "created_at": "2014-11-05T04:24:31+00:00",
    "closed_at": "2015-08-28T18:16:56+00:00",
    "resolver": "ringw",
    "resolved_in": "59da7811fa567352cbbab5f10a296e543ec9e1fe",
    "resolver_commit_num": 1,
    "title": "Strange output from `DataFrame.apply` when applied func creates a dict",
    "body": "Just had something odd come up while trying to come up with something for [this SO question](-pandas-dataframe-rows-to-a-pandas-series).\n\nIf we use `DataFrame.apply()` to try and create dictionaries from the rows of a dataframe, it seems to return the `dict.values()` method rather than returning the dict itself.\n\n\n\nLooks like it's probably something to do with trying to grab the `values` attribute when the output of the applied function is a Series or something similar.\n\nLibrary versions:\n\n\n",
    "labels": [
      "API Design",
      "Bug"
    ],
    "comments": [
      "This inference is done in cython. It is indeed trying to get the values attribute (e.g. `_values_from_object`). I suppose you could fix it.\n\nDo you want something like this?\n\n```\nIn [1]: df = pd.DataFrame({'k': ['a', 'b', 'c'], 'v': [1, 2, 3]})\n\nIn [2]: def f(row):\n   ...:         return pd.Series({row['k']: row['v']})\n   ...: \n\nIn [3]: df.apply(f,axis=1)\nOut[3]: \n    a   b   c\n0   1 NaN NaN\n1 NaN   2 NaN\n2 NaN NaN   3\n```\n",
      "Hi all, I've run into the same issue myself. I actually need to pass a list of dicts for each row as my output, and it would be nice to be able to do `list(df.apply(lambda x: x.to_dict(), 0))`. This would require `df.apply` to return a Series of dtype object, where the elements are dicts.\n\n`apply` returning a Series of dicts would be consistent with the behavior when the passed function returns an object Pandas doesn't understand (apparently, any non-numeric without a `values` attribute). I guess I can add a check to `_values_from_object` to make sure the input is, say, a subclass of PandasObject.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "labeled",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 29,
    "deletions": 6,
    "changed_files_list": [
      "doc/source/whatsnew/v0.17.0.txt",
      "pandas/core/frame.py",
      "pandas/src/reduce.pyx",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 8848,
    "reporter": "jreback",
    "created_at": "2014-11-18T12:26:04+00:00",
    "closed_at": "2016-02-27T16:02:50+00:00",
    "resolver": "andrew-rosenfeld-ts",
    "resolved_in": "91967c89baffa03baa875934521c9e2fe978b124",
    "resolver_commit_num": 0,
    "title": "ENH: add missing methods to .dt for Period",
    "body": "methods\n- to_timestamp\n- asfreq\n\nproperties\n- start_time/end_time\n",
    "labels": [
      "Good as first PR",
      "API Design",
      "Period",
      "Compat",
      "Effort Low"
    ],
    "comments": [
      "@jreback I just want to make sure I understand this issue. If there's a series of Period objects, there are some additional methods desired on the .dt accessor? And these would be the same for PeriodIndex, as from looking at the code it seems that PeriodIndex is what's really behind the scenes of the .dt accessor?\n\nI should note there's a whole host of methods that are available to datetime_series.dt that aren't on period_series.dt, including: ceil/floor/round, is_{month,quarter,year}_{start,end}, microsecond, nanosecond, normalize, tz, tz_convert, tz_localize.\n",
      "Now that I've played with this code a bit, not sure a lot of those methods make sense for PeriodIndex/period_series.dt. I think a new issue can be raised if some of them should be implemented. (a harder task than what this issue was, which was mostly just exposing on .dt what was already done on PeriodIndex)\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "referenced",
      "commented"
    ],
    "changed_files": 4,
    "additions": 23,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/tests/series/test_datetime_values.py",
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 8860,
    "reporter": "nsriram13",
    "created_at": "2014-11-19T21:26:03+00:00",
    "closed_at": "2016-12-06T19:10:40+00:00",
    "resolver": "mroeschke",
    "resolved_in": "1725d24639a7c350a48fd201ca71b4548ea7186b",
    "resolver_commit_num": 7,
    "title": "Maintaining the order of the categorical variable that is passed into pd.crosstab",
    "body": "xref #8731, soln might be the same\n\nCurrently when we do a `crosstab`, the distinct values in each column is reported in the lexical order. But crosstabs are usually useful when we have categorical data (that may have an inherent ordering). \n\n\n\nBoth the cross-tab statements above result in the same output as below - essentially the code I believe is performing a lexical sort on the contents of the `Series` being passed.\n\n\n\nWould it be possible for `crosstab` to maintain the ordering of the categorical variable if column.cat.ordered on the passed column is True? Thanks!\n",
    "labels": [
      "Categorical",
      "Reshaping"
    ],
    "comments": [
      "this sounds sensible. Would you be interested in doing a pull-request for this? should be straightforward\n",
      "I have never worked with pandas code base and have no idea where to look. If you can point me to any resource that can help me get going I can definitely dabble around. But if it is an easy update to make for someone who is more familiar with the code - I would definitely recommend you assign it to them.\n",
      "https://github.com/pydata/pandas/wiki/Contributing\n\nalways a good time to become familar!\n\nlmk know how it goes\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "unlabeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced"
    ],
    "changed_files": 1,
    "additions": 69,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tools/tests/test_pivot.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9002,
    "reporter": "pybokeh",
    "created_at": "2014-12-04T21:09:36+00:00",
    "closed_at": "2016-02-27T15:09:05+00:00",
    "resolver": "BranYang",
    "resolved_in": "78d671fbcd18736c3fa8f47aead07b26b895ac4b",
    "resolver_commit_num": 3,
    "title": "StopIteration error when trying to import specific columns from Excel file using parse_cols",
    "body": "\n\nDocumentation says the following:\nparse_cols : int or list, default None\n- If None then parse all columns\n- If int then indicates last column to be parsed\n- If list of ints then indicates list of column numbers to be parsed\n- If string then indicates comma separated list of column names and column ranges (e.g. \u201cA:E\u201d or \u201cA,C,E:F\u201d)\n\nAlso did:\n\n\n\nBut still got the same error.\n\n\n",
    "labels": [
      "Bug",
      "Excel"
    ],
    "comments": [
      "create a csv and try to parse, then show what you have. \n",
      "This worked using usecols parameter:\n`df = pd.read_csv(r'D:\\temp\\four_years.csv', sep=',', usecols=['DTF','MTF'])`\n\nNot sure how this is relevant to my read_excel problem.  \n\nAlso, not sure why we have parse_cols for read_excel() versus usecols for read_csv(), but I see there is already an issue created for parameter name inconsistency.\n",
      "it could be a bug. Interested in looking at it? \n",
      "@jreback Sorry, wish I could.  But admittedly, my Python knowledge is not that great.  Been using Python for a few years as a data analyst, but I have not personally created a large project or debugged a large project before. \n",
      "Just FYI I am having exactly the same problem with pandas 0.17.1\n",
      "@pybokeh @cardosan  Do we have an excel file to reproduce this bug?\n",
      "Here it is\n[010215_BAU45_SUMMARY_GIU_tot_dem_codes.xls.zip](https://github.com/pydata/pandas/files/144080/010215_BAU45_SUMMARY_GIU_tot_dem_codes.xls.zip)\n",
      "@cardosan Sorry but the file you uploaded cannot reproduce this bug on master now. Could you please verify the bug using you own environment and paste your results here? \n\n``` Python\nimport pandas as pd\ndf = pd.read_csv(\"010215_BAU45_SUMMARY_GIU_tot_dem_codes.xls\")\npd.__version__\n```\n\nHere is my results running on most recent master code (up to commit fe584e7)\n\n``` Python\nIn [1]: import pandas as pd\n\nIn [2]: df=pd.read_excel(\"C:/D/tmp/010215_BAU45_SUMMARY_GIU_tot_dem_codes.xls\")\n# It successfully completed.\nIn [3]: pd.__version__\nOut[3]: '0.18.0rc1+47.gfe584e7'\n```\n\nAnd using 0.17.1 on MY computer\n\n``` Python\nIn [1]: import pandas as pd\n\nIn [2]: df=pd.read_excel(\"C:/D/tmp/010215_BAU45_SUMMARY_GIU_tot_dem_codes.xls\")\n# It successfully completed on my computer\nIn [3]: pd.__version__\nOut[3]: '0.17.1'\n```\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 75,
    "deletions": 15,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/io/excel.py",
      "pandas/io/tests/test_excel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9052,
    "reporter": "TomAugspurger",
    "created_at": "2014-12-10T19:47:53+00:00",
    "closed_at": "2016-02-15T20:28:58+00:00",
    "resolver": "jreback",
    "resolved_in": "cac5f8b33fee92395cae4a9b4b469e2130322281",
    "resolver_commit_num": 3897,
    "title": "DOC: Dict of Dicts for renaming Groupby Aggregations",
    "body": "I didn't realize this was possible, and didn't see it in the docs.\n\n\n",
    "labels": [
      "Groupby",
      "API Design",
      "Difficulty Novice",
      "Effort Low",
      "Prio-medium"
    ],
    "comments": [
      "xref is #8593  (which would replace / enhance this)\n",
      "Thanks for the tip. Didn't realize this was possible either, this will save me from building my multicolumns \"by hand\".\n\n@jreback are you planning any API change for 0.16.0 on this? #8593 does not seem to interfere with this behaviour, but maybe a deeper change is planned?\n\nI'd rather not rely on this if it's not tested atm. Or would you accept a test for this?\n",
      "@Gimli510 this IS implemented. Its basically the same as the following (except the name determination is slightly different).\n\n```\nIn [5]: df.groupby('B').agg({'A': ['mean','median'], 'C': ['mean','median']})\nOut[5]: \n     A           C       \n  mean median mean median\nB                        \na  1.5    1.5  3.5    3.5\nb  3.0    3.0  5.0    5.0\n```\n\nI haven't carefully looked thru, but I suspect their is at least 1 tests. Though would for sure accept a PR which makes these tests more prominent (e.g. test_agg_api or something).\n\n`pd.Summary` will enhance this API, the existing will remain.\n",
      "from [mailing list](https://groups.google.com/forum/#!topic/pydata/nXutBGUDEYw)\n\n```\nIn [2]: df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n   ...:                           'foo', 'bar', 'foo', 'foo'],\n   ...:                    'B' : ['one', 'one', 'two', 'three',\n   ...:                           'two', 'two', 'one', 'three'],\n   ...:                    'C' : np.random.randn(8),\n   ...:                    'D' : np.random.randn(8)})\n\nIn [3]: \n\nIn [3]: grouped = df.groupby(['A', 'B'])\n\nIn [4]: grouped[['D','C']].agg({'r':np.sum, 'r2':np.mean})\nOut[4]: \n                    r        r2\nA   B                          \nbar one   D -0.460078 -0.460078\n          C  0.798220  0.798220\n    three D  1.599986  1.599986\n          C -0.554798 -0.554798\n    two   D  0.124900  0.124900\n          C  0.084758  0.084758\nfoo one   D -0.466082 -0.233041\n          C -0.585512 -0.292756\n    three D -0.184726 -0.184726\n          C  0.130756  0.130756\n    two   D -1.985586 -0.992793\n          C  1.275138  0.637569\n\nIn [5]: grouped[['D','C']].agg({'r': { 'C' : np.sum }, 'r2' : { 'D' : np.mean }})\nOut[5]: \n                    r        r2\n                    C         D\nA   B                          \nbar one   D -0.460078 -0.460078\n          C  0.798220  0.798220\n    three D  1.599986  1.599986\n          C -0.554798 -0.554798\n    two   D  0.124900  0.124900\n          C  0.084758  0.084758\nfoo one   D -0.466082 -0.233041\n          C -0.585512 -0.292756\n    three D -0.184726 -0.184726\n          C  0.130756  0.130756\n    two   D -1.985586 -0.992793\n          C  1.275138  0.637569\n\nIn [6]: grouped[['D','C']].agg([np.sum, np.mean])\nOut[6]: \n                  D                   C          \n                sum      mean       sum      mean\nA   B                                            \nbar one   -0.460078 -0.460078  0.798220  0.798220\n    three  1.599986  1.599986 -0.554798 -0.554798\n    two    0.124900  0.124900  0.084758  0.084758\nfoo one   -0.466082 -0.233041 -0.585512 -0.292756\n    three -0.184726 -0.184726  0.130756  0.130756\n    two   -1.985586 -0.992793  1.275138  0.637569\n```\n\nwith a trivial patch\n\n```\ndiff --git a/pandas/core/groupby.py b/pandas/core/groupby.py\nindex add5080..b885b6f 100644\n--- a/pandas/core/groupby.py\n+++ b/pandas/core/groupby.py\n@@ -2837,9 +2837,6 @@ class NDFrameGroupBy(GroupBy):\n             keys = []\n             if self._selection is not None:\n                 subset = obj\n-                if isinstance(subset, DataFrame):\n-                    raise NotImplementedError(\"Aggregating on a DataFrame is \"\n-                                              \"not supported\")\n\n                 for fname, agg_how in compat.iteritems(arg):\n                     colg = SeriesGroupBy(subset, selection=self._selection,\n```\n\nof course need some tests......\n",
      "I do not really like this:\n\n```\nIn [3]: grouped = df.groupby(['A', 'B'])\n\nIn [4]: grouped[['D','C']].agg({'r':np.sum, 'r2':np.mean})\nOut[4]: \n                    r        r2\nA   B                          \nbar one   D -0.460078 -0.460078\n          C  0.798220  0.798220\n    three D  1.599986  1.599986\n...\n```\n\nThe fact that `C` and `D` end up as rows, feels so different as all other groupby things, that I would not add it to the API. It is rather easy to get with a `stack` after `groupby` if you want it that way.\n\nAlso, it is a bit strange that it works on `grouped[['C', 'D']]`, but not on `grouped` itself, while it both are DataFrameGroupby objects (but this is also inconsistent with current master). I would expect both to be the same, but that is not the case now:\n\n```\nIn [27]: grouped[['C', 'D']].agg({'r':np.sum, 'r2':np.mean})\n...\nNotImplementedError: Aggregating on a DataFrame is not supported\n\nIn [28]: grouped.agg({'r':np.sum, 'r2':np.mean})\n...\nKeyError: 'r'\n```\n\n```\nIn [21]: grouped[['D','C']].agg({'D':np.sum, 'C':np.mean})\n...\nNotImplementedError: Aggregating on a DataFrame is not supported\n\nIn [22]: grouped.agg({'D':np.sum, 'C':np.mean})\nOut[22]:\n                  C         D\nA   B\nbar one    1.249205 -1.576279\n    three -0.262759 -0.352865\n    two    1.151419 -0.670436\nfoo one   -0.259004 -0.135123\n    three  0.588044 -0.523053\n    two    0.817821  2.095902\n```\n\nSo apparantly `grouped` and `grouped[['C', 'D']]` is not the same, but this difference has never occured to me (I though it was the same just like `df` and `df[['C', 'D']]` is the same for a frame with those two columns). So I think this difference is also too subtle to base different behaviour upon.\n",
      "I think the most important is to have a clear set of rules how the (nested) dicts are interpreted. Eg:\n- For a SeriesGroupby: \n  - list: applies functions, and different column names are inferred from function \n    - eg `grouped['C'].agg(['mean', 'std'])`)\n  - flat dict: different columns names are taken from the dict keys \n    - eg `grouped['C'].agg({'C_mean': 'mean', 'C_std': 'std'})`)\n  - nested dict (with lists/dicts as elements): not allowed\n- For a DataFrameGroupby:\n  - list: functions are applied to each column, column names are inferred from functions, end up with MultiIndex\n    - eg `grouped[['C', 'D']].agg(['mean', 'std'])`\n  - flat dict: to specify a different (set of) function(s) to be applied to each column\n    - eg `grouped.agg({'C': 'mean', 'D': ['mean', 'std']})`\n    - and I think `grouped[['C', 'D']].agg({'C': 'mean', 'D': ['mean', 'std']})` should be equivalent\n  - nested dict: first level is column to which function is applied, second level can be used to give custom names to columns (current behaviour)\n    - eg `grouped.agg({'C': {'mean1': 'mean', 'med1': 'median'}, 'D': {'mean2': 'mean', 'med2': 'median'}})`\n    - and I think `grouped[['C', 'D']].agg(..)` should be equivalent. \n\nThe above are the current rules (as far as I know from using it, I don't know if everything is explicitly meant/tested/documented to be the rules). \nYou can do almost everything, but indeed the case for a DataFrameGroupby of \"apply several functions to all columns but with custom name\" is not really easy. Because the first level of the dict is interpreted as the column names, you now have to repeat this twice:\n\n```\nIn [48]: grouped.agg({'C': {'r':np.sum, 'r2':np.mean},'D': {'r':np.sum, 'r2':np.\nmean}})\nOut[48]:\n                  C                   D\n                  r        r2         r        r2\nA   B\nbar one    1.249205  1.249205 -1.576279 -1.576279\n    three -0.262759 -0.262759 -0.352865 -0.352865\n    two    1.151419  1.151419 -0.670436 -0.670436\nfoo one   -0.518008 -0.259004 -0.135123 -0.067562\n    three  0.588044  0.588044 -0.523053 -0.523053\n    two    1.635643  0.817821  2.095902  1.047951\n```\n\nBut allowing to let the first level of the dict to be the custom names instead of column names (as implemented in #11603) to ease this case:\n\n```\nIn [130]: grouped[['D','C']].agg({'r':np.sum, 'r2':np.mean})     ## not behaviour of master\nOut[130]:\n                    r        r2\nA   B\nbar one   D -0.435276 -0.435276\n          C  0.602266  0.602266\n    three D -2.090016 -2.090016\n          C -1.138887 -1.138887\n    two   D -1.012663 -1.012663\n          C  1.069958  1.069958\nfoo one   D -0.609795 -0.304898\n          C -0.812805 -0.406402\n    three D -1.957863 -1.957863\n          C  0.655829  0.655829\n    two   D  1.139243  0.569621\n          C -0.327944 -0.163972\n```\n\nseems like a potential rabbit hole to me .. (possible conflicts between existing column names / custom names, behaviour dependent on the presence of a column with a certain name, ..)\n\nSo I would rather vote to keep the rules as above, and remove the distinction between `grouped` and `grouped[['C', 'D']]`. \nI agree that it makes the specified use case of above (and of https://groups.google.com/forum/#!topic/pydata/nXutBGUDEYw) more cumbersome, but I do not directly see a way to do this cleanly\n\ncc @jreback @TomAugspurger @sinhrks @shoyer \n",
      "acutally not closing this\n",
      "The following raises `SpecificationError` in 0.18.0, although there is no [ambiguity](https://github.com/jreback/pandas/commit/e243f1808fd97c746990bc8a6f7cbc449584bec9) (SeriesGroupby):\n\n``` python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'bar',\n                         'foo', 'bar', 'foo', 'foo'],\n                   'B': ['one', 'one', 'two', 'three',\n                         'two', 'two', 'one', 'three'],\n                   'C': np.random.randn(8),\n                   'D': np.arange(8)})\n\ngrouped = df.groupby(['A', 'B'])\n\ngrouped['D'].agg({'D': np.sum, 'result2': np.mean})\n```\n\nIs this intended or a bug (I'd prefer to be able to reuse the series column name)?\n",
      "This should work (it is also a regression, as it worked before). \nI think this should work because for a SeriesGroupBy, the dict keys can/should always be interpreted as new column names, and not to select existing columns names.\n",
      "@xflr6 \n\nthis is fixed in #12329 \n\n```\nIn [3]: grouped['D'].agg({'D': np.sum, 'result2': np.mean})\nOut[3]: \n           result2  D\nA   B                \nbar one          1  1\n    three        3  3\n    two          5  5\nfoo one          3  6\n    three        7  7\n    two          3  6\n```\n\nNote that this works as well, though maybe not as to the users intent (e.g. the C is exactly a label here, nothing to do with the actual aggregation columns.\n\n```\nIn [4]: grouped['D'].agg({'D': np.sum, 'c': np.mean})\nOut[4]: \n           C  D\nA   B          \nbar one    1  1\n    three  3  3\n    two    5  5\nfoo one    3  6\n    three  7  7\n    two    3  6\n```\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "closed",
      "commented",
      "reopened",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "closed",
      "commented",
      "commented",
      "reopened",
      "milestoned",
      "demilestoned",
      "referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 55,
    "deletions": 23,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9114,
    "reporter": "shoyer",
    "created_at": "2014-12-20T08:56:33+00:00",
    "closed_at": "2016-07-24T13:54:50+00:00",
    "resolver": "sinhrks",
    "resolved_in": "6cae23dae668e1e5ac465f5868557f00dfcb5e77",
    "resolver_commit_num": 348,
    "title": "BUG: datetime64[us] arrays with NaT cannot be cast to DatetimeIndex",
    "body": "\n\nThe second line should give the same result as the last one.\n",
    "labels": [
      "Bug",
      "Timeseries"
    ],
    "comments": [
      "this is more of a case of `DatetimeIndex` should have less code in its new method (and just defer to `pd.to_datetime`).\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced"
    ],
    "changed_files": 2,
    "additions": 13,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9167,
    "reporter": "jmcnamara",
    "created_at": "2014-12-29T16:36:50+00:00",
    "closed_at": "2015-01-06T00:12:48+00:00",
    "resolver": "jmcnamara",
    "resolved_in": "07a573561422c83441b0e4023a73421d495d89f1",
    "resolver_commit_num": 9,
    "title": "BUG: ExcelWriter with xlsxwriter adds default format to cells preventing column formats",
    "body": "The `xlsxwriter` engine (which I helped implement) erroneously adds a default `General` format to cells that don't require it.\n\nThis doesn't affect spreadsheets created with `to_excel()`. However, it does prevent the user from overwriting the cell format with a column format.\n\nFor example consider the following program:\n\n\n\nThe gives the following output:\n\n![screenshot 2](-8f78-11e4-9259-98f40ef53cbd.png)\n\nHowever, it should look like this (note the format in the B column cells):\n\n![screenshot](-8f78-11e4-8af3-8ece50d9f144.png)\n\nI'll submit a PR to fix this as soon as I get a working test.\n\nVersions:\n\n\n",
    "labels": [
      "Data IO",
      "Excel",
      "Bug"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 50,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.16.0.txt",
      "pandas/io/excel.py",
      "pandas/io/tests/test_excel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9180,
    "reporter": "okdistribute",
    "created_at": "2015-01-01T23:46:25+00:00",
    "closed_at": "2016-07-24T14:14:06+00:00",
    "resolver": "aterrel",
    "resolved_in": "6efd743ccbf2ef6c13ea0c71b7b2e2a022a99455",
    "resolver_commit_num": 0,
    "title": "Support ndjson -- newline delimited json -- for streaming data.",
    "body": "Hey all,\n\nI'm a developer on [dat project]() (git for data) and we are building a python library to interact with the data store. \n\nEverything in dat is streaming, and we use newline delimited json as the official transfer format between processes. \n\n[Take a look at the specification for newline delimited json here](-spec/)\n\nDoes pandas support this yet, and if not, would you consider adding a `to_ndjson` function to the existing output formats?\n\nFor example, the following table:\n\n\n\nWould be converted to \n\n\n\nFor general streaming use cases, it might be nice to also consider other ways of supporting this format, like a generator function that outputs ndjson-able objects\n",
    "labels": [
      "JSON",
      "Enhancement",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "something like this, perhaps:\n\n``` python\ndef iterndjson(df):\n    generator = df.iterrows()\n    ndjson = []\n    row = True\n    while row:\n        try:\n            row = next(generator)\n            ndjson.append(row[1].to_dict())\n        except StopIteration:\n            row = None\n\n    return ndjson\n```\n\n``` python\n> df = pd.DataFrame({'one': [1,2,3,4], 'two': [3,4,5,6]})\n> iterndjson(df)\n[{'one': 1, 'two': 3},\n {'one': 2, 'two': 4},\n {'one': 3, 'two': 5},\n {'one': 4, 'two': 6}]\n```\n",
      "have a look here http://pandas.pydata.org/pandas-docs/stable/io.html#json\n",
      "this would also be great for things like BigQuery, which outputs JSON files as new line delimited JSON.  The issue is that it's not actually valid JSON (since it ends up as multiple objects).\n\n@karissa maybe you could hack around this by reading the file row by row, using ujson/json to read each row into a python dictionary and then passing the whole thing to the DataFrame constructor?\n",
      "@mrocklin and I looked into this. The simplest solution we came up was loading the file into a buffer, add the appropriate commas and brackets then passing back to read_json. \n\nBelow are a few timings on this approach, it seems the current implementation of read_json is a bit slower than ujson, so we felt the simplicity of this approach didn't make anything too slow.\n\n```\nIn [3]: def parse():\n    with open(\"reviews_Clothing_Shoes_and_Jewelry_5.json\") as fp:\n        list(map(json.loads, fp))\n   ...:\n\nIn [4]: %timeit parse()\n1 loop, best of 3: 4.05 s per loop\n\nIn [18]: import ujson as json\n\nIn [19]: def parse():\n    with open(\"reviews_Clothing_Shoes_and_Jewelry_5.json\") as fp:\n        list(map(json.loads, fp))\n   ....:\n\nIn [20]: %timeit parse()\n1 loop, best of 3: 1.43 s per loop\n\nIn [22]: %time _ = pd.read_json('reviews_Clothing_Shoes_and_Jewelry_5_comma.json', convert_dates=False)\nCPU times: user 4.75 s, sys: 520 ms, total: 5.27 s\nWall time: 5.49 s\n```\n\nI'll try to get a patch together unless someone thinks there is a better solution. The notion would be to add a flag 'line=True' to the reader.\n",
      "@karissa is there any difference between ndjson and jsonlines (http://jsonlines.org/)  I've never heard of ndjson but it seems to be the same thing.\n",
      "Looks like jsonlines includes a few more rules, including a specification about UTF-8 encoding. http://jsonlines.org/ vs http://ndjson.org/\n"
    ],
    "events": [
      "commented",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "milestoned",
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "assigned",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented"
    ],
    "changed_files": 5,
    "additions": 142,
    "deletions": 11,
    "changed_files_list": [
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/generic.py",
      "pandas/io/json.py",
      "pandas/io/tests/json/test_pandas.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9217,
    "reporter": "shoyer",
    "created_at": "2015-01-09T20:05:25+00:00",
    "closed_at": "2017-02-14T13:34:23+00:00",
    "resolver": "mroeschke",
    "resolved_in": "ff0deecbc8f8e9ae3d274e5e7cd7c0056de1a6c2",
    "resolver_commit_num": 30,
    "title": "BUG/API: interpolate with limit=0 should mean no interpolation",
    "body": "Current behavior:\n\n\n\nBut in fact, `limit=0` in contrast to `limit=None` should mean no interpolation.\n",
    "labels": [
      "Bug",
      "API Design"
    ],
    "comments": [
      "very confusing to have None and 0 mean different things\n\nwhy would you do this?\n",
      "I'm testing the results for different interpolation limits and thought a simple way to trigger no interpolation would be use `limit=0`, e.g., `df = pd.DataFrame({lim: s.interpolate(limit=lim) for lim in range(5)})`.\n\nI can see why `limit=None` makes sense as a sentinel value. But I interpreted `limit=0` as \"interpolate at most zero values\".\n",
      "Another sane API design would be to disallow `limit=0` entirely, e.g., `assert limit > 0`.\n",
      "ok I understand the reason and it makes sense\n\nthough maybe a bit confusing (I get for testing this is useful) but from a user perspective u r like why have an option that makes the function do nothing?\n",
      "yep maybe limit > 0 makes more sense (but raise a ValueError)\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 6,
    "additions": 56,
    "deletions": 17,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/generic.py",
      "pandas/core/internals.py",
      "pandas/core/missing.py",
      "pandas/src/algos_common_helper.pxi.in",
      "pandas/tests/series/test_missing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9249,
    "reporter": "yarikoptic",
    "created_at": "2015-01-14T14:25:22+00:00",
    "closed_at": "2016-04-09T14:34:51+00:00",
    "resolver": "gliptak",
    "resolved_in": "979e31db31eb719cd9cb4dd83682798099edf8db",
    "resolver_commit_num": 19,
    "title": "*__tmp_to_csv_deprecated_options__ are left behind",
    "body": "which fills up the /tmp on a test box:\n\n\n\nwhich seems to come from one of the test_frame tests which is not among failing tests (-py2.x-sid-sparc/builds/1141/steps/shell_5/logs/stdio) so should be cleaned up:\n",
    "labels": [
      "Good as first PR",
      "Testing",
      "Effort Low"
    ],
    "comments": [
      "yeh, these should all be done with the `ensure_clean` as a context manager to clean up. \n",
      "```\njreback-~/pandas] grin tmp_to_csv\n./pandas/tests/test_frame.py:\n 5897 :         pname = '__tmp_to_csv_deprecated_options__'\n 5914 :         pname = '__tmp_to_csv_from_csv__'\n 6062 :         path = '__tmp_to_csv_moar__'\n 6254 :         pname = '__tmp_to_csv_no_index__'\n 6268 :         pname = '__tmp_to_csv_headers__'\n 6283 :         pname = '__tmp_to_csv_multiindex__'\n13551 :         pname = '__tmp_to_csv_date_format__'\n\n # these ok\n #6427 :         with ensure_clean('__tmp_to_csv_float32_nanrep__.csv') as path:\n #6439 :         with ensure_clean('__tmp_to_csv_withcommas__.csv') as path:\n```\n",
      "I only saw the ***tmp_to_csv_deprecated_options**\n\nOn Wed, 14 Jan 2015, jreback wrote:\n\n>  jreback-~/pandas] grin tmp_to_csv\n>  ./pandas/tests/test_frame.py:\n>   5897 :         pname = '**tmp_to_csv_deprecated_options**'\n>   5914 :         pname = '**tmp_to_csv_from_csv**'\n>   6062 :         path = '**tmp_to_csv_moar**'\n>   6254 :         pname = '**tmp_to_csv_no_index**'\n>   6268 :         pname = '**tmp_to_csv_headers**'\n>   6283 :         pname = '**tmp_to_csv_multiindex**'\n> \n>   # these ok\n>   #6427 :         with ensure_clean('**tmp_to_csv_float32_nanrep**.csv') as path:\n>   #6439 :         with ensure_clean('**tmp_to_csv_withcommas**.csv') as path:\n> \n>  13551 :         pname = '**tmp_to_csv_date_format**'\n> \n>    -\n>    Reply to this email directly or view it on GitHub.\n> \n>    Link: https://github.com/pydata/pandas/issues/9249#issuecomment-69922986\n\n## \n\nYaroslav O. Halchenko, Ph.D.\nhttp://neuro.debian.net http://www.pymvpa.org http://www.fail2ban.org\nResearch Scientist,            Psychological and Brain Sciences Dept.\nDartmouth College, 419 Moore Hall, Hinman Box 6207, Hanover, NH 03755\nPhone: +1 (603) 646-9834                       Fax: +1 (603) 646-1419\nWWW:   http://www.linkedin.com/in/yarik\n",
      "I reviewed the occurences of `__tmp_` in HEAD and they all use `ensure_clean`.  Consider closing this issue.\n",
      "we don't need ANY paths explicity set when they are not directly in a `with ensure_clean`.\nI think the `_tmp_to_csv_moar` might not be wrapped in a context manger. \n\n```\npandas/tests/frame/test_to_csv.py:\n   41 :         pname = '__tmp_to_csv_from_csv__'\n  217 :         path = '__tmp_to_csv_moar__'\n  431 :         pname = '__tmp_to_csv_no_index__'\n  454 :         pname = '__tmp_to_csv_headers__'\n  469 :         pname = '__tmp_to_csv_multiindex__'\n  635 :         with ensure_clean('__tmp_to_csv_float32_nanrep__.csv') as path:\n  647 :         with ensure_clean('__tmp_to_csv_withcommas__.csv') as path:\n 1025 :         pname = '__tmp_to_csv_date_format__'\n```\n",
      "@jreback Yes, I did change the `__tmp_to_csv_moar__` flow.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "commented",
      "milestoned",
      "commented",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "labeled",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 68,
    "deletions": 67,
    "changed_files_list": [
      "pandas/io/tests/test_excel.py",
      "pandas/tests/frame/test_to_csv.py",
      "pandas/tests/test_panel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9266,
    "reporter": "rmorgans",
    "created_at": "2015-01-16T05:33:51+00:00",
    "closed_at": "2015-07-21T10:52:45+00:00",
    "resolver": "captainsafia",
    "resolved_in": "384eb45587172b67c5de358b3e3c05cb0f61cdbd",
    "resolver_commit_num": 2,
    "title": "uin8 should be uint8 in io/parsers.py",
    "body": "while using read_fwf I had a bug that had this line from parsers.py\n\n#L989\n\nin which there is a np.uin8 which I'm pretty sure should be a np.uint8\n\napologise if this has already been submitted - I did a search on uin8 and only got the offending line in the code.\n\nI just fixed on my arch source code to keep working on my problem (I'm an engineer not a software guy!). I will try and find time for a patch and test (I failed on my last one)\n\nCHeers\n\nRick\n",
    "labels": [
      "Bug",
      "Dtypes",
      "CSV",
      "Good as first PR",
      "Effort Low"
    ],
    "comments": [
      "can you show some code that actually triggered this? (I agree its buggy/not tested). and I think I know what you did, but just to be clear. Pls show `pd.show_versions()` as well.\n",
      "Hi - in the next comment is some self contained code that reproduces the bug -\nincluding pd.show_versions. It's basically a head of the file I was trying\nto load in and the code that triggered the bug. When I next get some time\nI'll try and make this smaller & more suitable for a unit test.\n\nCheers\n\nRick\n",
      "``` python\nimport pandas as pd\nprint(pd.show_versions())\nwith open('test.txt','w') as f:\n    f.write(\n\"\"\"1421302964.213420    PRI=3 PGN=0xef00      DST=0x17 SRC=0x28    04 154 00 00 00 00 00 127\n1421302964.226776    PRI=6 PGN=0xf002               SRC=0x47    243 00 00 255 247 00 00 71\"\"\"\n    )\nd=pd.read_fwf('test.txt',\n              colspecs=[(0,17),(25,26),(33,37),(49,51),(58,62),(63,1000)],\n              names=['time','pri','pgn','dst','src','data'],\n              converters={'pgn':lambda x: int(x,16),\n                          'src':lambda x: int(x,16),\n                          'dst':lambda x: int(x,16),\n                          'data':lambda x: len(x.split(' '))},\n              index_col='time')\n```\n\nrunning this gives\n\n``` bash\npython test_pandas_uin8_bug.py\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.4.2.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.14.28-1-lts\nmachine: x86_64\nprocessor:\nbyteorder: little\nLC_ALL: None\nLANG: en_AU.UTF-8\npandas: 0.15.2\nnose: 1.3.4\nCython: 0.21.2\nnumpy: 1.9.1\nscipy: 0.15.0\nstatsmodels: 0.6.1\nIPython: 2.3.1\nsphinx: 1.2.3\npatsy: 0.3.0\ndateutil: 2.4.0\npytz: 2014.10\nbottleneck: 0.8.0\ntables: 3.1.1\nnumexpr: 2.4\nmatplotlib: 1.4.2\nopenpyxl: 2.1.4\nxlrd: 0.9.3\nxlwt: None\nxlsxwriter: 0.6.5\nlxml: 3.4.1\nbs4: None\nhtml5lib: 0.999\nhttplib2: None\napiclient: None\nrpy2: None\nsqlalchemy: 0.9.8\npymysql: None\npsycopg2: 2.5.4 (dt dec pq3 ext)\nNone\nTraceback (most recent call last):\n  File \"/usr/lib/python3.4/site-packages/pandas/io/parsers.py\", line 987,\nin _convert_to_ndarrays\n    values = lib.map_infer(values, conv_f)\n  File \"pandas/src/inference.pyx\", line 1046, in pandas.lib.map_infer\n(pandas/lib.c:57158)\n  File \"test_pandas_uin8_bug.py\", line 13, in <lambda>\n    'dst':lambda x: int(x,16),\nValueError: invalid literal for int() with base 16: ''\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"test_pandas_uin8_bug.py\", line 15, in <module>\n    index_col='time')\n  File \"/usr/lib/python3.4/site-packages/pandas/io/parsers.py\", line 496,\nin read_fwf\n    return _read(filepath_or_buffer, kwds)\n  File \"/usr/lib/python3.4/site-packages/pandas/io/parsers.py\", line 251,\nin _read\n    return parser.read()\n  File \"/usr/lib/python3.4/site-packages/pandas/io/parsers.py\", line 710,\nin read\n    ret = self._engine.read(nrows)\n  File \"/usr/lib/python3.4/site-packages/pandas/io/parsers.py\", line 1563,\nin read\n    data = self._convert_data(data)\n  File \"/usr/lib/python3.4/site-packages/pandas/io/parsers.py\", line 1600,\nin _convert_data\n    self.verbose, clean_conv)\n  File \"/usr/lib/python3.4/site-packages/pandas/io/parsers.py\", line 989,\nin _convert_to_ndarrays\n    mask = lib.ismember(values, na_values).view(np.uin8)\nAttributeError: 'module' object has no attribute 'uin8'\n```\n",
      "This typo is still present in the 1.6.2 release, and in the 1.7 development branch. I did the same as rmorgans, fixed it in my source code and recompiled. Maybe it should not be marked as closed?\n",
      "this is issue is still open\nthere have been several pull requests to fix but haven't been followed thru - u are welcome to pick one up and finish up the tests\n",
      "note - iirc there were several bug reports to this - u might be looking at one of those \n",
      "I would be more than happy to help out, as I think it is a fantastic piece of software - but the overhead for me to fix just this small typo is a bit big for me right now, unfortunately. Sorry.\n",
      "I think I'll take this on.\n\nI'll try to write some solid tests for it.\n\nPR coming soon.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "labeled",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 22,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.17.0.txt",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9424,
    "reporter": "stevenmanton",
    "created_at": "2015-02-05T22:20:07+00:00",
    "closed_at": "2016-05-23T21:43:04+00:00",
    "resolver": "gfyoung",
    "resolved_in": "9a6ce07ce19adb6d8ded8af2ef66326d6750171e",
    "resolver_commit_num": 21,
    "title": "read_csv clobbers values of columns with duplicate names",
    "body": "xref #10577 (has test for duplicates with empty data)\n\nI don't expect this is the correct behavior, although it's always possible I'm doing something wrong. Importing data using the `names` keyword will clobber the values of columns where the name is duplicated. For example:\n\n\n\nreturns \n\n\n\nHowever, this produces the correct result:\n\n\n\n\n\nInterestingly, it works if the field names are in the header:\n\n\n\n\n\nIs this a bug or am I doing something wrong?\n",
    "labels": [
      "Bug",
      "CSV"
    ],
    "comments": [
      "I've came across something similar. When using `mangle_dupe_cols=False` I get duplicate column names but the column data is all the same, although it's not so in the data I read. So it seems that the last column of the same name overrides all other columns.\n\n``` py\nimport pandas as pd\nfrom StringIO import StringIO\n\ndata = \"\"\"A,A,B,B,B\n    1,2,3,4,5\n    6,7,8,9,10\n    11,12,13,14,15\n    \"\"\"\n\ndf1 = pd.read_table(StringIO(data), sep=',', mangle_dupe_cols=True)\ndf2 = pd.read_table(StringIO(data), sep=',', mangle_dupe_cols=False)\n```\n\nNow `df1` is:\n\n|  | A | A.1 | B | B.1 | B.2 |\n| --: | --: | --: | --: | --: | --: |\n| 0 | 1 | 2 | 3 | 4 | 5 |\n| 1 | 6 | 7 | 8 | 9 | 10 |\n| 2 | 11 | 12 | 13 | 14 | 15 |\n\nwhich has the original data but non-duplicate column names;\n\nand `df2` is:\n\n|  | A | A | B | B | B |\n| --: | --: | --: | --: | --: | --: |\n| 0 | 2 | 2 | 5 | 5 | 5 |\n| 1 | 7 | 7 | 10 | 10 | 10 |\n| 2 | 12 | 12 | 15 | 15 | 15 |\n\nwhich has duplicate column names but their respecrive data has been overriden.\n\nReproducible bug in IPython notebook: http://nbviewer.ipython.org/github/yoavram/ipython-notebooks/blob/master/pandas%20duplicate%20column%20bug.ipynb\n\nPandas version 0.16.0. Python 2.7.\n",
      "This seems very strange to me -- I don't think there's any good reason for this behavior. I'm going to label it as a bug.\n",
      "xref #7160\n"
    ],
    "events": [
      "commented",
      "referenced",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 7,
    "additions": 149,
    "deletions": 38,
    "changed_files_list": [
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/io/parsers.py",
      "pandas/io/tests/parser/c_parser_only.py",
      "pandas/io/tests/parser/common.py",
      "pandas/io/tests/parser/test_parsers.py",
      "pandas/io/tests/parser/test_unsupported.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9455,
    "reporter": "amelio-vazquez-reina",
    "created_at": "2015-02-09T22:11:29+00:00",
    "closed_at": "2016-04-25T14:38:03+00:00",
    "resolver": "nbonnotte",
    "resolved_in": "bb9b9c584638329900b4d7f12247091498de781b",
    "resolver_commit_num": 11,
    "title": "ERR: better error message on invalid on with multi-index columns",
    "body": "Consider the following:\n\n\n\nand \n\n\n\nI would like to do an inner join in `object_uid`. When I do:\n\n\n\nI get:\n`AttributeError: 'numpy.ndarray' object has no attribute 'start'`\n\nThis is with: \n\n\n\nThe full error trace is below:\n\n\n\nAlso, the following works:\n\n\n\nbut it flattens my columns:\n\n\n",
    "labels": [
      "Error Reporting",
      "MultiIndex",
      "Reshaping",
      "Good as first PR"
    ],
    "comments": [
      "show .info() on both frames\n",
      "Thanks @jreback \n\n```\n> a_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 10 entries, 0 to 9\nData columns (total 2 columns):\nobject_uid      10 non-null object\nitem_uid        10 non-null object\ndtypes: object(2)\nmemory usage: 240.0+ bytes\n```\n\nand\n\n```\n> b_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 10 entries, 0 to 9\nData columns (total 4 columns):\n(object_uid, )           10 non-null object\n(after, micro_spend)     10 non-null float64\n(before, micro_spend)    10 non-null float64\n(change_rate, )          10 non-null float64\ndtypes: float64(3), object(1)\nmemory usage: 400.0+ bytes\n```\n",
      "how did you create the columns ?\n",
      "the columns index mean\n",
      "I created the multi-index one (`b_df` as follows):\n\n```\nb_df            = pd.concat([left_df, right_df], keys=['after', 'before'], axis=1)\nb_df.index.name = 'object_uid'\nb_df            = b_df.reset_index()\n```\n\nOther than that, `a_df` and the data in `left_df` and `right_df` comes from SQL queries and perhaps a few other joins before that.\n",
      "That said @jreback I have tried using `.copy()` on both dataframes with the hope that that may perhaps recreate the dataframes with no luck.\n",
      "Also, the following (flattening columns for `b_df`) works:\n\n```\nb_df.columns = ['object_uid', 'after', 'before', 'change_rate']\npd.merge(a_df, b_df, on='object_uid')\n```\n",
      "When you have a multi-index column, you have to specify the entire column depth in the on, IOW, a tuple. The error message could be better I think though. Want to submit a pull-request to improve that?\n",
      "Thanks @jreback I wonder if the following problem is related to this:\n\nConsider a dataframe that we populate as follows:\n\n```\ndf = pd.DataFrame()\ndf['col1'] = ...\ndf['col2'] = ...\n\nidx = pd.IndexSlice\ndf[idx['higher_col_1', 'col3']] = ...\ndf[idx['higher_col_2', 'col4']] = ...\n```\n\nI end up with \n\n```\nlist(df.columns)\n['col1',\n'col2',\n('higher_col_1', 'col3')\n('higher_col_1', 'col4')\n]\n```\n\nand when I do:\n\n```\ndf.sort_index(axis=1\n```\n\nI get:\n\n`TypeError: unorderable types: str() > tuple()`\n",
      "And on that note, is there any way to recreate the index in the example above automatically? (e.g. pushing any index value with lower depth than the max depth of the axis to the highest and lower level, and creating, empty level values for any depth missing?)\n",
      "what you are doing is not supported, you need to use `df.loc[...]`\n",
      "Thanks @jreback. That's good to know. I changed all my `df[idx[..]]` commands to `df.loc[:, idx[..]]` and, for what is worth, i get the same exact result (unable to sort the index). \n\nI will definitely try to improve the error message with a PR, but I think the problem I am having in my last two posts is different (unable to sort the index with columns with different depths).\n",
      "@amelio-vazquez-reina you cannot have columns of different depths; you think you do, but they are really all the same depth.\n",
      "Thanks @jreback. I understand now the problem.\n",
      "Xref #12078, #11640\n\nThe initial issue has been solved with PR #12158\n\nI'll do a PR to add some tests\n"
    ],
    "events": [
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "renamed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 39,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/tests/frame/test_axis_select_reindex.py",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9467,
    "reporter": "shoyer",
    "created_at": "2015-02-11T19:12:29+00:00",
    "closed_at": "2016-04-10T14:32:46+00:00",
    "resolver": "sinhrks",
    "resolved_in": "083db2a8b7f3b59ec7309ca909497bf419b1dfae",
    "resolver_commit_num": 284,
    "title": "BUG/CLN: SparseSeries __getitem__ uses questionable, buggy logic",
    "body": "SparseSeries currently relies on the assumption that all non-scalar input will raise a `TypeError` (because it won't be hashable) when used as input to `Index.get_loc`:\n#L361\n\nHowever, some non-scalar input is hashable (e.g., `Ellipsis`):\n\n\n\n(this should return the entire sparse series)\n\nThis logic is also problematic because it relies on `get_loc` always throwing `TypeError` for invalid input. So this could use some cleanup, ideally checking directly if the input is hashable. And it probably shouldn't be raising a generic `Exception`, either.\n\nxref tslib.pyx change/discussion in #9258\n",
    "labels": [
      "Bug",
      "Sparse"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "cross-referenced",
      "cross-referenced",
      "milestoned"
    ],
    "changed_files": 3,
    "additions": 11,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_indexing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9534,
    "reporter": "ruoyu0088",
    "created_at": "2015-02-23T07:27:33+00:00",
    "closed_at": "2016-12-06T19:10:40+00:00",
    "resolver": "mroeschke",
    "resolved_in": "1725d24639a7c350a48fd201ca71b4548ea7186b",
    "resolver_commit_num": 7,
    "title": "pivot_table(aggfunc=\"count\") with category column raise \"ValueError: Cannot convert NA to integer\"",
    "body": "Here is the test code, that return the right table:\n\n\n\nwhen convert column to category, ValueError is raised:\n\n\n\ngroupby also raise the same error:\n\n\n\nadd `dropna()` to `count()` in `groupby.py` fix this problem:\n\n\n",
    "labels": [
      "Bug",
      "Groupby",
      "Reshaping",
      "Categorical",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "xref #8731 \n\npls `pd.show_versions()`\n\npull-requests are welcome\n",
      "Here is the output of show_versions:\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.9.final.0\npython-bits: 32\nOS: Windows\nOS-release: 7\nmachine: x86\nprocessor: x86 Family 6 Model 42 Stepping 7, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.15.2.dev\nnose: 1.3.4\nCython: 0.21.2\nnumpy: 1.9.1\nscipy: 0.15.0\nstatsmodels: 0.6.1\nIPython: 3.0.0-dev\nsphinx: 1.2.3\npatsy: 0.3.0\ndateutil: 2.3\npytz: 2014.10\nbottleneck: None\ntables: 3.1.1\nnumexpr: 2.4\nmatplotlib: 1.4.2\nopenpyxl: None\nxlrd: 0.9.3\nxlwt: None\nxlsxwriter: 0.6.5\nlxml: 3.4.1\nbs4: 4.3.2\nhtml5lib: None\nhttplib2: None\napiclient: None\nrpy2: 2.5.4\nsqlalchemy: 0.9.8\npymysql: None\npsycopg2: None\n```\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 1,
    "additions": 69,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tools/tests/test_pivot.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9708,
    "reporter": "flying-sheep",
    "created_at": "2015-03-23T12:11:47+00:00",
    "closed_at": "2015-03-25T22:56:58+00:00",
    "resolver": "flying-sheep",
    "resolved_in": "f7c7ee0e92c61870626256eea93f64fda940cb95",
    "resolver_commit_num": 0,
    "title": "Allow to select index in drop_duplicates and duplicated",
    "body": "there\u2019s no way to drop rows with duplicated index using `drop_duplicates`.\r\n\r\nwe\u2019d have to add a copy of the index as column, or do this:\r\n\r\n\r\n",
    "labels": [
      "Docs",
      "Groupby"
    ],
    "comments": [
      "Typically I'll use a `df.groupby(level=0).last()` (or more typically `.first()`). It works fine, but a groupby isn't necessarily the first thought for deduplication.\n\nI'm +0 on whether we should have a dedicated method for this.\n",
      "As @TomAugspurger indicates the following are equivalent.\n\nI suppose the `drop_duplicates` section could have this an alterative example. If you would like to pull-request for a doc update would be ok.\n\n```\nIn [6]: df = pd.DataFrame({'A' : range(4), 'B' : list('aabb')})            \n\nIn [7]: df                                                                 \nOut[7]:                                                                    \n   A  B                                                                    \n0  0  a                                                                    \n1  1  a                                                                    \n2  2  b                                                                    \n3  3  b                                                                    \n\nIn [9]: df2 = df.set_index('B')                                            \n\nIn [10]: df2                                                               \nOut[10]:                                                                   \n   A                                                                       \nB                                                                          \na  0                                                                       \na  1                                                                       \nb  2                                                                       \nb  3   \n\n```\n\n```\nIn [13]: df2.groupby(level=0).first()                        \nOut[13]:                                                     \n   A                                                         \nB                                                            \na  0                                                         \nb  2                                                         \n\nIn [16]: df2.reset_index().drop_duplicates(subset='B',take_last=False).set_index('B')                                                      \nOut[16]:                                                                                                                                   \n   A                                                                                                                                       \nB                                                                                                                                          \na  0                                                                                                                                       \nb  2\n```\n",
      "sorry, i don\u2019t get it. you mean i should add the second code block as exemple to the docs?\n",
      "I would add the groupby method as an alternative as its is another common way of performing this task \n",
      "to which file? `indexing.rtf`?\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#duplicate-data\n (which is in `indexing.rst`)\n"
    ],
    "events": [
      "commented",
      "closed",
      "reopened",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 1,
    "additions": 10,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/indexing.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9714,
    "reporter": "wetchler",
    "created_at": "2015-03-24T00:39:48+00:00",
    "closed_at": "2016-01-26T14:32:58+00:00",
    "resolver": "chris-b1",
    "resolved_in": "5de6b84f5117b005a8f010d4510a758b50f3d14e",
    "resolver_commit_num": 29,
    "title": "to_datetime 1000x slower for timezone-aware strings vs timezone-agnostic",
    "body": "When converting a string date column to datetime, if the string has a GMT timezone suffix (e.g. \"-0800\"), it takes 1000x longer to parse:\n\n\n\nNote microseconds vs milliseconds. 3 orders of magnitude... seems unnecessary. This can make loading CSVs into correctly-typed dataframes very, very, very slow for large datasets.\n",
    "labels": [
      "Enhancement",
      "Performance",
      "Timezones"
    ],
    "comments": [
      "Indeed, this is a known issue: pandas does not have a timezone aware `Block` (the internal data structure we use for holding data). Thus, we create `dtype=object` arrays of datetime objects. If I recall correctly, this is on @jreback's to-do list.\n",
      "This is actually a different issue\n\nno TZ specified\n\n```\nIn [11]: %timeit pd.to_datetime(string_dates)\n1000 loops, best of 3: 435 us per loop\n```\n\nWhat you gave; this fallsback to dateutil parsing, which is why its so slow, going\nback-forth from cython-python\n\n```\nIn [12]: tz_string_dates = string_dates.apply(lambda dt: dt + ' -0800')\n\nIn [13]: %timeit pd.to_datetime(tz_string_dates)\n1 loops, best of 3: 194 ms per loop\n```\n\nTry this\n\n```\nIn [15]: tz_string_dates = string_dates.apply(lambda dt: dt + '-0800')\n\nIn [16]: %timeit pd.to_datetime(tz_string_dates)\n10 loops, best of 3: 23 ms per loop\n\nIn [17]: tz_string_dates[0]\nOut[17]: '2000-01-01 00:00:00-0800'\n```\n\nThe space before the fixed TZ designation throws this off. Its actually an easy fix if you want to look (see `src/np_datetime_string.c`. That said, I am not 100% correct if this makes this non-ISO (but I agree should prob be parsable in c).\n\nFurther it is slowed down relative to non-tz strings because the TZ has to be interpreted for each string. This could be cached actually. So this is point 2 of speedups.\n",
      "Interesting -- thanks for the tips. Removing the space works, though I'll have to just be vigilant for now about what format csvs are automatically dumped to (in my case I believe the dataset is from a mysql dump). Cheers.\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 7,
    "additions": 192,
    "deletions": 48,
    "changed_files_list": [
      "asv_bench/benchmarks/timeseries.py",
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/src/datetime/np_datetime_strings.c",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tests/test_tslib.py",
      "pandas/tseries/tools.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9746,
    "reporter": "amcpherson",
    "created_at": "2015-03-29T04:41:40+00:00",
    "closed_at": "2016-01-30T19:27:56+00:00",
    "resolver": "amcpherson",
    "resolved_in": "de460567ecde0f2a846116e210541d52c8f682f2",
    "resolver_commit_num": 0,
    "title": "fill_value kwarg for unstack",
    "body": "Currently:\n\n\n\nIf I want to fill with -1, i need to `fillna` and then `astype` back to `int`.  Ideally:\n\n\n",
    "labels": [
      "Missing-data",
      "API Design",
      "Usage Question"
    ],
    "comments": [
      "You can do this by specifying the `downcast` keyword. This is NOT automatic as a general operation this _can_ be expensive.\n\n```\nIn [10]: df.set_index(['x','y']).unstack().fillna(-1,downcast='infer')\nOut[10]: \n   z   \ny  j  k\nx      \na  0  1\nb  2 -1\n\nIn [11]: df.set_index(['x','y']).unstack().fillna(-1,downcast='infer').dtypes\nOut[11]: \n   y\nz  j    int64\n   k    int64\ndtype: object\n```\n",
      "There may be some merit to this being allowed directly, even if the functionality can be accomplished with a series of operations.  For instance, when trying to limit memory usage on a big dataset, perhaps it would be preferable to keep the data as `np.int8`.\n\n``` python\nIn [15]: idx = np.array([0, 0, 1], dtype=np.int32)\n\nIn [16]: idx2 = np.array([0, 1, 0], dtype=np.int8)\n\nIn [17]: value = np.array([0, 1, 2], dtype=np.int8)\n\nIn [18]: df = pd.DataFrame({'idx':idx, 'idx2':idx2, 'value':value})\n\nIn [19]: df.dtypes\nOut[19]:\nidx      int32\nidx2      int8\nvalue     int8\ndtype: object\n\nIn [20]: df.set_index(['idx', 'idx2']).unstack().dtypes\nOut[20]:\n       idx2\nvalue  0       float64\n       1       float64\ndtype: object\n```\n\nAfter the unstack my data table is suddenly much larger than necessary.\n\nAlso, from looking at the code this would be fairly trivial to implement, without much impact on existing code.\n",
      "@amcpherson ok, if you can find a reasonable way to do this w/o affecting perf then would be ok to have a `fill_value` argument.\n"
    ],
    "events": [
      "commented",
      "closed",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "reopened",
      "milestoned"
    ],
    "changed_files": 7,
    "additions": 200,
    "deletions": 13,
    "changed_files_list": [
      "doc/source/reshaping.rst",
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/common.py",
      "pandas/core/frame.py",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/tests/frame/test_reshape.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9755,
    "reporter": "brechtm",
    "created_at": "2015-03-31T08:01:57+00:00",
    "closed_at": "2016-04-06T19:17:26+00:00",
    "resolver": "gfyoung",
    "resolved_in": "c6c201e27c7ed57a823ec7261340dfeec1e0226a",
    "resolver_commit_num": 7,
    "title": "read_csv with names, usecols and parse_dates",
    "body": "xref #12203 \n\nThe arrays passed to the date_parser function is different when `names` and `use_cols` are specified to limit the number of parsed columns.\n\nWhen running the example code below, the date_parser function receives two arguments, one array with '20140101' strings, and one array with integers. The default `date_parser` fails to process this input.\n\nWhen assigning an empty list to `DROPPED_COLUMNS` (so that all columns are parsed), the second array contains strings instead of integers, and the datetimes are parsed correctly.\n\nThe problem doesn't occur with `engine='python'`. I haven't tested the influence of the `header` and `index_cols` options.\n\nPython script:\n\n\n\nContents of 2014.csv:\n\n\n",
    "labels": [
      "CSV",
      "Bug",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "I can confirm this with master. \n\nWith as simplified example:\n\n```\ns = \"\"\"0,1,20140101,0900,4\n0,1,20140102,1000,4\"\"\"\n\n# using all cols -> no problem\npd.read_csv(StringIO(s), header=None, names=list('abcde'), parse_dates=[[2,3]], engine='c')\n\n# using certain cols -> problem\npd.read_csv(StringIO(s), header=None, names=list('acd'), usecols=[0,2,3], parse_dates=[[1,2]], engine='c')\n\n# but not with python engine\npd.read_csv(StringIO(s), header=None, names=list('acd'), usecols=[0,2,3], parse_dates=[[1,2]], engine='python')\n```\n\nBut it seems the header/names do not matter for reproducing it. So this also reproduces it:\n\n```\ns = \"\"\"a,b,c,d,e\n0,1,20140101,0900,4\n0,1,20140102,1000,4\"\"\"\n\n# using all cols -> no problem\npd.read_csv(StringIO(s), parse_dates=[[2,3]], engine='c')\n\n# using certain cols -> problem\npd.read_csv(StringIO(s), usecols=[0,2,3], parse_dates=[[1,2]], engine='c')\n\n# but not with python engine\npd.read_csv(StringIO(s), usecols=[0,2,3], parse_dates=[[1,2]], engine='python')\n```\n\ngives:\n\n```\n# using all cols -> no problem\nIn [16]: pd.read_csv(StringIO(s), parse_dates=[[2,3]], engine='c')\nOut[16]:\n                  c_d  a  b  e\n0 2014-01-01 09:00:00  0  1  4\n1 2014-01-02 10:00:00  0  1  4\n\n# using certain cols -> problem\nIn [18]: pd.read_csv(StringIO(s), usecols=[0,2,3], parse_dates=[[1,2]], engine='c')\nOut[18]:\n             c_d  a\n0   20140101 900  0\n1  20140102 1000  0\n\n# but not with python engine\nIn [20]: pd.read_csv(StringIO(s), usecols=[0,2,3], parse_dates=[[1,2]], engine='python')\nOut[20]:\n                  c_d  a\n0 2014-01-01 09:00:00  0\n1 2014-01-02 10:00:00  0\n```\n",
      "@brechtm Thanks for reporting! (interested in looking into it?)\n\ncc @mdmueller @selasley\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 166,
    "deletions": 27,
    "changed_files_list": [
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9765,
    "reporter": "RasterBurn",
    "created_at": "2015-03-31T19:51:46+00:00",
    "closed_at": "2016-04-25T13:42:20+00:00",
    "resolver": "sinhrks",
    "resolved_in": "5ae1bd8feb705a2c337bde3eb5de15486d85a19e",
    "resolver_commit_num": 292,
    "title": "concat erroneously sets series to NaN",
    "body": "# problem\n\n`concat` fails when:\n1. Using a SparseDataFrame\n2. AND that SparseDataFrame has a column of all `0.0` and a `fill_value` of `0.0`\n\nIf that SparseDataFrame doesn't have an all `0.0` column, it works beautifully.\n\n\n# output\n\n\n# metadata\n\n\n",
    "labels": [
      "Bug",
      "Reshaping",
      "Sparse"
    ],
    "comments": [
      "hmm, sparse concats have not gotten a lot of attention, so certainly looks like a bug. like to do a pull-request?\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced"
    ],
    "changed_files": 10,
    "additions": 422,
    "deletions": 121,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/internals.py",
      "pandas/sparse/array.py",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_array.py",
      "pandas/sparse/tests/test_combine_concat.py",
      "pandas/sparse/tests/test_series.py",
      "pandas/tests/test_reshape.py",
      "pandas/tools/merge.py",
      "pandas/types/concat.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9795,
    "reporter": "ghost",
    "created_at": "2015-04-02T23:39:12+00:00",
    "closed_at": "2015-04-07T10:28:15+00:00",
    "resolver": "kshedden",
    "resolved_in": "52a30d93a9d153ba1d8b30c430f6c49a6ce461c4",
    "resolver_commit_num": 4,
    "title": "Unexpected in-place changes when saving a DataFrame to Stata with write_index=False",
    "body": "After executing `df.to_stata()` with `write_index=False`, all `NaNs` in `df` were automatically replaced by `8.988466e+307`. Please see the following code:\n\n`df = pd.DataFrame(np.random.randn(5,4), columns=list('abcd'))`\n`df.ix[2, 'a':'c'] = np.nan`\n`print df`\n`df.to_stata('test.dta', write_index=False)`\n`print df`\nI am using `pandas v0.16.0`. Thanks.\n",
    "labels": [
      "Bug",
      "Stata"
    ],
    "comments": [
      "cc @bashtage\ncc @kshedden \n"
    ],
    "events": [
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "milestoned",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 13,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.16.1.txt",
      "pandas/io/stata.py",
      "pandas/io/tests/test_stata.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9807,
    "reporter": "hayd",
    "created_at": "2015-04-04T02:05:10+00:00",
    "closed_at": "2015-04-14T13:19:45+00:00",
    "resolver": "jcrist",
    "resolved_in": "30580e7acd6cafd55616d91bedbb8999d65177f5",
    "resolver_commit_num": 1,
    "title": "Groupby mean transform not converting to float",
    "body": "\n\n\n",
    "labels": [
      "Bug",
      "Groupby",
      "Dtypes",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 12,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.16.1.txt",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9809,
    "reporter": "jreback",
    "created_at": "2015-04-04T17:32:17+00:00",
    "closed_at": "2017-01-25T23:48:06+00:00",
    "resolver": "jorisvandenbossche",
    "resolved_in": "2619ee3cbf88b274d475b9a73d2d1e6e72c0a9ab",
    "resolver_commit_num": 654,
    "title": "DOC: merge FAQ / Caveats & Gotchas",
    "body": "xref #9802\n- I think making it FAQ would most appropriate\n- want to think about the orderings\n- need a release note to indicate that these have been merged\n",
    "labels": [
      "Docs",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "Agreed, would be nice to consolidate these.\n\nLooking over these pages again, several of the FAQ sections (\"Migrating from scikits.timeseries\" and \"Visualizing Data in Qt applications\") look very outdated and could probably be removed.\n",
      "the Visualizing Data is marked as `deprecated` already (but its only docs here, can prob just take it out)\n",
      "should also think about the 3 indexing mentions in the Caveats and Gotchas and whether these should really be in one of the indexing sections.\n\n```\n- Integer indexing -> move to advanced.rst\n- Label-based slicing conventions -> move to indexing.rst\n  - Non-monotonic indexes require exact matches\n  - Endpoints are inclusive\n- Miscellaneous indexing gotchas\n  - Reindex versus ix gotchas (move to advanced.rst with the Integer Indexing from above)\n  - Reindex potentially changes underlying Series dtype (needs a note in reindex section / indexing)\n```\n",
      "Also agree we can merge these sections. \nAlthough FAQ is maybe a good name, I also like the 'gotchas' wording, as it really are some typical 'gotchas' (frequently asked questions sounds more oriented to newcomers to me with questions about the project, while the content is not quite like that).\n\nI think we should also move some parts to the actual documentation on that issue, eg the indexing questions that @jreback noted, but I think also the html parsing issues\n",
      "In the earlier part of this thread, it mentions parts of the documentation which \"look very outdated and could probably be removed.\"\n\nI don't know if removing parts of the documentation is a novice level task.  I am a novice when it comes to python and pandas, and when I read the documentation of a python library, I generally believe it all.  I have not really thought to myself, this is can be removed when I have read the docs.  I think the difficulty level of this task should be raised to intermediate, since you need to know how things have changed in the different pandas versions over time.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 7,
    "additions": 299,
    "deletions": 374,
    "changed_files_list": [
      "doc/source/advanced.rst",
      "doc/source/basics.rst",
      "doc/source/ecosystem.rst",
      "doc/source/faq.rst",
      "doc/source/gotchas.rst",
      "doc/source/install.rst",
      "doc/source/io.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9815,
    "reporter": "jreback",
    "created_at": "2015-04-05T18:05:26+00:00",
    "closed_at": "2016-04-27T14:01:13+00:00",
    "resolver": "leifwalsh",
    "resolved_in": "1e0b2286cd341da2cbb6ba42ae5bbc186485ef2c",
    "resolver_commit_num": 0,
    "title": "DOC: expanding comparison with R section",
    "body": "might be worth expanding the [comparison with R section](-docs/stable/comparison_with_r.html); maybe have a basics section?\n\nwith some of the verbs from [here]()\n\n@TomAugspurger \n",
    "labels": [
      "Docs",
      "Difficulty Novice",
      "Effort Medium"
    ],
    "comments": [],
    "events": [
      "labeled",
      "mentioned",
      "subscribed",
      "milestoned",
      "labeled",
      "labeled",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 1,
    "additions": 73,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/comparison_with_r.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9850,
    "reporter": "sinhrks",
    "created_at": "2015-04-11T02:20:38+00:00",
    "closed_at": "2016-04-09T18:01:56+00:00",
    "resolver": "sinhrks",
    "resolved_in": "f813425a1b6153df0d06863fcf84678186fdfa4a",
    "resolver_commit_num": 279,
    "title": "API: SparseSeries.to_frame and SparseDataFrame.to_panel result in dense structures",
    "body": "Related to #9802. I think these should return sparse.\n\n\n",
    "labels": [
      "Bug",
      "API Design",
      "Sparse"
    ],
    "comments": [
      "Agreed. We discussed this here: https://github.com/pydata/pandas/issues/8048\n"
    ],
    "events": [
      "renamed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 23,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9854,
    "reporter": "amelio-vazquez-reina",
    "created_at": "2015-04-11T16:50:54+00:00",
    "closed_at": "2015-04-14T13:56:16+00:00",
    "resolver": "hsperr",
    "resolved_in": "39fa180cf6b861981871b4a61bf0f3a31eaa2b98",
    "resolver_commit_num": 2,
    "title": "asfreq drops the name of the index",
    "body": "Here's a simple repro\n\n\n\n---\n\nThis bug was originally reported on StackOverflow [here](-re-indexing-one-level-with-forward-fill-in-a-multi-index-dataframe/29404203?noredirect=1#comment47161025_29404203).  \n### Original question\n\nThe question was asking how to efficiently re-index one level of a multi-index DataFrame with \u201cforward-fill\u201d using the following input DataFrame as an example:\n\n\n### Andy's answer:\n\n> You have a couple of options, the easiest IMO is to simply unstack the first level and then ffill. I think this make it much clearer about what's going on than a groupby/resample solution (I suspect it will also be faster, depending on the data):\n\n\n\n> If you're missing some dates you have to reindex (assuming the start and end are present, otherwise you can do this manually e.g. with `pd.date_range`):\n\n\n\n**Note: `asfreq` drops the name of the index (which is most likely a bug!)**\n",
    "labels": [
      "Bug",
      "Indexing",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "starting on this one\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 34,
    "deletions": 17,
    "changed_files_list": [
      "doc/source/whatsnew/v0.16.1.txt",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9903,
    "reporter": "ssanderson",
    "created_at": "2015-04-15T02:26:52+00:00",
    "closed_at": "2016-04-29T15:59:08+00:00",
    "resolver": "sinhrks",
    "resolved_in": "62bedfdffdb537bbbf9caf1f8b2b7074d9cf2e9f",
    "resolver_commit_num": 295,
    "title": "Calling shift on a DatetimeIndex of length 0 returns an Index instead of a DatetimeIndex ",
    "body": "Minimal repro:\n\n\n",
    "labels": [
      "Timeseries",
      "Indexing"
    ],
    "comments": [
      "(This happens because DatetimeIndex delegates to Index.shift, which then calls the Index.**new** with an empty list, and it can't infer that the output should be a DatetimeIndex.)\n",
      "This is fixed by #9904.\n",
      "This has been already fixed in current master (maybe by #11211?). Adding tests then close.\n",
      "@sinhrks looks good.\n\n```\nIn [1]: DatetimeIndex([]).shift(3, 'H')\nOut[1]: DatetimeIndex([], dtype='datetime64[ns]', freq=None)\n```\n\nI since we have 0 len we don't infer the freq, but in this case we _know_ the freq (as we are shifting by it), or is that too much carry-over?\n",
      "`DatetimeIndex.freq` is common frequencies between elem, and it can be different from shifting freq. Thus it sounds making an exception in length=0 case. Thoughts? \n\n```\ns = pd.DatetimeIndex(['2011-01-01', '2011-01-02'], freq='D')\ns.shift(1, 'H')\n# DatetimeIndex(['2011-01-01 01:00:00', '2011-01-02 01:00:00'], dtype='datetime64[ns]', freq=None)\n```\n",
      "cc @ssanderson \ncc @llllllllll \n",
      "@sinhrks no, I was more thinking that If I shift an index, then I want to have it set the freq (as opposed to infer it which won't work if its < 3 elements). not really strong here opinion here, except that this was from a user action.\n",
      "@jreback I don't think that shifting an index by a specific frequency should be taken to imply that the index's data has that frequency.  If I have an hourly-frequency index, and I shift it forward by 5 minutes, it's still hourly frequency.  That doesn't change if my index is empty.\n",
      "https://github.com/pydata/pandas/pull/13026 looks good to me btw.  Thanks @sinhrks.\n",
      "@ssanderson hmm, that is right. ok then!\n"
    ],
    "events": [
      "commented",
      "referenced",
      "commented",
      "cross-referenced",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 1,
    "additions": 59,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tseries/tests/test_base.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 9915,
    "reporter": "brownan",
    "created_at": "2015-04-16T17:36:44+00:00",
    "closed_at": "2016-04-17T20:37:34+00:00",
    "resolver": "gliptak",
    "resolved_in": "b585b5ae3901aee8625e45f030d341db768ad8ad",
    "resolver_commit_num": 21,
    "title": "Resampling produces all NaN values for a particular dataset",
    "body": "I ran into a particular dataset that seems to cause Series.resample() to produce incorrect results. Each datapoint is almost but not exactly a minute apart. (This is trimmed down from a larger dataset, but this still reproduces the problem)\n\n\n\nExpected output:\n\n\n\nChanging any one of the timestamps or taking one of the two datapoints out works correctly. Something about this particular dataset causes resample to fail.\n\nI have more examples on this ipython notebook: \n\n\n\nedit: I can also reproduce this with the latest version on master: 0.16.0-157-g161f38d\n",
    "labels": [
      "Bug",
      "Resample"
    ],
    "comments": [
      "Looks like we forget to pass along a kwarg, or our inference for downsampling is buggy:\n\n``` python\nIn [19]: data1.resample('10S', how='mean')\nOut[19]: \n2015-03-31 21:48:50     2\n2015-03-31 21:49:00   NaN\n2015-03-31 21:49:10   NaN\n2015-03-31 21:49:20   NaN\n2015-03-31 21:49:30   NaN\n2015-03-31 21:49:40   NaN\n2015-03-31 21:49:50     1\n2015-03-31 21:50:00   NaN\n2015-03-31 21:50:10   NaN\n2015-03-31 21:50:20   NaN\n2015-03-31 21:50:30   NaN\n2015-03-31 21:50:40   NaN\n2015-03-31 21:50:50     2\nFreq: 10S, dtype: float64\n```\n\nPassing `how='mean'` explicitly works, but that should be the default here.\n",
      "This shows expected result running with `pandas: 0.18.0+114.g6c692ae.dirty` \n",
      "@gliptak Do you want to do a PR to explicitly add a test for this to confirm that it is fixed and to close this issue?\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 1,
    "additions": 15,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10001,
    "reporter": "a59",
    "created_at": "2015-04-27T19:58:18+00:00",
    "closed_at": "2016-05-06T12:10:40+00:00",
    "resolver": "kordek",
    "resolved_in": "1296ab39d32acaf2c77ed0c185fafe4ebcfedcb3",
    "resolver_commit_num": 0,
    "title": "pandas.ExcelFile ignore parse_dates=False",
    "body": "I am trying to read an Excel file which someone else created and the wrongly formatted a column as \"date\" when it is not. It has a large integer in it, which triggers an error\n\nOverflowError: normalized days too large to fit in a C int\n\nBut I have \"parse_dates=False\" so I thought pandas.ExcelFile would not try to parse the dates and return a string instead. Is this a bug?\n",
    "labels": [
      "Excel",
      "Bug"
    ],
    "comments": [
      "you would have to show an example and `pd.show_versions()`\n",
      "In Excel (2013 Windows 7), I created a new Workbook. In Sheet1, I entered A in A1 and 10000000 in B1. I then formatted B1 to be a Short Date which displays the cell as #################. I saved the files as 'test.xlsx'.\n\nI then ran the following python code\n\n``` python\nimport pandas as pd\npd.show_versions()\nxl_file = pd.ExcelFile('test.xlsx')\nsheet = xl_file.parse('Sheet1',parse_dates=False)\n```\n\nwhich gives me the following output\n\n> ## INSTALLED VERSIONS\n> \n> commit: None\n> python: 2.7.9.final.0\n> python-bits: 64\n> OS: Darwin\n> OS-release: 14.3.0\n> machine: x86_64\n> processor: i386\n> byteorder: little\n> LC_ALL: None\n> LANG: en_US.UTF-8\n> \n> pandas: 0.16.0\n> nose: 1.3.6\n> Cython: 0.22\n> numpy: 1.9.2\n> scipy: 0.15.1\n> statsmodels: 0.6.1\n> IPython: 3.1.0\n> sphinx: 1.2.3\n> patsy: 0.3.0\n> dateutil: 2.4.2\n> pytz: 2015.2\n> bottleneck: None\n> tables: 3.1.1\n> numexpr: 2.3.1\n> matplotlib: 1.4.3\n> openpyxl: 2.0.2\n> xlrd: 0.9.3\n> xlwt: 1.0.0\n> xlsxwriter: 0.7.2\n> lxml: 3.4.3\n> bs4: 4.3.2\n> html5lib: None\n> httplib2: None\n> apiclient: None\n> sqlalchemy: 1.0.1\n> pymysql: None\n> psycopg2: None\n> Traceback (most recent call last):\n>  File \"test.py\", line 5, in <module>\n>    sheet = xl_file.parse('Sheet1',parse_dates=False)\n>  File \"/Users/myname/anaconda/lib/python2.7/site-packages/pandas/io/excel.py\", line 287, in parse\n>    **kwds)\n>  File \"/Users/myname/anaconda/lib/python2.7/site-packages/pandas/io/excel.py\", line 418, in _parse_excel\n>    row.append(_parse_cell(value,typ))\n>  File \"/Users/myname/anaconda/lib/python2.7/site-packages/pandas/io/excel.py\", line 342, in _parse_cell\n>    epoch1904)\n>  File \"/Users/myname/anaconda/lib/python2.7/site-packages/xlrd/xldate.py\", line 130, in xldate_as_datetime\n>    return epoch + datetime.timedelta(days, seconds, 0, milliseconds)\n> OverflowError: date value out of range\n",
      "`parse_dates` is not implemented. But I think could be in the `_should_parse` function. pull-requests are welcome.\n",
      "Hi, I am adding this patch here, in case it's useful for those who do not want to parse dates from excel file by setting parse_dates=False.  Please review.  I had trouble parsing the following excel file from Crunchbase Excel Export which had really old dates which gave OverflowError.\n\n```\nFrom aae19c65e2a4b3a965f91bdffa5bd4595b0b7d7b Mon Sep 17 00:00:00 2001\nFrom: Kwan Lee <kwan@openviewpartners.com>\nDate: Tue, 29 Mar 2016 14:52:03 +0300\nSubject: [PATCH] parse_dates=False in read_excel should prevent from parsing\n dates.  It was throwing overflowerror when the dates were too old. update\n #10001\n\n---\n pandas/io/excel.py | 6 +++---\n 1 file changed, 3 insertions(+), 3 deletions(-)\n\ndiff --git a/pandas/io/excel.py b/pandas/io/excel.py\nindex 5656c36..b4b8996 100644\n--- a/pandas/io/excel.py\n+++ b/pandas/io/excel.py\n@@ -321,11 +321,11 @@ class ExcelFile(object):\n\n         epoch1904 = self.book.datemode\n\n-        def _parse_cell(cell_contents, cell_typ):\n+        def _parse_cell(cell_contents, cell_typ, parse_dates=True):\n             \"\"\"converts the contents of the cell into a pandas\n                appropriate object\"\"\"\n\n-            if cell_typ == XL_CELL_DATE:\n+            if cell_typ == XL_CELL_DATE and parse_dates:\n                 if xlrd_0_9_3:\n                     # Use the newer xlrd datetime handling.\n                     cell_contents = xldate.xldate_as_datetime(cell_contents,\n@@ -405,7 +405,7 @@ class ExcelFile(object):\n                         should_parse[j] = self._should_parse(j, parse_cols)\n\n                     if parse_cols is None or should_parse[j]:\n-                        row.append(_parse_cell(value, typ))\n+                        row.append(_parse_cell(value, typ, parse_dates))\n                 data.append(row)\n\n             if sheet.nrows == 0:\n-- \n2.7.2\n\n```\n",
      "@kwantopia It'll be easier to review that if you put it up as a pull request. Then we can comment inline.\n\nWhat's the desired behavior here? `read_csv` seems to silently \"fail\" to parse the columns that can't be represented as datetime64s.\n\n``` python\nIn [5]: !cat foo.csv\ndate,val\n1500-01-01,1\n1600-01-02,2\n1700-01-01,3\n1800-01-01,4\n1900-01-01,5\n2000-01-01,6\n\nIn [1]: pd.read_csv('foo.csv', parse_dates='date')\nOut[1]:\n         date  val\n0  1500-01-01    1\n1  1600-01-02    2\n2  1700-01-01    3\n3  1800-01-01    4\n4  1900-01-01    5\n5  2000-01-01    6\n```\n",
      "actually maybe @jorisvandenbossche can comment here. IIRC the `parse_dates` kw only matters if in excel its NOT a date already (and in your case these are out-of-range, so they are strings and will be parsed to `object` even in pandas).\n",
      "this is related to the issue in #11544 and looks to be a dupe of these (there is a somewhat convoluted chain as to what the original issues actually though). maybe someone can figure this chain out and we can create a master issue so its more clear.\n",
      "@TomAugspurger it's a problem in read_excel, but I guess I was also misunderstanding parse_dates field.  I was assuming that parse_dates=True means parse the dates and parse_dates=False means do not parse the dates for pandas.read_excel\n"
    ],
    "events": [
      "commented",
      "labeled",
      "commented",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "demilestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "milestoned",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 6,
    "additions": 23,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/io/excel.py",
      "pandas/io/tests/data/testdateoverflow.xls",
      "pandas/io/tests/data/testdateoverflow.xlsm",
      "pandas/io/tests/data/testdateoverflow.xlsx",
      "pandas/io/tests/test_excel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10008,
    "reporter": "mortada",
    "created_at": "2015-04-28T20:16:45+00:00",
    "closed_at": "2016-05-13T13:24:58+00:00",
    "resolver": "sinhrks",
    "resolved_in": "82f54bd1dd53cb031e5d801405b34f062155d823",
    "resolver_commit_num": 312,
    "title": "ENH: Index StringMethods should return MultiIndex when result dimension is more than one",
    "body": "Currently the `Index.str` methods only support returning `Index` results. However, analogous to the expansion from `Series.str.*` -> `DataFrame`, we should be able to do `Index.str.*` -> `MultiIndex` in certain cases, as discussed in #issuecomment-92416184 \n\nSome of the string methods that can support this are: `str.get_dummies`, `str.extract`, `str.split`\n- [x] `split` (#10085)\n- [x] `rsplit` (#10303)\n- [x] `partition` / `rpartition` (#9773)\n- [x] `extract` (#11386, only supports one group )\n- [x] `extractall` (not implemented in Index), #13156 \n- [x] `get_dummies` (#12842)\n\nHere are the related PRs: , , , \n",
    "labels": [
      "API Design",
      "Strings",
      "Master Tracker",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "I should have comment here... #9773 is going to have a general logic for `expand`. Affected methods (in future) are summarized in #9870.\n",
      "~~I understand current status is:~~\n-> Checklist is moved to the top.\n",
      "`Index.str.extractall` raises `AttributeError`. Just fix and close.\n\n```\nidx = pd.Index([\"a1a2\", \"b1\", \"c1\"], [\"A\", \"B\", \"C\"])\nidx.str.extractall(\"[ab](?P<digit>\\d)\")\n# AttributeError: 'Index' object has no attribute 'iteritems'\n```\n\nCC: @tdhock\n",
      "thanks @sinhrks \n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "commented",
      "labeled",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced"
    ],
    "changed_files": 4,
    "additions": 68,
    "deletions": 17,
    "changed_files_list": [
      "doc/source/text.rst",
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/core/strings.py",
      "pandas/tests/test_strings.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10041,
    "reporter": "multiloc",
    "created_at": "2015-05-01T15:59:58+00:00",
    "closed_at": "2016-03-06T20:44:23+00:00",
    "resolver": "thejohnfreeman",
    "resolved_in": "60b307f2ea2c6ccdd58dfeaa0c1d6b308bd2b74b",
    "resolver_commit_num": 1,
    "title": "Loss of nanosecond resolution when constructing Timestamps from str",
    "body": "Looks like a bug: When passing a YYYYMMDDTHHMMSS.f string to `pd.Timestamp`, the resolution seems limited to micro- rather than nano-seconds:\n\n\n\nThe behavior seems correct when using a different string format:\n\n\n\nUsing pandas 0.16.0, python 2.7.6\n",
    "labels": [
      "Timeseries",
      "Bug",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "Because dateutil doesn't support nanosecond, it looks little difficult to cover all the cases to nanoseconds (see #7907).\n\nOne option is to prepare a nanosecond parser to support some popular date formats. For example, change (or prepare separate function like) `np_datetime_string.c` to allow formats with other separators or without separators (like suggested in #9714).\n\nA PR would be appreciated :)\n"
    ],
    "events": [
      "labeled",
      "commented",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "closed",
      "reopened",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 168,
    "deletions": 140,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/src/datetime/np_datetime_strings.c",
      "pandas/tseries/tests/test_tslib.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10153,
    "reporter": "esafak",
    "created_at": "2015-05-16T16:38:47+00:00",
    "closed_at": "2016-08-06T22:53:22+00:00",
    "resolver": "chris-b1",
    "resolved_in": "a292c13a7f831145f3daac9881813aeb7ff08138",
    "resolver_commit_num": 41,
    "title": "Support categorical variables with CSVs",
    "body": "It would be nice to be able to read CSVs with categorical variables using read_csv's dtype parameter instead of casting the columns after the fact.\n",
    "labels": [
      "API Design",
      "CSV",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "I'm not opposed to this in principle, but I think the API will necessarily be clunky. Would we require (or allow) the user to specify all categories in the call to `read_csv`.\n\n@esafak we do support categoricals in `read/write_hdf` if that's an option for you (it may not be).\n",
      "Can't we already declare the dtypes of selected columns? I thought the problem was limited to categoricals, but if not, please expand my request to all dtypes.\n",
      "You can specify the types. I was just thinking\n\n``` python\npd.read_csv('file.csv', dtypes={'A': np.int64, 'B': pd.CategoricalDtype(['cat1', 'cat2', 'cat3'])})\n```\n\nwhich means you'd need to know all the categories up front. Or we infer them and you'll need to check that they're aren't any surprising categories.\n",
      "Nice workaround, but I think it is still nice to support `category` arg. \n\nAs a first step, how about converting the specified columns to `Categorical` after parsing? Though it is very nice to have optimized IO logic...\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "milestoned",
      "labeled",
      "labeled",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 6,
    "additions": 367,
    "deletions": 77,
    "changed_files_list": [
      "asv_bench/benchmarks/parser_vb.py",
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/io/tests/parser/c_parser_only.py",
      "pandas/parser.pyx",
      "pandas/tools/tests/test_concat.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10193,
    "reporter": "rekcahpassyla",
    "created_at": "2015-05-22T09:44:57+00:00",
    "closed_at": "2015-06-18T15:36:30+00:00",
    "resolver": "rekcahpassyla",
    "resolved_in": "7da7c97602412f28bf51e46f775bc39bad255a26",
    "resolver_commit_num": 2,
    "title": "BUG: indexing error when setting item in empty Series which has a frequency",
    "body": "Another somewhat degenerate case that works in 0.15.2, fails in 0.16.1\n\n\n\n0.15.2 output:\n\n\n\n0.16.1 output:\n\n\n\nProposed \n",
    "labels": [
      "Bug",
      "Indexing",
      "API Design",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "This actually is a bit tricker. You are slicing another object, then trying to set it. This actually should hit the `SettingWIthCopyWarning` path, though it should actually work.\n\nSide note is that we don't use `TimeSeries` anymore (its just a synonym for `Series` and will be deprecated in 0.17.0)\n\n```\nIn [2]: ts2 = Series(0, pd.date_range('2011-01-01', '2011-01-01')) \n\nIn [3]: ts2\nOut[3]: \n2011-01-01    0\nFreq: D, dtype: int64\n\nIn [4]: ts3 = ts2[:0]\n\nIn [5]: ts3\nOut[5]: Series([], Freq: D, dtype: int64)\n\n# this is the original structure\nIn [8]: ts3._data.blocks[0].values.base\nOut[8]: array([0])\n```\n",
      "Actually, I've just found that setting an item on a newly created empty series with frequency still returns an error. Since it's a new series, there is no view involved. \n\n``` python\nIn [142]: pd.show_versions()\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.10.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 26 Stepping 5, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.16.2\nnose: 1.3.4\nCython: 0.22\nnumpy: 1.9.2\nscipy: 0.14.0\nstatsmodels: 0.6.1\nIPython: 3.0.0\nsphinx: 1.3\npatsy: 0.3.0\ndateutil: 2.4.2\npytz: 2015.4\nbottleneck: 0.8.0\ntables: 3.1.1\nnumexpr: 2.3.1\nmatplotlib: 1.4.3\nopenpyxl: 1.8.5\nxlrd: 0.9.3\nxlwt: 0.7.5\nxlsxwriter: 0.6.7\nlxml: 3.4.2\nbs4: 4.3.2\nhtml5lib: 0.999\nhttplib2: None\napiclient: None\nsqlalchemy: 0.9.9\npymysql: None\npsycopg2: None\n\nIn [143]: series = pd.Series([], freq='D')\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-143-123100163557> in <module>()\n----> 1 series = pd.Series([], freq='D')\n\nTypeError: __init__() got an unexpected keyword argument 'freq'\n\nIn [144]: series = pd.Series([], pd.DatetimeIndex([], freq='D'))\n\nIn [145]: dt = pd.Timestamp('2011-01-01')\n\nIn [146]: series.loc[dt] = 9\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n<ipython-input-146-c577a9a95757> in <module>()\n----> 1 series.loc[dt] = 9\n\nC:\\dev\\bin\\Anaconda\\envs\\pandas-0.16.2\\lib\\site-packages\\pandas-0.16.2-py2.7-win-amd64.egg\\pandas\\core\\indexing.pyc in __setitem__(self, key, value)\n    113     def __setitem__(self, key, value):\n    114         indexer = self._get_setitem_indexer(key)\n--> 115         self._setitem_with_indexer(indexer, value)\n    116\n    117     def _has_valid_type(self, k, axis):\n\nC:\\dev\\bin\\Anaconda\\envs\\pandas-0.16.2\\lib\\site-packages\\pandas-0.16.2-py2.7-win-amd64.egg\\pandas\\core\\indexing.pyc in _setitem_with_indexer(self, indexer, value)\n    281                 if self.ndim == 1:\n    282                     index = self.obj.index\n--> 283                     new_index = index.insert(len(index),indexer)\n    284\n    285                     # this preserves dtype of the value\n\nC:\\dev\\bin\\Anaconda\\envs\\pandas-0.16.2\\lib\\site-packages\\pandas-0.16.2-py2.7-win-amd64.egg\\pandas\\tseries\\index.pyc in insert(self, loc, item)\n   1499             # check freq can be preserved on edge cases\n   1500             if self.freq is not None:\n-> 1501                 if (loc == 0 or loc == -len(self)) and item + self.freq == self[0]:\n   1502                     freq = self.freq\n   1503                 elif (loc == len(self)) and item - self.freq == self[-1]:\n\nC:\\dev\\bin\\Anaconda\\envs\\pandas-0.16.2\\lib\\site-packages\\pandas-0.16.2-py2.7-win-amd64.egg\\pandas\\tseries\\base.pyc in __getitem__(self, key)\n     73         getitem = self._data.__getitem__\n     74         if np.isscalar(key):\n---> 75             val = getitem(key)\n     76             return self._box_func(val)\n     77         else:\n\nIndexError: index 0 is out of bounds for axis 0 with size 0\n\n\nIn [148]: series_nofreq = pd.Series([], pd.DatetimeIndex([]))\n\nIn [149]: series_nofreq.loc[pd.Timestamp('2011-01-01')] = 47\n\nIn [150]:\n```\n"
    ],
    "events": [
      "referenced",
      "cross-referenced",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 14,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.17.0.txt",
      "pandas/tests/test_series.py",
      "pandas/tseries/index.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10208,
    "reporter": "jseabold",
    "created_at": "2015-05-26T14:12:06+00:00",
    "closed_at": "2016-04-03T15:41:14+00:00",
    "resolver": "dukebody",
    "resolved_in": "8776596346b2e7717a81ecd19bb78bb399f5621e",
    "resolver_commit_num": 0,
    "title": "Regex C Engine Warning",
    "body": "Using `pd.read_csv(..., sep=\", \", ...)` I'm now getting a warning about falling back to the C engine because regex parsing isn't supported in the C engine. That's fine, but this isn't actually using regex.\n\nI don't have an idea for a good transition strategy, and maybe the ship has sailed, but perhaps there should be a separate `read_regex` or a `regex` keyword instead of emitting this warning for any string greater than length 1.\n\nPandas 0.16.0.\n",
    "labels": [
      "CSV",
      "Docs",
      "Difficulty Novice",
      "Error Reporting",
      "Effort Low"
    ],
    "comments": [
      "The warning has to do with the fact that your separator is > 1 character, which is not supported by the parser (Prob isn't that hard to implement, but would need someone to do it). Here is a nice way to do this (and use the c-parser). \n\n```\nIn [38]: data = \"\"\"a, b, c\\n1, 2, 3\"\"\"\n\nIn [39]: read_csv(StringIO(data),sep=\",\",engine='c',skipinitialspace=True)\nOut[39]: \n   a  b  c\n0  1  2  3\n\nIn [40]: read_csv(StringIO(data),sep=\", \",engine='python')\nOut[40]: \n   a  b  c\n0  1  2  3\n```\n",
      "From the documentation it is not clear when a separator is considered a regex and when it isn't. I was trying to use '::' as separator (MovieLens dataset) when reading a file and pandas was interpreting it as a regex, when it really isn't.\n\nI think a separate `sep_regex` keyword would be cleaner. For the time being, we can also raise an exception \"non-regex separators of more than 1 character are not supported\". If it's the C engine that doesn't support >1 char separators, we can warn \"C engine doesn't support separators longer than 1 character, falling back to Python engine\".\n",
      "I don't think there's any need to adjust the API, just a clearer warning message.\n",
      "I think documentation should also be amended.\n\n> **sep**: Delimiter to use. If sep is None, will try to automatically determine this. Regular expressions are accepted and will force use of the python parsing engine and will ignore quotes in the data.\n\nWhen I first read this I wondered how pandas knows when am I using a regexp as delimiter and when am I using a normal string. I would change this by:\n\n> **sep**: Delimiter to use. If sep is None, will try to automatically determine this. If it is longer than 1 character, it will be interpreted as a regular expression, will force use of the python parsing engine and will ignore quotes in the data.\n\nAnyhow I still believe that accepting string separators larger than 1 character is a good feature, but might need a separate ticket/issue.\n",
      "IIRC if its > 1 length, then it by defintion defers to the python engine.\n",
      "no need to add any more options to the parsers. But as @TomAugspurger points out a clearer error message would be fine.\n",
      "@dukebody pull-requests welcome.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 2,
    "additions": 10,
    "deletions": 6,
    "changed_files_list": [
      "doc/source/io.rst",
      "pandas/io/parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10235,
    "reporter": "jreback",
    "created_at": "2015-05-30T00:40:14+00:00",
    "closed_at": "2016-07-25T12:03:19+00:00",
    "resolver": "sinhrks",
    "resolved_in": "2166ac1394da3fcad4c152cc1cb16e40b89ba08f",
    "resolver_commit_num": 354,
    "title": "PERF: core/base/IndexOpsMixin duplicated should be changed to use same impl as frame.duplicated",
    "body": "`DataFrame.duplicated` correctly uses `hashtable.duplicated_int64` a specialized routine, while `core.series.IndexOpsMixin.duplicated` uses `lib.duplicated` an object based one.\n\nas its almost always better to factorize then use the fast routine, than to do object comparisons.\nbut YMMV, so needs a couple of perf tests.\n\n\n",
    "labels": [
      "Performance",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "Nice to have a single flow. Because current series logic looks faster for object dtype, branching the logic depending on dtype?\n\n``` python\ns = pd.Series(pd.util.testing.rands_array(nchars=10, size=1000000))\n\n%timeit s.duplicated()\n# 1 loops, best of 3: 448 ms per loop\n\ndf = s.to_frame()\n%timeit df.duplicated()\n# 1 loops, best of 3: 764 ms per loop\n\n# case which has some duplicates\ns = pd.Series(np.repeat(pd.util.testing.rands_array(nchars=10, size=1000), 1000))\n%timeit s.duplicated()\n# 10 loops, best of 3: 32.4 ms per loop\n\ndf = s.to_frame()\n%timeit df.duplicated()\n# 10 loops, best of 3: 83.3 ms per loop\n```\n",
      "for sure branch on type \nI think create a float_64 one (put in hashtable.pyx) - this can release Gil as well (I'll create this in my Gil branch)\n\nthe object one can stay as it is \n",
      "OK. I worked a little and it additionally needs datetime and float handling...\n\nhttps://github.com/pydata/pandas/compare/master...sinhrks:dup_perf\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 9,
    "additions": 314,
    "deletions": 88,
    "changed_files_list": [
      "asv_bench/benchmarks/algorithms.py",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/algorithms.py",
      "pandas/core/base.py",
      "pandas/hashtable.pyx",
      "pandas/lib.pyx",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tests/test_algos.py",
      "pandas/tests/test_lib.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10291,
    "reporter": "songhuiming",
    "created_at": "2015-06-05T23:20:24+00:00",
    "closed_at": "2016-02-12T03:02:26+00:00",
    "resolver": "tshauck",
    "resolved_in": "dcc7cca72eb6ee699e4b8f269a909cdeaaa99a28",
    "resolver_commit_num": 2,
    "title": "KeyError: '__dummy__'  for pd.crosstab in pandas",
    "body": "get ~~ KeyError: '**dummy**' ~~ when I run the following:\n\n\n",
    "labels": [
      "Bug",
      "Reshaping",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "Even simpler example. Perhaps something to do with the indices not overlapping at all.\n\n```\ns1 = pd.Series([1, 2, 3], index=[1, 2, 3])\ns2 = pd.Series([4, 5, 6], index=[4, 5, 6])\npd.crosstab(s1, s2)\n```\n",
      "Believe this is the root cause of things:\n\nhttp://pandas.pydata.org/pandas-docs/stable/groupby.html#na-group-handling\n",
      "Yes http://pandas.pydata.org/pandas-docs/stable/groupby.html#na-group-handling is this cause.\n\nBecause the 2 indices have no overlapping indexes, this means that each groupby ends up including a nan which then excludes it from groupby result.\n\nYou then end up with an empty dataframe and that is the cause of the  KeyError, as you're accessing df['**dummy**'] on an empty dataframe.\n",
      "yeh, this should just be an empty frame, as there are no cross-tabulations.\n",
      "So this is not a bug?\n\nshould we:\n- raise exception\n- return an empty dataframe?\n",
      "return an empty frame\n",
      "I'm getting the same KeyError: '**dummy**' for my grouped data.\n\nAnd I'm not really sure how to fix it / what you mean by 'return an empty frame.' Care to dumb it down/show precisely what you mean?\n\nThanks!\n",
      "@dan7davis this needs a fix that would return an empty frame when catching the `KeyError` exception raised by the example above\n\nhttps://github.com/pydata/pandas/blob/master/pandas/tools/pivot.py#L151, just need something like:\n\n```\ntry:\n    table = table[values[0]]\nexcept KeyError:\n    pass\n```\n",
      "@jreback problem solved. thank you! really appreciate the alacrity\n",
      "want to do a pull request to fix in master?\n",
      "I'm (very) new to coding/python/GitHub, so unfortunately I have no idea\nwhat that means. But it sounds useful for me to know & helpful for others,\nso I'd be happy to learn/try..\n\nOn Tue, Jan 5, 2016 at 3:32 PM, Jeff Reback notifications@github.com\nwrote:\n\n> want to do a pull request to fix in master?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/pydata/pandas/issues/10291#issuecomment-169017196.\n",
      "contributing is a great way to learn ...., see our docs: http://pandas.pydata.org/pandas-docs/stable/contributing.html\n\nany questions, pls ask.\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 16,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/tools/pivot.py",
      "pandas/tools/tests/test_pivot.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10331,
    "reporter": "thrasibule",
    "created_at": "2015-06-11T16:45:05+00:00",
    "closed_at": "2016-03-21T12:27:56+00:00",
    "resolver": "markroth8",
    "resolved_in": "7e71a44a93cdcf9a1a4716ad9fdd4d83764ea4b3",
    "resolver_commit_num": 0,
    "title": "Partial string matching for timestamps with multiindex",
    "body": "I'm trying to get a slice from a multiindex. I find the behavior pretty inconsistent. Here is a simple dataframe:\n\n\n\nWith a single index, I can select all the data for a given day as follows:\n\n\n\nBut it doesn't work for a multiindex:\n\n\n\nSo I can make it work if I specify the slice explicitely, but it would be nice if the behavior for the 1D index carried over to Multiindices.\n",
    "labels": [
      "Enhancement",
      "Timeseries",
      "Indexing",
      "MultiIndex",
      "API Design",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "this is almost a dupe of #9732 \nThis is simply not implemented (though not that difficult). See that issue for an easy work-around.\n",
      "Added some unit tests that should pass once this feature works. @jreback did a quick review pass on the tests.\n\n@jreback, you said you'd have a look at the code path and give some recommendations on how to fix this issue.\n",
      "@jreback, per our discussion, I have an implementation that seems to fix this issue (in the private branch above), but it does not fail the test for `df.loc[('2016-01-01', 'a'), :]` which you thought it should fail. Before I make a formal pull request, can you have a look at the branch?\n\nThese tests all pass:\n`$ nosetests -A \"not slow\" tests/frame tests/series tests/indexes tests/test_indexing.py`\n",
      "Additionally, `df_swap.loc[idx[:, '2016-01-02':], :]` fails with an error, but I believe this can be considered a separate bug, as it did so before my proposed fix.\n\nThe cause is that the code in `multi.py` is trying to call `.stop` on an `int` instead of first checking to see if `stop` is a `slice`:\n\n``` python\nif isinstance(start, slice) or isinstance(stop, slice):\n                # we have a slice for start and/or stop\n                # a partial date slicer on a DatetimeIndex generates a slice\n                # note that the stop ALREADY includes the stopped point (if\n                # it was a string sliced)\n                return convert_indexer(start.start, stop.stop, step)\n```\n\nPlease let me know how you'd like me to proceed with the pull request.\n",
      "thanks @markroth8 I will be visting this next week.\n",
      "While trying to update the docs, I think I found an issue with the implementation. I'm not sure if it's new:\n\n``` python\ndft2 = DataFrame(randn(200000,1),columns=['A'],index=MultiIndex.from_product([dft.index, ['a', 'b']]))\n```\n\n```\nIn [32]: dft2.loc['2013-03']\nOut[32]: \n                              A\n2013-01-01 00:00:00 a  0.145858\n2013-01-01 00:01:00 a  0.007413\n2013-01-01 00:02:00 a  0.286948\n2013-01-01 00:03:00 a -0.695290\n2013-01-01 00:04:00 a -0.948675\n2013-01-01 00:05:00 a -2.454434\n2013-01-01 00:06:00 a  0.920254\n2013-01-01 00:07:00 a -0.614170\n2013-01-01 00:08:00 a  2.039961\n2013-01-01 00:09:00 a -1.326936\n2013-01-01 00:10:00 a  0.500509\n2013-01-01 00:11:00 a -0.433509\n2013-01-01 00:12:00 a  0.466115\n2013-01-01 00:13:00 a  1.245106\n2013-01-01 00:14:00 a  1.077567\n2013-01-01 00:15:00 a  0.603533\n2013-01-01 00:16:00 a -1.153203\n2013-01-01 00:17:00 a -0.932540\n2013-01-01 00:18:00 a -0.942356\n2013-01-01 00:19:00 a  0.078370\n2013-01-01 00:20:00 a -0.421003\n2013-01-01 00:21:00 a -0.700034\n2013-01-01 00:22:00 a  0.956662\n2013-01-01 00:23:00 a -1.009236\n2013-01-01 00:24:00 a -1.325032\n2013-01-01 00:25:00 a -1.633084\n2013-01-01 00:26:00 a -0.281456\n2013-01-01 00:27:00 a  0.394617\n2013-01-01 00:28:00 a -1.875757\n2013-01-01 00:29:00 a  1.056043\n...                         ...\n2013-03-11 10:25:00 a  0.212931\n                    b  0.419044\n2013-03-11 10:26:00 a  1.571333\n                    b  0.059788\n2013-03-11 10:27:00 a -0.271262\n                    b -0.167976\n2013-03-11 10:28:00 a -1.111855\n                    b -0.525583\n2013-03-11 10:29:00 a -0.801815\n                    b  0.557208\n2013-03-11 10:30:00 a -0.004837\n                    b -0.452653\n2013-03-11 10:31:00 a  1.270449\n                    b -0.775152\n2013-03-11 10:32:00 a -0.773299\n                    b  0.258476\n2013-03-11 10:33:00 a  0.376850\n                    b  1.430959\n2013-03-11 10:34:00 a  0.566823\n                    b -0.464938\n2013-03-11 10:35:00 a  0.889925\n                    b  1.714770\n2013-03-11 10:36:00 a -1.593098\n                    b -0.938460\n2013-03-11 10:37:00 a  0.046490\n                    b  0.543553\n2013-03-11 10:38:00 a  0.527221\n                    b  0.117193\n2013-03-11 10:39:00 a -0.459528\n                    b -1.791615\n\n[115040 rows x 1 columns]\n```\n\nNote how rows with 'b' are only present for '2013-03' rows but 'a' appears for all rows.\n",
      "Confirmed the same behavior exists in master branch with:\n\n`dft2.loc[IndexSlice['2013-03':'2013-03',:],:]`\n\nIs this a bug we should file separately?\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "labeled",
      "cross-referenced",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 4,
    "additions": 146,
    "deletions": 10,
    "changed_files_list": [
      "doc/source/timeseries.rst",
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/indexing.py",
      "pandas/tests/indexes/test_multi.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10381,
    "reporter": "TomAugspurger",
    "created_at": "2015-06-17T18:17:28+00:00",
    "closed_at": "2016-12-11T21:54:59+00:00",
    "resolver": "toobaz",
    "resolved_in": "e833096244d71c7253cf763556f51f0bece1d6f4",
    "resolver_commit_num": 17,
    "title": "BUG: DataFrame.to_hdf doesn't pass along min_itemsize for index",
    "body": "Unless I'm seeing something wrong\n\n\n\nand FYI this raises (not sure if it should work)\n\n\n\nJust a report right now... no time.\n",
    "labels": [
      "Bug",
      "HDF5",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "FYI the solution [here](http://stackoverflow.com/questions/15988871/hdfstore-appendstring-dataframe-fails-when-string-column-contents-are-longer) works. Something like\n\n``` python\nstore.append('test', df, min_itemsize={'index': 30})\n```\n\nso it should just be a matter of passing along arguments.\n",
      "I think maybe add to the docs a bit more [here](http://pandas.pydata.org/pandas-docs/stable/io.html#string-columns). This is like saying that the default for column A should be the same as B, which is not very explicit. \n\nThat said it should work for `min_itemsize=30` (e.g. defaults all object columns)\n",
      "Notice that if I do\n\n```\nddf = pd.DataFrame([['a', 'b', 1],\n                    ['a', 'b', 2]],\n                    columns=['A', 'B', 'C']).set_index(['A', 'B'])\n```\n\nand then\n\n```\nddf['C'].to_hdf('/tmp/store.hdf', 'test',\n          format=\"table\",\n          min_itemsize={'index' : 3})\n```\n\n(as far as I understand, the suggested workaround), I still get the error.\n",
      "Just for the records: the bug doesn't have to do with ``to_hdf()`` specifically, but rather with storing in ``table`` format without (explicitly) appending:\r\n\r\n``store.put(df, 'key', format='table', min_itemsize={'index' : 10})``\r\n\r\nwill fail the same.\r\n\r\nI'm pushing a PR in few seconds."
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "labeled",
      "labeled",
      "referenced",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "commented",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 39,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.2.txt",
      "pandas/io/pytables.py",
      "pandas/io/tests/test_pytables.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10409,
    "reporter": "amelio-vazquez-reina",
    "created_at": "2015-06-23T00:18:46+00:00",
    "closed_at": "2017-03-10T23:06:03+00:00",
    "resolver": "jreback",
    "resolved_in": "026e748e4ff558de80c92c04986a78754b430902",
    "resolver_commit_num": 4277,
    "title": "BUG: merge with categoricals does not preserve categories dtype",
    "body": "xref #14351\n\nNone of the following merge operations retain the `category` types. Is this expected? How can I keep them?\n#### Merging on a `category` type:\n\nConsider the following:\n\n\n\nif I do the merge, we end up with:\n\n\n#### Merging on a `non-category` type:\n\n\n\nif I do the merge, we end up with:\n\n\n",
    "labels": [
      "Reshaping",
      "API Design",
      "Categorical",
      "Difficulty Advanced",
      "Effort Medium",
      "Prio-high"
    ],
    "comments": [
      "ok, this works for a simple `concat`, but merging will not preserve the categoricals. Needs some post processing logic. Keep in mind this will ONLY work for identically categorized blocks. I think we originally left this out and didn't automagically recategorize, but let me see what I can do.\n",
      "@jreback \n\n> Keep in mind this will ONLY work for identically categorized blocks.\n\nDo you mean that attempting to merge on `Categorical` with one of categories missing should normally fail like this:\n\n```\nA = pd.DataFrame({'X': np.random.choice(['foo', 'bar'],size=(5,)), \n                  'Y': np.random.choice(['one', 'two', 'three'], size=(5,))})\nA['X'] = A['X'].astype('category')\n\nB = pd.DataFrame({'X': np.random.choice(['foo', 'baz'],size=(5,)),   # baz instead of bar\n                  'Z': np.random.choice(['jjj', 'kkk', 'sss'], size=(5,))})\nB['X'] = B['X'].astype('category')\n\npd.merge(A, B, on='X', how='outer')\n```\n\nReturning error:\n\n```\nd:\\Anaconda\\envs\\py2k\\lib\\site-packages\\pandas\\core\\common.pyc in take_nd(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\n    829         out_shape[axis] = len(indexer)\n    830         out_shape = tuple(out_shape)\n--> 831         if arr.flags.f_contiguous and axis == arr.ndim - 1:\n    832             # minor tweak that can make an order-of-magnitude difference\n    833             # for dataframes initialized directly from 2-d ndarrays\n\nAttributeError: 'Categorical' object has no attribute 'flags'\n```\n\nPerhaps it is linked to issue [#10374](https://github.com/pydata/pandas/issues/10374)...\n\nIdeally this would return a new `Categorical` with inserted `NaN`s.\n\nThe above was tried on pandas 0.16.2 \n",
      "well aside from the bug in #10324 \n- An ordered Categorical merge should raise as the new ordering would be impossible to determine. \n- A merge on a Categorical and an object column would be ok with nan for any non-represented categories. \n- A merge with 2 different Categoricals I suppose could succeed with a union of the categoricals. \n",
      "Additionally the performance benefits of the underlying int representation of the categories are lost when doing the merge. For example:\n\n``` python\n# Here we are joining on a int column and appending two string columns\nA = pd.DataFrame({'X': np.random.choice(range(0, 10), size=(100000,)), \n                                'Y': np.random.choice(['one', 'two', 'three'], size=(100000,))})\n\nB = pd.DataFrame({'X': np.random.choice(range(0, 10), size=(100000,)), \n                                'Z': np.random.choice(['jjj', 'kkk', 'sss'], size=(100000,))})\n\n%time r = pd.merge(A, B, on='X')\nWall time: 57.3 s\n\n# Lets do the same but convert the appending columns 'Y' and 'Z' to a category\nA['Y'] = A['Y'].astype('category')\nB['Z'] = B['Z'].astype('category')\n\n%time r = pd.merge(A, B, on='X')\nWall time: 54.9 s          # Slightly less\n\n# Now lets use the int representation of the categorical columns and merge\nA['Y'] = A['Y'].cat.codes\nB['Z'] = B['Z'].cat.codes\n\n%time r = pd.merge(A, B, on='X')\nWall time: 41.7 s           # 72% of the original time taken\n```\n",
      "@joshlk all true\n\nif u would like to try an impl would be great\nI will get to this but prob not for a little bit\n",
      "\r\nHi,\r\n\r\nis there any plans to solve this issue ? losing categorical types when merging data frames ?\r\n\r\nThanks.\r\n\r\nJCG\r\n\r\n",
      "@littlegreenbean33 its an open issue. A community pull-request would make this go faster.",
      "Thanks for taking the time to respond. Regrettably my skills and competences won't allow me to pitch in.  "
    ],
    "events": [
      "renamed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "assigned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "renamed",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 10,
    "additions": 364,
    "deletions": 71,
    "changed_files_list": [
      "asv_bench/benchmarks/join_merge.py",
      "doc/source/categorical.rst",
      "doc/source/merging.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/internals.py",
      "pandas/tests/test_categorical.py",
      "pandas/tests/tools/test_merge.py",
      "pandas/tests/tools/test_merge_asof.py",
      "pandas/tests/types/test_common.py",
      "pandas/tools/merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10452,
    "reporter": "sinhrks",
    "created_at": "2015-06-27T13:43:35+00:00",
    "closed_at": "2016-03-30T12:39:02+00:00",
    "resolver": "sinhrks",
    "resolved_in": "b550acc16d262f22320437d74b3ff7df087b8360",
    "resolver_commit_num": 263,
    "title": "API/BUG: SparseSeries.shape ignores fill_value ",
    "body": "`SparseSeries.shape` seems to return incorrect result ignoring shapes filled by `fill_value`.\n\n\n",
    "labels": [
      "Bug",
      "API Design",
      "Sparse"
    ],
    "comments": [
      "This refers to the shape of the sparse values, not the shape of the series itself. \n\n```\nIn [19]: s._data.blocks[0].sp_values\nOut[19]: array([1])\n\nIn [20]: s._data.blocks[0].sp_values.shape\nOut[20]: (1,)\n```\n",
      "I see. This is a spec and not be fixed?\n\nMay be a option to fix docstring to say `underlying` means sparse values.\n\n```\nhelp(pd.SparseSeries.shape)\n# return a tuple of the shape of the underlying data\n```\n",
      "I don't remember where exactly this is used and whether its just user-facing. If its user facing only, then it _should_ be consistent and return the shape of a densified frame, and we should have a `sparse_shape` or something. I am not sure what this really should do. If you want to experiment and report back would be gr8.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned"
    ],
    "changed_files": 3,
    "additions": 49,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_sparse.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10476,
    "reporter": "michaelaye",
    "created_at": "2015-06-30T15:40:42+00:00",
    "closed_at": "2016-05-25T12:09:25+00:00",
    "resolver": "gfyoung",
    "resolved_in": "b638f18469725eaad3dfe82dd7b01e285010a990",
    "resolver_commit_num": 22,
    "title": "read_csv python engine errors",
    "body": "Only thing I changed from my usually working reduction pipeline is to try `engine=\"python\"` (because I wanted to use `nrows` for a smaller test-read, but that fails as well, and I thought maybe the python engine is buggy currently):\n\n\n\nMy function call is this:\n\n\n\nUsing pandas-0.16.2_58_g01995b2-py3.4\n",
    "labels": [
      "Bug",
      "CSV"
    ],
    "comments": [
      "`1e6` is `float`. \n",
      "Changing title, because the apparent same error happens in Py2.7:\n\n``` python\n$ python reduction.py ~/data/planet4/2015-06-21_planet_four_classifications.csv\nINFO:Starting reduction.\nTraceback (most recent call last):\n  File \"reduction.py\", line 258, in <module>\n    args.test_n_rows, args.remove_duplicates)\n  File \"reduction.py\", line 182, in main\n    data = [chunk for chunk in reader]\n  File \"/Users/klay6683/miniconda3/envs/py27/lib/python2.7/site-packages/pandas/io/parsers.py\", line 686, in __iter__\n    yield self.read(self.chunksize)\n  File \"/Users/klay6683/miniconda3/envs/py27/lib/python2.7/site-packages/pandas/io/parsers.py\", line 710, in read\n    ret = self._engine.read(nrows)\n  File \"/Users/klay6683/miniconda3/envs/py27/lib/python2.7/site-packages/pandas/io/parsers.py\", line 1534, in read\n    content = self._get_lines(rows)\n  File \"/Users/klay6683/miniconda3/envs/py27/lib/python2.7/site-packages/pandas/io/parsers.py\", line 1985, in _get_lines\n    for _ in range(rows):\nTypeError: integer argument expected, got float\n(py27)\n```\n",
      "> 1e6 is float.\n> Yes, but it works with the C engine. Shouldn't the same function call be transparent in terms of syntax validity towards both engines?\n",
      "ok, will mark as a bug. should be easy test/fix. want to do a PR?\n"
    ],
    "events": [
      "commented",
      "commented",
      "renamed",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 46,
    "deletions": 8,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/io/parsers.py",
      "pandas/io/tests/parser/common.py",
      "pandas/io/tests/parser/test_unsupported.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10480,
    "reporter": "Wilfred",
    "created_at": "2015-07-01T09:32:27+00:00",
    "closed_at": "2016-09-28T10:08:51+00:00",
    "resolver": "chris-b1",
    "resolved_in": "c084bc1807374b06ebe9cf2cf350bcb6e8307e0f",
    "resolver_commit_num": 50,
    "title": "numeric_only inconsistency with pandas Series",
    "body": "\n",
    "labels": [
      "API Design",
      "Compat",
      "Docs"
    ],
    "comments": [
      "The docstring suggests this is a legitimate argument:\n\n```\nReturn the sum of the values for the requested axis\n\nParameters\n----------\naxis : {index (0)}\nskipna : boolean, default True\n    Exclude NA/null values. If an entire row/column is NA, the result\n    will be NA\nlevel : int or level name, default None\n        If the axis is a MultiIndex (hierarchical), count along a\n        particular level, collapsing into a scalar\nnumeric_only : boolean, default None\n    Include only float, int, boolean data. If None, will attempt to use\n    everything, then use only numeric data\n\nReturns\n-------\nsum : scalar or Series (if level specified)\n```\n\nHowever, strangely, there's an explicit test that this throws an exception: https://github.com/pydata/pandas/blob/054821dc90ded4263edf7c8d5b333c1d65ff53a4/pandas/tests/test_series.py#L2724\n",
      "this is just for compat as its a general parameter that matters for DataFrames. (and the function is auto-generated). If you can find a way to not-expose it without jumping thru hoops would be ok.\n",
      "OK, so `numeric_only` is accepted by `Series.sum` simply for compatibility with `DataFrame.sum`. You're proposing we find a way to hide this specific parameter in the docstring.\n\nHave I understood correctly?\n",
      "Ok\n",
      "I'll freely admit I'm a pandas novice, but I ran headlong into what I think was this bug just now.  I wanted numeric_only with Series.mean rather than sum; I assume that falls under this issue as well.  The documentation says this option exists but the code says it doesn't.  pandas version 0.18.1, documentation from a matching-version manual (http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.mean.html) (although obviously that link may age out).\n",
      "@smlewis - can you show an example of some data where you needed this and what you you expected to happen?  Note that the implemented usecase is for selecting numeric _columns_, like\n\n```\ndf = pd.DataFrame({'a': [2,3,4], 'b': pd.timedelta_range('1s', periods=3)})\n\ndf\nOut[63]: \n   a               b\n0  2 0 days 00:00:01\n1  3 1 days 00:00:01\n2  4 2 days 00:00:01\n\ndf.mean()\nOut[65]: \na                  3\nb    1 days 00:00:01\ndtype: object\n\ndf.mean(numeric_only=True)\nOut[64]: \na    3.0\ndtype: float64\n\n```\n",
      "The input file for my dataframe was constructed in a stupid way (by me...): several similar data sources were concatenated so I could process their averages all at once instead of running the script N times.  The concatenation meant that each group had its header repeated (except the first, which I'd edited manually to properly name the column; that column was a mangling of the source filename inserted at concatenation time).  So you get a data set like this:\n\n```\nsource  score \nalpha   2 \nalpha   3 \nalpha   2 \nbeta    score \nbeta    9 \nbeta    8 \nbeta    7 \ngamma   score \ngamma   4 \ngamma   4 \ngamma   1 \n```\n\nThis snippet:\n\n```\nimport pandas as pd\n\nall_scores = pd.read_csv(\"scores_for_averaging.csv\", delim_whitespace=True)\n\nexperiments = all_scores['source'].unique()\n\nfor each in experiments:\n    exp_slice = all_scores.loc[all_scores['source'] == each]\n    #print each, exp_slice['score'].mean(numeric_only=True) #fails: NotImplementedError: Series.mean does not implement numeric_only.\n    #print each, exp_slice['score'].mean() #fails: TypeError: Could not convert score987 to numeric\n```\n\nfailed because mean() couldn't accept numeric_only to throw out the spurious extra header line for beta, gamma, etc.  I just reprocessed my input to not have the header line repeated and then it worked fine.  I guess the problem is that the documentation and the code don't match?\n",
      "Thanks, just curious what the expected use was.  Yes, the documentation/method should be updated to match, just tricky to actually do in this case (PR welcome!).\n\nFYI,  for a conversion like this (assuming you actually do have a valid mixed type object array), the function you likely want is `to_numeric`\n\n```\npd.to_numeric(exp_slice['score'], errors='coerce').mean()\n```\n",
      "I suppose this could be better documented, but the arg is there for consistency with DataFrame. It really doesn't do anything as a Series is a single dtyped object. Either you get all elements or None (even if mixed). We don't deeply introspect mixed (or object) things.\n"
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "labeled",
      "renamed",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "labeled",
      "commented",
      "cross-referenced",
      "milestoned"
    ],
    "changed_files": 1,
    "additions": 6,
    "deletions": 6,
    "changed_files_list": [
      "pandas/core/generic.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10486,
    "reporter": "DSLituiev",
    "created_at": "2015-07-01T23:50:49+00:00",
    "closed_at": "2016-05-07T17:12:30+00:00",
    "resolver": "tsstchoi",
    "resolved_in": "881a690c370c411c555e10ea7665688fd9014912",
    "resolver_commit_num": 0,
    "title": "BUG: query with invalid dtypes should fallback to python engine",
    "body": "The following call of `df.query()` produces an error:\n\n\n\nin pandas 0.15.2\n\n\n",
    "labels": [
      "Bug",
      "Dtypes",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "well, `numexpr` does not support this, nor are string ops actually passed. You should simply use regular indexing\n\n`df[df.gene == \"Actb\"]`\n\nI suppose this is a bug in that this should fall back to the python engine.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "renamed",
      "labeled",
      "labeled",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 1,
    "additions": 1,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10530,
    "reporter": "JonasAbernot",
    "created_at": "2015-07-08T16:20:46+00:00",
    "closed_at": "2015-08-15T22:41:03+00:00",
    "resolver": "captainsafia",
    "resolved_in": "433602022bfbb9ee2424b77694c60f276065dee8",
    "resolver_commit_num": 1,
    "title": "BUG: 'base' argument when resampling TimedelataIndex has no effect",
    "body": "\n\nThe resampled index is still aligned to 0. \nThe base argument seems to never be used, it doesn't raise any error when passing `base=1223456879`, or `base='gjhmljpouj'`, or even `base=df`\n",
    "labels": [
      "Enhancement",
      "Difficulty Novice",
      "Timedelta",
      "Effort Low",
      "Resample"
    ],
    "comments": [
      "this is not implemented. I don't think very difficult to do this as its just a freq shift.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "labeled",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 22,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/whatsnew/v0.17.0.txt",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10536,
    "reporter": "artemyk",
    "created_at": "2015-07-09T18:16:36+00:00",
    "closed_at": "2016-04-18T19:55:34+00:00",
    "resolver": "sinhrks",
    "resolved_in": "26782a9bdefb82670859490c8d008048f9f22fb3",
    "resolver_commit_num": 277,
    "title": "Concat does not work for sparse series",
    "body": "Concatenating two sparse series does not return sparse data structures as expected:\n\n\n\nThe above _does_ work correctly for SparseDataFrames.\n",
    "labels": [
      "Bug",
      "Reshaping",
      "Sparse"
    ],
    "comments": [
      "xref #10531 \n"
    ],
    "events": [
      "renamed",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 13,
    "additions": 474,
    "deletions": 289,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/categorical.py",
      "pandas/core/common.py",
      "pandas/core/internals.py",
      "pandas/core/reshape.py",
      "pandas/indexes/base.py",
      "pandas/sparse/array.py",
      "pandas/sparse/tests/test_series.py",
      "pandas/tools/merge.py",
      "pandas/tseries/common.py",
      "pandas/tseries/index.py",
      "pandas/tseries/tdi.py",
      "pandas/types/concat.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10560,
    "reporter": "mcwitt",
    "created_at": "2015-07-13T05:12:40+00:00",
    "closed_at": "2016-04-03T14:20:30+00:00",
    "resolver": "sinhrks",
    "resolved_in": "02478d097692509e4848cf1ffbe2c8ca7fdafe42",
    "resolver_commit_num": 270,
    "title": "TypeError in SparseSeries.__repr__ when series longer than max_rows",
    "body": "\n\nSet `max_rows=3` for demonstration, but occurs also for the default `max_rows=60`.\n",
    "labels": [
      "Output-Formatting",
      "Sparse",
      "Bug",
      "Prio-medium",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      " FIX : If you just create the SparseSeries in a script without printing, it will work.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "labeled",
      "labeled",
      "labeled",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 11,
    "additions": 361,
    "deletions": 170,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/io/tests/test_pickle.py",
      "pandas/sparse/array.py",
      "pandas/sparse/frame.py",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_array.py",
      "pandas/sparse/tests/test_indexing.py",
      "pandas/sparse/tests/test_list.py",
      "pandas/sparse/tests/test_sparse.py",
      "pandas/tests/test_format.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10567,
    "reporter": "iyer",
    "created_at": "2015-07-14T14:50:06+00:00",
    "closed_at": "2016-08-12T15:44:54+00:00",
    "resolver": "sinhrks",
    "resolved_in": "29d9e24f4c778b0c9ebe9288bfc217808d2c6edb",
    "resolver_commit_num": 377,
    "title": "DataFrame combine_first() loses timezone information for datetime columns",
    "body": "xref addl example in #13650 \n\ncombine_first() loses timezone information for datetime columns\n\n\n",
    "labels": [
      "Bug",
      "Timezones",
      "Internals",
      "Indexing",
      "MultiIndex",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "this is actually some older code `combine` which really needs to be pushed into the internal classes, e.g. see this issue #3025 so a bit non-trivial. that said, pull-requests are welcome!\n",
      "Thanks. I'll await the change when it happens\nIn the meantime I'll consume the DATE in my index, so as to work around the issue\n",
      "well pull-requests are always welcome. There are a ton of issues. So this will prob not be addressed for quite some time.\n",
      "@jreback Could you give some instructions on how to fix this bug? \n",
      "well, this needs to be pushed to the block manager (e.g. need a method `Block.combine`), then all of this dtype handling can be dispatched via the internal block types. Further https://github.com/pydata/pandas/pull/10477 will allow this to handle nicely. \n\nDoing it now is going to be a bit hacky.\n\nSo if you want to look to start moving it internally as in #3025 would be a good start.\n",
      "FWIW, I noticed that, in certain cases (i.e., when you are well aware of the TZs), this workaround can be used:\n\n``` python\nIn [16]: n['ts'].index.tz\nOut[16]: <DstTzInfo 'America/Los_Angeles' PST-1 day, 16:00:00 STD>\n\nIn [17]: n['ev'].index.tz\nOut[17]: <DstTzInfo 'America/Los_Angeles' LMT-1 day, 16:07:00 STD>\n\nIn [18]: n['ts'].combine_first(n['ts']).index.tz\nOut[18]: <DstTzInfo 'America/Los_Angeles' PST-1 day, 16:00:00 STD>\n\nIn [19]: n['ts'].combine_first(n['ev']).index.tz\nOut[19]: <UTC>\n\nIn [20]: n['ts'].combine_first(n['ev']).tz_convert('America/Los_Angeles').index.tz\nOut[20]: <DstTzInfo 'America/Los_Angeles' LMT-1 day, 16:07:00 STD>\n```\n\n**Warning:** this is my first dig into pandas.\n",
      "This looks to be fixed on master (I don't look into detail, but maybe by changes in `concat_compat`?)\n\nAdding tests.\n",
      "Ah, even though tz is preserved, datetimes are incorrectly shifts (maybe the same as #12619). \n\n```\ndts1 = pd.DatetimeIndex(['2011-01-01', 'NaT', '2011-01-03', '2011-01-04'], tz='US/Eastern')\ndf1 = pd.DataFrame({'DATE': dts1}, index=[1, 3, 5, 7])\ndts2 = pd.DatetimeIndex(['2012-01-01', '2012-01-02', '2012-01-03'], tz='US/Eastern')\ndf2 = pd.DataFrame({'DATE':dts2}, index=[2, 4, 5])\ndf = df1.combine_first(df2)\ndf\n#                        DATE\n# 1 2011-01-01 05:00:00-05:00\n# 2 2012-01-01 05:00:00-05:00\n# 3                       NaT\n# 4 2012-01-02 05:00:00-05:00\n# 5 2011-01-03 05:00:00-05:00\n# 7 2011-01-04 05:00:00-05:00\n```\n",
      "Note that the example at the top is not really showing the issue (that seems to work in master)\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "commented",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 7,
    "additions": 410,
    "deletions": 227,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/frame.py",
      "pandas/tests/frame/test_combine_concat.py",
      "pandas/tests/frame/test_operators.py",
      "pandas/tests/types/test_cast.py",
      "pandas/types/cast.py",
      "pandas/types/common.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10571,
    "reporter": "tzinckgraf",
    "created_at": "2015-07-14T18:40:03+00:00",
    "closed_at": "2016-03-23T17:56:45+00:00",
    "resolver": "jreback",
    "resolved_in": "85f8cf74b733a6782730f17b4a68fa4b028f2013",
    "resolver_commit_num": 3953,
    "title": "pd.concat DataFrames with an all None object column converts None to nan",
    "body": "For example,\n\n\n\nI have found that this is a direct result of line 4102-4103 in core/internals.py\n\n\n",
    "labels": [
      "Reshaping",
      "Dtypes"
    ],
    "comments": [
      "np.nan are the missing value marker\n\nwhy would this be any different?\n\nusing None while possible is not guaranteed under most transformations \n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 7,
    "additions": 127,
    "deletions": 23,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/indexing.py",
      "pandas/core/internals.py",
      "pandas/tests/indexing/test_indexing.py",
      "pandas/tests/test_format.py",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10598,
    "reporter": "jreback",
    "created_at": "2015-07-16T00:19:18+00:00",
    "closed_at": "2016-06-24T00:01:47+00:00",
    "resolver": "nparley",
    "resolved_in": "ab116a7139c2c377555650b001b461a2b1eaf15c",
    "resolver_commit_num": 2,
    "title": "CI: migrate to travis container infrastructure",
    "body": "-ci.com/user/migrating-from-legacy/\n\ntrivial I think to remove the `sudo`. but prob have to change some stuff (as we\nuse _some_ apt stuff)\n\ncurrently we externally cache, so that would have to be figure out (at a later date is ok)\n",
    "labels": [
      "CI"
    ],
    "comments": [
      "I haven't seen any major improvements in testing speed when using this.  I switched pandas-datareader to this and the build time dropped form 3 to 1.5, which is something for a small project.\n\nIn terms of buidl time, I don't think it woudl do much for pandas.  However, if you get faster access to build servers then it could be worth it - the backlog of PRs can get pretty big at times.\n",
      "@bashtage thanks for the fb.\n\nI have actually been noticiing that travis _seems_ to be building faster lately (on the old infrastructure).\n\nI think main benefit is we _could_ use the travis caching (as opposed to the only pyx build cache that we use from iron-cache).\n",
      "Might be a good reference: https://github.com/libdynd/libdynd/pull/481\nalso scipy recently changed.\n",
      "It seems that the travis job `27_slow_nnet_LOCALE` is having problems with the container-based infrastructure. Also travis is not detecting the use of sudo and is running in containers on my fork at least. Compare https://travis-ci.org/nparley/pandas/jobs/124695152 and https://travis-ci.org/nparley/pandas/jobs/124778492. I have created a PR to add sudo: required for the time being.\n"
    ],
    "events": [
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "cross-referenced",
      "commented",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 11,
    "additions": 283,
    "deletions": 210,
    "changed_files_list": [
      ".travis.yml",
      "ci/before_install_travis.sh",
      "ci/check_cache.sh",
      "ci/install-2.7_NUMPY_DEV.sh",
      "ci/install-3.5_NUMPY_DEV.sh",
      "ci/install_travis.sh",
      "ci/prep_ccache.sh",
      "ci/prep_cython_cache.sh",
      "ci/submit_ccache.sh",
      "ci/submit_cython_cache.sh",
      "pandas/util/print_versions.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10623,
    "reporter": "kawochen",
    "created_at": "2015-07-19T04:45:58+00:00",
    "closed_at": "2016-02-16T14:50:50+00:00",
    "resolver": "kawochen",
    "resolved_in": "3358afc7157ac67a29fa94a8c81f84d497e994e7",
    "resolver_commit_num": 42,
    "title": "DEPR: msgpack encode 'items'",
    "body": "At some point deprecate `items` in `msgpack.encode()` for blocks. xref #10527 \n",
    "labels": [
      "Deprecate",
      "Msgpack"
    ],
    "comments": [
      "This is just for forward compat, so can prob just remove in 0.18.0\n",
      "@kawochen  you want to do a PR for this?\n"
    ],
    "events": [
      "cross-referenced",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 13,
    "additions": 370,
    "deletions": 246,
    "changed_files_list": [
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/common.py",
      "pandas/core/internals.py",
      "pandas/io/packers.py",
      "pandas/io/tests/data/legacy_msgpack/0.17.1/0.17.1_AMD64_windows_2.7.11.msgpack",
      "pandas/io/tests/data/legacy_msgpack/0.17.1/0.17.1_AMD64_windows_3.5.1.msgpack",
      "pandas/io/tests/data/legacy_msgpack/0.17.1/0.17.1_x86_64_darwin_2.7.11.msgpack",
      "pandas/io/tests/data/legacy_msgpack/0.17.1/0.17.1_x86_64_darwin_3.5.1.msgpack",
      "pandas/io/tests/data/legacy_msgpack/0.17.1/0.17.1_x86_64_linux_2.7.11.msgpack",
      "pandas/io/tests/data/legacy_msgpack/0.17.1/0.17.1_x86_64_linux_3.4.4.msgpack",
      "pandas/io/tests/generate_legacy_storage_files.py",
      "pandas/io/tests/test_packers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10648,
    "reporter": "ebolyen",
    "created_at": "2015-07-21T18:02:20+00:00",
    "closed_at": "2016-04-03T14:24:21+00:00",
    "resolver": "sinhrks",
    "resolved_in": "2d134107ffc14c40ac9f7bd425ed55dfb7ba598d",
    "resolver_commit_num": 269,
    "title": "to_dense does not preserve dtype in SparseArray",
    "body": "This isn't a huge deal, but it seems a little odd:\n\n\n\nI would have expected `d` to retain the dtype of `bool`. I can cast down, but I am still wasting 7 bytes per element in the process. \n",
    "labels": [
      "Bug",
      "Dtypes",
      "Sparse"
    ],
    "comments": [
      "## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.6.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.13.0-57-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.16.2\nnose: 1.3.7\nCython: 0.22.1\nnumpy: 1.9.2\nscipy: 0.15.1\nstatsmodels: None\nIPython: 3.2.0\nsphinx: 1.2.2\npatsy: None\ndateutil: 2.4.2\npytz: 2015.4\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.4.3\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\n",
      "prob a bug, not really well tested and not really a lot of dev support in sparse \npull requests are welcome\n",
      "I may be speaking too soon, but after skimming the code for this, it looks pretty simple to fix. I'll work on a PR!\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "renamed",
      "commented",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 33,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/sparse/array.py",
      "pandas/sparse/tests/test_array.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10690,
    "reporter": "shishirsharma",
    "created_at": "2015-07-28T18:40:45+00:00",
    "closed_at": "2016-07-08T15:16:46+00:00",
    "resolver": "haleemur",
    "resolved_in": "ba82b511c76d87421c8900348efebe4577548ec6",
    "resolver_commit_num": 0,
    "title": "to_html() formatters does not work for objects with 'datetime64[ns]' type",
    "body": "I am using a to_html() to convert a dataframe into a table. i have various datatypes in Dataframe including float, int, datetime and timedelta. \n\nI am using formatters to control outputs, It seems to be working for int and float but not for datetime. \n\n\n",
    "labels": [
      "Timeseries",
      "IO HTML"
    ],
    "comments": [
      "pls show a small but minimally reproducible example, along with `pd.show_versions()`\n",
      "I encountered the same problem, and then did some digging. The issue is reproduced with the following example.\n\n```\nimport pandas as pd\ndf = pd.DataFrame({'months': ['2015-01-01', '2015-10-10', '2016-01-01']})\ndf.months = pd.to_datetime(df.months)\ndf.to_html(formatters={'months': lambda x: x.strftime('%Y-%m')})\n```\n\nthis prints:\n\n```\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>months</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2015-01-01</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2015-10-10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2016-01-01</td>\n    </tr>\n  </tbody>\n</table>\n```\n\nAs you can see, the dates are not formatted using the specified function.\n\nThe class `pandas.formats.format.Datetime64Formatter` is responsible for the formatting, however it does nothing if a `formatter` is passed to it's init. Browsing through the file, I found a class `pandas.formats.format.Datetime64TZFormatter` which inherits from Datetime64Formatter, which does respect `formatter`.\n\n```\nfrom pandas.formats.format import Datetime64Formatter, Datetime64TZFormatter\n\nDatetime64TZFormatter(df.months, formatter=lambda x: x.strftime('%Y-%m')).get_result()\n#prints\n['2015-01', '2015-10, '2016-01']\n\nDatetime64Formatter(df.months, formatter=lambda x: x.strftime('%Y-%m')).get_result()\n#prints\n['2015-01-01', '2015-10-10', '2016-01-01']\n```\n\nI propose to change method `Datetime64Formatter._format_strings` from \n\n```\n    def _format_strings(self):\n        \"\"\" we by definition have DO NOT have a TZ \"\"\"\n\n        values = self.values\n        if not isinstance(values, DatetimeIndex):\n            values = DatetimeIndex(values)\n\n        fmt_values = format_array_from_datetime(\n            values.asi8.ravel(),\n            format=_get_format_datetime64_from_values(values,\n                                                      self.date_format),\n            na_rep=self.nat_rep).reshape(values.shape)\nreturn fmt_values.tolist()\n```\n\nto\n\n```\n    def _format_strings(self):\n        \"\"\" we by definition have DO NOT have a TZ \"\"\"\n\n       if self.formatter is not None:\n           return [formatter(x) for x in values]\n\n        values = self.values\n        if not isinstance(values, DatetimeIndex):\n            values = DatetimeIndex(values)\n\n        fmt_values = format_array_from_datetime(\n            values.asi8.ravel(),\n            format=_get_format_datetime64_from_values(values,\n                                                      self.date_format),\n            na_rep=self.nat_rep).reshape(values.shape)\nreturn fmt_values.tolist()\n```\n\nHere's the output of `pd.show_versions()` on my system.\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.3.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_CA.UTF-8\n\npandas: 0.18.1\nnose: None\npip: 8.1.2\nsetuptools: 23.0.0\nCython: 0.24\nnumpy: 1.11.0\nscipy: 0.17.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.2.0\nsphinx: None\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: 2.6.0\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.13\npymysql: None\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\njinja2: 2.8\nboto: None\npandas_datareader: None\n```\n"
    ],
    "events": [
      "commented",
      "labeled",
      "commented",
      "cross-referenced",
      "milestoned",
      "labeled",
      "labeled",
      "unlabeled"
    ],
    "changed_files": 3,
    "additions": 133,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/formats/format.py",
      "pandas/tests/formats/test_format.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10712,
    "reporter": "sinhrks",
    "created_at": "2015-07-31T15:50:00+00:00",
    "closed_at": "2016-02-27T15:09:04+00:00",
    "resolver": "sinhrks",
    "resolved_in": "8c41e62089549bb9f3a1387275b3d69d6994f5f3",
    "resolver_commit_num": 255,
    "title": "BUG: Series.dt ops reset name",
    "body": "`DatetimeIndex.tz_localize` preserves name.\n\n\n\nBut `Series` doesn't. I don't check all methods  which can return `Series` yet.\n\n\n",
    "labels": [
      "Bug",
      "Timeseries"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "cross-referenced",
      "cross-referenced",
      "milestoned"
    ],
    "changed_files": 1,
    "additions": 69,
    "deletions": 55,
    "changed_files_list": [
      "pandas/tests/series/test_datetime_values.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10758,
    "reporter": "jreback",
    "created_at": "2015-08-06T12:28:09+00:00",
    "closed_at": "2016-05-20T14:16:17+00:00",
    "resolver": "thejohnfreeman",
    "resolved_in": "72164a8471be0e9f41476ae094a3b46479c7a6d2",
    "resolver_commit_num": 2,
    "title": "API/COMPAT: add kw args to Timestamp constructor",
    "body": "from [SO](-to-create-a-pandas-timestamp-object)\n\n`Timestamp(year, month, day, second, millisecond, microsecond, nanosecond)`\n\nas optional keyword args might be nice for compat with `datetime.datetime`. We do something similar for `Timedelta` construction.\n",
    "labels": [
      "Enhancement",
      "Timeseries",
      "Difficulty Novice",
      "API Design",
      "Effort Low"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 114,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/timeseries.rst",
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/tseries/tests/test_tslib.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10778,
    "reporter": "blink1073",
    "created_at": "2015-08-09T15:55:45+00:00",
    "closed_at": "2016-04-26T18:54:47+00:00",
    "resolver": "Komnomnomnom",
    "resolved_in": "37a7e69cea0ba518adc4328aac92ac7d925ed843",
    "resolver_commit_num": 41,
    "title": "df.to_json segfaults with categorical column types",
    "body": "This code from the Categorical tutorial:\n\n\n\nWill segfault when `df.to_json` is called.  Tested on:\n\n\n",
    "labels": [
      "JSON",
      "Categorical",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "If I convert using `df['B'] = df['B'].astype(str)`, then `df.to_json` works fine.\n",
      "xref #10321 \n\nThis has not been implemented. pull-requests are welcome.\n",
      ":+1: \n",
      "My apologies if this is simply extra noise, but I think I came across the same issue in (maybe) a slightly different situation. Thought this might help to give more insight into this issue.\n\nWith me this happens when trying to convert a DataFrame with binned data into JSON.\n\nThe kernel reports back the following:\n\nJan 30 18:18:04 debian kernel: [ 1632.371968] python[24794]: segfault at ffffffffffffffff ip 00007f58f570655d sp 00007ffe13a8c410 error 7 in multiarray.so[7f58f55bf000+1d9000]\n\nVersion info:\n- Linux Debian Jessie x86_64\n- Python 2.7.10 (GCC 4.9.2)\n- NumPy 1.10.4\n- Pandas 0.17.1\n\nGreatly simplified piece of code which reproduces the error:\n\n```\nimport pandas\n\ndef calculate_age_dist(df):\n    # Setup the bins to divide. Setting a final bin\n    # of 200 to mimic 60+ behavior.\n    age_ranges = [0,2,10,20,30,40,50,60,200]\n    age_ranges_labels = ['0-1', '2-9', '10-19', '20-29',\n                         '30-39', '40-49', '50-59', '60+']\n\n    # Divide the data into the bins. Using \"right=False\" to\n    # exclude the upper range (so 30 is only included in bin 30-40\n    # and not in bin 20-30).\n    df['age_cat'] = pandas.cut(x = df['age'],\n                               bins = age_ranges,\n                               right = False,\n                               labels = age_ranges_labels)\n\n    # Leave only the top ten sports per age group\n    # and convert the whole back to a DataFrame.\n    age_dist = age_dist.groupby('age_cat').head(10).reset_index(drop=True)\n\n    return age_dist\n\n# The data.\ndf = pandas.DataFrame(data = {'age': [55,20,30,40,50,60,65,23],\n                              'sport': ['soccer', 'baseball', 'soccer', 'football',\n                                        'swimming', 'tennig', 'swimming', 'baseball']},\n                      columns = {'age', 'sport'})\n\n# The call which causes the segfault.\nprint calculate_age_dist(df).to_json()\n```\n\nConverting the \"age_cat\" column with `df['age_cat'] = df['age_cat'].astype(str)` does indeed circumvent the segfault.\n"
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "labeled",
      "labeled",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 9,
    "additions": 284,
    "deletions": 42,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/io/tests/json/__init__.py",
      "pandas/io/tests/json/data/tsframe_iso_v012.json",
      "pandas/io/tests/json/data/tsframe_v012.json",
      "pandas/io/tests/json/test_json_norm.py",
      "pandas/io/tests/json/test_pandas.py",
      "pandas/io/tests/json/test_ujson.py",
      "pandas/src/ujson/python/objToJSON.c",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10789,
    "reporter": "max-sixty",
    "created_at": "2015-08-10T16:43:54+00:00",
    "closed_at": "2016-03-06T14:57:59+00:00",
    "resolver": "evectant",
    "resolved_in": "547c784c927a1affadc551d368e5ee0b2a1d32cb",
    "resolver_commit_num": 0,
    "title": "ENH: Allow exponential weighing functions to specify alpha, in addition to span / com / halflife",
    "body": "Currently the exponential functions take one of three arguments to specify the length of backhistory. Each of these numerically converts to alpha. Outlined here: -docs/stable/computation.html#exponentially-weighted-moment-functions\n\nIs there a reason we don't allow users to just pass in `alpha`? This is how I think about weighing some of the time.\n\nI'm happy to do a PR in time; this issue is to solicit feedback.\n",
    "labels": [
      "Difficulty Novice",
      "API Design",
      "Numeric",
      "Effort Low"
    ],
    "comments": [
      "This would be useful - I also find it strange that alpha is always indirectly parameterized.\n",
      "this would be a pretty easy add. I don't know the original reason why this is, nor do I see the harm. As long as the docs are updated, ok by me.\n\nBut maybe need a note about why so many ways to specify the same thing (e.g. because like to think about it differently, maybe their is a reference somewhere, e.g. wikipedia or somesuch)\n"
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 6,
    "additions": 171,
    "deletions": 76,
    "changed_files_list": [
      "doc/source/computation.rst",
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/generic.py",
      "pandas/core/window.py",
      "pandas/stats/moments.py",
      "pandas/tests/test_window.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10806,
    "reporter": "jreback",
    "created_at": "2015-08-12T15:21:36+00:00",
    "closed_at": "2016-07-21T15:05:26+00:00",
    "resolver": "IamJeffG",
    "resolved_in": "bb6b5e54edaf046389e8cce28e7cd27ee87f5fcc",
    "resolver_commit_num": 1,
    "title": "BUG/ENH: sort_values(by=index_label, axis=1)",
    "body": "xref #10726 \n\nThis is possible, just not implemented\n\n\n",
    "labels": [
      "Enhancement",
      "Reshaping",
      "Difficulty Intermediate",
      "Effort Low",
      "Prio-low"
    ],
    "comments": [
      "Old issue was #2940, will close that one\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "labeled",
      "unlabeled",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 45,
    "deletions": 10,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/tests/frame/test_sorting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10862,
    "reporter": "jreback",
    "created_at": "2015-08-20T14:08:34+00:00",
    "closed_at": "2016-03-25T13:30:15+00:00",
    "resolver": "jreback",
    "resolved_in": "4e7974aef2b114c62f010c3c77fe80ed8ac41aee",
    "resolver_commit_num": 3955,
    "title": "TST: create legacy_pickles/msgpacks for older versions",
    "body": "using \n\nits quite easy to produce a library of pickles/msgpacks from prior versions that we then store to test for back-compat with the current version (IOW, these have generated tests which run).\n\nIdeally like to fill this out and regenerate all of the older ones (you generate using that released version). May have to add some skips for features which didn't exist in particular versions.\n- if we can do this for multi-platforms is good.\n- updating the contributing docs would be good (on how to do this)\n",
    "labels": [
      "Testing",
      "Prio-medium",
      "Difficulty Novice",
      "Msgpack",
      "Compat",
      "Effort Medium"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files": 10,
    "additions": 0,
    "deletions": 0,
    "changed_files_list": [
      "pandas/io/tests/data/legacy_msgpack/0.18.0/0.18.0_AMD64_windows_2.7.11.msgpack",
      "pandas/io/tests/data/legacy_msgpack/0.18.0/0.18.0_AMD64_windows_3.5.1.msgpack",
      "pandas/io/tests/data/legacy_msgpack/0.18.0/0.18.0_x86_64_darwin_2.7.11.msgpack",
      "pandas/io/tests/data/legacy_msgpack/0.18.0/0.18.0_x86_64_darwin_3.5.1.msgpack",
      "pandas/io/tests/data/legacy_pickle/0.17.1/0.17.1_AMD64_windows_2.7.11.pickle",
      "pandas/io/tests/data/legacy_pickle/0.17.1/0.17.1_x86_64_darwin_2.7.11.pickle",
      "pandas/io/tests/data/legacy_pickle/0.18.0/0.18.0_AMD64_windows_2.7.11.pickle",
      "pandas/io/tests/data/legacy_pickle/0.18.0/0.18.0_AMD64_windows_3.5.1.pickle",
      "pandas/io/tests/data/legacy_pickle/0.18.0/0.18.0_x86_64_darwin_2.7.11.pickle",
      "pandas/io/tests/data/legacy_pickle/0.18.0/0.18.0_x86_64_darwin_3.5.1.pickle"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10904,
    "reporter": "cglwn",
    "created_at": "2015-08-25T22:25:12+00:00",
    "closed_at": "2016-05-07T14:42:56+00:00",
    "resolver": "gliptak",
    "resolved_in": "b2009e6284a078da768c4d9ec65d18ede83c5513",
    "resolver_commit_num": 24,
    "title": "pandas.Series.tolist should convert Series contents from numpy datatypes to native Python datatypes",
    "body": "Noticed this in Pandas v0.16.1 when JSON  serializing a large nested dictionary containing mixes of arrays, numpy arrays, and pandas series. Calling tolist() on numpy arrays allows the standard library JSON serializer to serialize them, but this is not the case for pandas.\n\nnumpy.array.tolist converts all of the array's contents to native Python datatypes. For compatibility with numpy, pandas.Series.tolist should also convert its contents to native Python datatypes. Currently, the list returned by pandas contains elements which have types like numpy.int64 or numpy.float64.\n\nFor example:\n\n\n",
    "labels": [
      "Enhancement",
      "Prio-low",
      "Dtypes",
      "Compat",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "this is a bit tricker than it looks. pandas correctly handles boxing of dtypes (e.g. to Timestamp/categorical and such). So this is really just `list(self)`. I suppose you could box integer/floats. Though IMHO this is a pretty useless method and really just for numpy compat.\n\nIf you'd like to do a pull-request would prob accept.\n\nyou should simply use `.to_json()`\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 88,
    "deletions": 32,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/core/series.py",
      "pandas/tests/series/test_io.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10911,
    "reporter": "cstegemann",
    "created_at": "2015-08-27T10:31:02+00:00",
    "closed_at": "2016-04-22T15:20:13+00:00",
    "resolver": "gfyoung",
    "resolved_in": "5688d2771a00675bb530fc2d61fb8e356d8e134d",
    "resolver_commit_num": 12,
    "title": "read_csv parse issue with newline in quoted items combined with skiprows",
    "body": "Now I don't know if this is known or the desired behaviour but when I try to read certain rows from a large file that uses \"~\" (tilde) as a quotechar and use skiprows at the same time, the parser screws up as follows:\nNote: I use \"\" in the output even though that isn't shown, if I didn't the markup would become messed up - sorry...\n\n\n\nwhile the output I wish to get would be in this artificial case:\n\n\n\nit seems when skipping rows, the parser ignores custom quotation - which in this case is undesired from my point of view. \n\nEDIT: It might well be that in the quoted texts newlines are not always \\n but sometimes also \\r.\n\nEDIT2 (31.8.):\nThe lineterminator fix fails as far as I can see with the following example:\n\n\n\nThe problem is that there is a \"text\"-column in the csv with html-formatted textblocks as content. However, there is no saying what kind of newline the creators of the html used originally and the textblocks stem from different sources.\nI might also add that it respects the quoting perfectly if one does not use \"skiprows\".\n\nversioninfo: \n\n\n",
    "labels": [
      "CSV",
      "Usage Question"
    ],
    "comments": [
      "This looks like what you want. `quotechar` doesn't apply to line endings (which are normally a different character, e.g. `\\n`\n\n```\nIn [2]: pd.read_csv(StringIO('a,b,c\\r~a\\n b~,~e\\n d~,~f\\n f~\\r1,2,~12\\n 13\\n 14~'), quotechar='~', skiprows=range(1,2), lineterminator='\\r' )\nOut[2]: \n   a  b             c\n0  1  2  12\\n 13\\n 14\n```\n",
      "That would work for this example, but what if the quoted texts in the csv, coming from many different sources, use not only \\n but sometimes also \\r as a newline?\n",
      "well if u show an example that would help\n",
      "I added an example to the original post\n",
      "A combination of universal newline mode and the python parsing engine seems to work\n\n```\n# create a text file with mixed newlines\nwith open('testunl.txt', 'wb') as tstunl:\n    tstunl.write('a,b,c\\r~a\\n b~,~e\\n d~,~f\\n f~\\r1,2,~12\\n 13\\n 14~')\n\nwith open('testunl.txt', 'U') as tstunl:\n    print(pd.read_csv(tstunl, quotechar=\"~\", skiprows=range(1,2), engine='python'))\n\n   a  b             c\n0  1  2  12\\n 13\\n 14\n\nwith open('testunl.txt', 'wb') as tstunl:\n    tstunl.write('Text,url\\r~example\\r sentence\\r one~,url1\\r~example\\n sentence\\n two~,url2')\n\nwith open('testunl.txt', 'U') as tstunl:\n    print(pd.read_csv(tstunl, quotechar=\"~\", skiprows=range(1,2), engine='python'))\n\n                       Text   url\n0  example\\n sentence\\n two  url2\n```\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned"
    ],
    "changed_files": 4,
    "additions": 129,
    "deletions": 20,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/parser/tokenizer.c",
      "pandas/src/parser/tokenizer.h"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 10972,
    "reporter": "TomAugspurger",
    "created_at": "2015-09-02T18:00:54+00:00",
    "closed_at": "2017-02-27T21:38:08+00:00",
    "resolver": "jreback",
    "resolved_in": "251826f0861159160bd1d51eafadb6e0b4161f77",
    "resolver_commit_num": 4249,
    "title": "BUG: output of a transform is cast to dtype of input",
    "body": "xref #11444, #13046 for addtl tests\n\n\n\nI expected a float. No idea how difficult this will be so I marked it for 0.18. I won't have time to get to it any earlier, but if someone else wants to...\n",
    "labels": [
      "Groupby",
      "Dtypes",
      "Prio-medium",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "This is only a problem with transform; apply does this kind of inference\n\n```\nIn [6]: df.groupby('a').b.apply(lambda x: x.dt.dayofweek - x.dt.dayofweek.mean()).head()\nOut[6]: \n0    0.214286\n1    1.054795\n2    1.837209\n3    2.837209\n4   -3.162791\ndtype: float64\n```\n",
      "Yeah, I've switched to apply for now. My actual case was transforming an integer to categorical (which raised an exception).\n",
      "doesn't make sense to `transform` int->cat, rather just `.astype`\n",
      "Not that simple in my case. Have to groupby a level and do some shift / diff logic to get my result.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 57,
    "deletions": 18,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/groupby.py",
      "pandas/tests/groupby/test_filters.py",
      "pandas/tests/groupby/test_transform.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11043,
    "reporter": "shoyer",
    "created_at": "2015-09-10T06:01:50+00:00",
    "closed_at": "2016-02-12T03:10:54+00:00",
    "resolver": "Moisan",
    "resolved_in": "2ce897f99b82b756926806a35473e3db0f3f4822",
    "resolver_commit_num": 5,
    "title": "DOC: consider updating visualization.rst to use plot submethods rather than kind",
    "body": "Another followup to #9321:\n\nMost of the documentation examples in visualization.rst (the [main plotting docs](-docs/stable/visualization.html)) currently use the `kind` argument. We should probably switch these to use the plotting submethods instead, to push users in that direction.\n",
    "labels": [
      "Docs",
      "Visualization",
      "Difficulty Novice",
      "Effort Low",
      "Prio-medium"
    ],
    "comments": [
      "Question. I'm using analytics.quant-platform. Plotting submethod does not give me a graph just object. Kind argument works? \n\nIn [38]:\nimport pandas as pd\nimport numpy as np\n%matplotlib inline\n\ndf = pd.DataFrame(np.random.rand(10, 2), columns=['a', 'b']).plot(kind='barh')\n\n![image](https://cloud.githubusercontent.com/assets/4602526/12283523/0d9b9d4e-b975-11e5-9c8b-f238fd516319.png)\n\nIn [41]:\nimport pandas as pd\nimport numpy as np\n%matplotlib inline\n\ndf = pd.DataFrame(np.random.rand(10, 2), columns=['a', 'b'])\n\ndf.plot.bar\n\nOut[38]:\nbound method FramePlotMethods.bar of <pandas.tools.plotting.FramePlotMethods object at 0x7f04b10fa908\n",
      "@MattRijk you need to call the method `df.plot.bar()`, not `df.plot.bar`\n",
      "Alright. Works. That's weird i used .bar() a few times and nothing. Thanks\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "milestoned",
      "demilestoned",
      "labeled",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 1,
    "additions": 49,
    "deletions": 45,
    "changed_files_list": [
      "doc/source/visualization.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11082,
    "reporter": "sinhrks",
    "created_at": "2015-09-12T22:50:09+00:00",
    "closed_at": "2016-04-10T14:22:42+00:00",
    "resolver": "sinhrks",
    "resolved_in": "c03f5456676eb349deefb1ed7ec52f9dc3419ee5",
    "resolver_commit_num": 281,
    "title": "BUG: empty Series concat has no effect",
    "body": "\n",
    "labels": [
      "Bug",
      "Reshaping"
    ],
    "comments": [
      "xref #10698 prob getting excluded here (pr is #10723)\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "commented",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 91,
    "deletions": 31,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11089,
    "reporter": "parthea",
    "created_at": "2015-09-13T17:23:53+00:00",
    "closed_at": "2016-08-31T12:47:53+00:00",
    "resolver": "parthea",
    "resolved_in": "f92cd7e41cfb8ad56875e1313791f010e5efb202",
    "resolver_commit_num": 9,
    "title": "BigQuery integration tests are skipped on travis",
    "body": "Certain BigQuery integration tests are being skipped because they require a project id. \n\nValid BigQuery credentials are required to properly test the gbq module on travis.\n\nSee PR #10857 \n\n\n",
    "labels": [
      "Testing",
      "Google I/O"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 197,
    "deletions": 100,
    "changed_files_list": [
      ".travis.yml",
      "ci/requirements-2.7.pip",
      "ci/travis_gbq.json.enc",
      "pandas/io/tests/test_gbq.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11169,
    "reporter": "yarikoptic",
    "created_at": "2015-09-22T16:28:36+00:00",
    "closed_at": "2016-07-19T13:06:23+00:00",
    "resolver": "sinhrks",
    "resolved_in": "31c2e5ffa9c8008e2d84dc5ffa02f2d938a32294",
    "resolver_commit_num": 340,
    "title": "test_constructor_compound_dtypes and test_invalid_index_types fail in parse_datetime_string",
    "body": "Debian stretch/sid, cython 0.22.1-2, pandas v0.17.0rc1-92-gc6bcc99\n\n\n\n`nosetest pandas` leads to\n\n\n",
    "labels": [
      "Testing"
    ],
    "comments": [
      "yeh, the we these on windows because its actually not giving us the correct error (its in a weird encoding).\n\nI never could figure out why this works on linux/osx but not elsewhere.\n\n@yarikoptic maybe the same issue. \n\n@sinhrks \n",
      "The same as ... ?  in my case it is as non-windows as it gets (Debian GNU/Linux ;))\n",
      "what I mean is that we skip these on windows (never could really figure it out), yet it works on our travis builds (linux) and macosx. So _something_ is different. I think its some kind of encoding issue.\n",
      "Both also fail with [Nix](http://nixos.org/nix/). See #11287.\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "labeled",
      "referenced",
      "commented",
      "cross-referenced",
      "milestoned"
    ],
    "changed_files": 9,
    "additions": 74,
    "deletions": 178,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/io/parsers.py",
      "pandas/tests/indexes/test_datetimelike.py",
      "pandas/tseries/index.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tests/test_tslib.py",
      "pandas/tseries/tools.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11276,
    "reporter": "sebhahn",
    "created_at": "2015-10-09T20:30:11+00:00",
    "closed_at": "2017-04-02T22:55:23+00:00",
    "resolver": "jreback",
    "resolved_in": "cd24fa95f1781b14d35eac4953bab02691fd9d04",
    "resolver_commit_num": 4346,
    "title": "implementation of from_julian_date()",
    "body": "`pandas.DatetimeIndex` supports a method called `to_julian_date()` would it be easily possible to support a function/method doing the opposite, i.e. `from_julian_date()`?\n",
    "labels": [
      "Enhancement",
      "Timeseries"
    ],
    "comments": [
      "fyi, this is somewhat easy to do with the current `to_datetime`, just subtract the offset and specify that the unit is days.\n\n``` python\nIn [66]: julian_dates = pd.date_range('2014-1-1', periods=10).to_julian_date().values\n\nIn [67]: julian_dates\nOut[67]: \narray([ 2456658.5,  2456659.5,  2456660.5,  2456661.5,  2456662.5,\n        2456663.5,  2456664.5,  2456665.5,  2456666.5,  2456667.5])\n\nIn [68]: pd.to_datetime(julian_dates - pd.Timestamp(0).to_julian_date(), unit='D')\nOut[68]: \nDatetimeIndex(['2014-01-01', '2014-01-02', '2014-01-03', '2014-01-04',\n               '2014-01-05', '2014-01-06', '2014-01-07', '2014-01-08',\n               '2014-01-09', '2014-01-10'],\n              dtype='datetime64[ns]', freq=None)\n```\n",
      "interesting solution, thank you very much for that!\n",
      "I think this would not be unreasonable to have `pd.to_datetime(....., ,unit='julian')`\nI guess as long as making things up `DatetimeIndex.astype('datetime64[julian]')` prob a bit much\n\nWe don't have any `from_*` for this kind of thing\n",
      "Good idea! So you plan to support this in the future?\n",
      "@sebhahn if you'd like to do a PR with the `unit='julian'` I think would be reasonable to start.\n",
      "@jreback @sebhahn \nIs this enhancement being included, and is someone working on the same? If not, I would like to give it a try :-)\n",
      "@sumitbinnani I haven't started yet, so feel free to implement it :smile: \n",
      "@sebhahn: Thanks. Will soon submit a patch :-)\n"
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "referenced",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 8,
    "additions": 317,
    "deletions": 26,
    "changed_files_list": [
      "doc/source/timeseries.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/_libs/tslib.pyx",
      "pandas/tests/indexes/datetimes/test_tools.py",
      "pandas/tests/indexes/timedeltas/test_ops.py",
      "pandas/tests/scalar/test_timedelta.py",
      "pandas/tseries/tdi.py",
      "pandas/tseries/tools.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11287,
    "reporter": "FRidh",
    "created_at": "2015-10-11T12:18:30+00:00",
    "closed_at": "2016-07-19T13:06:23+00:00",
    "resolver": "sinhrks",
    "resolved_in": "31c2e5ffa9c8008e2d84dc5ffa02f2d938a32294",
    "resolver_commit_num": 340,
    "title": "TEST: failing test pandas.tseries.tests.test_frequencies.TestFrequencyInference",
    "body": "While packaging pandas 0.17 for Nix, I noticed this test to fail. I've tested with both Python 2.7 and 3.4.\n\n\n\nSince Travis does not seem to be failing I could imagine this is a Nix issue. If so, do you have any suggestion regarding dependencies of software this specific piece of code that is tested needs? \n",
    "labels": [
      "Testing",
      "Unicode"
    ],
    "comments": [
      "this is the same issue I noted here: https://github.com/pydata/pandas/issues/10822\n\nit fails on windows (so is skipped), but cannot repro on travis/macosx. Its supposed to fail parsing and a raise a `ValueError` instead in raises a `TypeError`; note that this is actually hits `dateutil`. If you'd like to dig in and see where it is failing then we can modify the test.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "cross-referenced",
      "cross-referenced",
      "milestoned"
    ],
    "changed_files": 9,
    "additions": 74,
    "deletions": 178,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/io/parsers.py",
      "pandas/tests/indexes/test_datetimelike.py",
      "pandas/tseries/index.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tests/test_tslib.py",
      "pandas/tseries/tools.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11299,
    "reporter": "jlec",
    "created_at": "2015-10-12T14:11:42+00:00",
    "closed_at": "2016-04-26T18:54:47+00:00",
    "resolver": "Komnomnomnom",
    "resolved_in": "37a7e69cea0ba518adc4328aac92ac7d925ed843",
    "resolver_commit_num": 41,
    "title": "pandas.io.tests.test_json.test_ujson.NumpyJSONTests testOdArray segfaults under python2.7/3.5 in 0.17.0",
    "body": "\n\n\n\nAbrt report: \n",
    "labels": [
      "Testing",
      "IO JSON"
    ],
    "comments": [
      "might be a locale issue. again I cannot repro this, can you see what the issue is, propose a fix?\n",
      "python-3.5 shows also the segf. python-3.4 is fine. aa94ae4 doesn't fix the issue\n",
      "the commit is pointing to something else\n\nsince u r only getting this error (and works on all other platforms / versions)\n\ncan u see if u can see where happening \n",
      "> the commit is pointing to something else\n\nThat's the one reference\n\n> since u r only getting this error (and works on all other platforms / versions)\n> can u see if u can see where happening\n\nThe best I can get is\nhttps://gist.github.com/jlec/6282f0dae0586d6749af\n",
      "I'm seeing this segfault too. It's acting like a [Heisenbug](https://en.wikipedia.org/wiki/Heisenbug). I've been testing with 0.17.1 under python2.7 and python3.4, and the only way I've been able to reliably reproduce it is with python 3.4, pandas compiled with debugging turned off (and optimizations on), and by running the whole `NumpyJSONTests` class, not just the `test0dArray` test case.\n\n```\n% nosetests-3.4 -v pandas.io.tests.test_json.test_ujson:NumpyJSONTests.testOdArray\ntestOdArray (pandas.io.tests.test_json.test_ujson.NumpyJSONTests) ... ok\n\n----------------------------------------------------------------------\nRan 1 test in 0.001s\n\nOK\n% nosetests-3.4 -v pandas.io.tests.test_json.test_ujson:NumpyJSONTests\ntestArrayNumpyExcept (pandas.io.tests.test_json.test_ujson.NumpyJSONTests) ... ok\ntestArrayNumpyLabelled (pandas.io.tests.test_json.test_ujson.NumpyJSONTests) ... ok\ntestArrays (pandas.io.tests.test_json.test_ujson.NumpyJSONTests) ... ok\ntestBool (pandas.io.tests.test_json.test_ujson.NumpyJSONTests) ... ok\ntestBoolArray (pandas.io.tests.test_json.test_ujson.NumpyJSONTests) ... ok\ntestFloat (pandas.io.tests.test_json.test_ujson.NumpyJSONTests) ... ok\ntestFloatArray (pandas.io.tests.test_json.test_ujson.NumpyJSONTests) ... ok\ntestFloatMax (pandas.io.tests.test_json.test_ujson.NumpyJSONTests) ... ok\ntestInt (pandas.io.tests.test_json.test_ujson.NumpyJSONTests) ... ok\ntestIntArray (pandas.io.tests.test_json.test_ujson.NumpyJSONTests) ... ok\ntestIntMax (pandas.io.tests.test_json.test_ujson.NumpyJSONTests) ... ok\ntestOdArray (pandas.io.tests.test_json.test_ujson.NumpyJSONTests) ... Segmentation fault (core dumped)\n```\n\nIn the following `print_versions` output, you'll notice that I have exclude as many dependencies as possible.\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.4.3.final.0\npython-bits: 64\nOS: FreeBSD\nOS-release: 10.1-STABLE\nmachine: amd64\nprocessor: amd64\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.17.1\nnose: 1.3.7\npip: None\nsetuptools: 17.0\nCython: None\nnumpy: 1.10.1\nscipy: None\nstatsmodels: None\nIPython: None\nsphinx: None\npatsy: None\ndateutil: 2.3\npytz: 2015.7\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: None\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\nJinja2: None\n```\n\nHere is [a backtrace from the coredump](https://gist.github.com/neirbowj/b1720c42a9908feedf7c).\n\nAny suggestions for how to proceed?\n"
    ],
    "events": [
      "renamed",
      "commented",
      "labeled",
      "labeled",
      "referenced",
      "commented",
      "commented",
      "commented",
      "renamed",
      "commented",
      "cross-referenced",
      "milestoned"
    ],
    "changed_files": 9,
    "additions": 284,
    "deletions": 42,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/io/tests/json/__init__.py",
      "pandas/io/tests/json/data/tsframe_iso_v012.json",
      "pandas/io/tests/json/data/tsframe_v012.json",
      "pandas/io/tests/json/test_json_norm.py",
      "pandas/io/tests/json/test_pandas.py",
      "pandas/io/tests/json/test_ujson.py",
      "pandas/src/ujson/python/objToJSON.c",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11305,
    "reporter": "jreback",
    "created_at": "2015-10-12T22:03:18+00:00",
    "closed_at": "2015-11-13T15:13:07+00:00",
    "resolver": "michaelaye",
    "resolved_in": "47778004fbe2594d0f877fbf49b685ad55469557",
    "resolver_commit_num": 8,
    "title": "PERF: rendering of large number of categories",
    "body": "xref #11304 \n\n\n\nThis renders the string of all of the categories before chopping them, see [here](#L1392)\n\nThis should check for a large number of categories first, then render the chopped portions.\n",
    "labels": [
      "Output-Formatting",
      "Difficulty Novice",
      "Categorical",
      "Effort Low"
    ],
    "comments": [
      "cc @michaelaye \n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 23,
    "deletions": 5,
    "changed_files_list": [
      "asv_bench/benchmarks/categoricals.py",
      "doc/source/whatsnew/v0.17.1.txt",
      "pandas/core/categorical.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11359,
    "reporter": "FlxVctr",
    "created_at": "2015-10-18T03:31:36+00:00",
    "closed_at": "2016-09-13T10:24:02+00:00",
    "resolver": "parthea",
    "resolved_in": "f3632368ed703f3f0586b4f79e26f3eeb4e2a2c2",
    "resolver_commit_num": 11,
    "title": "pandas.io.gbq verify_schema seems to be too strict.",
    "body": "[This line](#L330) seems for me to be too strict for repeated insertion, because apparently GBQ is not consistent in the order of fields in the schema (or my application screws the order of fields up, anyway, I would say the verification is too strict).\n\nSo for example:\n\n\n\nwould make verification fail, though insertion would work, as the insert as JSON makes the order of fields irrelevant.\n\nSolved that for myself for the moment with:\n\n\n",
    "labels": [
      "Google I/O",
      "Error Reporting"
    ],
    "comments": [
      "cc @parthea\n",
      "The strict ordering of fields is by design (also in 0.16.2). From the [docs](http://pandas.pydata.org/pandas-docs/stable/io.html#io-bigquery-writer):\n\n```\nThe dataframe must match the destination table in column order, structure, and data types.\n```\n\n> because apparently GBQ is not consistent in the order of fields in the schema (or my application screws the order of fields up\n\nCan you confirm whether it is your application that is changing the order of fields? If we can understand how the ordering of fields is changing, we may be better off fixing that problem instead.\n\nMy personal preference would be to fail if the column order of the DataFrame being inserted is different. It is trivial to alter the column order of a DataFrame prior to inserting. \n\nCertainly, the proposed changed would be more flexible, but I think that the user should be aware of the column order, in case the BigQuery table schema has actually changed.\n",
      "Thanks for your answer. Wasn't reading the docs closely enough apparently. It's very likely that it is my application as I am transforming raw JSON from the Twitter API to a dataframe and the order there is arbitrary. I can try to verify that though. But order of columns seemed (before formatting anything for readable output) pretty unnecessary to me and only costing additional programming time as well as introducing more possibilities for errors to sneak in.\n\nOn the other hand, but now we are talking about particular project requirements and personal preferences, this use case it not that unlikely. And I don't know how the order of fields in BigQuery would matter for most applications of BigQuery in the end, as you query for fields by name anyway. For my project it's just another step processing the data (which will come in large volume) before inserting, and I need to minimise those. Have to admit that ordering is trivial, indeed. So yes, that's a design decision. I would plead for a less strict option (maybe with an optional argument in to_gbq?) though.\n",
      "The number of optional arguments in `to_gbq` is growing, so it may be better to decide whether to incorporate the change by default or not. The proposed change would certainly make inserting data into BigQuery easier, as long as we are ok with not maintaining column order.  \n\nI'd like @jacobschaer to comment. The note in the docs to maintain column order was added in #6937.\n",
      "just to note that HDF5 requires the orderings to be exactly the same as does SQL. I don't think this should be relaxes. Ok to have a 'better' error message though.\n",
      "Yes, a more detailed error message would be an improvement. Because neither Python dicts, nor pandas DataFrames nor GBQ really cares about column order, so I did not expect it to matter, it took me quite a while to find out what is wrong with my schema.\n",
      "Note that for the SQL functions the ordering does not need to be the same (as we use named parameters and a dict with the data to insert)\n",
      "@jreback Can this be closed, or do you expect a PR for this? I improved the error message in #11401 (already merged)\n",
      "does thi example work?\neg maybe just need a test to validate this \n",
      "is schema ordered?\n",
      "The verify_schema function is currently very strict. Column order matters in the schema. The example solution works. I'll submit a PR to make the verify_schema function less strict and add tests to validate this. I'll also update the docs.\n"
    ],
    "events": [
      "renamed",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned"
    ],
    "changed_files": 4,
    "additions": 78,
    "deletions": 9,
    "changed_files_list": [
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/io/gbq.py",
      "pandas/io/tests/test_gbq.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11382,
    "reporter": "saroele",
    "created_at": "2015-10-20T09:42:58+00:00",
    "closed_at": "2016-05-07T16:53:42+00:00",
    "resolver": "gliptak",
    "resolved_in": "744d27e4e8d8fbc931231e6a271988c13a74dd9d",
    "resolver_commit_num": 23,
    "title": "truncate() raises IndexError on large timeseries",
    "body": "Hi,\n\nI found a bug, tested on pandas 0.16 and 0.17.  On large time series, the `truncate()` method throws an IndexError.  Here's code that reproduces the bug:\n\n\n\nInfo version: \n\nINSTALLED VERSIONS\n\ncommit: None\npython: 2.7.8.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.16.0-41-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_GB.UTF-8\n\npandas: 0.17.0\nnose: 1.3.4\npip: 7.1.2\nsetuptools: 5.5.1\nCython: 0.20.2\nnumpy: 1.8.2\nscipy: 0.14.0\nstatsmodels: None\nIPython: 3.0.0\nsphinx: 1.2.2\npatsy: None\ndateutil: 2.4.2\npytz: 2015.6\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.3.1\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: 4.3.2\nhtml5lib: None\n 0.9\napiclient: None\nsqlalchemy: 0.9.7\npymysql: None\npsycopg2: None\n",
    "labels": [
      "Timeseries",
      "Error Reporting",
      "Difficulty Intermediate",
      "Effort Low",
      "Testing"
    ],
    "comments": [
      "actually what you are doing should raise a `TypeError` in both cases.\n\nthe input arguments have to be dates.\n\nThese are converted to dates (but incorrectly).\n\nwant to do a pull-request to fix?\n",
      "so for integer indexes, I should use `.loc[start:end]` ? \nAnd check myself first if the start and end are contained in the index (raises an IndexError if not)?\n\nI'll think about the pull-request.  I'm not a pandas developer yet, but maybe it's a good start and it will teach me a thing or two :-) \n",
      "integer indexing is via iloc\n\ndocs are http://pandas.pydata.org/pandas-docs/stable/indexing.html\n",
      "@jreback Could you describe the expected behaviour?\nThe description of `def truncate(self, before=None, after=None, axis=None, copy=True)` refers to dates, while the implementation (and the comment in `truncate`) uses `slice` on `ax` (only converting `before/after` if `ax.is_all_dates`)\n",
      "if this is not a dateindex the this should raise\n",
      "There are a number of tests failing after changing this (as per below), as they use `truncate` on non datetime indexes ...\n\nhttps://travis-ci.org/gliptak/pandas/jobs/125287589\n\n```\n======================================================================\n\nERROR: test_truncate (pandas.tests.test_panel.TestLongPanel)\n\n----------------------------------------------------------------------\n\nTraceback (most recent call last):\n\n  File \"/home/travis/build/gliptak/pandas/pandas/tests/test_panel.py\", line 48, in wrapper\n\n    return func(self, *args, **kwargs)\n\n  File \"/home/travis/build/gliptak/pandas/pandas/tests/test_panel.py\", line 2409, in test_truncate\n\n    trunced = self.panel.truncate(start, end).to_panel()\n\n  File \"/home/travis/build/gliptak/pandas/pandas/core/generic.py\", line 4622, in truncate\n\n    raise TypeError(\"not ax.is_all_dates\")\n\nTypeError: not ax.is_all_dates\n\n======================================================================\n\nERROR: test_truncate_fillna_bug (pandas.tests.test_panel.TestPanel)\n\n----------------------------------------------------------------------\n\nTraceback (most recent call last):\n\n  File \"/home/travis/build/gliptak/pandas/pandas/tests/test_panel.py\", line 1558, in test_truncate_fillna_bug\n\n    result = self.panel.truncate(before=None, after=None, axis='items')\n\n  File \"/home/travis/build/gliptak/pandas/pandas/core/generic.py\", line 4622, in truncate\n\n    raise TypeError(\"not ax.is_all_dates\")\n\nTypeError: not ax.is_all_dates\n```\n\nhttps://travis-ci.org/gliptak/pandas/jobs/125287587\n\n```\n======================================================================\n\nERROR: testOLSWithDatasets_copper (pandas.stats.tests.test_ols.TestOLS)\n\n----------------------------------------------------------------------\n\nTraceback (most recent call last):\n\n  File \"/home/travis/build/gliptak/pandas/pandas/stats/tests/test_ols.py\", line 92, in testOLSWithDatasets_copper\n\n    self.checkDataSet(sm.datasets.copper.load())\n\n  File \"/home/travis/build/gliptak/pandas/pandas/stats/tests/test_ols.py\", line 150, in checkDataSet\n\n    self.checkMovingOLS('rolling', x, y)\n\n  File \"/home/travis/build/gliptak/pandas/pandas/stats/tests/test_ols.py\", line 209, in checkMovingOLS\n\n    x_iter[k] = v.truncate(before=prior_date, after=date)\n\n  File \"/home/travis/build/gliptak/pandas/pandas/core/generic.py\", line 4622, in truncate\n\n    raise TypeError(\"not ax.is_all_dates\")\n\nTypeError: not ax.is_all_dates\n\n======================================================================\n\nERROR: testOLSWithDatasets_scotland (pandas.stats.tests.test_ols.TestOLS)\n\n----------------------------------------------------------------------\n\nTraceback (most recent call last):\n\n  File \"/home/travis/build/gliptak/pandas/pandas/stats/tests/test_ols.py\", line 96, in testOLSWithDatasets_scotland\n\n    self.checkDataSet(sm.datasets.scotland.load())\n\n  File \"/home/travis/build/gliptak/pandas/pandas/stats/tests/test_ols.py\", line 150, in checkDataSet\n\n    self.checkMovingOLS('rolling', x, y)\n\n  File \"/home/travis/build/gliptak/pandas/pandas/stats/tests/test_ols.py\", line 209, in checkMovingOLS\n\n    x_iter[k] = v.truncate(before=prior_date, after=date)\n\n  File \"/home/travis/build/gliptak/pandas/pandas/core/generic.py\", line 4622, in truncate\n\n    raise TypeError(\"not ax.is_all_dates\")\n\nTypeError: not ax.is_all_dates\n```\n",
      "@jreback How do you see this moving forward? Thanks\n",
      "@gliptak sorry this is not related to the timeseries. something in `index.pyx` is raising a `IndexError` when it should be a `KeyError` (on big index)\n",
      "@jreback From what I understand this issue discusses datetime index vs,. other indices, and is reproducible with small indices.\n",
      "@gliptak not sure what you mean. its a non-datetime large index.\n",
      "@jreback OK, I read this again. I think https://github.com/pydata/pandas/pull/12921 will correct `IndexError vs KeyError`. But there is a second issue here, `truncate` documentation states it is only applicable for `DateTimeIndex`, and you stated earlier in this issue, that it should raise `TypeError` when `non DateTimeIndex`. I patched to raise, but testcases in `pandas` codebase use truncate for `non DateTimeIndex`. \n",
      "@gliptak yeah, maybe just update the doc-string. its 'sort of legit' to do this with a non-datetimelike (a bit odd, but that's ok).\n",
      "@gliptak you are right, we just need a validation test for this (and doc-update).\n",
      "I wasn't correct, https://github.com/pydata/pandas/pull/12921 takes a different path ... So what do you see raised for these?\n\n```\nbig = pd.Series(data=range(int(2e6)), index=range(int(2e6)))\nbig.truncate() # NOOP?\nbig.truncate(before=0, after=3e6) # what to raise?\n```\n",
      "@gliptak that example is correct. I think something fixed this issue. I would just write a confirming test for this (validating the existing behavior).\n",
      "@jreback I modified docstring and added tests to highlight current behaviour. Please review.\n"
    ],
    "events": [
      "referenced",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 5,
    "additions": 67,
    "deletions": 16,
    "changed_files_list": [
      "pandas/core/generic.py",
      "pandas/tests/test_generic.py",
      "pandas/tests/test_panel4d.py",
      "pandas/tseries/tests/test_plotting.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11412,
    "reporter": "toobaz",
    "created_at": "2015-10-22T14:03:45+00:00",
    "closed_at": "2015-10-22T14:22:17+00:00",
    "resolver": "toobaz",
    "resolved_in": "53bf1b27c7dac2a8d72d9bcfe75f514dae8e8c96",
    "resolver_commit_num": 16,
    "title": "min_itemsize not working on MultiIndex columns for Series, with format=\"table\"",
    "body": "If I do\n\n\n\nand then\n\n\n\nI get the following: \n\n\n\nAll goes smoothly instead if I don't specify \"format=table\", or if I don't specify the min_itemsize, or if I save as DataFrame (ddf[['C']]) rather than a as Series.\n\nTested with up to date pandas from git and pytables 3.2.2-1.\n",
    "labels": [],
    "comments": [
      "dupe of #11364 \n\nits a bug, specify 'index' as the key to make it work\n"
    ],
    "events": [],
    "changed_files": 3,
    "additions": 14,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.2.txt",
      "pandas/io/pytables.py",
      "pandas/io/tests/test_pytables.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11473,
    "reporter": "davidszotten",
    "created_at": "2015-10-29T16:26:24+00:00",
    "closed_at": "2016-04-26T18:54:47+00:00",
    "resolver": "Komnomnomnom",
    "resolved_in": "37a7e69cea0ba518adc4328aac92ac7d925ed843",
    "resolver_commit_num": 41,
    "title": "to_json segfaults with timezone-aware datetimes",
    "body": "\n",
    "labels": [
      "Enhancement",
      "Timeseries",
      "IO JSON",
      "Timezones",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "similarly to #10778  it is waiting for an implementation\n",
      "should there be a validation somewhere so that it doesn't segfault even with unsupported types?\n",
      "@kawochen IIRC their is supposed to be something that does exactly that (and use the `defautl` handler in that case). maybe its not working (or borking on this check)\n",
      "cc @Komnomnomnom \n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 9,
    "additions": 284,
    "deletions": 42,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/io/tests/json/__init__.py",
      "pandas/io/tests/json/data/tsframe_iso_v012.json",
      "pandas/io/tests/json/data/tsframe_v012.json",
      "pandas/io/tests/json/test_json_norm.py",
      "pandas/io/tests/json/test_pandas.py",
      "pandas/io/tests/json/test_ujson.py",
      "pandas/src/ujson/python/objToJSON.c",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11485,
    "reporter": "jreback",
    "created_at": "2015-10-30T18:47:42+00:00",
    "closed_at": "2016-04-29T17:05:49+00:00",
    "resolver": "sinhrks",
    "resolved_in": "7bbd031104ee161b2fb79ba6f5732910661f94f8",
    "resolver_commit_num": 297,
    "title": "API: chained reshaping ops",
    "body": "accept callables\n- ~~`.query`~~ (not changed, see #12539)\n- [x] `.where/.mask` (#12539)\n- [x] `.loc` (and indexers) (#12539)\n\n\n\nan operation that changes the shape of the DataFrame\n\n\n\ncan be done like this\n\n\n\nSQL calls this `select`, which pandas has, but both `select/filter` are used for filtering LABELS (and not data).\n\nI suppose making this work:\n\n`df.dropna().loc[lambda x: x[x.B=='a']]` is _maybe_ a slight enhancement of this\n\nany thoughts?\n",
    "labels": [
      "Indexing",
      "Reshaping",
      "API Design",
      "Needs Discussion",
      "Enhancement",
      "Prio-medium",
      "Difficulty Intermediate",
      "Effort Medium",
      "Master Tracker"
    ],
    "comments": [
      "cc @jorisvandenbossche @TomAugspurger @sinhrks @shoyer \n\nFYI @TomAugspurger I really do like `.pipe` & chaining!\n\ncomes from this example\n\n```\n(tidy\n     .dropna()\n     .pipe(lambda df: df[df.team == 'Los Angeles Lakers'])\n     .pipe(sns.FacetGrid, col='team', hue='team')\n     .map(sns.barplot, \"variable\", \"rest\")\n )\n```\n",
      "forgot about `.query` which is a nice soln for this actually\n\n```\nIn [63]: df.dropna().query('B==\"a\"')            \nOut[63]: \n   A  B\n0  1  a\n3  4  a\n```\n",
      "Yes, `.query` works but I hate coding in strings. I would be supportive of accepting a `lambda` in query, e.g., `df.query(lambda df: df.B == 'a')`\n",
      "yeh, then of course\n\n`df[lambda x: x.B == 'a']`\n`df.loc[lambda x: x.B == 'a']`\n\nfor consistency\n\nok, then it allows easy chaining based on values, which is nice to do (and then is consistent with how `.assign` works, e.g. accepts an expression or a lambda)\n",
      "Yep, putting support for functions in indexing makes sense to me.\n",
      "Would be nice if this also worked on `Series` (unlike query which currently just works on DFs). \n\nAnd +1 for @shoyer's suggestion re extending query for this: `df.query(lambda df: df.B == 'a')`, despite the inability to set those values.\n",
      "@MaximilianR I'm using your example from #12226:\nWhat's wrong with the following?\n\n```\nIn [15]: pd.Series(range(10)).mul(5).pipe(lambda x: x**2).pipe(lambda x: x-500).pipe(lambda x: x[x>200])\nOut[15]:\n6     400\n7     725\n8    1100\n9    1525\ndtype: int64\n```\n",
      "@kawochen Ah - that's very nice. Not sure how I missed that. Thanks!\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "closed",
      "commented",
      "commented",
      "reopened",
      "milestoned",
      "labeled",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "labeled",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 13,
    "additions": 588,
    "deletions": 23,
    "changed_files_list": [
      "doc/source/indexing.rst",
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/common.py",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/indexing.py",
      "pandas/core/panel.py",
      "pandas/core/series.py",
      "pandas/tests/frame/test_indexing.py",
      "pandas/tests/frame/test_query_eval.py",
      "pandas/tests/indexing/test_callable.py",
      "pandas/tests/series/test_indexing.py",
      "pandas/tests/test_panel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11514,
    "reporter": "r-pluss",
    "created_at": "2015-11-03T20:36:55+00:00",
    "closed_at": "2016-04-10T22:12:08+00:00",
    "resolver": "sinhrks",
    "resolved_in": "a1f5ef3b9dd02b0ca9a7bea74c30a7e24b4434d5",
    "resolver_commit_num": 285,
    "title": "Mixing CustomBusinessDay and BusinessHours Classes",
    "body": "Is it currently possible to combine the CustomBusinessDay and BusinessHour classes in any way?\n\nMy use-case is that I want to be able to create a SubClass of BusinessHour that would skip over all time occurring in the 'holidays' or outside the 'weekmask' defined within a CustomBusinessDay instance. This would be incredibly useful for me, and, I suspect, many others.\n\nI have currently tried subclassing BusinessHour and replacing the next_bday attribute with in an instance of a CustomBusinessDate. Unfortunately, it does not rollforward past days defined as holidays, and seems to function the same as a typical BusinessDay object.\n\nIf such functionality is not currently available, please consider this a feature request.\n",
    "labels": [
      "Frequency",
      "Enhancement"
    ],
    "comments": [
      "Currently no.\n\nI **assume** it should work by replacing `BusinessDay` to `CustomBusinessDay` used in `BusinessHour`. Pull request is appreciated!\n"
    ],
    "events": [
      "labeled",
      "commented",
      "cross-referenced",
      "milestoned",
      "labeled"
    ],
    "changed_files": 4,
    "additions": 419,
    "deletions": 68,
    "changed_files_list": [
      "doc/source/timeseries.rst",
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/tseries/offsets.py",
      "pandas/tseries/tests/test_offsets.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11526,
    "reporter": "benjello",
    "created_at": "2015-11-05T16:00:38+00:00",
    "closed_at": "2016-09-11T13:55:39+00:00",
    "resolver": "kshedden",
    "resolved_in": "54ab5be78fac125ae6efcd650d8d3f418e60754d",
    "resolver_commit_num": 16,
    "title": "read_stata issue",
    "body": "I have the following error when reading a bunch of stata file\n\n\n\nI do not know the way to check the stata version of the file but I suspect it is fairly recent.\nOpening the file with stata 12 and saving it solves the problem. \nBut I do not have stata installed on my machine and I cannot do that everytime.\nHow can I check the version of the file ?\nIs there anybody kind enough  to have a look at the problematic file (personnal email please) ?\nI suspect encoding problem. data is a sample of a french survey with many string with accent etc.\n\nThanks for help\n",
    "labels": [
      "IO Stata",
      "Compat"
    ],
    "comments": [
      "try with encoding='utf-8' and see if that works\n",
      "Same error.\n",
      "this seems peculiar to your case and out of scope of pandas\n",
      "I have the same problem with DTA files generated by SAS. Stata users have no problem opening these files, and neither do I if I use R, but I can't open them up using pandas (v 0.18.1)\n",
      "@torstees Can you provide a reproducible example? (eg example file that fails to open)\n",
      "Here you go. I included a minimal python and R script along with the output I'm seeing on current OSX and a recent linux system. The dataset was exported with the current version of SAS using proc export. It contains a simple list of ids and nothing else. \n[testdata.zip](https://github.com/pydata/pandas/files/447700/testdata.zip)\n",
      "There is a huge probability that the stata file I do use are generated by SAS since it is the main software used by the french statistical institute. I tried @torstees data and got the same error.\n",
      "cc @bashtage or @kshedden if you could have a look\n",
      "The dta file format code for the file supplied by @torstees is 111.  According to the R docs here:\n\nhttps://stat.ethz.ch/R-manual/R-devel/library/foreign/html/read.dta.html\n\nformat 111 corresponds to Stata 7SE.  But the SAS docs linked below state that SAS writes dta files compatible with Stata 8 and later.\n\nhttp://support.sas.com/documentation/cdl/en/acpcref/63184/HTML/default/viewer.htm#a003103776.htm\n\nThere is no mention of format version 111 in the Stata dta format docs:\n\nhttp://www.stata.com/help.cgi?dta\n",
      "I also noticed that in the References section below:\n\nhttps://stat.ethz.ch/R-manual/R-devel/library/foreign/html/read.dta.html\n\nthey state that the spec for dta's written by Stata 7 is contained in the printed programming manual.  I can't find it on-line.\n",
      "The small SAS program below exports a Stata dta file.  You can then use Stata to check its version, with `dtaversion tmp.dt`.  Under SAS 9.1 and Stata 14.1 I get:\n\n```\n. dtaversion tmp.dta\n  (file \"tmp.dta\" is .dta-format 111 from Stata 7)\n```\n\nHere is the SAS program (you need to have a small \"tmp.csv\" file in the working directory):\n\n```\nlibname mydata \".\";\n\nproc import datafile=\"tmp.csv\"\n    dbms=csv\n    out=tmp;\n\nproc export\n    file=\"tmp.dta\"\n    dbms=stata replace;\n\nrun;\n```\n"
    ],
    "events": [
      "commented",
      "labeled",
      "commented",
      "commented",
      "closed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "reopened",
      "milestoned",
      "labeled",
      "labeled",
      "unlabeled",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 20,
    "deletions": 6,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/io/stata.py",
      "pandas/io/tests/data/stata7_111.dta",
      "pandas/io/tests/test_stata.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11544,
    "reporter": "jreback",
    "created_at": "2015-11-07T14:47:00+00:00",
    "closed_at": "2017-03-28T12:49:58+00:00",
    "resolver": "jreback",
    "resolved_in": "1dab800b412be3613e8f666eb1be88458b631312",
    "resolver_commit_num": 4331,
    "title": "ERR: raise NotImplemented error if keywords are passed to read_excel which are not supported",
    "body": "xref #11527 \nxref #10001\n\ncurrently we pass thru keywords to the `TextReader` but some of these don't do anything. Should catch and raise on these.\n",
    "labels": [
      "Difficulty Novice",
      "Error Reporting",
      "IO Excel",
      "Effort Low"
    ],
    "comments": [
      "Did #11870 close this fully? There are not other keywords that also should raise?\n",
      "I think we need to change `read_excel` to have explict keywords, maybe 'accepting' some common ones where we raise `NotImplementedError` to give a nicer error messsage.\n\nso let's reopen this one.\n",
      "cc @grahamjeffries \n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "closed",
      "referenced",
      "commented",
      "commented",
      "reopened",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "renamed",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 20,
    "deletions": 11,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/io/excel.py",
      "pandas/io/parsers.py",
      "pandas/tests/io/test_excel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11555,
    "reporter": "jreback",
    "created_at": "2015-11-09T00:24:42+00:00",
    "closed_at": "2016-02-12T03:07:36+00:00",
    "resolver": "frankcleary",
    "resolved_in": "a424bb21e47ad8550a01bca6da8ed88356f1326b",
    "resolver_commit_num": 0,
    "title": "DOC: pd.read_csv doc-string clarification",
    "body": "need to update the docs for `read_csv` to match the doc-string. Let's make sure they have the same text / examples as much as possible (and are in the same order and such).\n\nIn particular, the `header` option doesn't have `header='infer'` which is the default in the docs, but it exists in the doc-string.\n\nFurther, should show what this does:\n\n\n\nmeaning if the the `header` kw is not specified, this it is set to the first line if no `names` are specified, else `None`.\n",
    "labels": [
      "Docs",
      "Difficulty Novice",
      "CSV"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 384,
    "deletions": 290,
    "changed_files_list": [
      "doc/source/io.rst",
      "doc/source/merging.rst",
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/io/parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11558,
    "reporter": "mikepqr",
    "created_at": "2015-11-09T04:32:58+00:00",
    "closed_at": "2016-03-29T20:09:45+00:00",
    "resolver": "sinhrks",
    "resolved_in": "896454e65a85df0db9bad72f211ce0ab62a31231",
    "resolver_commit_num": 262,
    "title": "groupby categorical column fails with unstack",
    "body": "Replicating example\n\n\n\nThe behaviour in [this notebook]() seems like a bug to me. This is pandas 0.17.0.\n\nIn it, `g` and `gcat` are the results of two `df.groupby(['medium', 'artist']).count().unstack()` operations. The only difference is that one of those operations is on `df` where one of the columns that the `groupby` operates over has been converted to Categorical.\n\n`g` and `gcat` behave very differently. I've tried to pin this down to the exact operation in the split-apply-combine that causes the problem without much luck. \n\nSlicing a column out of `g` returns a Series as expected, while slicing a column out of `gcat` returns a DataFrame (see cells 4 and 5).\n\n`g.describe()` works as expected, but `gcat.describe()` raises the exception\n\n\n\nand `g['painting'] + g['sculpture']` works as expected but `g['painting'] + g['sculpture']` raises\n\n\n",
    "labels": [
      "Bug",
      "Prio-medium",
      "Indexing",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "this is a tricky bug actually; when indexing into a frame that has duplicates (or is a `CategoricalIndex`), you get a frame back from `.iteritems` even though it may be unique. So there are 2 paths here that need checking actually.\n",
      "This seems related to this incongruency I also ran into:\n\n```\n>>> data = pd.DataFrame([[1,2,3],[3,4,5]], index=['one', 'two'])\n>>> print(data.ix['one'].shape)\n(3,)\n>>> data = pd.DataFrame([[1,2,3],[3,4,5]], index=pd.Categorical(['one', 'two']))\n>>> print(data.ix['one'].shape)\n(1, 3)\n```\n\nIf this dataframe is coming from a `groupby`, then it's guaranteed to be uniquely indexed, so it's doubly inconsistent.\n",
      "this has to do with how we handle uniques vs non-uniques. A Categorical Index is by definition non-unique (its actually unique in this case).\n\nBut this might be a a buggie.\n\n```\nIn [36]: data1 = pd.DataFrame([[1,2,3],[3,4,5]], index=['one', 'two'])\n\nIn [37]: data2 = pd.DataFrame([[1,2,3],[3,4,5]], index=pd.Categorical(['one', 'two']))\n\nIn [40]: data1.ix['one']\nOut[40]: \n0    1\n1    2\n2    3\nName: one, dtype: int64\n\nIn [41]: data2.ix['one']\nOut[41]: \n     0  1  2\none  1  2  3\n```\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 9,
    "additions": 105,
    "deletions": 13,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/generic.py",
      "pandas/indexes/base.py",
      "pandas/indexes/category.py",
      "pandas/indexes/multi.py",
      "pandas/indexes/range.py",
      "pandas/tests/frame/test_analytics.py",
      "pandas/tests/test_categorical.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11559,
    "reporter": "rserbitar",
    "created_at": "2015-11-09T08:21:53+00:00",
    "closed_at": "2016-04-17T13:54:53+00:00",
    "resolver": "sinhrks",
    "resolved_in": "e0ee3a149d3e4e9e93b60ee00234c715e273be19",
    "resolver_commit_num": 288,
    "title": "Subclassing not working as expected",
    "body": "Running the code:\n\n\n\nworks as expected.\n\nHowever running:\n\n\n\nthrows an exception as:\n\n\n\nreturns a Series instance instead of an SublcassedSeries instance as expected.\n",
    "labels": [
      "Bug",
      "Indexing",
      "Compat",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "ok, subclassing is not recommended, see docs [here](http://pandas.pydata.org/pandas-docs/stable/internals.html#subclassing-pandas-data-structures), but would take a pull-request to fully test out the indexers (as this is not currently done).\n",
      "This is already fixed on master (maybe by #12787). Needs tests though.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 4,
    "additions": 89,
    "deletions": 8,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/tests/frame/test_subclass.py",
      "pandas/tests/series/test_subclass.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11584,
    "reporter": "dickster77",
    "created_at": "2015-11-12T21:32:54+00:00",
    "closed_at": "2016-02-12T15:04:32+00:00",
    "resolver": "sinhrks",
    "resolved_in": "b6f0292cd17b743528390a9e9efe92db96f8c19e",
    "resolver_commit_num": 254,
    "title": " util.testing.assert_almost_equal() gives Key Error ",
    "body": "Test 1 works successfully with a square dataframe\n\n\n\nTest 2 fails with a dataframe where number rows > number of columns\n\n\n\nKeyError: 5L\n",
    "labels": [
      "Testing",
      "Error Reporting"
    ],
    "comments": [
      "You should use `pd.util.testing.assert_frame_equal(a,b)` for `DataFrame` comparison. \n\nBetter to add docstring of `assert_almost_equal` to describe it.\n",
      "Sorry sinhrks - I meant to highlight the fact there are small differences in the dataframes. Hence i specifically need  assert_almost_equal()\n",
      "Did you updated the example? Use `pd.util.testing.assert_almost_equal(a.values, b.values)`. \n\nStrangely `assert_frame_equal(a, b, check_less_precise=True)` should work for your data by definition, it raises.\n",
      "Yes see update in the example `b += 0.00001`\n\nWhen I use the .values property to return an ndarray the test assert_almost_equal() works for me.\n\nHowever I would want to do a dataframe test. One that ensure indexes etc. match. Yet it allows for the small variation in the values.\n",
      "Currently it should be possible by `check_less_precise` option, but it seems not applied to all cases.\n\nMore flexible comparison tolerance is being discussed in #10788. \n",
      "Thanks - when should I use .assert_almost_equal()  with respect to dataframes?\n",
      "Let me summarize: \n- `assert_almost_equal` shoun't be used for `DataFrame`\n- `assert_frame_equal` can compare `index` and `columns`. It should be always used for `DataFrame` comparison. \n- `assert_frame_equal(check_less_precise=True)` should compare 3 digits after decimal points (similar to `assert_almost_equal`). (Your case should be covered by the option, but it looks not work properly because of a bug)\n\nAs a workaround until the bug is fixed,  you can use `assert_almost_equal(a.values, b.values)` to compare `DataFrame` values (`ndarray`).  To compare `index` and `columns`, use `assert_index_equal` each.\n",
      "see #9457 for the enhancement issue to add equiv of `np.allclose` to this.\n\n@sinhrks let's rerpose this issue to enable `assert_almost_equal` to raise a nicer error message (and suggest using one of the `assert_*pandasobject*_equal` methods\n\nI suppose we could just call one of these methods directly, no?\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "milestoned"
    ],
    "changed_files": 21,
    "additions": 190,
    "deletions": 130,
    "changed_files_list": [
      "pandas/computation/tests/test_eval.py",
      "pandas/io/tests/test_cparser.py",
      "pandas/sparse/tests/test_sparse.py",
      "pandas/tests/frame/test_alter_axes.py",
      "pandas/tests/frame/test_constructors.py",
      "pandas/tests/frame/test_indexing.py",
      "pandas/tests/frame/test_mutate_columns.py",
      "pandas/tests/series/test_analytics.py",
      "pandas/tests/series/test_constructors.py",
      "pandas/tests/series/test_operators.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_internals.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tests/test_stats.py",
      "pandas/tests/test_strings.py",
      "pandas/tests/test_testing.py",
      "pandas/tests/test_window.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tools/tests/test_tile.py",
      "pandas/tseries/tests/test_timedeltas.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11594,
    "reporter": "jorisvandenbossche",
    "created_at": "2015-11-13T17:18:47+00:00",
    "closed_at": "2016-03-23T17:56:45+00:00",
    "resolver": "jreback",
    "resolved_in": "85f8cf74b733a6782730f17b4a68fa4b028f2013",
    "resolver_commit_num": 3953,
    "title": "BUG: displayed dtype of series inferred from shown subset instead of series",
    "body": "All the same issue\n- [ ] #12045 \n- [ ] #12211 \n\n\n\nSo in some cases, you think you have a `datetime64` series, but actually you don't and eg `dt` properties don't work which can lead to quite some confusion ... :-)\n",
    "labels": [
      "Bug",
      "Output-Formatting",
      "Difficulty Novice",
      "Effort Low",
      "Master Tracker"
    ],
    "comments": [
      "you have an out-of-range date which forces it back to object.\n\nif you had done this via `pd.to_datetime` it would raise.\n\n```\nIn [4]: s = pd.to_datetime([datetime.datetime(2012, 1, 1)]*10 + [datetime.datetime(1012,1,2)] + [datetime.datetime(2012, 1, 3)]*10)\nOutOfBoundsDatetime: Out of bounds nanosecond timestamp: 1012-01-02 00:00:00\n```\n",
      "you want something like this maybe?\n\n`AttributeError: Can only use .dt accessor with datetimelike values, you have [dtype->object]`\n",
      "Well, yes, that is the point, I _don't_ have a datetime64 series (because of the out of range datetime), but the series display indicates _I do_ have a datetime64 series. \nWhich makes it a bit difficult to see that something went wrong in my datetime conversion.\n\nI know that I can convert it, but this is just about the displaying thing (the actual case where I ran into it were some excel data that apparantly had some wrong data. But I didn't directly spot this because the displayed told me everything was fine.\n",
      "@jorisvandenbossche ahh I see, we should be displaying the _original_ dtype, not the sliced ones, hmm (and shouldn't be inferring on those), but I think we may be automatically doing that....\n",
      "So `.iloc` with a list is re-inferring the dtype when slicing, while `[[]]` is not. hmm. I think we should _not_ be re-inferring here. The `.concat` will follow naturally if we don't reinfer.\n\n```\nIn [2]: s = Series([pd.Timedelta(1,unit='s'),'foo'])\n\nIn [3]: s\nOut[3]: \n0    0 days 00:00:01\n1                foo\ndtype: object\n\nIn [5]: s[[0]]\nOut[5]: \n0    0 days 00:00:01\ndtype: object\n\nIn [6]: s[[1]]\nOut[6]: \n1    foo\ndtype: object\n\nIn [7]: s.iloc[[0]]\nOut[7]: \n0   00:00:01\ndtype: timedelta64[ns]\n\nIn [8]: s.iloc[[1]]\nOut[8]: \n1    foo\ndtype: object\n\n```\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "milestoned",
      "cross-referenced",
      "labeled",
      "labeled",
      "cross-referenced",
      "commented",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "labeled",
      "referenced",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 7,
    "additions": 127,
    "deletions": 23,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/indexing.py",
      "pandas/core/internals.py",
      "pandas/tests/indexing/test_indexing.py",
      "pandas/tests/test_format.py",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11617,
    "reporter": "samueljohn",
    "created_at": "2015-11-16T16:36:29+00:00",
    "closed_at": "2017-01-12T22:43:12+00:00",
    "resolver": "jreback",
    "resolved_in": "82ab26af28549cd2a58483e89455296eb3300182",
    "resolver_commit_num": 4162,
    "title": ".loc on DataFrame returning coerced dtype for single rows",
    "body": "xref #14205 \n\nThe `.loc` method of `DataFrame` with different dtypes yields coerced type **even if the resulting slice does only contain elements from one type**. This happens only when selecting a single row.\nI can guess that this might be intended because the implementation of `loc` seems to first lookup the row _as a single Series_, doing the coercion and _then_ applying the second (column) indexer.\n\nHowever, when the column indexer narrows down the selection such that the upcasting would not have been necessary in the first place, it can be very surprising and may even cause bugs (on user-side) if it goes unnoticed. (Like, \"I was sure that those column was `int64`\").\n\n\n\nFeel free to close if the behavior s intended. Maybe this this a \"bug\" or an suggested API change. I dunno.\n\nPerhaps related to #10503, #9519, #9269, #11594 ?\n\n\n",
    "labels": [
      "Bug",
      "Indexing",
      "Dtypes",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "hmm, that is a bit inconsistent. I would expect all of these to give the same result (and not coerce), adding `.iloc` here and not change the dtype.\n\n```\nIn [23]: d\nOut[23]: \n      a    b\n0  1.23  666\n\nIn [24]: d.dtypes\nOut[24]: \na    float64\nb      int64\ndtype: object\n\nIn [25]: d.ix[0, \"b\"]\nOut[25]: 666\n\nIn [26]: d.loc[0, \"b\"]\nOut[26]: 666.0\n\nIn [28]: d.iloc[0,1]\nOut[28]: 666.0\n```\n\nif you'd like to dig in would be great!\n",
      "Wow ... I have been deep down in the 5k LOC internals.py... I don't think I wanna go there again :-)\nI somehow assumed pandas was something \"lightweight\" on top of numpy. \n\nSo, indeed a creation of a `Series` seems involved.\n\nIn the following, I used the latest release for tracing but I do point into the master codebase. Perhaps If you have installed pandas master you could try if this still applies (I think yes).\n\nI have traced it so far as first a Series is created for the first key in the tuple `(0,\"b\")`. The call to `d.loc.obj._xs(0, axis=0)` calls `d.loc.obj._data.fast_xs(0)` here:\nhttps://github.com/pydata/pandas/blob/master/pandas/core/generic.py#L1498-L1500\n\nIn the creation of the Series, the blocks are still correct:\n`(FloatBlock: slice(0, 1, 1), 1 x 1, dtype: float64, IntBlock: slice(1, 2, 1), 1 x 1, dtype: int64)`\n\nBut then in the dtype is determined by `dtype = _interleaved_dtype(self.blocks)` (https://github.com/pydata/pandas/blob/master/pandas/core/internals.py#L3170) and returns `float64` which makes sense from a number theoretical POV. That method is also in internals:\nhttps://github.com/pydata/pandas/blob/master/pandas/core/internals.py#L4114\n\nI think this is how pandas Series are defined (they must contain just one type).\n\nBut the question is if the creation of the series should perhaps better be done _after_ the second key (in this example the column `\"b\"`) is evaluated. Because then the dtype would not need to be a float64 at all.\n\nNot sure if this is still `Effort Low`.\n",
      "@samueljohn haha, indexing is pretty complex!\n\nWe don't distinguish between all scalar keys upfront, hence the serial conversions. Easiest thing to do is try changing and see if your tests for this behavior (and original tests pass). That is the part about indexing, preserving the API when making changes.\n",
      "Hi @jreback , @samueljohn.\n\nI also encountered this problem today. After a little digging around, the following may help:\n\nFirstly, the dataframe behaves correctly if there is a non-numeric object in the dataframe:\n\n```\n>>> df = pd.DataFrame({'x': [1,2,3], 'y': [1.0, 2.0, 3.0]}, columns=['x', 'y'])\n>>> df.loc[0]\nx    1.0\ny    1.0\nName: 0, dtype: float64\n>>> [type(v) for v in df.loc[0]]\n[numpy.float64, numpy.float64]\n\n>>> df['z'] = 'foo'\n>>> df.loc[0]\nx    1  \ny    1  \nz    foo\nName: 0, dtype: object\n>>> [type(v) forv in df.loc[0]]\n[numpy.int64, numpy.float64, str]\n```\n\nSecondly, this may be fixed by simply changing how the Series constructor is called:\n\n```\n>>> s = pd.Series([np.int64(1), np.float64(1.0)])\n>>> print s\n0    1.0\n1    1.0\ndtype: float64\n>>> [type(v) for v in s]\n[numpy.float64, numpy.float64]\n\n>>> s = pd.Series([np.int64(1), np.float64(1.0)], dtype='object')\n>>> print s\n0    1\n1    1\ndtype: object\n>>> [type(v) for v in s]\n[numpy.int64, numpy.float64]\n```\n\nAny thoughts on possible performance hits if `df.loc[...]` always returns a series with `dtype='object'`?\n\nCheers,\n",
      "returning as `object` is only appropriate if it actually includes things that are not representable as baser types. right now we coerce ints to floats if needed, this is pretty standard practice as it leads to much more efficiency.\n",
      "I propose a fix in  _interleaved_dtype(blocks).\n\nI think there are use cases for both scenarios:\n- always coerce numeric dtypes into a dtype that supports all dtypes in the block, for calculation-heavy applications which don't care too much about preserving numerical dtypes\n- always preserve numerical dtypes, using dtype('object') where different numerical types are present. For applications where preserving the dtypes of a data frame is important.\n\nMaybe it could be added as a pandas option, perhaps 'mode.coerce_numerical_dtypes'?\n",
      "Try to address the specific change of verifying that all of the cases above return the same dtype. Doing something more complicated like returning an `object` dtype is prob ok, but only in very certain circumnstances.\n\nDoing what you are suggesting above is not going to back-compat and likely break lots of things. Start small.\n\nAdding an option is also a non-starter.\n",
      "I think I will submit a separate issue. I currently require a way of retrieving a row of a DataFrame that preserves numerical dtypes, which is separate to this issue, but very related.\n",
      "I suppose a:\n\n`df.loc(coerce=False)[0]` might be ok (with a default of `True`) for back compat.\n",
      "That could work, but would require more thought into how it plugs in to other data frame methods (e.g. df.apply(..., axis=1, coerce=True))\n\nI will have a go at it, but it might take me some time.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "unlabeled",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 102,
    "deletions": 9,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/indexing.py",
      "pandas/tests/frame/test_indexing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11623,
    "reporter": "pckapps",
    "created_at": "2015-11-17T04:17:48+00:00",
    "closed_at": "2016-05-12T13:12:58+00:00",
    "resolver": "jreback",
    "resolved_in": "4de83d25d751d8ca102867b2d46a5547c01d7248",
    "resolver_commit_num": 4025,
    "title": "Pandas quantile function very slow",
    "body": "The quantile function is almost 10 000 times slower than the equivalent percentile function in numpy. See code below:\n\n\n",
    "labels": [
      "Performance",
      "Numeric"
    ],
    "comments": [
      "This seems to be because it computes the quantiles series by series -- so computing 10k quantiles like this example does is going to have a lot of overhead.  This was presumably done as a simplification to handle different types such as TimeStamp. It also handles nulls by default (as do most Pandas functions), which also affects performance (lots of `notnull()`) that aren't run by NumPy.\n\nUltimately `df.quantile` is just calling `np.percentile` N times where N is the shape of the axis.  The simplest thing to do would be to have a fast path for `numeric_only = True` where there are no nulls, although the requirement to drop nulls can't be easily done inside of the NumPy implementation using the block.\n",
      "these can simply be done block-by-block. we do this with almost all other functions already. \n",
      "I think the null-handling prevents trivial application even block-by-block. See my revised comment.\n",
      "There's an `np.nanpercentile` which could be used to go block by block\n",
      "NumPy >= 1.9, so would require some special casing.\n\nKevin\n\nOn Thu, Nov 19, 2015 at 9:34 AM Maximilian Roos notifications@github.com\nwrote:\n\n> There's an np.nanpercentile which could be used to go block by block\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/pydata/pandas/issues/11623#issuecomment-158074146.\n",
      "After #12752\n\n```\n# Output:\n# Pandas took 20180.231 ms\n# Numpy took 6.843 ms\n```\n\nCompared to numpy, #12752 has some improvements but needs further.\n\n```\n# current\n# 15337.531 / 1.653 = 9278.603145795523\n\n# 12752:\n# 20180 / 6.843 = 2948.9989770568463\n```\n",
      "@sinhrks it needs to be done on a block-basis. Then it will be the same.\n",
      "The another bottleneck is transposition caused by `axis=1`. I think this transposition can be skipped in some condition (numeric_only, etc)\n",
      "numpy handles the 2-d just fine. so when it is done by blocks, we just transpose quantile and transpose (e.g. kind of like what `.eval` does). But again can fix this after.\n",
      "after #13122\n\n```\nIn [16]: %timeit df.quantile(q, axis=1)\n100 loops, best of 3: 2.06 ms per loop\n\nIn [17]: %timeit np.percentile(data, q*100, axis=1)\n1000 loops, best of 3: 1.23 ms per loop\n```\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 12,
    "additions": 352,
    "deletions": 115,
    "changed_files_list": [
      "asv_bench/benchmarks/frame_methods.py",
      "codecov.yml",
      "doc/source/whatsnew/v0.18.1.txt",
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/core/frame.py",
      "pandas/core/internals.py",
      "pandas/core/series.py",
      "pandas/io/pytables.py",
      "pandas/src/inference.pyx",
      "pandas/tests/frame/test_quantile.py",
      "pandas/tests/series/test_quantile.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11630,
    "reporter": "ktownv",
    "created_at": "2015-11-17T16:35:11+00:00",
    "closed_at": "2016-05-20T14:16:17+00:00",
    "resolver": "thejohnfreeman",
    "resolved_in": "72164a8471be0e9f41476ae094a3b46479c7a6d2",
    "resolver_commit_num": 2,
    "title": "Timestamp AttributeError",
    "body": "pandas 0.17.0, python 2.7.10:\n\n> > > import pandas as pd\n> > > p = pd.Timestamp(2015, 11, 12)\n> > > p\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File \"pandas\\tslib.pyx\", line 339, in pandas.tslib.Timestamp.**repr** (pandas\\tslib.c:9660)\n> > > AttributeError: 'int' object has no attribute 'freqstr'\n",
    "labels": [
      "Error Reporting",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "You are not calling correct.y\n\n```\nIn [40]: pd.Timestamp?\nType:        type\nString form: <class 'pandas.tslib.Timestamp'>\nFile:        c:\\miniconda\\envs\\neat\\lib\\site-packages\\pandas\\tslib.pyd\nDocstring:\nTimeStamp is the pandas equivalent of python's Datetime\nand is interchangable with it in most cases. It's the type used\nfor the entries that make up a DatetimeIndex, and other timeseries\noriented data structures in pandas.\n\nParameters\n----------\nts_input : datetime-like, str, int, float\n    Value to be converted to Timestamp\noffset : str, DateOffset\n    Offset which Timestamp will have\ntz : string, pytz.timezone, dateutil.tz.tzfile or None\n    Time zone for time which Timestamp will have.\nunit : string\n    numpy unit used for conversion, if ts_input is int or float\n```\n",
      "then it should fail at the creation of the object, not leave me with something in an inconsistent state that doesn't play nicely in the interpreter.  \n\nfor example\n\n> > > str(p)\n> > > '1970-01-01 00:00:00.000002015+00:00'\n\nworks.\n\ncalls to **repr** should never raise an exception.\n",
      "actually it should raise if the offset is not `None` or a string.\n\ncode is [here](https://github.com/pydata/pandas/blob/master/pandas/tslib.pyx#L303)\n\npull-requests to fix are welcome.\n",
      "I can work on this.\n",
      "I'm having some troubles solving this bug. I'm not familiar with `cython` and its workflow. Should I just fix the code in the .pyx file and then rebuild the C dependencies, right? Because when I try to run `python setup.py build_ext --inplace` I get lots of warnings and can't complete building the `Timedelta` C extension.\n\nI've created a Question in [StackOverflow](http://stackoverflow.com/questions/33971809/import-pandas-tseries-offsets-dateoffset-class-in-cython) with all the details. \n",
      "yes, you need to fix in the `.pyx` and then run setup. if their are compilation errors then it won't import, warnings are ok.\n\nSO in general is not helpful with this kind of thing.\n",
      "I don't understand what's wrong here. This is the [log](https://gist.github.com/IamGianluca/6a8c2828455b219f59f8) of the build. There are lots of warnings but nothing that help me identify the route cause of the issue. The build also doesn't fail, however when I try to run the nosetests immediately after I get this error message:\n\n```\ngrossi-ml:pandas-iamgianluca grossi$ nosetests pandas/\nE\n======================================================================\nERROR: Failure: ImportError (C extension: Timedelta not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext --inplace' to build the C extensions first.)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Users/grossi/anaconda/lib/python2.7/site-packages/nose/loader.py\", line 418, in loadTestsFromName\n    addr.filename, addr.module)\n  File \"/Users/grossi/anaconda/lib/python2.7/site-packages/nose/importer.py\", line 47, in importFromPath\n    return self.importFromDir(dir_path, fqname)\n  File \"/Users/grossi/anaconda/lib/python2.7/site-packages/nose/importer.py\", line 94, in importFromDir\n    mod = load_module(part_fqname, fh, filename, desc)\n  File \"/Users/grossi/PycharmProjects/pandas-iamgianluca/pandas/__init__.py\", line 13, in <module>\n    \"extensions first.\".format(module))\nImportError: C extension: Timedelta not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext --inplace' to build the C extensions first.\n\n----------------------------------------------------------------------\nRan 1 test in 0.001s\n\nFAILED (errors=1)\n```\n\nInterestingly if I remove my changes I can build and then run nosetests without problems.\n\nMy code however seems fine to me. I've just added an else statement and imported the DateOffset class:\n\n```\n        if util.is_string_object(offset):\n            from pandas.tseries.frequencies import to_offset\n            offset = to_offset(offset)\n        else:\n            from pandas.tseries.offsets import DateOffset\n            if not isinstance(offset, DateOffset):\n                raise ValueError\n```\n",
      "trying putting a debugging statement right before the import then you can see where its called. you have a circular import is the issue.\n",
      "Changes to isoformat in current HEAD corrected this.\n\nFile \"pandas/tslib.pyx\", line 567, in pandas.tslib.Timestamp.isoformat (pandas/tslib.c:13019)\n",
      "No longer sure that this works as expected, I will pull the change by @IamGianluca from above ...\n"
    ],
    "events": [
      "commented",
      "closed",
      "labeled",
      "commented",
      "commented",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "unlabeled",
      "reopened",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 114,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/timeseries.rst",
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/tseries/tests/test_tslib.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11633,
    "reporter": "DSLituiev",
    "created_at": "2015-11-18T01:10:01+00:00",
    "closed_at": "2016-05-18T13:22:36+00:00",
    "resolver": "sinhrks",
    "resolved_in": "86f68e6a48bc0219493f093e4224fe772f24ecac",
    "resolver_commit_num": 314,
    "title": "Pivot to SparseDataFrame: TypeError: ufunc 'isnan' not supported in sparse matrix conversion",
    "body": "I want to convert a DataFrame to SparseDataFrame before pivoting it (when it gets really sparse, see also [this discussion](-create-sparse-pivot-tables-in-pandas) ). I have a textual key, which I need to keep (\"chr\"):\n\n\n\nFor this small table it works well with regular `DataFrame`:\n\n\n\nBut I need to do it on much larger matrices. So I tried to do following trick:\n\n\n\nbut I am getting:\n\n\n\nAre there any plans to include a functionality option into `pivot` function for automatic conversion into SparseDataFrame?\n",
    "labels": [
      "Reshaping",
      "Sparse",
      "Bug",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "If I include `default_fill_value=0`, which makes sense in my case I get yet another error:\n\n```\n>>> dfsp = pd.SparseDataFrame(df, default_fill_value=0)\nValueError: could not convert string to float: '<value from \"chr\" column>'\n```\n",
      "you would have to show a copy-pastable example. and `pd.show_versions()`\n",
      "please see updated post with an example above\n",
      "```\npd.show_versions()\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.4.3.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 14.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.17.0\nnose: 1.3.7\npip: 7.1.2\nsetuptools: 18.4\nCython: 0.23.4\nnumpy: 1.10.1\nscipy: 0.16.0\nstatsmodels: None\nIPython: 4.0.0\nsphinx: 1.3.1\npatsy: 0.3.0\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2.dev0\nnumexpr: 2.4.3\nmatplotlib: 1.4.3\nopenpyxl: 2.2.6\nxlrd: 0.9.3\nxlwt: 1.0.0\nxlsxwriter: 0.7.3\nlxml: 3.4.4\nbs4: 4.3.2\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.5\npymysql: None\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\n```\n",
      "this is quite easy to fix, need to replace `~np.isnan(arr)` with `pd.notnull(arr)`\n\npull-requests are welcome\n",
      "Do you have a test file dedicated to sparse?\n",
      "https://github.com/pydata/pandas/blob/master/pandas/sparse/tests/test_sparse.py\n"
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced"
    ],
    "changed_files": 6,
    "additions": 126,
    "deletions": 8,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/sparse/array.py",
      "pandas/sparse/tests/test_array.py",
      "pandas/sparse/tests/test_groupby.py",
      "pandas/sparse/tests/test_pivot.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11640,
    "reporter": "nbonnotte",
    "created_at": "2015-11-18T14:27:07+00:00",
    "closed_at": "2016-01-29T14:40:50+00:00",
    "resolver": "nbonnotte",
    "resolved_in": "b291dd68356ffdfa9dec25b8d6259e717fa97de9",
    "resolver_commit_num": 8,
    "title": "BUG AttributeError: 'DataFrameGroupBy' object has no attribute '_obj_with_exclusions'",
    "body": "I guess it will be clearer with an example. First, let's prepare the dataframe:\n\n\n\nNow, the exception raised:\n\n\n\nMaybe I'm doing something wrong, and it's not a bug, but then the exception raised should definitely be more explicit than a reference to an internal attribute :-)\n\nThis attribute, by the way, is (only) referenced in one file and in issue #5264. It might be connected, but the discussion is a bit long and technical. \n\nI'll try to have a look at what's going on.\n",
    "labels": [
      "Groupby",
      "Error Reporting"
    ],
    "comments": [
      "it should be a better error message, but you are grouping on something which is not a column, your\ncolumns are a multi-index.\n\n```\nIn [16]: df.columns\nOut[16]: \nMultiIndex(levels=[[u'b1', u'b2', u'a'], [u'c1', u'c2', u'']],\n           labels=[[2, 0, 1], [2, 0, 1]],\n           names=[u'b', u'c'])\n\nIn [17]: df.index\nOut[17]: Int64Index([0], dtype='int64')\n\nIn [18]: df.columns.values\nOut[18]: array([('a', ''), ('b1', 'c1'), ('b2', 'c2')], dtype=object)\n```\n\nwhat exactly are you trying to do?\n",
      "I'm trying to group according to the column `a`, or `('a','')`. What would be the proper way?\n",
      "```\nIn [27]: df = pd.DataFrame(columns=['a','b','c','d'], data=[[1,'b1','c1',3], [1,'b2','c2',4]])\n\nIn [28]: df\nOut[28]: \n   a   b   c  d\n0  1  b1  c1  3\n1  1  b2  c2  4\n\nIn [29]: df.groupby('a').mean()\nOut[29]: \n     d\na     \n1  3.5\n```\n",
      "But that's not the result I would expect: with my dumb example, I would like to get the same dataframe.\n\nBTW, if `df['a']` works whatever the status of `a`, wouldn't it be nice to be able to group according to `a` as well?\n",
      "what are your expecattions for a result here? pls show an example.\n\n`a` is not a group in your example\n",
      "i would like that\n\n```\nb  a b1 b2\nc    c1 c2\n0  1  3  4\n1  1  5  5\n```\n\nafter grouping by `a` and taking the mean, yields\n\n```\nb b1   b2\nc c1   c2\na        \n1  4  4.5\n```\n\nwhere the first dataframe is for instance obtained with\n\n```\nIn [88]: df = pd.DataFrame(columns=['a','b','c','d'], data=[[1,'b1','c1',3], [1,'b2','c2',4], [2,'b1','c1',5], [2,'b2','c2',5]]).pivot_table(index='a', columns=['b','c'], values='d').reset_index()\n\nIn [89]: df\nOut[89]: \nb  a b1 b2\nc    c1 c2\n0  1  3  4\n1  2  5  5\n\nIn [90]: df['a'] = 1\n\nIn [91]: df\nOut[91]: \nb  a b1 b2\nc    c1 c2\n0  1  3  4\n1  1  5  5\n```\n",
      "```\nIn [17]: df.groupby([('a','')]).mean()\nOut[17]: \nb     b1   b2\nc     c1   c2\n(a, )        \n1      4  4.5\n```\n",
      "So that was that... I had tried \n\n```\nIn [99]: df.groupby(('a', '')).mean()\nOut[99]: \nb  a b1 b2\nc    c1 c2\n   1  5  5\na  1  3  4\n```\n\n(the result of which I quite don't understand, but never mind) but not enclosing it betweens brackets. Thanks!\n",
      "gr8. \n\nif u are interested in improving he error message on he above case would be great\n",
      "Sure!\n",
      "@jreback digging about this issue, I think what is happening here is not so much a problem about reporting as a real bug. Indeed, my example just shows that after all issue #11185 was only partially solved by the PR #11202:\n\n```\nIn [3]: df = pd.DataFrame(columns=['a', 'b', 'c', 'd'],\n                       data=[[1, 'b1', 'c1', 3]])\n\nIn [4]: df.groupby('z').mean()\nOut[4]: <pandas.core.groupby.DataFrameGroupBy object at 0x7f57f363d510>\n```\n\nThis should produce a `KeyError`. The fact that a `KeyError` is not raised then allows for the `AttributeError` that is the subject of this issue, and is caused by the fact that the list of keys passed (here `['z']`) is of the same length as the index, which in turn causes `match_axis_length` to be `True` in the following line:\n\nhttps://github.com/pydata/pandas/blob/b07dd0cbd6d18c55aaa0043d85f42a483eab7dbb/pandas/core/groupby.py#L2210\n\nI'll dig a bit deeper before making a PR\n",
      "hmm, that does looks like a bug. I agree should give a `KeyError` (though a bit lower down in the code that where you pointed). \n",
      "Well, this is quite interesting. I've found a correction of the last bug, which does not solve the first problem though. But digging a bit further, I've found another bug\n\n```\nIn [16]: df = pd.DataFrame(columns=['a', 'b', 'c', 'd'],\n                       data=[[1, 'b1', 'c1', 3],\n                             [1, 'b2', 'c2', 4]])\n\nIn [17]: dg = df.pivot_table(index='a', columns=['b', 'c'], values='d').reset_index()\n\nIn [18]: dg.drop('a', axis=1)\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-18-90595ac9cb8f> in <module>()\n----> 1 dg.drop('a', axis=1)\n\n/home/nicolas/Git/pandas/pandas/core/generic.pyc in drop(self, labels, axis, level, inplace, errors)\n   1615                 new_axis = axis.drop(labels, level=level, errors=errors)\n   1616             else:\n-> 1617                 new_axis = axis.drop(labels, errors=errors)\n   1618             dropped = self.reindex(**{axis_name: new_axis})\n   1619             try:\n\n/home/nicolas/Git/pandas/pandas/core/index.py in drop(self, labels, level, errors)\n   5011                 else:\n-> 5012                     inds.extend(lrange(loc.start, loc.stop))\n   5013             except KeyError:\n   5014                 if errors != 'ignore':\n\nAttributeError: 'numpy.ndarray' object has no attribute 'start'\n```\n\nTurns out, this is the `AttributeError` which is mistakenly displayed as\n\n```\nAttributeError: 'DataFrameGroupBy' object has no attribute '_obj_with_exclusions'\n```\n\nI've not checked yet if there is already an issue for this.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "demilestoned",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 47,
    "deletions": 1,
    "changed_files_list": [
      "pandas/indexes/multi.py",
      "pandas/tests/frame/test_axis_select_reindex.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11655,
    "reporter": "mattilyra",
    "created_at": "2015-11-20T10:57:04+00:00",
    "closed_at": "2016-08-04T11:43:55+00:00",
    "resolver": "TomAugspurger",
    "resolved_in": "5d163ceb8a39218aba61747cdd4c0b508aa510b5",
    "resolver_commit_num": 37,
    "title": "Conditional HTML styling hides MultiIndex structure",
    "body": "I noticed that the styling doesn't quite work with `MultiIndex` data frames. For example if we have a dataframe with two cross validated scores and take their `mean`and `sem` using `groupby` to get a `MultiIndex` dataframe\n\n\n\n![screen shot 2015-11-20 at 12 04 13](-8f7e-11e5-92f8-a7b1bfbaff77.png)\n\nwhere as without the styling the index names are preserved and the `score_1` and `score_2` column headers span 2 columns which improves legibility.\n\n![screen shot 2015-11-20 at 12 02 56](-8f7e-11e5-9b7a-49e71c2b87b4.png)\n",
    "labels": [
      "Output-Formatting",
      "IO HTML",
      "MultiIndex",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "IIRC this is 'not implemented', cc @TomAugspurger as wanted to get basic stuff working.\n\npatches are welcome!\n",
      "see last check box in #11610 \n",
      "Yep. This one isn't too bad to do, I just ran out of time. It's a matter of modifying the template to be aware of the index levels.\n",
      "@jreback ah I see, didn't read the post properly. I guess in that case this issue is redundant and can be closed.\n",
      "@mattilyra its fine. you can post a fix and directly reference this.\n",
      "this still open @TomAugspurger ?\n",
      "Yeah, still open.\n",
      "@TomAugspurger did you have a fix for this already? (even if not prod ready), just want to give a try.\n",
      "@jreback I can get something together tonight, if that's not too late.\n\nThe basic idea is to use the logic in `pandas.core.format._get_level_lengths` on each of the MI elements. We'll put a couple more pieces of state in the internal dict `is_visible`, `row_span` and `col_span`.\n",
      "that would be totally fantastic!\n",
      "@jreback OK, the minimal version (1 whole test) is here: https://github.com/TomAugspurger/pandas/tree/style-sparse-mi-2\n\nI'll see what refactoring I can do and submit a pull request sometime soonish.\n",
      "![image](https://cloud.githubusercontent.com/assets/953992/16652616/b221a88e-4419-11e6-8713-d7eae7b4624a.png)\n\nhmm, doesn't seem to change the display. (using pandas 0.18.0 as a comparison). The column labels are not showing sparse (as what we normally do in an html display)\n",
      "Ahh, only did index labels. Give me 20 minutes :)\n",
      "@jreback pushed https://github.com/pydata/pandas/commit/4655534a40e65e0d380116607d983100c513da20 which fix it.\n\nhttps://gist.github.com/643402b241e6d945118942cb8ccff3ac\n\nIndex names are messed up with a  MultiIndex, but if you just set those to `\"\"` then the None shouldn't appear.\n",
      "that looks great! thanks!\n\nI realize that the CSS injection can be done thru `set_table_style` I think as well.\n",
      "@TomAugspurger so this fixes the single level index case (I don't have a good repro :<\nbut); also only barely tested on a single level columns (this works with the multi-level nicley as that was the point).\n\n```\ndef _get_level_lengths(index):\n    '''\n    Given an index, find the level lenght for each element.\n\n    Result is a dictionary of (level, inital_position): span\n    '''\n    sentinel = com.sentinel_factory()\n    levels = index.format(sparsify=sentinel, adjoin=False, names=False)\n\n    if index.nlevels == 1:\n        return { (0, i):1 for i, value in enumerate(levels) }\n\n    lengths = {}\n    for i, lvl in enumerate(levels):\n        for j, row in enumerate(lvl):\n            if row != sentinel:\n                last_label = j\n                lengths[(i, last_label)] = 1\n            else:\n                lengths[(i, last_label)] += 1\n\n    return lengths\n```\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "renamed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "labeled",
      "labeled",
      "commented",
      "labeled",
      "unlabeled",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 301,
    "deletions": 38,
    "changed_files_list": [
      "doc/source/html-styling.ipynb",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/formats/style.py",
      "pandas/tests/formats/test_style.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11666,
    "reporter": "gfairchild",
    "created_at": "2015-11-20T17:45:15+00:00",
    "closed_at": "2017-03-09T15:25:04+00:00",
    "resolver": "goldenbull",
    "resolved_in": "0cfc95055ca78ae0ba5189dd84f9319d175586a8",
    "resolver_commit_num": 0,
    "title": "ENH: add gzip/bz2 compression to read_pickle() (and perhaps other read_*() methods)",
    "body": "Right now, `read_csv()` has a `compression` option, which allows the user to pass in a gzipped or bz2-compressed CSV file directly into Pandas to be read. It would be great if `read_pickle()` supported the same option. Pickles actually compress surprisingly well; I have a 567M Pandas pickle (resulting from `DataFrame.to_pickle()`) that packs down to 45M with `pigz --best`. An order of magnitude difference in size is pretty significant. This makes storing static pickles long-term as gzipped archives a very attractive option. Workflow would be made easier if Pandas could natively handle my `dataframe.pickle.gz` files in the same way it does compressed CSV files.\n\nMore generally, a `compression` option should probably be allowed for most `read_*` methods. Many of the `read_*` methods involve formats that compress very well.\n",
    "labels": [
      "Enhancement",
      "Data IO",
      "Difficulty Novice",
      "Compat",
      "Effort Low"
    ],
    "comments": [
      "yeh, this wouldn't be hard for gzip/bz2\n",
      "xref #5924 \n",
      "Yes please. Especially for `read_json`!\n",
      "I like xz/lzma2 format for pickle format :smile: \n",
      "@goldenbull pull-requests are welcome! (this is not very difficult, more of a bit of code reorg to share the compression code)\n"
    ],
    "events": [
      "renamed",
      "renamed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files": 6,
    "additions": 324,
    "deletions": 19,
    "changed_files_list": [
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/generic.py",
      "pandas/io/common.py",
      "pandas/io/pickle.py",
      "pandas/tests/io/test_pickle.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11692,
    "reporter": "schwallie2",
    "created_at": "2015-11-24T16:47:07+00:00",
    "closed_at": "2016-02-12T20:42:51+00:00",
    "resolver": "TomAugspurger",
    "resolved_in": "cf8b7f8ac8f4ffab6136e933fb878446dd27cb03",
    "resolver_commit_num": 29,
    "title": "Style not able to take formatters",
    "body": "When using the new CSS styling, there doesn't seem to be a way to add formatters for the numbers. So trying to make the number data display as $10,000 when using any styling options currently seems off the table.\nDocumented here:\n-number-formatting-to-pandas-html-css-styling\n\nRelated to this PR: \n",
    "labels": [
      "Output-Formatting",
      "API Design",
      "IO HTML"
    ],
    "comments": [
      "I suppose the formatting should be expanded to cover all the functions in `Styler` not just `.set_precision`, so the following should take some kind of `format={}` kwarg that defines the format per column.\n- `.highlight_min`\n- `.highlight_max`\n- `.highlight_max`\n- `.bar`\n\nThis is partly implemented in #11667.\n",
      "API wise it'd be best to have a dedicated method for this, something like `.format` that takes a list of dictionaries mapping column to a python string formatting spec.\n",
      "Agree with the .format being better than implementing this everywhere. Can\nwe reuse formatters from to_HTML?\nOn Nov 25, 2015 7:15 AM, \"Tom Augspurger\" notifications@github.com wrote:\n\n> API wise it'd be best to have a dedicated method for this, something like\n> .format that takes a list of dictionaries mapping column to a python\n> string formatting spec.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/pydata/pandas/issues/11692#issuecomment-159604111.\n\n## \n\nThis e-mail is only intended for the person(s) to whom it is addressed and \nmay contain confidential information.  Any unauthorized review, use, \ndisclosure, or distribution is prohibited. If you received this e-mail in \nerror, please notify the sender by reply e-mail and then delete this \nmessage and any attachments from your system. Thank you for your \ncooperation.\n",
      "> Can we reuse formatters from to_HTML?\n\nYeah we should match that API if possible.\n\nI think our best bet implementation is to build up a more complicated dictionary outside of the template that contains another field with the formatted / to be displayed value. We'll look for that field in the template.\n"
    ],
    "events": [
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 240,
    "deletions": 75,
    "changed_files_list": [
      "doc/source/api.rst",
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/style.py",
      "pandas/tests/test_style.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11693,
    "reporter": "brendene",
    "created_at": "2015-11-24T16:52:53+00:00",
    "closed_at": "2016-02-01T21:38:25+00:00",
    "resolver": "varunkumar-dev",
    "resolved_in": "fc77cafe2a70cc08a81ebf2e855bca096f3bb7e6",
    "resolver_commit_num": 7,
    "title": "Inconsistent concat behavior between datetime64[ns] and tz-aware version in 0.17.1",
    "body": "This edge case appears when concatenating a timezone aware datetime series with another that is filled with only pd.NaT.  This works if the second series is only partially filled with pd.NaT.  It also works if the second series is another timezone (in that case the resulting Series is timezone unaware).\n\n\n",
    "labels": [
      "Bug",
      "Timeseries",
      "Reshaping",
      "Difficulty Novice",
      "Timezones",
      "Effort Low"
    ],
    "comments": [
      "0.17.1 I  c\n\ncan u see if this worked prior to 0.17.0?\n",
      "This works in 0.16.2, the dtype of timezoned series is object.\n",
      "hmm, looks like a bug, want to do a pull-request to fix?\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 11,
    "additions": 165,
    "deletions": 19,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/common.py",
      "pandas/core/series.py",
      "pandas/tests/indexes/test_datetimelike.py",
      "pandas/tests/series/test_constructors.py",
      "pandas/tests/test_groupby.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tseries/common.py",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11718,
    "reporter": "kdebrab",
    "created_at": "2015-11-28T18:03:48+00:00",
    "closed_at": "2016-02-11T23:35:54+00:00",
    "resolver": "sinhrks",
    "resolved_in": "d838db7b92a1cc79b4a36eff8c93d09339ac391f",
    "resolver_commit_num": 253,
    "title": "Timestamp subtraction of NaT with timezones",
    "body": "In pandas 0.17.1, a TypeError is returned when trying to subtract a timezone-aware timestamp from a NaT timestamp:\n\n\n\nSubtracting a timezone unaware timestamp from a NaT timestamp is no problem. Also subtracting a NaT from a timezone aware timestamp works:\n\n\n",
    "labels": [
      "Bug",
      "Missing-data",
      "Timezones",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "Thanks for the report. There seems to be several problems related to this. PR is appreciated!\n\n```\n# OK\npd.Timestamp('2011-01-01', tz='UTC') - pd.NaT\n# NaT  \n\npd.DatetimeIndex(['2011-01-01'], tz='UTC') - pd.NaT\n# TypeError: Timestamp subtraction must have the same timezones or no timezones\n\npd.NaT - pd.Timestamp('2011-01-01', tz='UTC')\n# TypeError: Timestamp subtraction must have the same timezones or no timezones\n\npd.NaT - pd.DatetimeIndex(['2011-01-01'], tz='UTC')\n# TypeError: Timestamp subtraction must have the same timezones or no timezones\n\n# NG\npd.DatetimeIndex(['2011-01-01']) - pd.NaT\n# TimedeltaIndex(['-91777 days +00:12:43.145224'], dtype='timedelta64[ns]', freq=None)\n\n# OK\npd.NaT - pd.DatetimeIndex(['2011-01-01'])\n# NaT\n\n# OK\npd.NaT - pd.Timestamp('2011-01-01')\n# NaT\n\n# OK\npd.Timestamp('2011-01-01') - pd.NaT\n# NaT\n```\n",
      "iirc there is an issue open about this - we need to have arithmetic methods on the  NaT scalar itself\n",
      "some of this is being addressed by #11564 \n",
      "@sinhrks / @kawochen \n\ncan you update this and see what we have left (e.g. that #11564) didn't already cover\n",
      "Looks no changes from the output attached above. I'll take a look.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "milestoned",
      "demilestoned",
      "labeled",
      "labeled",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 7,
    "additions": 178,
    "deletions": 50,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/tseries/base.py",
      "pandas/tseries/index.py",
      "pandas/tseries/tdi.py",
      "pandas/tseries/tests/test_base.py",
      "pandas/tseries/tests/test_tslib.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11732,
    "reporter": "jreback",
    "created_at": "2015-12-01T12:17:44+00:00",
    "closed_at": "2016-02-02T15:16:18+00:00",
    "resolver": "jreback",
    "resolved_in": "1dc49f51afe67fdc17fa2670545c053775765ebc",
    "resolver_commit_num": 3866,
    "title": "API: change .resample to be a groupby-like API",
    "body": "similar to #11603 \n\nthis would transform:\n\n`s.resample('D',how='max')`\n\nto\n\n`s.resample('D').max()`\n\nThis would be a breaking API change, as the default is `how='mean'`, meaning, that `s.resample('D')` returns the `mean` of the resampled data. However it would be visible at the very least and not simply change working code.\n\nThis would bring `.resample` (which is just a groupby type operation under the hood anyhow) into the API syntax for `.groupby` and `.rolling` et. al.\n\nFurthermore this would allow geitem / aggregate type operations with minimal effort\ne.g.\n\n`s.resample('D').agg(['min','max'])`\n",
    "labels": [
      "API Design",
      "Resample",
      "Difficulty Advanced",
      "Effort Medium"
    ],
    "comments": [
      "This change would also eliminate the need many of the current use cases for `pd.TimeGrouper`, which is a nice thing because that API is pretty well hidden right now.\n\nThis API will work well for downsampling (to a coarser time resolution), but it's not clear to me how it would work for upsampling or combined down/upsampling. For example, how would you upsample from daily to hourly data using forward filling with the new API? `s.resample('H').mean(fill_method='pad')`? Using a method like `mean` is a bit confusing in this context.\n",
      "`s.resample('H').pad()`\n",
      "I am not sure that combined up/downsampling is even possible now?\n",
      "or maybe to be more in-line\n\n`s.resample('H').ffill()`\n`s.resample('H').fillna(method='pad')` \n\n(or all the above)\n\nI guess \n\n`s.upsample('H').ffill()` is also possible :)\n",
      "Here's a simple example of combined up/downsampling:\n\n```\nIn [25]: idx = pd.to_datetime(['2000-01-01T06', '2000-01-01T12', '2000-01-03T00'])\n\nIn [26]: s = pd.Series(range(3), idx)\n\nIn [27]: s\nOut[27]:\n2000-01-01 06:00:00    0\n2000-01-01 12:00:00    1\n2000-01-03 00:00:00    2\ndtype: int64\n\nIn [28]: s.resample('1D')\nOut[28]:\n2000-01-01    0.5\n2000-01-02    NaN\n2000-01-03    2.0\nFreq: D, dtype: float64\n\nIn [29]: s.resample('1D', fill_method='pad')\nOut[29]:\n2000-01-01    0.5\n2000-01-02    0.5\n2000-01-03    2.0\nFreq: D, dtype: float64\n```\n",
      "I suppose we could have an optional `fill_method` kw in the `Resample` object\ne.g. in `s.resample('D',fill_method='pad')` if necessary (similar to how `.reindex` has this, but normally you would do a: `.reindex().ffill()`\n\ne.g.\n\n```\nIn [23]: s.resample('1D',how='mean').ffill()\nOut[23]: \n2000-01-01    0.5\n2000-01-02    0.5\n2000-01-03    2.0\nFreq: D, dtype: float64\n```\n\nwhich I would do like:\n`s.resample('1D').mean().ffill()`\n\nI guess `fill_method` would apply _while_ doing the mean intra-day I guess (though I don't think I can see a case for this).\n",
      "POC\n\n```\nIn [3]: s = Series(np.random.rand(1000), pd.date_range('20130101 09:00:00',freq='Min',periods=1000))\n\nIn [4]: r = s.resample2('H')\n\nIn [5]: r\nOut[5]: DatetimeIndexResampler [freq-><Hour>,axis->0,closed->left,label->left,convention->start,base->0]\n\nIn [6]: r.\nr.agg        r.aggregate  r.ax         r.mean       r.name       \n\nIn [6]: r.mean()\nOut[6]: \n2013-01-01 09:00:00    0.463474\n2013-01-01 10:00:00    0.496552\n2013-01-01 11:00:00    0.467690\n2013-01-01 12:00:00    0.542037\n2013-01-01 13:00:00    0.500808\n2013-01-01 14:00:00    0.541115\n2013-01-01 15:00:00    0.549489\n2013-01-01 16:00:00    0.567870\n2013-01-01 17:00:00    0.466067\n2013-01-01 18:00:00    0.468675\n2013-01-01 19:00:00    0.520051\n2013-01-01 20:00:00    0.495800\n2013-01-01 21:00:00    0.496541\n2013-01-01 22:00:00    0.437051\n2013-01-01 23:00:00    0.514727\n2013-01-02 00:00:00    0.517313\n2013-01-02 01:00:00    0.501945\nFreq: H, dtype: float64\n```\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 23,
    "additions": 2784,
    "deletions": 997,
    "changed_files_list": [
      "doc/source/api.rst",
      "doc/source/cookbook.rst",
      "doc/source/release.rst",
      "doc/source/timedeltas.rst",
      "doc/source/timeseries.rst",
      "doc/source/whatsnew/v0.10.0.txt",
      "doc/source/whatsnew/v0.18.0.txt",
      "doc/source/whatsnew/v0.9.1.txt",
      "pandas/core/base.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/ops.py",
      "pandas/core/window.py",
      "pandas/io/tests/test_excel.py",
      "pandas/tests/test_generic.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tests/test_window.py",
      "pandas/tseries/plotting.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11741,
    "reporter": "dwyatte",
    "created_at": "2015-12-02T16:37:39+00:00",
    "closed_at": "2016-01-17T03:43:25+00:00",
    "resolver": "nbonnotte",
    "resolved_in": "e9e85988ec1240803379ae35ce8590e94537e747",
    "resolver_commit_num": 5,
    "title": "Unexpected behavior with groupby on single-row dataframe?",
    "body": "Do single-row dataframes (not series) get special treatment in some way? It seems you can do arbitrary groupby operations on them on non-existent columns without errors.\n\n\n",
    "labels": [
      "Groupby",
      "Bug"
    ],
    "comments": [
      "Yeah, there is a problem. I discovered that while working on issue #11640\n\nI've a PR ready\n",
      "Solved with PR #12063\n"
    ],
    "events": [
      "commented",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "commented",
      "labeled",
      "milestoned",
      "labeled",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 10,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11742,
    "reporter": "stephen-hoover",
    "created_at": "2015-12-02T16:53:20+00:00",
    "closed_at": "2016-04-01T13:13:27+00:00",
    "resolver": "stephen-hoover",
    "resolved_in": "2c79a5039826a993d6b7aec152fe25b217968101",
    "resolver_commit_num": 5,
    "title": "Inconsistent return type when grouping dates by frequency with custom reduction function",
    "body": "If I group a `DataFrame` by a column of dates, the return type varies depending on whether I just group or whether I also apply a frequency in the `Grouper`. \n\nGrouping without resampling dates returns a `DataFrame` when I apply a function which returns a labeled `Series`, or a `Series` if the function returns a scalar:\n\n\n\nIf I apply a frequency in the `Grouper`, I get a `Series` with a multi-index when the function returns a labeled `Series`, or a `TypeError` when it returns a scalar.\n\n\n\nSince in this example, assigning dates to months still leaves the same groups, I would have expected identical results whether I set `freq='M'` or not. I'm guessing that the difference is that the `freq='M'` causes an extra `groupby` to happen under the hood, yes? When I ran into this, what I expected to happen was for `pd.Grouper(freq='M', key='date')` to do a single `groupby`, combining rows where dates happened to fall into the same month.\n\nPandas version:\n\n\n",
    "labels": [
      "Groupby",
      "Dtypes",
      "API Design",
      "Resample",
      "Difficulty Advanced",
      "Effort Medium"
    ],
    "comments": [
      "I guess. this is a quite tricky code path. Welcome for you to take a stab at making them consistent.\n\nKeeping in mind that `.apply` may not always be able to do the same thing as it has to infer return shapes and such.\n\nYou should avoid custom functions this as they are non-performant anyhow.\n",
      "xref https://github.com/pydata/pandas/issues/9867\n",
      "I might be able to take a look at this over Christmas, but I think I'll be too busy before then. \n\nI wouldn't use a custom function for something like `sum`, but sometimes I have aggregations which aren't built-in.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 70,
    "deletions": 21,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11745,
    "reporter": "jtkiley",
    "created_at": "2015-12-02T23:25:07+00:00",
    "closed_at": "2017-04-02T22:55:23+00:00",
    "resolver": "jreback",
    "resolved_in": "cd24fa95f1781b14d35eac4953bab02691fd9d04",
    "resolver_commit_num": 4346,
    "title": "Add origin parameter to Timestamp/to_datetime epoch support.",
    "body": "When using SAS or Stata data, dates are represented as the number of days since 1/1/1960, and other statistical software uses different [origin dates](-dates-and-times-from-other-software/). With that in mind, it would be nice to have an origin date that can be specified. See also, #3969.\n\nIt's a relatively simple thing, and not hard to work around, of course. However, I end up dealing with date formatting on just about every data set I import, and I imagine that lots of others do, too.\n\nCurrently, I do something like this:\n\n\n\nIn R, the `as.Date()` function takes an origin parameter for numeric types (see, [manual](-manual/R-devel/library/base/html/as.Date.html)). So, in R, the date part would simply be:\n\n\n",
    "labels": [
      "Timeseries",
      "API Design",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "sure this could be a parameter to `to_datetime` (only) as that is the datetime converter\ncould work alongside the `unit` parameter (like https://github.com/pydata/pandas/issues/11276).\n\nIn fact that should be `origin` rather than `unit` now that I think about it. xref #11470 \n",
      "@jtkiley Is `read_stata` not returning the correct dates by default?\n",
      "It does not (pandas 0.19.1). Below, see one of my Stata datasets (originally written by R). In this example, I've formatted the date like my original post but without the epoch adjustment. I have some type stuff going on here that I'd fix in a real project, but you can see that the separate year variable is 10 years off from the date.\r\n\r\n~~~\r\nlpermno       date id_ticker    year  \\\r\n12060.0 2014-03-02        GE  2004.0   \r\n86868.0 2016-11-05        GS  2006.0   \r\n24643.0 2018-07-29        AA  2008.0   \r\nNaN 2011-02-01       DAL  2001.0\r\n\r\n~~~\r\n\r\n\r\nHere's the same data in Stata, also without adjustment, though I formatted the date using `format %td date`.\r\n\r\n~~~\r\nlpermno\tdate\tid_ticker\tyear\r\n12060\t01mar2004\tGE\t2004\r\n86868\t05nov2006\tGS\t2006\r\n24643\t28jul2008\tAA\t2008\r\n.z\t31jan2001\tDAL\t2001\r\n\r\n~~~\r\n",
      "@jtkiley Could you post a small DTA that demonstrated this issue?",
      "@jtkiley Any chance for sharing a DTA with this issue?",
      "Sorry for the delay. Here's one that I reduced down (columns and rows) to what you see above. It was originally written by R and then reduced and saved using Stata. It continues to exhibit this issue.\r\n\r\nIt's also zipped to make Github happy.\r\n[data_epoch.zip](https://github.com/pandas-dev/pandas/files/719757/data_epoch.zip)\r\n\r\n",
      "I can't reproduce it.  When I use `read_stata`, I get:\r\n\r\n```\r\n   lpermno     date id_ticker    year\r\n0  12060.0  16131.0        GE  2004.0\r\n1  86868.0  17110.0        GS  2006.0\r\n2  24643.0  17741.0        AA  2008.0\r\n3      NaN  15006.0       DAL  2001.0\r\n```\r\n\r\nwhich is identical to what Stata shows.  When I convert date to a data column in Stata using `format %td date` and save it as `date_epoch_td.dta', reading this gets\r\n\r\n```\r\n   lpermno       date id_ticker    year\r\n0  12060.0 2004-03-01        GE  2004.0\r\n1  86868.0 2006-11-05        GS  2006.0\r\n2  24643.0 2008-07-28        AA  2008.0\r\n3      NaN 2001-01-31       DAL  2001.0\r\n```\r\n\r\nwhich seems to be correct.",
      "@bashtage Right. The problem is when you convert the epoch time using `data['date'] = pd.to_datetime(data['date'],unit='D')`. If you use my adjustment above, it's right. If not, you end up with the results I showed above. The interpretation problem results from Stata using 1/1/1960 and pandas using 1/1/1970 as the base of epoch time.",
      "@jtkiley I\u00a0see.  I thought it was a bug in\u00a0`read_stata`.  FWIW if you export your Stata dates as dates, and not integers/floats then `read_stata` will correctly use the 1960 epoch date when reading the data in.",
      "@bashtage That makes sense. I was thinking of a `to_datetime` parameter for setting the origin, as Stata formats aren't the only place that this occurs, and R has such a parameter (presumably for the same reason).\r\n\r\nI often see it when moving data around or pulling it from sources that have a Stata export option, and those often don't come with the date formatting intact. I tend to use those export options (often with Stata for co-author accessibility), assemble data in pandas (R in the past), and then export it in Stata format for sharing and analysis. ",
      "It looks like #11470 has the `origin` parameter basically done. It just has some work left finishing it up.",
      "@bashtage note that #15828 just provides the tools to deal with this, should this be an additional parameter to those reader? (is there actually meta data that *tells* you the origin?). Could certainly just start this as a doc / post-processing step in any event.",
      "I don't see a strong reason to allow arbitrary offsets in the Stata interface code.  The present version is very loyal to the Stata dta format spec and allowing a semi-random option to be internalized rather than chained seems like the wrong way to do things.  \r\n\r\nI suppose without explicit support one would have to do something like\r\n\r\n```\r\npd.to_datetime(dates.astype(np.int64), origin='1-1-1960')\r\n```\r\n\r\nMaybe there would be an easier way to re-originate existing date-times.",
      "@bashtage makes sense."
    ],
    "events": [
      "commented",
      "cross-referenced",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 8,
    "additions": 317,
    "deletions": 26,
    "changed_files_list": [
      "doc/source/timeseries.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/_libs/tslib.pyx",
      "pandas/tests/indexes/datetimes/test_tools.py",
      "pandas/tests/indexes/timedeltas/test_ops.py",
      "pandas/tests/scalar/test_timedelta.py",
      "pandas/tseries/tdi.py",
      "pandas/tseries/tools.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11758,
    "reporter": "bhy",
    "created_at": "2015-12-04T13:40:16+00:00",
    "closed_at": "2016-04-30T18:47:41+00:00",
    "resolver": "jreback",
    "resolved_in": "286782da98a5598d36885b548c8ce9d41ea1b0cb",
    "resolver_commit_num": 4008,
    "title": "to_datetime returns NaT for epoch with errors='coerce'",
    "body": "xref #11760 \n\nParsing unix epoch timestamps give NaT with `errors='coerce'` while they can be parsed correctly without it:\n\n\n\npandas: 0.17.1\nnumpy: 1.10.1\nPython 2.7.7\n",
    "labels": [
      "Bug",
      "Timeseries",
      "Difficulty Novice",
      "Compat",
      "Effort Low"
    ],
    "comments": [
      "hmm, that does seem like a bug...\n\npull-requests welcome!\n",
      "I will take this one.\n",
      "@engelmav want to do the PR?\n",
      "hey Jeff, sure. I don't have the fix yet. had to pause due to issues at\nwork.\n\nOn Sunday, January 24, 2016, Jeff Reback notifications@github.com wrote:\n\n> @engelmav https://github.com/engelmav want to do the PR?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/pydata/pandas/issues/11758#issuecomment-174345613.\n",
      "Similarly, this is also not working as desired\n\n```\n>>> pd.to_datetime(11111111, unit='D', errors='ignore')\nOverflowError: long too big to convert\n```\n",
      "@jreback I was looking at this, and some questions:\n\nIt seems this is deliberate, given the comment here: https://github.com/pydata/pandas/blob/084391126cc4edda35b41378d2905c788e5b573a/pandas/tslib.pyx#L2041 (\"if we are coercing, dont' allow integers\"). And I suppose this is to deal with the case of mixed types. For example, if you are parsing strings: `pd.to_datetime(['2012-01-01', 1])`, you want the integer to be coerced to NaT. \nBut, of course when having only integers, this behaviour is not correct.\n\nBut, I  was also wondering, is there a reason we don't have a special path specifically for converting epochs (integers with `unit` specified), instead of using the general `tslib.array_to_datetime` where the types are checked for each element? \n(simplified idea)\n\n```\ndef to_datetime(arg, ...):\n\n    ....\n    elif com.is_integer(arg):\n        _epoch_to_datetime(arg, unit)\n    ....\n\ndef _epoch_to_datetime(vals, unit):\n    return vals.astype('datetime[{}]'.format(unit))\n\n```\n",
      "@jorisvandenbossche so it _could_ be in a routine. What you could do is multiply then feed it back to `to_datetime` with the same `coerce` option. you can directly astype if only if `errors='raise'`. I think ignore you might have to iterate as well.\n",
      "further I suspect we can error out on the above example as `unit` doesn't make sense when you don't have integers/floats (though you _can_ allow mixed things as long as you can astype them to ints/floats) eg.\n\n```\nIn [1]: pd.to_numeric(['1.0',2,3.0])\nOut[1]: array([ 1.,  2.,  3.])\n```\n\nin fact could be a pre-processing step assume we pass thru the `errors`, because that's the rub you have to potentially iterate here.\n",
      "I think I can fix this shortly.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "milestoned",
      "demilestoned",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 231,
    "deletions": 26,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/io/json.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tools.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11773,
    "reporter": "michaelaye",
    "created_at": "2015-12-06T12:46:27+00:00",
    "closed_at": "2016-05-16T12:09:51+00:00",
    "resolver": "quintusdias",
    "resolved_in": "62bed0e33397132bd4340c8da54c3feeb22e5083",
    "resolver_commit_num": 0,
    "title": "read_hdf not supporting pathlib.Path",
    "body": "According to the Enhancements text for 17.1:\n\n> pd.read_\\* functions can now also accept pathlib.Path, or py._path.local.LocalPath objects for the filepath_or_buffer argument. (GH11033) \n\nall read_\\* functions should support pathlib.Path now. It works for me with `read_csv` but not for `read_hdf`. \nInterestingly, I find in the squashed PR for this issue, 0d3dcbb, only a comment regarding `read_csv` and `read_table`, while the merge 82f0033 talks about `read_*`. Is there something missing for `read_hdf` ?\n",
    "labels": [
      "Testing",
      "IO HDF5",
      "Compat",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "yeh, this was not tested on all of the IO formats. pretty sure that `csv/excel/stat/msgpack/json` should work as they all use the same io calls. HDF uses a slightly different one as it doesn't support reading from buffers in the same way.\n\nSo need tests for each of these formats (and not just the IO call itself) on read/to\nHDF test/fix\n\ncc @flying-sheep \n",
      "jup, [you told me](https://github.com/pydata/pandas/issues/11033#issuecomment-138912214)\n\n> [here](https://github.com/pydata/pandas/blob/master/pandas/io/common.py#L127) is where all of this path inference is done (well not 100% sure of all but vast majority)\n\nso it only was the vast majority, not all :smile: \n\nPS: the link is broken by now (tip: press <kbd>y</kbd> on some github page to get a canonical url so that this doesn\u2019t happen anymore)\n",
      "`to_csv` also does not support pathlib.Path yet. (v0.17.1), while `read_csv` does, interestingly.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 47,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/io/pytables.py",
      "pandas/io/tests/test_pytables.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11786,
    "reporter": "mrocklin",
    "created_at": "2015-12-07T16:12:34+00:00",
    "closed_at": "2016-01-19T20:02:06+00:00",
    "resolver": "jdeschenes",
    "resolved_in": "567bc5ceb33d2147ca68b6eee9b180e6059ae247",
    "resolver_commit_num": 1,
    "title": "read_csv in multiple theads causes segmentation fault",
    "body": "The following script causes a segfault on my machine\n\n\n\n\n\nPython 3.4, Pandas 0.17.1, Ubuntu 14.04\n",
    "labels": [
      "Bug",
      "CSV",
      "Prio-high",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "FWIW on OSX that script just hangs at 99% CPU use. pandas 0.17.1, python 3.5.\n",
      "cc @jdeschenes can you have a look\n",
      "@mrocklin thanks for the repro!\n",
      "I think I found the issue, see my pull request. The issue was caused by a misplaced PyGilState_ensure(It was called after a Py_XDECREF being called.\n\nUsing read_csv with threads on such an object might have a big impact on performance.\n"
    ],
    "events": [
      "cross-referenced",
      "renamed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "labeled",
      "cross-referenced",
      "referenced",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 85,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/parser/io.c"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11831,
    "reporter": "jorisvandenbossche",
    "created_at": "2015-12-12T15:20:38+00:00",
    "closed_at": "2016-04-26T19:21:19+00:00",
    "resolver": "rs2",
    "resolved_in": "7fbc600e6f11ab256dc873175b971ca06d5ce456",
    "resolver_commit_num": 0,
    "title": "Performance issue with timeseries plotting on py3?",
    "body": "I noticed a performance issue with plotting timeseries. After some trying with different environments (different pandas, matplotlib and python versions), it seems there is a problem on python 3 -> up to 10 x slowdown compared to python 2.7:\n\nPython 2 and pandas 0.16.2 and 0.17.1:\n\n\n\n\n\nWith python 3, pandas 0.16.2 and 0.17.1:\n\n\n\n\n",
    "labels": [
      "Performance",
      "Regression",
      "Visualization"
    ],
    "comments": [
      "this is the period index conversion, which IIRC you fixed?\n",
      "You are referring to https://github.com/pydata/pandas/pull/11194 I think. This fixed a perf regression before 0.17.0 was released (introduced between 0.16.2 and 0.17.0). Do you see a possible reason that this fix would not work on py3?\n",
      "Has this issue been fixed or are there workarounds?\nI do have them same Problem (python3 is about 16 times slower).\n\nAre things moving on this issue?\n",
      "@Rittmeister123 Not that I know (I haven't had time myself to dive into it).\n\nIf you want to try to profile, to see where the slowdown is coming from, that would be very welcome!\n",
      "I'm new here (my first entry :) ), so please excuse possible format-issues or something else.\n\n@jorisvandenbossche I did some profiling with the example from above:\n\nMy Setup for Python 3:\n\n```\nIn [1]: sys.version\nOut[1]: '3.5.1 |Anaconda 2.4.1 (64-bit)| (default, Dec  7 2015, 15:00:12)\n[MSC v.1900 64 bit (AMD64)]\n\nIn[2]: pd.__version__\nOut[2]: '0.17.1'\n\nIn[3]: matplotlib.__version__\nOut[3]: '1.5.0'\n\nIn[4]: np.__version__\nOut[4]: '1.10.1'\n\n```\n\nMy Setup for Python 2:\n\n```\nIn [1]: sys.version\nOut[1]: '2.7.11 |Continuum Analytics, Inc.| (default, Dec  7 2015, 14:10:42)\n[MSC v.1500 64 bit (AMD64)]'\n\nIn[2]: pd.__version__\nOut[2]: u'0.17.1'\n\nIn[3]: matplotlib.__version__\nOut[3]: '1.5.0'\n\nIn[4]: np.__version__\nOut[4]: '1.10.1'\n\n```\n\nThe dataframe is generated with:\n\n```\nIn [1]: N = 2000\n        df = pd.DataFrame(np.random.randn(N, 5), index=pd.date_range('1/1/1975', periods=N))\n```\n\nTimeit on python2:\n\n```\nIn [1]: %timeit df.plot()\nOut[1]: 1 loops, best of 3: 111 ms per loop\n\n```\n\nTimeit on python3:\n\n```\nIn [1]: %timeit df.plot()\nOut[1]: 1 loops, best of 3: 1.01 s per loop\n```\n\nAttached you can find the Profiling files generated with\n\n```\nIn[1]: %prun -D python2_df_plot.prof df.plot()\n```\n\nand\n\n```\nIn[1]: %prun -D python3_df_plot.prof df.plot()\n\n```\n\n[profiling_pandas_plot.zip](https://github.com/pydata/pandas/files/82356/profiling_pandas_plot.zip)\n\nPlease have a look at it, since i have no knowledge on profiling\n",
      "@Rittmeister123 any luck with this?\n",
      "Unfortunately not!\nI'm quiet new in python and do not had time to take a close look whats going on and read the profiling..\n",
      "@jorisvandenbossche any thoughts on this?\n",
      "FYI: \nA few days ago I tried to plot a dataframe with the use_index Parameter set to false...and recognized a speed up...but hadn't had time to exactly verify this and proof it...\n",
      "Did some quick profiling, and one of the elements is in any case the difference in performance of `PeriodIndex._mpl_repr()`, which is just a call to `PeriodIndex._get_object_array`:\n\n```\nIn [8]: sys.version\nOut[8]: '3.5.0 |Anaconda 2.4.0 (64-bit)| (default, Nov  7 2015, 13:15:24) [MSC v.1900 64 bit (AMD64)]'\n\nIn [9]: pd.__version__\nOut[9]: '0.17.1'\n\nIn [10]: pidx = pd.period_range('1975-01-01', periods=2000)\n\nIn [11]: %timeit pidx._mpl_repr()\n1 loops, best of 3: 461 ms per loop\n```\n\nvs\n\n```\nIn [6]: sys.version\nOut[6]: '2.7.11 |Anaconda 1.7.0 (64-bit)| (default, Jan 19 2016, 12:08:31) [MSCv.1500 64 bit (AMD64)]'\n\nIn [7]: pd.__version__\nOut[7]: '0.16.2'\n\nIn [8]: pidx = pd.period_range('1975-01-01', periods=2000)\n\nIn [9]: %timeit pidx._mpl_repr()\n100 loops, best of 3: 5.25 ms per loop\n```\n\nIn turn, this boils down to calls to `Period.from_ordinal()`:\n\n```\nIn [12]: sys.version\nOut[12]: '3.5.0 |Anaconda 2.4.0 (64-bit)| (default, Nov  7 2015, 13:15:24) [MSCv.1900 64 bit (AMD64)]'\n\nIn [13]: pd.__version__\nOut[13]: '0.17.1'\n\nIn [14]: %timeit pd.Period._from_ordinal(ordinal=1, freq='D')\n1000 loops, best of 3: 476 \u00b5s per loop\n```\n\nvs\n\n```\nIn [6]: sys.version\nOut[6]: '2.7.11 |Anaconda 1.7.0 (64-bit)| (default, Jan 29 2016, 14:26:21) [MSCv.1500 64 bit (AMD64)]'\n\nIn [7]: pd.__version__\nOut[7]: '0.17.1+315.g62363d2'\n\nIn [8]: %timeit pd.Period._from_ordinal(ordinal=1, freq='D')\n10000 loops, best of 3: 42.1 \u00b5s per loop\n```\n\nNow, what would cause the dramatic difference in performance in the `from_ordinal` method between python 2 and 3, is still a mystery to me (and I also don't have time to look into further). \n@jreback any idea?\n\n@blbradley you did some work on the Period cython code. Do you have by any chance an idea where this peformance difference could be coming from?\n",
      "I think https://github.com/pydata/pandas/blob/master/pandas/src/period.pyx#L658\n\nneeds this instead of doing the string interpretation each time\n\n```\nif isinstance(freq, offsets.DateOffset):\n    return freq\n```\n\nalso the import can be replace by `offsets.to_offset` (below)\n\nas when `_mpl_repr` is called an already constructed freq object is passed (and not the string as in the case above).\n\nThere maybe something else going on in the actual `to_offset` to explain the py2/3 diff though (as string conversion should be similar perf)\n",
      "I actually did a wrong timeit, as the `freq` is already an offset in the case of plotting:\n\n```\n# python 3\nIn [24]: %timeit pd.Period._from_ordinal(ordinal=1, freq=pidx.freq)\n1000 loops, best of 3: 233 \u00b5s per loop\n\nIn [25]: type(pidx.freq)\nOut[25]: pandas.tseries.offsets.Day\n```\n\nvs\n\n```\nIn [11]: %timeit pd.Period._from_ordinal(ordinal=1, freq=pidx.freq)\nThe slowest run took 6.19 times longer than the fastest. This could mean that an\n intermediate result is being cached\n100000 loops, best of 3: 5.5 \u00b5s per loop\n```\n\nwhich makes the difference even larger\n",
      "Strange, I don't see a difference in performance of `to_offset`, while there is in `_maybe_convert_freq`:\n\npython 3:\n\n```\nIn [43]: %timeit pd.Period._maybe_convert_freq(pidx.freq)\n1000 loops, best of 3: 200 \u00b5s per loop\n\nIn [44]: %timeit to_offset(pidx.freq)\nThe slowest run took 10.28 times longer than the fastest. This could mean that a\nn intermediate result is being cached\n1000000 loops, best of 3: 479 ns per loop\n```\n\nvs python 2:\n\n```\nIn [19]:  %timeit pd.Period._maybe_convert_freq(pidx.freq)\nThe slowest run took 6.87 times longer than the fastest. This could mean that an\n intermediate result is being cached\n100000 loops, best of 3: 4.42 \u00b5s per loop\n\nIn [20]: %timeit to_offset(pidx.freq)\nThe slowest run took 13.08 times longer than the fastest. This could mean that a\nn intermediate result is being cached\n1000000 loops, best of 3: 502 ns per loop\n```\n",
      "try removing the import in `_maybe_convert_freq` (use `offsets.to_offset` instead)\nwould still add the check for `DateOffset` as its doing extra work\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 5,
    "additions": 56,
    "deletions": 53,
    "changed_files_list": [
      "asv_bench/benchmarks/period.py",
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/src/period.pyx",
      "pandas/tseries/frequencies.py",
      "pandas/tseries/tests/test_tslib.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11836,
    "reporter": "AlbertDeFusco",
    "created_at": "2015-12-13T21:11:01+00:00",
    "closed_at": "2016-02-13T13:34:34+00:00",
    "resolver": "jreback",
    "resolved_in": "a8be55ca0d5b816cdc827343aafa0ce8fcde9924",
    "resolver_commit_num": 3887,
    "title": "int64 Index in 0.17 typcasts '0' string to integer",
    "body": "Here's what I got. Is this expected behavior? I could not find a reference for this functionality in the release notes.\n## 0.16\n\n`conda create -n pd pandas=0.16 python=3.4 ipython`\n\nIn 0.16 the dtype of the index changes to object when adding a row with '0'.\n\n\n## 0.17.1\n\n`conda create -n pd pandas=0.17 python=3.4 ipython`\n\nIn 0.17.1 The index dtype does not change, but typecasts to the integer 0. Repeated assignment at the integer 0 index appends more 0s to the index.\n\n\n",
    "labels": [
      "Bug",
      "Indexing",
      "Dtypes",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "I suppose this should work to be consistent. Might be a bit tricky. This is a failing of `Index.insert` where the dtype of the inserted element is inferred using the dtype of the current index, which is a tricky thing; you almost alway want to do this because it will raise if its not a compatible element, except when it happens that a string version is DIRECTLY convertible (in this case to a numpy array). \n",
      "It's worse still though, because `m[0]=123` will modify an existing row, but `m[\"0\"]=123` will add more rows. So even in the crazy world of PHP-style type casting, the behavior is different depending on the type of the thing that gets cast.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 10,
    "additions": 547,
    "deletions": 227,
    "changed_files_list": [
      "doc/source/release.rst",
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/indexing.py",
      "pandas/indexes/base.py",
      "pandas/indexes/numeric.py",
      "pandas/tests/frame/test_indexing.py",
      "pandas/tests/indexes/test_base.py",
      "pandas/tests/indexes/test_numeric.py",
      "pandas/tests/test_indexing.py",
      "pandas/tseries/period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11847,
    "reporter": "potash",
    "created_at": "2015-12-15T18:08:24+00:00",
    "closed_at": "2016-12-10T15:38:14+00:00",
    "resolver": "kordek",
    "resolved_in": "d531718749ed686a975cae92a13e9ab9bd5aac6d",
    "resolver_commit_num": 1,
    "title": "Unstack with mixed dtypes coerces everything to object",
    "body": "Related to #2929, if I unstack a dataframe with mixed dtypes they all get coerced to object and I have to recast to go back which is surprisingly slow (30 seconds for 400k rows and 400 np.float32 columns)\n\nIs there any reason pandas doesn't keep the np.float32 dtype, especially since it supports missing values so even when there are missing index/column positions it shouldn't pose a problem?\n",
    "labels": [
      "Reshaping",
      "Dtypes",
      "Bug",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "pls show a copy-pastable example\n",
      "Ah, looking for an example helped me narrow down the bug. It is specific to passing a list of levels to unstack, even when that list only has a single entry. E.g. compare:\n\n```\n> df = pd.DataFrame({'state':['IL', 'MI'], 'index':['a','a'], 'value1':[1.0,1.0], 'value2':['c','c'] })\n> df.set_index(['state','index']).unstack(['index']).dtypes\n\n        index\nvalue1  a        object\nvalue2  a        object\ndtype: object\n\n> df.set_index(['state','index']).unstack('index').dtypes\nindex\nvalue1  a        float64\nvalue2  a         object\ndtype: object\n```\n\nSo a workaround in my case with multiple levels is to replace `unstack(['index1', 'index2'])` with `unstack('index1').unstack('index2')` and indeed I checked that it works.\n",
      "so looks like what you want is: https://github.com/pydata/pandas/pull/9023\n\nwhich is almost finished. in fact if you are looking for something to do...could use some updating :)\n",
      "i'll mark this as a bug, which may be independent. want to see if you can put in a fix with the existing framework?\n",
      "Thanks! I will try but I do not use pandas from master and I've never played with the source so it won't be quick.\n",
      "Picking this up to take a look\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "milestoned",
      "labeled",
      "cross-referenced",
      "commented",
      "labeled",
      "labeled",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 46,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.2.txt",
      "pandas/core/reshape.py",
      "pandas/tests/frame/test_reshape.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11852,
    "reporter": "ohadle",
    "created_at": "2015-12-16T09:35:21+00:00",
    "closed_at": "2016-03-23T20:38:17+00:00",
    "resolver": "terfilip",
    "resolved_in": "247fe0718c3b09a2de4d1af834cff8efb9f8edcc",
    "resolver_commit_num": 0,
    "title": "xz compression in to_csv()",
    "body": "I use compression directly in DataFrame.to_csv() to save on disk space / IO. Would be nice to have support for xz compression there.\n\nSimilar to , but I think in to_csv this has more added value.\n",
    "labels": [
      "Enhancement",
      "CSV",
      "Effort Medium",
      "Difficulty Novice"
    ],
    "comments": [
      "I suppose if someone wanted to add additional compressor to read/write csv, this would be ok.\n\ncan you point to the canonical package which implements this compressor?\n",
      "I haven't tried it myself, but it looks like for python 3 it would be https://docs.python.org/3/library/lzma.html. For 2.7 this looks like it: https://github.com/peterjc/backports.lzma\n",
      "sure that looks reasonable.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "unlabeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files": 12,
    "additions": 105,
    "deletions": 10,
    "changed_files_list": [
      "ci/requirements-2.7.pip",
      "doc/source/install.rst",
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/compat/__init__.py",
      "pandas/core/frame.py",
      "pandas/io/common.py",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/parser.pyx",
      "pandas/tests/frame/test_to_csv.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11853,
    "reporter": "kaotika",
    "created_at": "2015-12-16T11:08:03+00:00",
    "closed_at": "2016-05-26T12:43:40+00:00",
    "resolver": "gliptak",
    "resolved_in": "f2ce0ac6ecd31d9bf48366ecac293b092279c174",
    "resolver_commit_num": 27,
    "title": "ERR: better exception for converting bool to datetime with 0.17",
    "body": "While updating pandas-qt I stumbled about the following exception. Running the same with pandas 0.16 doesn't shows any exception. I think converting a bool to a datetime should raise some exception, but not the one shown. ;-)\n\n\n",
    "labels": [
      "Timeseries",
      "Difficulty Novice",
      "Error Reporting",
      "Effort Low"
    ],
    "comments": [
      "yep, could be a more informative message.\n\npull-requests are welcome!\n\ncc @sinhrks \n",
      "note that the default changes for how errors are handled changed in 0.17.0\n\n```\nIn [8]: df[\"bool\"].apply(pd.to_datetime,errors='ignore')\nOut[8]: \n0    True\nName: bool, dtype: bool\n```\n\nBut the error could still be more informative\n",
      "Thanks for reply. I should read the changelogs in the future.\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 43,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11854,
    "reporter": "JCalderan",
    "created_at": "2015-12-16T15:31:32+00:00",
    "closed_at": "2016-03-15T14:22:14+00:00",
    "resolver": "sinhrks",
    "resolved_in": "4da9b15f58cfcde998d6279a90d55a170145f8c2",
    "resolver_commit_num": 257,
    "title": "Can't get period code with frequency alias 'minute' or 'Minute'",
    "body": "Hi,\n\nWhile playing with the frequency module, I might have found a bug with the function __period_str_to_code_: \nthe function didn't return the code 8000 for the frequency string \"minute\", yet it works for frequency strings as 'T', 'Min', 'min'...\n\nHere's the code to reproduce the 'bug':\n\n\n\n(in pandas/tseries/frequencies.py line 783 to line 813)\nIt appears that 'minute' is converted into the alias 'Min' during the function **__period_str_to_code**_ (line 807).\n'Min' is then used as a key in the dictionnary **__period_code_map**_, but this dictionnary doesn't hold any value for this key (line 813).\nIndeed, the key 'Min' is stored in the dictionnary **__lite_rule_alias**_ where it indexes the value 'T' (the correct frequency string for minutes).\n\nThis particuliar situation could be solved by transforming the return of the function **__period_str_to_code**_ by using a recursive call which allows to 'safely' handle the converted frequency string (_period_str_to_code('minute') > 'Min' > _period_str_to_code('Min') > 8000).\nThe conversion from 'Min' to 'T' occures line 799, and the conversion from 'T' to 8000 occures line 804.\n\nBut I'm not sure of the implication of such a recursive call (performance impact, possibility of an 'infinite' recursive call in some situation, and so on).\nHere is the function with a recursive call (change in pandas/tseries/frequencies.py line 813):\n\n\n",
    "labels": [
      "Frequency",
      "Bug"
    ],
    "comments": [
      "cc @sinhrks \n",
      "Should we change this to `T`? Others are all mapped to single character representation.\n\nhttps://github.com/pydata/pandas/blob/master/pandas/tseries/frequencies.py#L708\n",
      "that seems right\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "commented",
      "labeled",
      "commented",
      "cross-referenced",
      "milestoned"
    ],
    "changed_files": 5,
    "additions": 67,
    "deletions": 6,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/src/period.pyx",
      "pandas/tseries/frequencies.py",
      "pandas/tseries/tests/test_frequencies.py",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11858,
    "reporter": "diazona",
    "created_at": "2015-12-17T14:51:14+00:00",
    "closed_at": "2016-05-13T13:19:16+00:00",
    "resolver": "gliptak",
    "resolved_in": "e5c18b4383bd49b7a6f42f9e3c299c8746b5a347",
    "resolver_commit_num": 25,
    "title": "Index without 0 in xerr/yerr causes KeyError",
    "body": "When creating an errorbar plot, if the data object being used for the errors (`xerr` or `yerr`) doesn't have 0 in its index, it produces a `KeyError` from matplotlib code. I can produce this with the following code sample:\n\n\n\nThe underlying bug (if it is a bug) in matplotlib is responsible for several other issues, including #4493 and #6127, but this case is different because it uses only Pandas API methods, rather than passing a Pandas object to a matplotlib method. So it's a little harder to justify passing this off on matplotlib to fix, as was done in e.g. #6127.\n\nIf the \"proper\" fix ever is implemented in matplotlib code, it should solve this as well as #4493 and #6127 and all the others of that nature, but until that point, Pandas can work around it by converting the error object (if it is a `Series` or `DataFrame`) to an `ndarray`. I'm working on this in 4b04f80c41684ba9ab05ce8c87b17b96fde87290 but I'm not sure if there's a better way to fix it, or if this breaks something. (If so, the tests don't indicate it.)\n",
    "labels": [
      "Visualization",
      "Bug",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "@diazona This also seems a regression (in pandas or matplotlib), as I don't get an error with pandas 0.16.2 and matplotlib 1.4.3, but do get one with 0.17.1 and 1.5.0\n",
      "Haven't looked, but sounds similar to https://github.com/matplotlib/matplotlib/issues/5550\n\n@diazona I'm assuming you're using matplotlib 1.5? Can you try with 1.4.3?\n",
      "Yeah, matplotlib/matplotlib#5550 seems like the exact same issue. That didn't come up in my search, so I hadn't realized the matplotlib developers were open to fixing it. If they do implement that fix, it renders my patch obsolete (though I think it's still a semantic improvement).\n\nLet me take some time to get a previous version of matplotlib and test with that. I am indeed using 1.5.0. Here's the complete version info:\n\n```\n>>> pd.show_versions()\n\nINSTALLED VERSIONS\n------------------\ncommit: 4b04f80c41684ba9ab05ce8c87b17b96fde87290\npython: 3.4.3.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.2.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.17.1+84.g4b04f80\nnose: 1.3.7\npip: 7.1.2\nsetuptools: 18.5\nCython: 0.23.4\nnumpy: 1.10.1\nscipy: 0.16.1\nstatsmodels: 0.6.1\nIPython: 4.0.1\nsphinx: None\npatsy: 0.4.1\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.4.6\nmatplotlib: 1.5.0\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\n```\n",
      "Update: matplotlib 1.4.3 indeed does not exhibit the error. After an enthralling `git bisect` run, I traced the `KeyError` to code introduced in matplotlib/matplotlib@5f33e7da3c466fe0c74e1e7087548ec941bb228b, on the 1.5.0 development branch.\n\nFor what it's worth, the current master HEAD matplotlib/matplotlib@9f6bb901f92a281dac54dde30b3c4319cd727d3f _does_ raise the `KeyError`.\n\nI haven't tested with other versions of Pandas.\n",
      "Can someone re-check if this is still a problem?  I think we fixed this is v1.5.1.\n",
      "yes, here v. 1.5.1 and the same problem\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "milestoned",
      "demilestoned",
      "labeled",
      "labeled",
      "cross-referenced",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 12,
    "deletions": 5,
    "changed_files_list": [
      "codecov.yml",
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/tools/plotting.py",
      "pandas/tseries/tests/test_plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11871,
    "reporter": "dpinte",
    "created_at": "2015-12-20T00:26:29+00:00",
    "closed_at": "2016-01-26T14:32:58+00:00",
    "resolver": "chris-b1",
    "resolved_in": "5de6b84f5117b005a8f010d4510a758b50f3d14e",
    "resolver_commit_num": 29,
    "title": "BUG: to_datetime issue parsing non-zero padded month in 0.17.1",
    "body": "In pandas 0.16.2, the following date (non-zero padded month) was parsing correctly:\n\n\n\nWith 0.17.1, it raises a ValueError:\n\n\n\nEven if `%m` is supposed to be used for zero-padded month definitions, Python's strptime function parses them properly.\n\nIs this a known issue? \n",
    "labels": [
      "Bug",
      "Timeseries"
    ],
    "comments": [
      "It sounds like the following works :\n\n```\n>>> pandas.to_datetime('2005-1-13', format='%Y-%m-%d', infer_datetime_format=True)\nTimestamp('2005-01-13 00:00:00')\n```\n\nThis could be related to #11142 and considered as a regression. Having to guess the datetime_format when the given format is the appropriate one is overkilll:\n\n```\n>>> from pandas.tseries import tools\n>>> tools._guess_datetime_format('2005-1-13')\n'%Y-%m-%d'\n```\n",
      "This PR (conveniently also mine) is a more likely cause for the problem - I'll take a look later.\nhttps://github.com/pydata/pandas/pull/10615\n",
      "This happens because there is a special fastpath (in C) for iso8601 formatted dates, but that code doesn't handle dates without leading 0s.  As a workaround, you can just not specify the format - \n\nTo fix this, probably either need to:\n1. Let fastpath code fall back to the regular parser.  This code is already pretty complex, and this would just make it more so.\n2.  Update C code to handle dates without leadings 0s. Not sure if this can be done in a performance neutral way?\n",
      "@chris-b1 The second option is definitely the best one as it would keep the behaviour closer to the standard behaviour of strptime. Even if it is not performance neutral, it should not add a serious overhead to support no leading-zero's in the C code.\n",
      "yes, more flexibility is good here. BTW this is quite straightforward to do as this is pretty straightforward c-code.\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "renamed",
      "cross-referenced"
    ],
    "changed_files": 7,
    "additions": 192,
    "deletions": 48,
    "changed_files_list": [
      "asv_bench/benchmarks/timeseries.py",
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/src/datetime/np_datetime_strings.c",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tests/test_tslib.py",
      "pandas/tseries/tools.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11879,
    "reporter": "kynnjo",
    "created_at": "2015-12-21T18:06:24+00:00",
    "closed_at": "2017-02-27T19:44:37+00:00",
    "resolver": "AlexisMignon",
    "resolved_in": "25dcff597162a12dbe419da2ae23d9b0d6322bee",
    "resolver_commit_num": 0,
    "title": "UnicodeEncodeError from DataFrame.to_records",
    "body": "The `DataFrame.to_records` method fails with a `UnicodeEncodeError` for some unicode column names.\n\n(This issue is related to   The example below extends the example given in that issue.)\n\n\n",
    "labels": [
      "Bug",
      "Reshaping",
      "Unicode",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "you are referring to a VERY old issue FYI. Pls show `pd.show_versions()`. This a bug in any event so pull-requests are welcome.\n\nthis should be: `lmap(compat.text_type, self.columns)` I think\n",
      "If you can't be bothered to verify the code I posted, then just delete the issue.  I don't give a damn.\n",
      "@kynnjo I did repro right after you posted that's why I marked it as a bug\nI asked nicely to have you post the diagnostic. I even put what I think the fix is.\n\nwe don't appreciate rude behavior. please use respectful language.\n",
      "just delete the issue and we're done\n",
      "I actually find this a valid issue. thank you for reporting. don't you wish to see pandas improved and others helped?\n",
      "This works on current HEAD:\n\n```\nIn [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame({u'c/\\u03c3':[1,2,3]})\n\nIn [3]: df\nOut[3]: \n   c/\u03c3\n0    1\n1    2\n2    3\n\nIn [4]: df.to_records()\nOut[4]: \nrec.array([(0, 1), (1, 2), (2, 3)], \n          dtype=[('index', '<i8'), ('c/\u03c3', '<i8')])\n```\n\nPlease consider closing.\n",
      "This fails in py2.\n\n```\nIn [1]: df = pandas.DataFrame({u'c/\\u03c3':[1,2,3]})\n\nIn [2]: df.to_records()\n---------------------------------------------------------------------------\nUnicodeEncodeError                        Traceback (most recent call last)\n<ipython-input-2-6d3142e97d2d> in <module>()\n----> 1 df.to_records()\n\n/Users/jreback/pandas/pandas/core/frame.pyc in to_records(self, index, convert_datetime64)\n   1063             elif index_names[0] is None:\n   1064                 index_names = ['index']\n-> 1065             names = lmap(str, index_names) + lmap(str, self.columns)\n   1066         else:\n   1067             arrays = [self[c].get_values() for c in self.columns]\n\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\u03c3' in position 2: ordinal not in range(128)\n```\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 25,
    "deletions": 5,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/frame.py",
      "pandas/tests/frame/test_convert_to.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11899,
    "reporter": "femtotrader",
    "created_at": "2015-12-25T09:31:41+00:00",
    "closed_at": "2016-01-26T14:32:58+00:00",
    "resolver": "chris-b1",
    "resolved_in": "5de6b84f5117b005a8f010d4510a758b50f3d14e",
    "resolver_commit_num": 29,
    "title": "PERF: allow even more flexible ISO 8601 datetime parsing",
    "body": "Hello,\n\nI noticed that there is a huge code speed difference with `to_datetime` execution when format is not given and when it's given.\n\nI wonder if there is not some room for improvements here!\n\n\n\nThere is x19.15 factor!!!\n\nSample data can be found here\n?usp=sharing\n\nSee also -datareader/issues/153\n",
    "labels": [
      "Timeseries",
      "Performance",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "this falls back to `dateutil` which is in python space for parsing. So it behooves you to specify the format in this case. No real easy way around this, unless the ISO 8601 parser was modified to accept this format (which is not that difficult as its the same format with no separators). \n\nIf you are another brave soul would like to submit a pull-request it would be great.\n",
      "Here's an easy way to repro things like this:\nYou can also use `infer_datetime_format=True` which will effectively guess the format for you (if its close to ISO 8601)\n\n```\nIn [16]: s = Series(pd.date_range('20130101',freq='ms',periods=10000)).dt.strftime('%Y%m%d %H:%M:%S.%f')\n\nIn [17]: s.head()\nOut[17]: \n0    20130101 00:00:00.000000\n1    20130101 00:00:00.001000\n2    20130101 00:00:00.002000\n3    20130101 00:00:00.003000\n4    20130101 00:00:00.004000\ndtype: object\n\nIn [18]: %timeit pd.to_datetime(s)\n1 loops, best of 3: 1.09 s per loop\n\nIn [19]: %timeit pd.to_datetime(s,format='%Y%m%d %H:%M:%S.%f')\n10 loops, best of 3: 61.9 ms per loop\n\nIn [20]: %timeit pd.to_datetime(s,infer_datetime_format=True)\n10 loops, best of 3: 62.8 ms per loop\n```\n",
      "Thanks @jreback for `infer_datetime_format` this is good for my use case.\n\nNot sure if I need to send PR to `pandas` or to `dateutil`\n\nhttps://github.com/pydata/pandas/blob/master/pandas/tseries/tools.py\n",
      "I am referring to pandas of course, code is [here](https://github.com/pydata/pandas/blob/master/pandas/src/datetime/np_datetime_strings.c)\n",
      "xref #9714 \n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "renamed",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 7,
    "additions": 192,
    "deletions": 48,
    "changed_files_list": [
      "asv_bench/benchmarks/timeseries.py",
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/src/datetime/np_datetime_strings.c",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tests/test_tslib.py",
      "pandas/tseries/tools.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11915,
    "reporter": "stharrold",
    "created_at": "2015-12-28T05:37:00+00:00",
    "closed_at": "2016-12-19T12:55:55+00:00",
    "resolver": "TomAugspurger",
    "resolved_in": "dc4b0708f36b971f71890bfdf830d9a5dc019c7b",
    "resolver_commit_num": 46,
    "title": "io/common.py: boto3 with python 3.5",
    "body": "Pandas v0.17.1 won't import for Python 3.5 due to boto. Replacing with boto3 appears to fix the issue. I'd like to suggest replacing dependencies on boto with boto3 for at least Python 3.5. I didn't test with other Python 3x versions. Thank you\n\nUpdate (2016-01-10T02:45:00Z): There are significant API changes between `boto` and `boto3` (). Doing `import boto3 as boto` as below will allow pandas to import for Python 3.5, but then AWS functionality is broken.\n\n\n",
    "labels": [
      "Build",
      "Compat",
      "Data IO",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "I think the import of boto should first try to `import boto3 as boto`, then try `import boto`, then pass on the `ImportError` (this is all in `pandas/io/common.py`).\n\nfurther would need:\n- to update the `install.rst` (recommend `boto3` for PY3). \n- `boto3` is ATM on only pip installable, so need to change `ci/requirements-` for the python 3 where we use `boto` now to pip install `boto3`.\n- `util\\print_versions.py` should then print `boto3/boto` (only 1, which ever one imports, or None if neither)\n\nwant to do a pull-request?\n",
      "I haven't looked much recently, but I think that `boto3` does have API changes from `boto`, so we would need a compatibility layer.\n",
      "I haven't used `boto` or `boto3`, but there do seem to be API changes: https://github.com/boto/boto/issues/3306. From the issue comment thread, it seems preferable to use `boto` instead of `boto3` where `boto` is sufficient.\n\nPandas's CI would have caught this if it were an issue for PY<3.5. Above import error only exists for PY3.5 if `boto` is installed. If PY3.5 but `boto` is not installed, there is no import error.\n\nIn-progress TODO list:  \n- Using `boto3` for all PY3: `if sys.version_info >= (3, ):` `try: import boto3 as boto` `except ImportError: pass`.\n- Inside `pandas` source:\n  - [ ] `pandas/io/common.py`: `import boto3` as above. Note: [API change for storing data](http://boto3.readthedocs.org/en/latest/guide/migrations3.html#storing-data) (`boto.key.Key` vs `boto3.Object.put`).\n  - [ ] `pandas/io/tests/test_excel.py`: `import boto3` as above.\n  - [ ] `pandas/io/tests/test_parsers.py`: `import boto3` as above.\n  - [ ] `pandas/util/print_versions.py`: add `boto`, `boto3`.\n- Outside `pandas` source:\n  - `tox.ini`:\n    - [x] Should there be a `[testenv:py35]`? Fixed with https://github.com/pydata/pandas/commit/c74b4b488ad1ef48a1cd62b1d336e37c9b1ef556\n    - [ ] Add `boto3` for PY3.\n  - [ ] `asv_bench/benchmarks/io_bench.py`: `import boto3` as above.\n  - [ ] `README.md`: Add `boto3` for PY3.\n  - [ ] `doc/source/install.rst`: `boto3 <https://pypi.python.org/pypi/boto3>`__: Recommended for Amazon S3 access in PY3.\n  - [ ] `ci/requirements-3*`: Replace `boto` with `boto3` as pip-installable (similar to `requirements-2.7*`) No file `requirements-3.5_SLOW`; omitting.\n\nComment last modified: 2016-01-10T02:55:00Z\n",
      "I think `boto3` should be used for all PY3. not really sure of any API changes. This is prob not tested very much ATM.\n",
      "From the terminal output above, it seems that `boto/plugin.py` failed when it used `glob` (also see https://github.com/boto/boto/issues/3413). There is a record in the Python 3.5 changelog for `glob` (https://docs.python.org/3/whatsnew/3.5.html, https://docs.python.org/3.5/whatsnew/changelog.html).\n",
      "I agree that using `boto3` for all PY3 is probably the best choice given the apparent movement of the AWS Python SDK project.\n\nAfter a little searching, there are significant changes in the AWS Python SDK API between `boto` and `boto3`: http://boto3.readthedocs.org/en/latest/guide/migrations3.html. I don't think I'm able to make a timely pull request since I'm inexperienced with both `nosetests` and `boto[3]`. Perhaps the users who originally included `boto` in pandas could help?\n\nThanks again for your help with the issue.\n",
      "@stharrold want to do a PR?\n",
      "@jreback Thanks for the offer. I'm sorry, I'm not in a position to devote the time that I think would be necessary to properly fix the issue.\n",
      "I wouldn't mind giving this a closer look, if nobody else beats me to it.\n",
      "that would be great!\n",
      "@jvkersch thanks.\n\nFWIW I've had to start porting a few things to use boto3 (apparently changes to ConfigParser in 3.4.3 and 3.5.1 broke boto), and the changes aren't too difficult.\n\nIt will be up to you as you try to implement this, but I think that requiring only `boto3` is entirely reasonable if it's easier. There's no real reason to clutter up the pandas code base with bunch of boto / boto3 compatibility stuff.\n",
      "@TomAugspurger @jreback I finally had time to look into this, and I agree the immediate issue is a bug in boto for Python 3.5 (reported as https://github.com/boto/boto/issues/3474, where I left a comment). That said, I wouldn't mind trying my hand at a switchover to boto3, using the excellent to-do list that @stharrold prepared as a guide, but I propose first adding a small check to `pandas.io.common` to check for Python 3.5 + boto, to ensure that Pandas still loads cleanly in the meantime. Does that sound reasonable?\n",
      "Is this a new version of `boto`? what version do you repro on?\n\n(so for sure would take a PR that adds `boto` to the version list)\n\n```\np(py3.5)bash-3.2$ python \nPython 3.5.1 |Continuum Analytics, Inc.| (default, Dec  7 2015, 11:24:55) \n[GCC 4.2.1 (Apple Inc. build 5577)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import pandas \n>>> quit()\n(py3.5)bash-3.2$ conda install boto\nFetching package metadata: ......\nSolving package specifications: ............\nPackage plan for installation in environment /Users/jreback/miniconda/envs/py3.5:\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    boto-2.39.0                |           py35_0         1.4 MB\n\nThe following NEW packages will be INSTALLED:\n\n    boto: 2.39.0-py35_0\n\nFetching packages ...\nboto-2.39.0-py 100% |##########################################################################################################################################################| Time: 0:00:00   4.58 MB/s\nExtracting packages ...\n[      COMPLETE      ]|#############################################################################################################################################################################| 100%\nLinking packages ...\n[      COMPLETE      ]|#############################################################################################################################################################################| 100%\n(py3.5)bash-3.2$ python\nPython 3.5.1 |Continuum Analytics, Inc.| (default, Dec  7 2015, 11:24:55) \n[GCC 4.2.1 (Apple Inc. build 5577)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import pandas\n>>> pandas.show_versions()\n\nINSTALLED VERSIONS\n------------------\ncommit: 6fecc9331d54cb3b7c710ac823aa29f02872097d\npython: 3.5.1.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 14.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.18.0rc1+13.g6fecc93\nnose: 1.3.7\npip: 8.0.2\nsetuptools: 19.6.2\nCython: 0.23.2\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: None\nxarray: None\nIPython: 4.0.3\nsphinx: None\npatsy: 0.4.0\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: None\ntables: 3.2.1.1\nnumexpr: 2.4.3\nmatplotlib: 1.5.0\nopenpyxl: None\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.7.3\nlxml: None\nbs4: None\nhtml5lib: 0.999\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: 0.6.6.None\npsycopg2: 2.6 (dt dec pq3 ext)\njinja2: None\n```\n",
      "@jreback it's a new package entirely called `boto3`. If we have an interested volunteer (which it sounds like we do! thanks @jvkersch), we could _hopefully_ just require boto3 and not have any compatibility code in pandas (maybe detect `boto` and warn, recommending an upgrade to `boto3` for a version?) `boto3` is pure python and can be install alongside `boto`, so that's not a problem. It also picks up AWS credentials so ideally a user won't have to change anything, just `pip install boto3`.\n",
      "the original report was with boto no?\n",
      "Yep. Currently with python 3.5 (and IIRC python 3.4.3) boto fails at import time. The in the original post swapped boto3 for boto, which let the import succeed, but didn't handle any of the API changes from boto to boto3.\n",
      "and I would like to see what version of `boto` fails this, as you can see above it works with the latest `boto`. I am guessing that this DID work with older versions of boto (as we never got this report before and have support py3 for quite a long time) and newer, but maybe a version in the middle broke things.\n",
      "@jreback It was an update to python (3.4.2 -> 3.4.3 and 3.5.0 -> 3.5.1) that broke boto. The boto version doesn't matter.\n\nI also mispoke about it failing at import time. It fails when you try to access a `boto` config file, e.g. with `boto.connect_s3()`.\n\n```\n$ python  # python 3.5.0 env\nPython 3.5.0 (default, Sep 14 2015, 02:37:27)\n[GCC 4.2.1 Compatible Apple LLVM 6.1.0 (clang-602.0.53)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import boto\n>>> boto.connect_s3()\nS3Connection:s3.amazonaws.com\n>>> quit()\n$ vf activate jday  # python 3.5.1 env\n$ python\nPython 3.5.1 (default, Dec  7 2015, 21:59:10)\n[GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.1.76)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import boto\n>>> boto.connect_s3()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/tom.augspurger/Envs/jday/lib/python3.5/site-packages/boto/__init__.py\", line 141, in connect_s3\n    return S3Connection(aws_access_key_id, aws_secret_access_key, **kwargs)\n  File \"/Users/tom.augspurger/Envs/jday/lib/python3.5/site-packages/boto/s3/connection.py\", line 191, in __init__\n    validate_certs=validate_certs, profile_name=profile_name)\n  File \"/Users/tom.augspurger/Envs/jday/lib/python3.5/site-packages/boto/connection.py\", line 569, in __init__\n    host, config, self.provider, self._required_auth_capability())\n  File \"/Users/tom.augspurger/Envs/jday/lib/python3.5/site-packages/boto/auth.py\", line 989, in get_auth_handler\n    'Check your credentials' % (len(names), str(names)))\nboto.exception.NoAuthHandlerFound: No handler was ready to authenticate. 1 handlers were checked. ['HmacAuthV1Handler'] Check your credentials\n```\n",
      "To add to @TomAugspurger's comment, it is definitely the most recent version (2.39.0) of boto that's broken with Python 3.5, and probably all prior versions too. The changes in `ConfigParser` in Python 3.5 (and maybe 3.4 too, I haven't tested that) make it so that boto's config object is simply unable to parse the `.boto` file. The fact that the original user had an error at import time is because he had a key `plugin_directory` in his `.boto` file, which is checked at import time. For other users, the error occurs at runtime, most notably whenever you need `aws_access_key_id` or some such key.\n",
      "@TomAugspurger is there is quick soln for 3.5 users? e.g. uninstall boto, install boto3?\n",
      "If you aren't using boto at all, either uninstalling boto or renaming / moving the `~/.boto` should do it. Or downgrade to python 3.5.0, which I believe works.\n\nJust installing boto3 won't work till we're using it. I've been porting some stuff at work to use boto3 so maybe I can put together a fix tomorrow night.\n",
      "ok, going to trap/ignore the error for now on 0.18.0\n",
      "Makes sense. If I have time to get to this before the next RC I'll remove those.\n",
      "@TomAugspurger what is the repro for this error?\n\ne.g. on py3.5.1 and 2.39 for boto it imports fine for me.\n",
      "@jreback I haven't been able to repro the _import time_ TypeError. I think these conditions should set it up...\n\n``` python\nwith open(os.path.expanduser('~/.boto'), 'wt') as f:\n    f.write('[Plugins]\\nplugin_directory = {}'.format(os.path.dirname(__file__))\n\nimport pandas\n```\n\nThere just needs to be a `.py` file in that directory. The actual error comes from joining the `plugin_directory` _value_ from that parsed Config file to a `glob.glob` result, one of which is bytes and the other is str in the original example. AFAIK those should always both be `strs`? But maybe something changed in 3.5.0.\n\nThe second bug, present in 3.4.4 and 3.5.1 which prevents the first one from even occurring, only raises when you try to authenticate. It repros with\n\n``` python\nimport boto\nboto.connect_s3()\n```\n\n``` pytb\nNoAuthHandlerFound: No handler was ready to authenticate. 1 handlers were checked. ['HmacAuthV1Handler'] Check your credentials\n```\n\nIf you're on `3.4.4` or `3.5.1` you should never get the first error since any key lookup from the `Config` will give `None`, even if the key is present.\n\nThat's defined in `boto.exception`.\n",
      "ok, currently master is clear: https://travis-ci.org/pydata/pandas/jobs/113383976 (though for some reason `boto` was not showing up in `show_versions()` (I added it), even though conda says its installing).\n\neven more odd is pip installing doesn't make this show up: see https://travis-ci.org/jreback/pandas/jobs/113398289 (is `print_versions()` doing something wrong)?\n",
      "FWIW I've had good experiences going with just `boto3` and deprecating `boto` entirely.\n",
      "so new package: [s3fs](http://s3fs.readthedocs.org/en/latest/) from @martindurant, @mrocklin, and @koverholt that might be of interest. This is a pure-python library only dependent on `boto3`, pip installable.\n\nI think we could make this a dep (rather than `boto3`), and remove a lot of code in `io/common.py` as this gives us a nice file-system like object.\n\ncould potentially close: #7682, #8508 as well as this issue.\n",
      "What would you all need from `s3fs` to make this happen?  Is there any particular logic or special cases that you've had to implement here that would we would need to ensure was well handled by `s3fs`?\n",
      "I'll run the test suite with s3fs swapped in tonight or this weekend and see what failure we have.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "milestoned",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "labeled",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "referenced",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 14,
    "additions": 72,
    "deletions": 120,
    "changed_files_list": [
      "asv_bench/benchmarks/io_bench.py",
      "ci/requirements-2.7-64.run",
      "ci/requirements-2.7.run",
      "ci/requirements-2.7_SLOW.run",
      "ci/requirements-3.5.run",
      "ci/requirements-3.5_OSX.run",
      "doc/source/install.rst",
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/io/common.py",
      "pandas/io/s3.py",
      "pandas/io/tests/parser/test_network.py",
      "pandas/io/tests/test_excel.py",
      "pandas/util/print_versions.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11934,
    "reporter": "yueatwork",
    "created_at": "2015-12-30T19:46:48+00:00",
    "closed_at": "2016-02-20T18:27:34+00:00",
    "resolver": "gfyoung",
    "resolved_in": "e39f63aff115e7bd8b48c6b273e2274b7057239c",
    "resolver_commit_num": 2,
    "title": "Bug: UTC in to_datetime() not working in 0.17.1",
    "body": "In the case below, setting `utc=True` in pandas.to_datetime actually removes the UTC attribute from the input:\n\n\n\nthe last function removes the UTC attribute from the input, even though `utc` is set to `True`.\n\nThis only happens in pandas 0.17.1.  pandas 0.16.x have a different implementation and does not have this issue.\n",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "Timezones",
      "Effort Low"
    ],
    "comments": [
      "This line should be: https://github.com/pydata/pandas/blob/master/pandas/tseries/tools.py#L308\n\n`arg = arg.tz_convert(None).tz_localize('UTC')`\n\nthough to be honest this flag is just confusing IMHO. Users should simply use `tz_convert`/`tz_localize` after `pd.to_datetime` if desired. This feels too automagicy to me.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 14,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tools.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11963,
    "reporter": "Tux1",
    "created_at": "2016-01-05T12:20:57+00:00",
    "closed_at": "2016-01-12T20:24:46+00:00",
    "resolver": "Tux1",
    "resolved_in": "4437e9c9e161b884450e8027daa3622f74c4b9e7",
    "resolver_commit_num": 1,
    "title": "Timestamp rounding wrong implementation",
    "body": "Hi,\nI was looking at the recent ENH #4314 and there is a wrong implementation of `round` method for datetime.\n\n\n\nYou are flooring the nano timestamp instead of rounding. You should replace `np.floor` by `np.round`.\nAlso I propose to add `floor` and `ceil` method to Timestamp in order to have the same behavior that `float`. (It is very usefull to get the upper/lower bound for a specific freq, likely more usefull than round)\n\nDo you agree @jreback ? \n",
    "labels": [
      "Bug",
      "Timeseries",
      "Timedelta"
    ],
    "comments": [
      "hmm, though looks right. Will need some specific tests that catch this.\n\nI would add addtl functions `.ceil` & `.floor` (with the same signature). You can of course re-use the impl completely. (just subst the functions). (maybe move the impl to a private `._round` which takes the rounding function).\n\nThis is used in `DatetimeIndex/TimedeltaIndex`, `Timestamp`, `Timedelta` (the scalars have slightly diff impl).\n\nJust add the this issue number onto the existing note in whatsnew.\n",
      "Yes, I will try to do it\n",
      "@Tux1 gr8!\n\nalso need to add for `.dt` as well (this is trivial, just add to the accessors list, and test in test_series)\n",
      "yeah, I have looked at your previous commit on that\n"
    ],
    "events": [
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 9,
    "additions": 156,
    "deletions": 49,
    "changed_files_list": [
      "doc/source/api.rst",
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/tests/test_categorical.py",
      "pandas/tests/test_series.py",
      "pandas/tseries/base.py",
      "pandas/tseries/common.py",
      "pandas/tseries/tests/test_timedeltas.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11981,
    "reporter": "ChristopherShort",
    "created_at": "2016-01-07T10:04:04+00:00",
    "closed_at": "2016-05-11T22:18:27+00:00",
    "resolver": "yaduart",
    "resolved_in": "d0734ba4d0f4c228110dc3974943ce4ec2adeea4",
    "resolver_commit_num": 0,
    "title": "interation of set_eng_float_format and pivot_tables",
    "body": "There appears to be an interaction between the option for float display length and pivot_table methods on integers. This issue arose when working on a dataset with both floats and integers - but this arose on operations on integers part of the dataset.\n\nThe following example throws an error from the `format.py` module.\n\nSwap the comments on the formatting options  (undoing the setting for floats) and it runs fine.\n\nIn my dataset - there were no missing values following the pivot table operation and the error still occurs. In the sample code below, NaNs may be causing the formatting issue, but that didn't appear to be the case for me.\n\n\n\ncheers\nChris\n",
    "labels": [
      "Bug",
      "Missing-data",
      "Output-Formatting",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "not so friendly, needs a nan-check before handing off to `Decimal`.\n\nwant to do a pull-request?\n",
      "OK - I'll have a go since you've classified it as a novice level issue. Last time I tried (with a csv issue in 2014), I got lost very quickly.  I'll get back to you in a week or so.\n\ncheers\n",
      "@ChristopherShort ok thanks. lmk.\n",
      "Hi,\nThis is my first bug fix. If this bug has been schedule to released in the next major release, which document should I update in doc/source/whatsnew/vx.y.z.txt (would it be the document v0.18.2)\n\nRegards,\nYadu\n",
      "0.18.2\n",
      "yadu - thanks - this has been on my conscience for a couple of months (the 1 week blew right past).\n\nI'll find something else I can contribute too.\n",
      "No worries. cheers.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "commented",
      "commented",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 21,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/formats/format.py",
      "pandas/tests/formats/test_format.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 11990,
    "reporter": "jseabold",
    "created_at": "2016-01-07T18:14:05+00:00",
    "closed_at": "2016-01-12T23:47:54+00:00",
    "resolver": "jseabold",
    "resolved_in": "138b8ba853a6f4356b23d37d76e69425373f3315",
    "resolver_commit_num": 88,
    "title": "select_dtypes fails on float16 typecode",
    "body": "I'll see if it's an easy fix.\n\n\n",
    "labels": [
      "Bug",
      "Dtypes"
    ],
    "comments": [
      "Oh `getattr(np, dtype)` doesn't work for `e`, obviously.\n",
      "I'm not sure what is handled by `getattr(np, dtype)` that isn't handled by `np.dtype(dtype)`, actually.\n",
      "have you had other issues with `float16` btw? its not support for _some_ of the numeric ops (and will get upcast). what are you doing with it?\n",
      "xref #9220, #10382 \n",
      "I'm not doing anything with `float16`. I was just lazily trying to differentiate between numeric, integer, and object types to do e.g. continuous vs categorical data handling without more specific types than numpy primitives, and I noticed this.\n"
    ],
    "events": [
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "milestoned",
      "demilestoned",
      "commented",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 11,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/common.py",
      "pandas/tests/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12021,
    "reporter": "mrocklin",
    "created_at": "2016-01-12T04:57:15+00:00",
    "closed_at": "2016-04-05T16:17:48+00:00",
    "resolver": "jreback",
    "resolved_in": "a69d628db77cc67f79590bbb3f13e178cb8b0e93",
    "resolver_commit_num": 3966,
    "title": "Serializing Pandas Functions",
    "body": "In recent efforts using Pandas on multiple machines I've found that some of the functions are tricky to serialize.  Apparently this might be due to runtime generation.  Here are a few examples of serialization breaking, occasionally in unpleasant ways:\n\n\n\nLest you think that this is just a problem with pickle (which has many flaws), `dill`, a much more robust function serialization library, also fails (the failure here is py35 only.) (cc @mmckerns)\n\n\n\nIn this particular case though `cloudpickle` will work.\n\nOther functions have this problem as well.  Consider the series methods:\n\n\n\nIn this case, concerningly `cloudpickle` completes, but returns a wrong result:\n\n\n\nI've been able to fix some of these in  but generally speaking I'm running into a number of problems here.  It would be useful if, during the generation of these functions we could at least pay attention to assigning metadata like `__name__` correctly.  This one in particular confused me for a while:\n\n\n### What would help?\n-  Testing that most of the API is serializable\n-  Looking at what metadata the serialization libraries use, and making sure that this metadata is enough to properly identify the function.  Some relevant snippets from cloudpickle follow:\n\n\n",
    "labels": [
      "Compat",
      "Effort Low",
      "Difficulty Intermediate"
    ],
    "comments": [
      "Why do you need to serialize the functions? If pandas is on the other machine, that shouldn't be necessary?\n",
      "I need to communicate to a process on the other machine which pandas function to run.\n",
      "@mrocklin: I'm not seeing `dill` have issue with `pd.Series.sum`.  It would be nice to have a list of the serialization issues you are seeing with `pandas`.  (Thanks for the pointer on the breakage for `pandas` with `dill` in python 3.5.)\n\nSome adjustments to `dill` could be made to help you here, however, the answer is typically changes to the package (`pandas`) is the better answer.  It's common that very little changes make all the difference, and usually have negligible negative impact on speed and functionality of the package.  `pandas` is a fundamental enough package that I'd be willing to digest some special cases.\n",
      "So the problem with the `cum` functions is trivial. Just naming them differently. So a PR for that is sort of easy.\n\n```\ndiff --git a/pandas/core/generic.py b/pandas/core/generic.py\nindex 958571f..9764ae2 100644\n--- a/pandas/core/generic.py\n+++ b/pandas/core/generic.py\n@@ -4701,7 +4701,7 @@ class NDFrame(PandasObject):\n             lambda y, axis: np.minimum.accumulate(y, axis),\n             np.inf, np.nan)\n         cls.cumsum = _make_cum_function(\n-            'sum', name, name2, axis_descr,\n+            'cumsum', name, name2, axis_descr,\n             \"cumulative sum\",\n             lambda y, axis: y.cumsum(axis), 0., np.nan)\n         cls.cumprod = _make_cum_function(\n```\n\nhow is the `cloudpickle` result for `Series.sum` wrong? this is just a closure\n",
      "``` python\nIn [1]: import pandas as pd\n\nIn [2]: import cloudpickle\n\nIn [3]: s = pd.Series([1, 2, 3])\n\nIn [4]: sum2 = cloudpickle.loads(cloudpickle.dumps(pd.Series.sum))\n\nIn [5]: pd.Series.sum(s)\nOut[5]: 6\n\nIn [6]: sum2(s)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-6-49effaf88fb4> in <module>()\n----> 1 sum2(s)\n\nTypeError: unbound method stat_func() must be called with NoneType instance as first argument (got Series instance instead)\n```\n\nThis I think was more an error with cloudpickle than with pandas.  This was fixed in a recent cloudpickle PR.\n",
      "So in your example above its serializing an unbound method.\n\n```\nIn [5]: sum2.__self__\n```\n\nYou can serialize a bound method then it works\n\n```\nIn [14]: sum2 = cloudpickle.loads(cloudpickle.dumps(s.sum))\n\nIn [15]: sum2\nOut[15]: \n<bound method Series.stat_func of 0    1\n1    2\n2    3\ndtype: int64>\n\nIn [16]: sum2()\nOut[16]: 6\n```\n",
      "In this case I want to serialize an unbound method.\n",
      "so this is a `cloudpickle`/`dill` issue then? (aside from the naming in the `cum*` funcs)?\n",
      "It can be handled in either.  Like Mike said earlier, a little bit of help from Pandas can go a long way here.  It's important to make sure that all functions maintain metadata like `__name__` and `__self__` and `im_class` through all of the manipulations that Pandas does.\n",
      "no problem with changing things. maybe @mmckerns has an example of how you are constructing the closures. maybe we aren't setting something up correctly. (we are setting `__name__` and `__doc__`), then assigning to the appropriate class.\n\n```\ndef _make_stat_function(name, name1, name2, axis_descr, desc, f):\n\n    @Substitution(outname=name, desc=desc, name1=name1, name2=name2, axis_descr=axis_descr)\n    @Appender(_num_doc)\n    def stat_func(self, axis=None, skipna=None, level=None,\n                  numeric_only=None, **kwargs):\n        if skipna is None:\n            skipna = True\n        if axis is None:\n            axis = self._stat_axis_number\n        if level is not None:\n            return self._agg_by_level(name, axis=axis, level=level,\n                                      skipna=skipna)\n        return self._reduce(f, name, axis=axis,\n                            skipna=skipna, numeric_only=numeric_only)\n    stat_func.__name__ = name\n    return stat_func\n```\n",
      "Not sure what `cloudpickle` does for closures\u2026 but I think it has failed on them for a long while (as @mrocklin noted above) and may recently have been fixed.  One major issue is that python uses/used the wrong fully qualified name for closures, and until `__qualname__` was introduced, it caused a serializer to have a lot of difficulty referencing the code.\n\n`dill` handles closures in most cases (but can still miss w/ bad qualnames) -- and it handles bound and unbound methods in most cases as well.  One issue tends to be referenced here: https://github.com/uqfoundation/dill/blob/cccbea9b715e16b742288e1e5a21a687a4d4081b/dill/dill.py#L1000\u2026 also when a reference to a class method is used at the module level, this is also difficult.  Also I think both `dill` and `cloudpickle` try to identify code in closures by looking at the source (to identify where the code lives)... and this can be aided by, say, not always using `inner` for all closured functions in the same file.\n\nI guess, in short, it's not so much about how `dill` or `cloudpickle` constructs closures\u2026 it's about which closure constructs a given serializer can handle.  Small things, like naming, and whether the decorator is in the same file as where it is used, etc, can all make a difference.\n\nMessing with `__name__` can also cause problems, but there are workarounds.  I think I see what you are doing\u2026 that function then gets referenced by some class as a method?  Yikes.  Maybe a reference to a full example would help.\n",
      "@mrocklin \nhttps://github.com/pydata/pandas/pull/12372\n\nanything else you need?\n",
      "The problem is more extensive.  I suspect you would need to be more careful about how you wrap things.  Here is a fail case not handled by that PR.\n\n``` python\nIn [1]: import pandas as pd\n\nIn [2]: from cloudpickle import dumps, loads\n\nIn [3]: pd.Series.sum\nOut[3]: <function pandas.core.generic._make_stat_function.<locals>.stat_func>\n\nIn [4]: loads(dumps(pd.Series.sum))\nOut[4]: <function stat_func>\n```\n",
      "I checkout 0.2.1 of `cloudpickle`\n\n```\nIn [1]: import cloudpickle\n\nIn [2]: from cloudpickle import dumps, loads\n\nIn [3]: loads(dumps(pd.Series.sum))\nOut[3]: <unbound method Series.sum>\n\nIn [4]: pd.__version__\nOut[4]: '0.18.0rc1+23.g8fba893.dirty'\n\nIn [5]: cloudpickle.__version__\nOut[5]: '0.2.1'\n```\n",
      "Try Python 3\n",
      "so if you have\n\n```\ndef make_function(.....):\n\n   def a_func(.....):\n         ....\n   a_func.__name__ = 'foo'\n   return a_func\n```\n\nhow do I set the name instead of `a_func`? (I tried `a_func.func_name = 'foo'`) but doesn't seem to do anything\n",
      "Looks like the attributes `__name__` and `__qualname__` come up in the definition of `functools.wraps`\n",
      "Looks like cloudpickle uses `__name__` in a couple places.  Doesn't seem to use `__qualname__`\n",
      "The `__module__` attribute may also come in handy\n",
      "Serializers need to handle `__qualname__` for display.\n",
      "@kawochen do you have a reference on how/what do with the `__qualname__`, I mean can simply do a substtitute is that the right way?\n",
      "yes I think you just change `__qualname__` upon (re)binding.  [PEP3155](https://www.python.org/dev/peps/pep-3155/).\n",
      "@mrocklin I updated #12372 can you see if this works for you?\n\nI could make this anything actually. What do you think `cloudpickle` actually wants?\n\n```\nIn [1]: pd.Series.sum.__qualname__\nOut[1]: '_make_stat_function.<locals>.sum'\n\nIn [2]: pd.Series.cumsum.__qualname__\nOut[2]: '_make_cum_function.<locals>.cumsum'\n\nIn [3]: pd.Series.any.__qualname__\nOut[3]: '_make_logical_function.<locals>.any'\n\nIn [3]: pd.Series.cumsum.__name__\nOut[3]: 'cumsum'\n```\n",
      "``` python\nIn [1]: import pandas as pd\npd\nIn [2]: pd.__version__\nOut[2]: '0.18.0rc1+28.gdcfadad'\n\nIn [3]: from cloudpickle import dumps, loads\n\nIn [4]: pd.Series.sum\nOut[4]: <function pandas.core.generic._make_stat_function.<locals>.sum>\n\nIn [5]: loads(dumps(pd.Series.sum))\nOut[5]: <function stat_func>\n\nIn [6]: s = pd.Series([1, 2, 3])\n\nIn [7]: pd.Series.sum(s)\nOut[7]: 6\n\nIn [8]: loads(dumps(pd.Series.sum))(s)\nOut[8]: 6\n\nIn [9]: pd.Series.sum.__name__\nOut[9]: 'sum'\n\nIn [10]: pd.Series.sum.__module__\nOut[10]: 'pandas.core.generic'\n\nIn [11]: pd.Series.sum.__qualname__\nOut[11]: '_make_stat_function.<locals>.sum'\n\nIn [12]: loads(dumps(pd.Series.sum)).__name__\nOut[12]: 'stat_func'\n\nIn [13]: loads(dumps(pd.Series.sum)).__module__\n\nIn [14]: loads(dumps(pd.Series.sum)).__qualname__\nOut[14]: 'stat_func'\n```\n",
      "ok latest push does this, but still not sure what `cloudpickle` is looking at. I think we actually need to go deeper into the closure\n\n```\nIn [1]: from cloudpickle import loads, dumps\n\nIn [2]: pd.Series.sum.__qualname__\nOut[2]: 'pandas.core.series.Series.sum'\n\nIn [3]: loads(dumps(pd.Series.sum)).__qualname__\nOut[3]: 'stat_func'\n```\n",
      "I just tried adding on `__module__` but no joy.\n",
      "I added breakpoints inside of cloudpickle to see where this would end up.  Arrived at the `save_function` method: https://github.com/cloudpipe/cloudpickle/blob/master/cloudpickle/cloudpickle.py#L173-L224\n\nAlthough it also ended up there seven times for one call to `dumps`, so it's presumably moving around a fair amount.  \n\nCloudpickle is only 700 lines.  I think it's worth skimming it to see the kinds of things they look for.\n",
      "yep, it _looks_ at first glance that the module is good if `pandas.core.series`, but it wants name `Series`, then not sure how to get it to find the actual method.\n\n```\n-> if getattr(themodule, name, None) is obj:\n(Pdb) l\n191             if modname == '__main__':\n192                 themodule = None\n193     \n194             if themodule:\n195                 self.modules.add(themodule)\n196  ->             if getattr(themodule, name, None) is obj:\n197                     return self.save_global(obj, name)\n198     \n199             # if func is lambda, def'ed at prompt, is in main, or is nested, then\n200             # we'll pickle the actual function object rather than simply saving a\n201             # reference (as is done in default pickler), via save_function_tuple.\n(Pdb) n\n> /Users/jreback/miniconda/envs/py3.5/lib/python3.5/site-packages/cloudpickle-0.2.1-py3.5.egg/cloudpickle/cloudpickle.py(202)save_function()\n```\n",
      "yeh, I think we are not setting it up correctly to save a closure that is actually an instancemethod. It tries to save like a module level function I think.\n",
      "If you want to see what `dill` does, all you have to do is turn on `trace`\u2026 so use `dill.detect.trace(True)`.  And if you want to use a version of `dill` that is the most similar to what `cloudpickle` does, first do `dill.settings['recurse'] = True`.  This will print out the path `dill` takes in serializing each object.  You can easily look at the matching code that is found in the source in `dill.py` for each object in the trace.   It's not going to be exactly the same as `cloudpickle`, but it should be close.\n",
      "@mmckerns FYI I've run into strange behavior when using `dill.settings['recurse'] = True` (things like `int` not being found upon reserialization), hence my recent move over to `cloudpickle`.\n\nProviding traces though is really slick.  \n",
      "thanks @mmckerns ok will get to this prob next week. In any event it _seems_  that what we are doing is using a static function which we then assign to a class at run-time (so its a method now at least in py3, in py2 have this bound nonsense...). I am thinking that on deserialization it cannot be found (even though I am settting the `__qualname__` and `__module__` which are supposed to provide the places to look up.\n\nis that a reasonable description?\n",
      "```\nIn [1]: import dill\n\nIn [2]: dill.detect.trace(True)\n\nIn [3]: from dill import loads, dumps\n\nIn [4]: loads(dumps(pd.Series.sum))\nF1: <function Series.sum at 0x10b8e8730>\nF2: <function _create_function at 0x10c00c6a8>\n# F2\nCo: <code object stat_func at 0x10b2d3150, file \"/Users/jreback/miniconda/envs/py3.5/pandas/pandas/core/generic.py\", line 5240>\nT1: <class 'code'>\nF2: <function _load_type at 0x10c00c598>\n# F2\n# T1\n# Co\nD4: <dict object at 0x10b29e2c8>\n# D4\nCe: <cell at 0x10b8e19a8: function object at 0x10ab9d268>\nF2: <function _create_cell at 0x10c00c9d8>\n# F2\nF2: <function nansum at 0x10ab9d268>\n# F2\n# Ce\nCe: <cell at 0x10b8e19d8: str object at 0x1003bc3b0>\n# Ce\nD2: <dict object at 0x10bfa13c8>\n# D2\n# F1\nOut[4]: <function pandas.core.generic.stat_func>\n```\n",
      "@jreback: I think you have a reasonable picture of it.\n\nNote that the trace starts by going into `F1` (which is your method), and ends with `# F1` (which is when `F1` is actually serialized).  So, it first serializes the helper function (this is `F2`) that will be used to create your target function.  Then it tries to serialize your target function's code object (`Co`), and apparently needs the `code` class to do this, and also needs another helper function to create this class object.  It gets those two (`T1` and `F2`), then finishes serializing the code object (`# Co`).  The dict `D4` tells me that it's grabbing the module dict as the global dict for the function\u2026 and I expect that things like `nansum` that follow are because they are referenced by the object at the module dict level.\n",
      "@mrocklin: I'm guessing you mean that `int` is missing you mean that `numpy.int64` is not found as `numpy.int64` upon deserialization.  I don't want to hijack the thread for that, so please feel free to open a ticket for whatever issues you are running into, and I'll try to fix them.\n"
    ],
    "events": [
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "labeled",
      "unlabeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "commented",
      "referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 15,
    "additions": 164,
    "deletions": 171,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/compat/__init__.py",
      "pandas/core/common.py",
      "pandas/core/generic.py",
      "pandas/core/internals.py",
      "pandas/core/missing.py",
      "pandas/core/ops.py",
      "pandas/core/panel.py",
      "pandas/indexes/base.py",
      "pandas/indexes/category.py",
      "pandas/indexes/multi.py",
      "pandas/io/tests/test_pickle.py",
      "pandas/tests/frame/test_constructors.py",
      "pandas/tests/test_generic.py",
      "pandas/util/misc.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12037,
    "reporter": "marcelnem",
    "created_at": "2016-01-14T10:40:40+00:00",
    "closed_at": "2016-02-10T17:37:09+00:00",
    "resolver": "BranYang",
    "resolved_in": "ab29f93f2a0300b8f79292faaec4c497f4cbabac",
    "resolver_commit_num": 2,
    "title": "ValueError: Values falls after last bin   when Resampling using pd.tseries.offsets.Nano as period",
    "body": "I have a timeseries in dataframe named dfi with non-eqispaced times as index\n\n\n\nOutput:\n\n\n\nI get an error when running this code:\n\n\n\nOutput + error:\n\n\n\npackages versions:\n\n\n\nOutput:\n\n\n",
    "labels": [
      "Bug",
      "Resample",
      "Difficulty Intermediate",
      "Effort Medium",
      "Timeseries"
    ],
    "comments": [
      "xref #9119 \n\ndoes look buggy. can you post an easily reproducible/simpler example that can be easily copy-pasted\n",
      "Here is reproducible/simpler example:\n\nrunning in ipython notebook and python 2\n\n```\nimport pandas as pd\nimport numpy as np\n\nstart=1443707890427\nend=1443916802040\ndif=end-start\nlength=1000\nnp.random.seed(seed=16516)\ntimestamps=np.random.random_integers(0,dif,length);\ntimestamps =timestamps+start\ntimestamps = np.sort(timestamps)\n\ndatetimes=pd.to_datetime(timestamps,unit=\"ms\")\nvalues = np.random.rand(length)\n\ndt_test=pd.DataFrame(values,columns=[\"value\"],index=datetimes)\nprint \"dt_test.head()\"\nprint dt_test.head()\nprint\n\nperiod_seconds=4.035752\nprint \"period_seconds\"\nprint period_seconds\nprint\nperiod_nanos=int(period_seconds*(10**9))\nprint \"period_nanos\"\nprint period_nanos\n\nres= dt_test.value.resample(pd.tseries.offsets.Nano(period_nanos), how=[np.min, np.max,'mean'])\n```\n\nOutput:\n\n```\ndt_test.head()\n                            value\n2015-10-01 13:59:11.020  0.795006\n2015-10-01 13:59:30.583  0.725395\n2015-10-01 14:01:31.597  0.731184\n2015-10-01 14:06:40.423  0.982237\n2015-10-01 14:08:28.432  0.014274\n\nperiod_seconds\n4.035752\n\nperiod_nanos\n4035751999\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-11-f6df1fcb5427> in <module>()\n     27 print period_nanos\n     28 \n---> 29 res= dt_test.value.resample(pd.tseries.offsets.Nano(period_nanos), how=[np.min, np.max,'mean'])\n\nC:\\Users\\USER1\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc in resample(self, rule, how, axis, fill_method, closed, label, convention, kind, loffset, limit, base)\n   3641                               fill_method=fill_method, convention=convention,\n   3642                               limit=limit, base=base)\n-> 3643         return sampler.resample(self).__finalize__(self)\n   3644 \n   3645     def first(self, offset):\n\nC:\\Users\\USER1\\Anaconda2\\lib\\site-packages\\pandas\\tseries\\resample.pyc in resample(self, obj)\n     80 \n     81         if isinstance(ax, DatetimeIndex):\n---> 82             rs = self._resample_timestamps()\n     83         elif isinstance(ax, PeriodIndex):\n     84             offset = to_offset(self.freq)\n\nC:\\Users\\USER1\\Anaconda2\\lib\\site-packages\\pandas\\tseries\\resample.pyc in _resample_timestamps(self, kind)\n    274         axlabels = self.ax\n    275 \n--> 276         self._get_binner_for_resample(kind=kind)\n    277         grouper = self.grouper\n    278         binner = self.binner\n\nC:\\Users\\USER1\\Anaconda2\\lib\\site-packages\\pandas\\tseries\\resample.pyc in _get_binner_for_resample(self, kind)\n    118             kind = self.kind\n    119         if kind is None or kind == 'timestamp':\n--> 120             self.binner, bins, binlabels = self._get_time_bins(ax)\n    121         elif kind == 'timedelta':\n    122             self.binner, bins, binlabels = self._get_time_delta_bins(ax)\n\nC:\\Users\\USER1\\Anaconda2\\lib\\site-packages\\pandas\\tseries\\resample.pyc in _get_time_bins(self, ax)\n    179 \n    180         # general version, knowing nothing about relative frequencies\n--> 181         bins = lib.generate_bins_dt64(ax_values, bin_edges, self.closed, hasnans=ax.hasnans)\n    182 \n    183         if self.closed == 'right':\n\npandas\\lib.pyx in pandas.lib.generate_bins_dt64 (pandas\\lib.c:20875)()\n\nValueError: Values falls after last bin\n```\n",
      "The issue is caused by line 164, 165 in pandas/tseries/resample.py\n\n``` python\nbinner = labels = DatetimeIndex(freq=self.freq,\n                                start=first.replace(tzinfo=None),\n                                # replace will truncate to millisecond \n                                end=last.replace(tzinfo=None),\n                                tz=tz,\n                                name=ax.name)\n```\n\nConsider this example\n\n``` python\nIn [1]: import pandas as pd\n\nIn [2]: from pandas.tseries.index import DatetimeIndex\n\nIn [3]: s_ns = 1443707950041939524\n\nIn [4]: itvl = 10**9\n\nIn [5]: e_ns = s_ns + itvl\n\nIn [6]: s = pd.Timestamp(s_ns).tz_localize(None)\n\nIn [7]: e = pd.Timestamp(e_ns).tz_localize(None)\n\nIn [8]: e\nOut[8]: Timestamp('2015-10-01 13:59:11.041939524')\n\nIn [9]: indx = DatetimeIndex(freq=pd.tseries.offsets.Nano(itvl/20),start=s, end=\ne,tz=None)\n\nIn [10]: indx[-1]\nOut[10]: Timestamp('2015-10-01 13:59:11.041939524', offset='50000000N')\n\nIn [11]: replaced = DatetimeIndex(freq=pd.tseries.offsets.Nano(itvl/20),start=s.\nreplace(tzinfo=None), end=e.replace(tzinfo=None),tz=None)\n\nIn [12]: replaced[-1]\nOut[12]: Timestamp('2015-10-01 13:59:11.041939', offset='50000000N')\n```\n\nThe last item clearly out of the bound if using replace.\nShould we consider not to use replace given its current behavior (i.e., throw away the nano second information)?\n",
      "@BranYang hmm, that does look likely.\n\n`Timestamp.replace` is pretty naive in that it doesn't understand nanoseconds at all. So it indeed dropping the nanos.\n\nWhat you need to do is fix that as I believe this is a symptom of an invalid replace.\n\nwant to take a crack at it? looking `tslib.pyx/Timestamp`\n",
      "a couple of other issues _might_ be showing similar symtoms, e.g. #6085 (and linked from there). If this proves to fix, we will want to add tests for those as well.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 32,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12042,
    "reporter": "jaradc",
    "created_at": "2016-01-15T06:11:22+00:00",
    "closed_at": "2016-02-08T15:28:58+00:00",
    "resolver": "BranYang",
    "resolved_in": "62363d2ff2079f4c7e0cc69b0a9dc4dc579a5bd3",
    "resolver_commit_num": 0,
    "title": "Pandas get_dummies() and n-1 Categorical Encoding Option to avoid Collinearity?",
    "body": "When doing linear regression and encoding categorical variables, perfect collinearity can be a problem.  To get around this, the suggested approach is to use n-1 columns.  It would be useful if `pd.get_dummies()` had a boolean parameter that returns n-1 for each categorical column that gets encoded.\n\nExample:\n\n\n\n\n\nInstead, I'd like to have some parameter such as `drop_first=True` in `get_dummies()` and it does something like this:\n\n\n\n**Sources**\n-categorical-data-into-numbers-with-pandas-and-scikit-learn/\n-to-get-pandas-get-dummies-to-emit-n-1-variables-to-avoid-co-lineraity\n\n",
    "labels": [
      "Stats",
      "Reshaping"
    ],
    "comments": [
      "Sounds good, interested in submitting a pull request?\n",
      ":+1: \n"
    ],
    "events": [
      "labeled",
      "labeled",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned"
    ],
    "changed_files": 3,
    "additions": 173,
    "deletions": 6,
    "changed_files_list": [
      "doc/source/reshaping.rst",
      "pandas/core/reshape.py",
      "pandas/tests/test_reshape.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12044,
    "reporter": "Winand",
    "created_at": "2016-01-15T09:03:23+00:00",
    "closed_at": "2016-07-19T01:52:00+00:00",
    "resolver": "pijucha",
    "resolved_in": "b225cacb1d2a34e3c4041533a0590133098756fa",
    "resolver_commit_num": 3,
    "title": "Index.difference performance",
    "body": "I need to append several big Series to a big categorical Series.\nTrying to update categories FAST i've found out that `Index.difference` uses Python's `set`, which is slow on creating LARGE set (i have up to 500k categories and 1.3M values).\nnumpy's `setdiff1` is more than an order of magnitude faster (as of datetime64 Categorical):\n\n\n\nNot so fast:\n\n\n",
    "labels": [
      "Indexing",
      "Performance",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "can u show the creation of tmp\n",
      "and to\n",
      "I've tried to implement this with string data and `difference` performed better. But `setdiff1d` is much better with `datetime64[ns]`\n\n```\nimport pandas as pd, time\n\nto=pd.Series(pd.DatetimeIndex(range(1000000))).astype('category')#pd.Series((\"hello%d\"%i for i in range(1000000))).astype('category')\ncats = to.cat.categorical._categories.values\ntmp=pd.Series(pd.DatetimeIndex(range(1000000, 1200000)))#pd.Series((\"bye%d\"%i for i in range(200000)))\n\ntmp_unique = tmp.unique()\ntmp_unique = tmp_unique[~pd.isnull(tmp_unique)]\n\n_=time.clock()\nnew_cats = pd.Index(tmp_unique).difference(cats)\nprint(\"Index.difference: %.3fs\"%(time.clock()-_))\n\n_=time.clock()\nnew_cats = pd.Index(pd.np.setdiff1d(tmp_unique, cats))\nprint(\"np.setdiff1d: %.3fs\"%(time.clock()-_))\n\n-----\n>>>Index.difference: 1.976s\n>>>np.setdiff1d: 0.104s\n```\n",
      "interesting, the is the first numpy setop that actually is fast. \n\nok, sure pull-requests are welcome (including an asv benchmark).\n",
      "xref https://github.com/pydata/pandas/pull/11279\n\nnote that you would have to use base forms for some of the index types (e.g. `.values`)\n\nso might need to upgrade tests for this (in fact should consolidate all of the `test_difference*` tests in `test_index.py`) and move them to `Base` for generic testing.\n",
      "@Winand I had a go at speeding this up, in the issue @jreback referenced. I didn't get it over the finish line, please do take the torch!\n\nMy understanding is that `set` is actually very fast. The slow part of the current implementation is the boxing & unboxing of values, for indexes that need to do conversions for each element for `list(self)`. So if you can delegated to `.values`, then `set` should be reasonable. numpy may still be faster - worth comparing apples to apples\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 11,
    "additions": 583,
    "deletions": 63,
    "changed_files_list": [
      "asv_bench/benchmarks/index_object.py",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/algorithms.py",
      "pandas/indexes/base.py",
      "pandas/tests/indexes/common.py",
      "pandas/tests/indexes/test_base.py",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tests/test_algos.py",
      "pandas/tests/test_groupby.py",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_join.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12045,
    "reporter": "lvphj",
    "created_at": "2016-01-15T12:17:04+00:00",
    "closed_at": "2016-03-23T17:56:45+00:00",
    "resolver": "jreback",
    "resolved_in": "85f8cf74b733a6782730f17b4a68fa4b028f2013",
    "resolver_commit_num": 3953,
    "title": "Printing None and NaN values in Pandas dataframe produces confusing results",
    "body": "Printing a dataframe where a variable contains None values produces confusing results. Large dataframes are automatically split to print to screen. If all the values on one side of the splits are None, they are actually displayed as NaN. This can be demonstrated with the following code.\n\n\n\nThis produces the following output:\n\n\n\nAbove the split, the variable 'text' has one cell which has a genuine string ('some words'). All the None values on that side of the split are correctly displayed as 'None'. However, on the bottom part of the split, all the cells contain None values but are confusingly displayed as Nan.\n\nExpected behaviour: All None values should be displayed as 'None' rather than 'NaN'.\n\n\n",
    "labels": [
      "Bug",
      "Output-Formatting",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "Thanks for the report! \nAs noted in a comment on SO, if the full dataframe consists of None values, you also get this:\n\n```\nIn [11]: with pd.option_context('display.max_rows', 4): print tempDF[3:]\n    id text\n3    4  NaN\n4    5  NaN\n..  ..  ...\n8    9  NaN\n9   10  NaN\n\n[7 rows x 2 columns]\n```\n\nBut of course, None's get converted to NaNs silently in a lot of pandas operations\n",
      "And if a cell in the bottom half of the split contains a real string, all the values in the top half are NaN:\n\n```\n     id        text\n0    1         NaN\n1    2         NaN\n2    3         NaN\n3    4         NaN\n..  ..         ...\n6    7        None\n7    8  some words\n8    9        None\n9   10        None\n```\n\nI suppose converting Nones to NaNs is reasonable \u2013 providing it is done consistently.\n",
      "this is the same issue as #11594 (though different dtypes).\n\nthe fix actually is pretty symbol. [here](https://github.com/pydata/pandas/blob/master/pandas/core/format.py#L433) is the code, need an `.astype` after the `.concat` to the original dtype of that column. \n\nAlso should be fixed for the Series formatter\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "labeled",
      "labeled",
      "milestoned",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 7,
    "additions": 127,
    "deletions": 23,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/indexing.py",
      "pandas/core/internals.py",
      "pandas/tests/indexing/test_indexing.py",
      "pandas/tests/test_format.py",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12048,
    "reporter": "Sereger13",
    "created_at": "2016-01-15T15:10:20+00:00",
    "closed_at": "2016-01-24T22:40:08+00:00",
    "resolver": "Sereger13",
    "resolved_in": "ab3291d7ddbe821733f199c6ce414d7a6b9c18e0",
    "resolver_commit_num": 0,
    "title": "ERR: read_csv with dtype specified on empty data",
    "body": "File has this content:\n\n\n\nWhen I do `pd.read_csv(filepath)` - all looks fine, however if I specify _skiprows_ and _dtype_ parameters it fails with the following error:\n\n\n\n> AttributeError: type object 'str' has no attribute 'items'\n\n---\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.10.final.0\npython-bits: 64\nOS: Linux\nOS-release: 2.6.18-238.9.1.el5\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US\n\n**pandas: 0.17.1**\n",
    "labels": [
      "Error Reporting",
      "CSV",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "This is equivalent to this. So this doesn't have any data, so the dtype converter should handle this properly.\n\n```\nIn [13]: read_csv(StringIO(data),header=0,skiprows=1)\nOut[13]: \nEmpty DataFrame\nColumns: [3, 3.1]\nIndex: []\n\nIn [14]: data\nOut[14]: 'A,A\\n3,3'\n```\n\nSo an even simpler repro is this.\n\n```\nIn [16]: read_csv(StringIO('A,B'),dtype=str)\nAttributeError: type object 'str' has no attribute 'items'\n```\n\ncare to do a pull-request to fix?\n",
      "Prepared the fix locally but getting this error when trying to send pull-request: \n'It seems you do not have permission to push your changes to this repository'.\n",
      "see contributing docs [here](http://pandas.pydata.org/pandas-docs/version/0.17.1/contributing.html)\n\nyou need to fork, then push to your branch and open a PR.\n\nyou are trying to push to master\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "renamed",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 19,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12052,
    "reporter": "TomAugspurger",
    "created_at": "2016-01-15T16:41:24+00:00",
    "closed_at": "2016-02-06T21:50:04+00:00",
    "resolver": "MattRijk",
    "resolved_in": "e8ef4f8b517de8bff8983c52bd6e9d200a795f14",
    "resolver_commit_num": 0,
    "title": "DOC: Put deprecation warning in convert_objects docstring",
    "body": "-docs/version/0.17.1/generated/pandas.DataFrame.convert_objects.html#pandas.DataFrame.convert_objects\n\nProbably other docstrings missing deprecations if anyone wants to track those down.\n",
    "labels": [
      "Docs",
      "Difficulty Novice",
      "Deprecate",
      "Effort Low"
    ],
    "comments": [
      "master list is in #6581 \n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 1,
    "additions": 17,
    "deletions": 8,
    "changed_files_list": [
      "pandas/core/generic.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12072,
    "reporter": "richjoyce",
    "created_at": "2016-01-17T17:45:03+00:00",
    "closed_at": "2016-02-02T15:16:18+00:00",
    "resolver": "jreback",
    "resolved_in": "1dc49f51afe67fdc17fa2670545c053775765ebc",
    "resolver_commit_num": 3866,
    "title": "Resampling fails with equal frequency",
    "body": "I've got a DataFrame with a TimedeltaIndex that fails when I try to resample it to the same frequency.\n\nMinimal working example\n\n\n\npandas versions\n\n\n\nThe error thrown is:\n\n\n\nThis appears to be a problem with the logic in `TimeGrouper._resample_timestamps()` lines 298 to 308, the error is being thrown on line 308. It looks like res_index is being set to the wrong thing but I'm not familiar enough with the code to know how to fix properly.\n",
    "labels": [
      "Bug",
      "Resample",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "this is fixed in the resample refactor, #11841 \nbut will add as a confirming test.\n"
    ],
    "events": [
      "cross-referenced",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "labeled",
      "labeled",
      "referenced"
    ],
    "changed_files": 23,
    "additions": 2784,
    "deletions": 997,
    "changed_files_list": [
      "doc/source/api.rst",
      "doc/source/cookbook.rst",
      "doc/source/release.rst",
      "doc/source/timedeltas.rst",
      "doc/source/timeseries.rst",
      "doc/source/whatsnew/v0.10.0.txt",
      "doc/source/whatsnew/v0.18.0.txt",
      "doc/source/whatsnew/v0.9.1.txt",
      "pandas/core/base.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/ops.py",
      "pandas/core/window.py",
      "pandas/io/tests/test_excel.py",
      "pandas/tests/test_generic.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tests/test_window.py",
      "pandas/tseries/plotting.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12077,
    "reporter": "Winand",
    "created_at": "2016-01-18T11:31:50+00:00",
    "closed_at": "2016-01-25T15:29:27+00:00",
    "resolver": "jreback",
    "resolved_in": "81bb972259d25d2ebd19db0a3fd08f35a0673750",
    "resolver_commit_num": 3854,
    "title": "Categorical(vals, cats) bad performance with NaNs",
    "body": "NaNs in `datetime64` data values GREATLY reduce performance of `Categorical(values, cats)`:\n\n\n\nSmall issue with printing Categorical datetime64:\n\n\n\nVersions:\n\n\n",
    "labels": [
      "Performance",
      "Categorical"
    ],
    "comments": [
      "pls show a copy pastable example and\npd.show_versions()\n",
      "can u show using \n%%timeit in ipython instead\nit's much easier to read \n",
      "At first i've tried to initialize like this:\n\n```\nto=pd.Series(pd.DatetimeIndex(range(1000000))).astype('category')\ncats = to.cat.categorical._categories.values\ntmp=pd.Series(pd.DatetimeIndex(range(1000000)))\n```\n\nbut it gives wrong results in the 1st case (a bug?):\n\n```\n>>>c1\n[1970-01-01, 1970-01-01, 1970-01-01, 1970-01-01, 1970-01-01, ..., 1970-01-01 00:00:00.000999, 1970-01-01 00:00:00.000999, 1970-01-01 00:00:00.000999, 1970-01-01 00:00:00.000999, 1970-01-01 00:00:00.000999]\nLength: 1000000\nCategories (1000000, datetime64[ns]): [1970-01-01 00:00:00.000000000, 1970-01-01 00:00:00.000000001,\n                                       1970-01-01 00:00:00.000000002, 1970-01-01 00:00:00.000000003, ...,\n                                       1970-01-01 00:00:00.000999996, 1970-01-01 00:00:00.000999997,\n                                       1970-01-01 00:00:00.000999998, 1970-01-01 00:00:00.000999999]\n\n>>>c2\n[1970-01-01 00:00:00.000000000, 1970-01-01 00:00:00.000000001, 1970-01-01 00:00:00.000000002, 1970-01-01 00:00:00.000000003, 1970-01-01 00:00:00.000000004, ..., 1970-01-01 00:00:00.000999995, 1970-01-01 00:00:00.000999996, 1970-01-01 00:00:00.000999997, 1970-01-01 00:00:00.000999998, 1970-01-01 00:00:00.000999999]\nLength: 1000000\nCategories (1000000, datetime64[ns]): [1970-01-01 00:00:00.000000000, 1970-01-01 00:00:00.000000001,\n                                       1970-01-01 00:00:00.000000002, 1970-01-01 00:00:00.000000003, ...,\n                                       1970-01-01 00:00:00.000999996, 1970-01-01 00:00:00.000999997,\n                                       1970-01-01 00:00:00.000999998, 1970-01-01 00:00:00.000999999]\nequal? False\n```\n",
      "@Winand \n\n#12128 should fix a multitude of categorical with `NaT` issues/perf.\n\nwas converting them to `object` dtype under the hood (bad) and not treating `NaT` like `nan`\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 9,
    "additions": 136,
    "deletions": 37,
    "changed_files_list": [
      "asv_bench/benchmarks/categoricals.py",
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/algos.pyx",
      "pandas/core/algorithms.py",
      "pandas/core/categorical.py",
      "pandas/hashtable.pyx",
      "pandas/tests/frame/test_analytics.py",
      "pandas/tests/test_categorical.py",
      "pandas/tseries/period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12078,
    "reporter": "nbonnotte",
    "created_at": "2016-01-18T12:10:28+00:00",
    "closed_at": "2016-01-28T21:27:38+00:00",
    "resolver": "nbonnotte",
    "resolved_in": "f673af1bb51bb465144ffd2f843672e23433384a",
    "resolver_commit_num": 7,
    "title": "Obscur AttributeError when dropping on a multi-index dataframe",
    "body": "\n\nThis is related to issue #11640. I have been working on a solution that I submitted in the pull request #11717, but the said solution was controversial, so I'm creating this issue to separate the problems.\n\nI'll make a PR soon enough.\n",
    "labels": [
      "Indexing",
      "Error Reporting",
      "MultiIndex"
    ],
    "comments": [
      "I'm a bit confused.\n\nAs I have understood the API, here `.drop` should not work, because `'a'` is not a column, and we should just have a more meaningful error message. If I wanted to remove the columns whose first level is `'a'`, I should do `df.drop('a', axis=1, level=0)`. Right?\n\nOn the other hand, if we consider\n\n```\nIn [4]: dg = pd.DataFrame([[1,3,4]],columns=pd.MultiIndex.from_tuples([('a',''),('b1','c1'),('b2','c2')],names=['b','c']))\n\nIn [5]: dg\nOut[5]: \nb  a b1 b2\nc    c1 c2\n0  1  3  4\n```\n\nthen `dg` and `df` are equivalent:\n\n```\nIn [7]: from pandas.util.testing import assert_frame_equal\n\nIn [8]: assert_frame_equal(df, dg) or \"No error raised\"\nOut[8]: 'No error raised'\n```\n\nbut\n\n```\nIn [14]: dg.drop('a', axis=1)\nOut[14]: \nb b1 b2\nc c1 c2\n0  3  4\n```\n\nHere is what happens:\n- In `MultiIndex.drop` (see  [here](https://github.com/pydata/pandas/blob/fab291b306ebf8b89d094379c99bd1b2a2b601b9/pandas/core/index.py#L5694)), in the  `try... except ...` the `ValueError` is raised because `labels ['a'] not contained in axis`, which is correct.\n- Then we go on, to `loc = self.get_loc(label)`, with here `label='a'`\n- In `MultiIndex.get_loc`, since the key `'a'` is not a tuple, the parameter `level=0` is automagically added (see [here](https://github.com/pydata/pandas/blob/fab291b306ebf8b89d094379c99bd1b2a2b601b9/pandas/core/index.py#L6136-L6138))\n\nDoes that mean that, in the API as it should be, in `.drop` the parameter `level=0` was intended to be superfluous? That is, `df.drop('a', axis=1)` should be equivalent to `df.drop('a', axis=1, level=0)` ?\n\nWhat should I do in my pull request?\n\nAs as side note, the reason why `.drop` fails for the first example `df` and not for the second example `dg` comes later: for the former, `.get_loc` returns a boolean mask, and the latter returns a slice, but `.drop` forgets to handle boolean mask (see [those lines](https://github.com/pydata/pandas/blob/fab291b306ebf8b89d094379c99bd1b2a2b601b9/pandas/core/index.py#L5728-L5731))\n\nAlso, I feel the need to say that I'm sorry if it seems that I am insisting a bit on those issues about `.drop`. I just like to understand things, and I'm confused about what the code pretends to be doing, what it should in theory do, and what it actually does. I guess that's bound to happen on such a complex project, and I'd be glad to help in any direction I can.\n",
      "```\nIn [1]: dg = pd.DataFrame([[1,3,4]],columns=pd.MultiIndex.from_tuples([('a',''),('b1','c1'),('b2','c2')],names=['b','c']))\n\nIn [6]: dg.columns.is_lexsorted()\nOut[6]: True\n\nIn [7]: df = pd.DataFrame(columns=['a','b','c','d'], data=[[1,'b1','c1',3], [1,'b2','c2',4]])\n\nIn [8]: df = df.pivot_table(index='a', columns=['b','c'], values='d').reset_index()\nIn [9]: df.columns.is_lexsorted()\nOut[9]: False\n```\n\nThe difference is that when the columns are not lexsorted this doesn't work, and the error message is incorrectly propogated, and an incorrect path is taken showing an error message which doesn't make sense. So you need to see where the difference is and what is happening to the exceptions.\n",
      "Oki doki, I'll do that ^^\n",
      "I couldn't find any other exception that would be raised but incorrectly propagated. Except the one that shows up, of course.\n\nAnd this exception is raised for the reason I gave: \n- when the multi-index is lexsorted, `.get_loc()` returns a slice\n- when it is not, it returns a boolean mask, but what comes next in `MultiIndex.drop` cant' handle that (see [those lines](https://github.com/pydata/pandas/blob/fab291b306ebf8b89d094379c99bd1b2a2b601b9/pandas/core/index.py#L5727-L5731))\n\n```\nIn [2]: ref = pd.MultiIndex.from_tuples([('a',''),('b1','c1'),('b2','c2')],names=['b','c'])\n\nIn [3]: pbm = pd.DataFrame(columns=['a','b','c','d'], data=[[1,'b1','c1',3], [1,'b2','c2',4]]).pivot_table(index='a', columns=['b','c'], values='d').reset_index().columns\n\nIn [6]: ref.is_lexsorted()\nOut[6]: True\n\nIn [7]: pbm.is_lexsorted()\nOut[7]: False\n\nIn [8]: ref.drop('a')\nOut[8]: \nMultiIndex(levels=[[u'a', u'b1', u'b2'], [u'', u'c1', u'c2']],\n           labels=[[1, 2], [1, 2]],\n           names=[u'b', u'c'])\n\nIn [9]: pbm.drop('a')\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-9-fcb8cd09713a> in <module>()\n----> 1 pbm.drop('a')\n\n/home/nicolas/Git/pandas/pandas/indexes/multi.py in drop(self, labels, level, errors)\n   1091                     inds.append(loc)\n   1092                 else:\n-> 1093                     inds.extend(lrange(loc.start, loc.stop))\n   1094             except KeyError as e:\n   1095                 if errors != 'ignore':\n\nAttributeError: 'numpy.ndarray' object has no attribute 'start'\n\nIn [10]: ref.get_loc('a')\nOut[10]: slice(0, 1, None)\n\nIn [11]: pbm.get_loc('a')\nOut[11]: array([ True, False, False], dtype=bool)\n\nIn [12]: ref.get_loc('a').start\nOut[12]: 0\n\nIn [13]: pbm.get_loc('a').start\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-13-2a974e7413c7> in <module>()\n----> 1 pbm.get_loc('a').start\n\nAttributeError: 'numpy.ndarray' object has no attribute 'start'\n```\n\nBut maybe I'm just not looking at the right place. Am I missing something?\n",
      "yeh, prob just not correctly implemented.\n",
      "Can I correct the implementation, so that `.drop` works for a non lexsorted multi-index in the same way as for a lexsorted one? :D\n\n```\nIn [2]: ref = pd.MultiIndex.from_tuples([('a',''),('b1','c1'),('b2','c2')],names=['b','c'])\n\nIn [3]: pbm = pd.DataFrame(columns=['a','b','c','d'], data=[[1,'b1','c1',3], [1,'b2','c2',4]]).pivot_table(index='a', columns=['b','c'], values='d').reset_index().columns\n\nIn [4]: ref.is_lexsort\nref.is_lexsorted            ref.is_lexsorted_for_tuple  \n\nIn [4]: ref.is_lexsorted()\nOut[4]: True\n\nIn [5]: pbm.is_lex\npbm.is_lexsorted            pbm.is_lexsorted_for_tuple  \n\nIn [5]: pbm.is_lexsorted()\nOut[5]: False\n\nIn [6]: ref.values\nOut[6]: array([('a', ''), ('b1', 'c1'), ('b2', 'c2')], dtype=object)\n\nIn [7]: pbm.values\nOut[7]: array([('a', ''), ('b1', 'c1'), ('b2', 'c2')], dtype=object)\n\nIn [8]: ref.drop('a')\nOut[8]: \nMultiIndex(levels=[[u'a', u'b1', u'b2'], [u'', u'c1', u'c2']],\n           labels=[[1, 2], [1, 2]],\n           names=[u'b', u'c'])\n```\n\nBeware that this simple correction might change the API of both `.drop` or `.groupby`, as we discussed in the pull request #11717 :innocent: \n\nSo perhaps a safer option would be to first have `ref.drop('a')` raise a `KeyError` or `ValueError` because `'a'` is not a correct value, the proper way being `ref.drop('a', level=0)`? And then correct the implementation.\n\nLet me know what I can do.\n",
      "I think `.drop` on a DataFrame is find (your example is not that). you can simply lexsort the pivot table I think.\n",
      "The problem with the DataFrame arises because of the problem with the MultiIndex, as shown in my examples.\n\nWhat can I do to remove the obscur error message?\n",
      "ahh, yes, see if you can\n\n`tm.assert_index_equal(pbm.drop('a'), ref.drop('a'))`\n\nthough you _may_ want to output a `PerformanceWarning` for `pbm.drop('a')`\n\nyou'll have to look and see how its used elsewhere.\n"
    ],
    "events": [
      "cross-referenced",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 39,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/indexes/multi.py",
      "pandas/tests/indexes/test_multi.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12089,
    "reporter": "JackKelly",
    "created_at": "2016-01-19T10:03:21+00:00",
    "closed_at": "2016-01-27T12:59:13+00:00",
    "resolver": "jreback",
    "resolved_in": "3152bdca818f44351ca1c8b63cb26fcfeae3cce7",
    "resolver_commit_num": 3857,
    "title": "Indexing into Series of tz-aware datetime64s fails using __getitem__",
    "body": "I'm a huge fan of Pandas.  Thanks for all the hard work!\n\nI believe I have stumbled across a small bug in Pandas 0.17.1 which was not present in 0.16.2.  Indexing into Series of _timezone-aware_ `datetime64`s fails using `__getitem__` but indexing succeeds if the `datetime64`s are timezone-_naive_.  Here is a minimal code example and the exception produced by Pandas 0.17.1:\n\n\n\nIf the dates are timezone-aware then we can access them using `loc` but, as far as I'm aware, we should be able to use `__getitem__` in this situation too:\n\n\n\nHowever, if the dates are timezone-_naive_ then indexing using `__getitem__` works as expected:\n\n\n\nSo indexing into a `Series` using `__getitem__` works if the data is a list of timezone-_naive_ `datetime64`s but indexing fails if the `datetime64`s are timezone-_aware_.\n\n\n",
    "labels": [
      "Bug",
      "Timeseries",
      "Regression",
      "Timezones"
    ],
    "comments": [
      "I can confirm this bug, also with current master. \n",
      "just need a `try: except:` around line 1780 catching `IndexError` and passing if its caught.\n\n@JackKelly want to do a PR?\n",
      "tests can go in the same place as in #12054 \n",
      "sure, I'll give it a go now...\n",
      "OK, I've attempted the fix.  [Here's the relevant commit on my fork of Pandas](https://github.com/JackKelly/pandas/commit/420c926932811d405c33d2c08bda122f6990684e).\n\nHowever, this hasn't fixed the issue and I'm not sure what's best to do.  My 'fix' has revealed a new issue.  The problem appears to be that, now, when we do `series['a']`, we get back a tz-_naive_ Timestamp (even though the `series` contains a bunch of tz-_aware_ datetime64s):\n\n``` python\nIn [5]: dates = pd.date_range(\"2011-01-01\", periods=3, tz='utc')\n\nIn [6]: dates\nOut[6]: DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03'], dtype='datetime64[ns, UTC]', freq='D')\n\nIn [7]: series = pd.Series(dates, index=['a', 'b', 'c'])\n\n# Note the lack of timezone:\nIn [8]: series['a']\nOut[8]: Timestamp('2011-01-01 00:00:00')\n\n# But using `loc` we do get the timezone:\nIn [9]: series.loc['a']\nOut[9]: Timestamp('2011-01-01 00:00:00+0000', tz='UTC')\n\nIn [10]: series\nOut[10]: \na   2011-01-01 00:00:00+00:00\nb   2011-01-02 00:00:00+00:00\nc   2011-01-03 00:00:00+00:00\ndtype: datetime64[ns, UTC]\n\nIn [11]: series['a'] == series.loc['a']\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-11-0de902e8919c> in <module>()\n----> 1 series['a'] == series.loc['a']\n\n/home/jack/workspace/python/pandas/pandas/tslib.pyx in pandas.tslib._Timestamp.__richcmp__ (pandas/tslib.c:19258)()\n    971                                 (type(self).__name__, type(other).__name__))\n    972 \n--> 973         self._assert_tzawareness_compat(other)\n    974         return _cmp_scalar(self.value, ots.value, op)\n    975 \n\n/home/jack/workspace/python/pandas/pandas/tslib.pyx in pandas.tslib._Timestamp._assert_tzawareness_compat (pandas/tslib.c:19638)()\n   1000         if self.tzinfo is None:\n   1001             if other.tzinfo is not None:\n-> 1002                 raise TypeError('Cannot compare tz-naive and tz-aware '\n   1003                                  'timestamps')\n   1004         elif other.tzinfo is None:\n\nTypeError: Cannot compare tz-naive and tz-aware timestamps\n```\n",
      "yeh, prob some issues down the path. lmk if you get stuck.\n",
      "Hmm, I think this is way over my head to be honest.  I'm really not very familiar with Pandas' internals.  I have had a quick shot at getting to the bottom of it.  Not sure if I've found any bugs or not.  Here are my notes:\n\nSet up a debugging session in IPython like this:\n\n``` python\ndates_with_tz = pd.date_range(\"2011-01-01\", periods=3, tz=\"US/Eastern\")\ns_with_tz = pd.Series(dates_with_tz, index=['a', 'b', 'c'])\n%debug s_with_tz['a']\n```\n\nwe find that:\n\nIn `Index.get_value()`, the line `s = _values_from_object(series)` sets `s` to be:\n\n``` python\n['2011-01-01T05:00:00.000000000+0000' '2011-01-02T05:00:00.000000000+0000'\n '2011-01-03T05:00:00.000000000+0000']\n```\n\ni.e. timezone is switched from \"US/Eastern\" to UTC.  I've tried stepping into `_values_from_object(series)` to find where the timezone is switched but I'm not sure I understand what I'm looking at.  My only hunch is that the following is broken (because the timezone should still be US/Eastern, surely?):\n\n``` python\nIn [32]: s_with_tz._values._values\nOut[32]: \narray(['2011-01-01T05:00:00.000000000+0000',\n       '2011-01-02T05:00:00.000000000+0000',\n       '2011-01-03T05:00:00.000000000+0000'], dtype='datetime64[ns]')\n```\n\nbut I'm really not sure!  Is `s_with_tz._values._values` supposed to return an array where the timezone is set to UTC instead of 'US/Eastern'?  Here are the other values (which look correct to me):\n\n``` python\nIn [33]: s_with_tz._values\nOut[33]: \nDatetimeIndex(['2011-01-01 00:00:00-05:00', '2011-01-02 00:00:00-05:00',\n               '2011-01-03 00:00:00-05:00'],\n              dtype='datetime64[ns, US/Eastern]', freq='D')\n\nIn [34]: s_with_tz\nOut[34]: \na   2011-01-01 00:00:00-05:00\nb   2011-01-02 00:00:00-05:00\nc   2011-01-03 00:00:00-05:00\ndtype: datetime64[ns, US/Eastern]\n```\n"
    ],
    "events": [
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "referenced",
      "commented",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 45,
    "deletions": 6,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/index.pyx",
      "pandas/indexes/base.py",
      "pandas/tests/series/test_indexing.py",
      "pandas/tseries/index.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12100,
    "reporter": "jreback",
    "created_at": "2016-01-20T14:41:50+00:00",
    "closed_at": "2016-01-25T15:31:23+00:00",
    "resolver": "jreback",
    "resolved_in": "2c03c5ec1af04eaa0355cdfdfa26db9cafa1112e",
    "resolver_commit_num": 3855,
    "title": "DEPR/TEST: fix numpy tz-aware parsing warnings",
    "body": "\n\nfull build [here](-ci.org/pydata/pandas/jobs/103526993)\nnote this is only on the numpy-dev builds (last 2)\n\n@shoyer recent change to show a deprecation warning on passing tz-aware strings to numpy [here]() is now causing loads of tests to show these.\n\nso in order to keep backward compat I suppose we need a wrapper function\n\n\n\ncurrent numpy\n\n\n\nso maybe \n\n`np_datetime64_compat(....)` that can then introspect our defined `_np_version_under1p11` (define in `pandas/__init__.py`) and add a 'Z' if needed (pre-suppose that we change the input formats to\n`2007-01-01 09:00:00.001'`) for clarity.\n",
    "labels": [
      "Testing",
      "Deprecate",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "Hmm. I wonder if we should adjust numpy to not issue a deprecation warning when the \"Z\" specifier is provided? Folks will be using that for quite a while for backwards compat, and it's actually 100% forwards compat in the typical case where you're time zone naive as UTC only...\n",
      "it IS technically wrong I think though. a `'Z'` implies by definition a tz-aware date (that is UTC). Since numpy doesn't support aware, then I guess it is clear what it is.\n\nBut you are right, it is no harm. OTOH, if you don't issue a `DeprecationWarning` then no-one will ever fix it.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "mentioned",
      "subscribed",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 7,
    "additions": 124,
    "deletions": 59,
    "changed_files_list": [
      "pandas/__init__.py",
      "pandas/compat/numpy_compat.py",
      "pandas/io/tests/test_date_converters.py",
      "pandas/tseries/tests/test_offsets.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tests/test_tslib.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12125,
    "reporter": "HHammond",
    "created_at": "2016-01-24T05:52:59+00:00",
    "closed_at": "2016-02-12T20:42:51+00:00",
    "resolver": "TomAugspurger",
    "resolved_in": "cf8b7f8ac8f4ffab6136e933fb878446dd27cb03",
    "resolver_commit_num": 29,
    "title": "Styler class fails to render numeric columns when 0 not in columns",
    "body": "The `core.style.Styler._translate` method uses `__getitem__` indexing when working with numerical cell locations (#L240). When using numeric dataframe columns the location based index causes confusion with the column name index, causing an error. \n\n\n\nThe expected behaviour should be to always use location based indexing. Changing [line 240](#L240) to:\n\n\n\nFixes the error.\n\nI do currently have a PR to fix this but wanted to file the bug report first. \n##### Error code\n\n\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.3.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.17.1\nnose: 1.3.7\npip: 7.1.2\nsetuptools: 19.4\nCython: None\nnumpy: 1.10.4\nscipy: 0.16.0\nstatsmodels: None\nIPython: 4.0.3\nsphinx: 1.3.1\npatsy: None\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.4.3\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\nJinja2: 2.8\n",
    "labels": [
      "Output-Formatting",
      "IO HTML",
      "Bug"
    ],
    "comments": [],
    "events": [
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 240,
    "deletions": 75,
    "changed_files_list": [
      "doc/source/api.rst",
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/style.py",
      "pandas/tests/test_style.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12133,
    "reporter": "Winand",
    "created_at": "2016-01-25T08:52:00+00:00",
    "closed_at": "2016-04-17T14:31:40+00:00",
    "resolver": "OXPHOS",
    "resolved_in": "3bed097b56db84891c891009c6bf57bb512040e5",
    "resolver_commit_num": 2,
    "title": "crosstab dropna=False breaks columns.names",
    "body": "Specifying `dropna=False` drops column level names:\n\n\n\nP.S. At first i thought `dropna=False` will help to count `NaN` values, though it seems that `fillna` is needed. ~~So i don't understand, why this parameter is needed at all.~~ If entire column is `NaN` then we'll get `KeyError: '__dummy__'` anyway.\n\npandas: 0.17.1\n",
    "labels": [
      "Bug",
      "Reshaping",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "xref #10291 \n",
      "fix: #12327\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 65,
    "deletions": 5,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/tools/pivot.py",
      "pandas/tools/tests/test_pivot.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12134,
    "reporter": "xflr6",
    "created_at": "2016-01-25T16:03:20+00:00",
    "closed_at": "2016-02-12T20:42:51+00:00",
    "resolver": "TomAugspurger",
    "resolved_in": "cf8b7f8ac8f4ffab6136e933fb878446dd27cb03",
    "resolver_commit_num": 29,
    "title": "style.set_precision(0) displays spurious .0",
    "body": "The expected display format would be as in [2]:\n![screenshot](-c385-11e5-89e1-69a2bdd0a049.png)\n",
    "labels": [
      "Bug",
      "Output-Formatting",
      "IO HTML",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "looks a bit buggy, want to do a PR?\n",
      "Okay, I guess the [Jinja2 docs](http://jinja.pocoo.org/docs/dev/templates/#round) recommend to do this by changing [this](https://github.com/pydata/pandas/blob/a783d1d4f8d129087c806d326a141b6052bed359/pandas/core/style.py#L120-L122) to something like this:\n\n``` jinja2\n{% if c.value is number %}\n    {% if precision %}\n        {{c.value|round(precision)}}\n     {% else %}\n         {{c.value|round|int}}\n     {% endif %}\n{% else %}\n```\n\nOkay to put another branch like this into the inner loop?\n",
      "I think that is ok, the rendering time is not usually an issue (unless you are rendering millions of lines, which itself is an issue anyhow).\n\n@TomAugspurger \n",
      "`{{c.value|round|int}}` fails with `nan` and `inf`, so the submitted PR uses percent-formatting\n",
      "Sorry, I didn't see this until now. I've got branch that is changing how the display formatting works, which is going to conflict with your PR in #12137.  I believe I've fixed this bug in the change ([here](https://github.com/TomAugspurger/pandas/commit/a929d6d4f611f00466871a1c8e2e159d95191133) if you want to take a look at the relevant bit). I'm going to clean that up and submit sometime this week.\n\nIt boils down to using formatting like\n\n``` python\nIn [4]: '{:0g}'.format(100.0)\nOut[4]: '100'\n```\n\nwhen your precision is 0\n",
      "No problem, more control over formatting is nice, stepping back.\n\nDoes this unify rendering of NaNs (IIRC df.style displays `nan` and normal html rendering `NaN`)?\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 240,
    "deletions": 75,
    "changed_files_list": [
      "doc/source/api.rst",
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/style.py",
      "pandas/tests/test_style.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12139,
    "reporter": "jreback",
    "created_at": "2016-01-25T20:17:18+00:00",
    "closed_at": "2016-01-27T22:39:25+00:00",
    "resolver": "jreback",
    "resolved_in": "9bc82438268742575e501d099c6f22ce0a2dcac1",
    "resolver_commit_num": 3860,
    "title": "CI: windows 27-64 build failing",
    "body": "This has been working for quite some time and I don't think the code changes broke this: -465/build/1.0.145\n\nso something changed in what conda is pull in I think (as I cannot repro this, though I have a slightly older build env)\n\nif anyone wants to take a look\n",
    "labels": [
      "CI",
      "Windows"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 13,
    "deletions": 15,
    "changed_files_list": [
      "conda.recipe/meta.yaml",
      "pandas/__init__.py",
      "pandas/compat/numpy_compat.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12140,
    "reporter": "jreback",
    "created_at": "2016-01-25T21:52:05+00:00",
    "closed_at": "2016-02-02T15:16:18+00:00",
    "resolver": "jreback",
    "resolved_in": "1dc49f51afe67fdc17fa2670545c053775765ebc",
    "resolver_commit_num": 3866,
    "title": "ENH: followup for resample, #11841 ",
    "body": "@jorisvandenbossche comments for followup on #11841 \n- [x] the repr: can we follow here PEP8 as well? :-) (I mean spaces after the comma's, I think this would make it a bit more readable, maybe also quotes around the strings)\n- [x] `upsample` has been removed? (but is still in the documentation) Or is this now `asfreq`?\n\n> doc are updated, it IS now `.asfreq`\n- [x] `apply gives Exception: Must produce aggregated value` (for a series) if you pass it a function that does not return a aggregated value. For groupby this works, would be nice to have this consistent. \n  When applying it on a resampled dataframe, you get the cryptic `ValueError: cannot copy sequence with size 3 to array axis with dimension 180` message. Typical example is just a `.apply(lambda x: x)`\n- [x] `Resampler.fillna` has no explanation\n- [x] `DatetimeIndexResampler` has no docstring yet\n- [x] `apply, agg, aggregate` have no docstring, transform a very brief\n  In theory it would be best if this were addressed in this PR, but given the PEP8 changes waiting on this PR, it's OK for me to leave this for a follow-up PR\n- [x] Further, an inconsistency between `r.agg()` and `r[].agg()` in:\n\n\n- [x] validate that we have tests for all combinations of aggregation\n\n\n",
    "labels": [
      "Enhancement",
      "Docs",
      "Resample"
    ],
    "comments": [
      "@jorisvandenbossche \nthis should address all of the remaining points: https://github.com/jreback/pandas/commit/2cccc709003f4b20d742f0e224911180d3bfe12d\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "mentioned",
      "subscribed",
      "milestoned",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 23,
    "additions": 2784,
    "deletions": 997,
    "changed_files_list": [
      "doc/source/api.rst",
      "doc/source/cookbook.rst",
      "doc/source/release.rst",
      "doc/source/timedeltas.rst",
      "doc/source/timeseries.rst",
      "doc/source/whatsnew/v0.10.0.txt",
      "doc/source/whatsnew/v0.18.0.txt",
      "doc/source/whatsnew/v0.9.1.txt",
      "pandas/core/base.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/ops.py",
      "pandas/core/window.py",
      "pandas/io/tests/test_excel.py",
      "pandas/tests/test_generic.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tests/test_window.py",
      "pandas/tseries/plotting.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12152,
    "reporter": "lexual",
    "created_at": "2016-01-27T04:36:59+00:00",
    "closed_at": "2016-03-14T13:02:22+00:00",
    "resolver": "robintw",
    "resolved_in": "4f5099b4ee5f5c32f6b299de93902c9a074ac228",
    "resolver_commit_num": 2,
    "title": "better docs for read_csv() argument infer_datetime_format",
    "body": "This gives pretty decent explanation of the infer_datetime_format parameter:\n\n-docs/version/0.17.1/whatsnew.html#id55\n\nThe api docs are fairly opaque.\n\nImprove api docs to make it clearer:\n\n-docs/stable/generated/pandas.read_csv.html\n",
    "labels": [
      "Docs",
      "Timeseries",
      "CSV",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "xref #12061 \n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 2,
    "additions": 23,
    "deletions": 6,
    "changed_files_list": [
      "pandas/io/parsers.py",
      "pandas/tseries/tools.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12153,
    "reporter": "toobaz",
    "created_at": "2016-01-27T10:55:54+00:00",
    "closed_at": "2016-02-06T20:04:13+00:00",
    "resolver": "toobaz",
    "resolved_in": "45a83a08c9af59e033382217fbc1a2bbbe748466",
    "resolver_commit_num": 8,
    "title": "TypeError: 'TextFileReader' object is not an iterator",
    "body": "\n\nyields\n\n\n\nwhich I didn't expect. Sure, I can do `csv = iter(pd.read_csv(file_path, chunksize=10))`, but since I don't see any advantage (i.e. indexing) in having a non-iterator iterable, maybe read_csv could directly return an iterator? In other words,\n\n\n\nis maybe not what people expect from a non-iterator iterable.\n\n(Even) I should be able to submit a PR. The only question is whether 1) this is worth fixing 2) this is worth fixing in pandas rather than in pytables.\n",
    "labels": [
      "API Design",
      "CSV",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "`TextFileReader` and the `HDFStore` are _iterables_, not _iterators_, kind of like lists, so that they are reusable, see a good description [here](http://stackoverflow.com/questions/13054057/confused-with-python-lists-are-they-or-are-they-not-iterators)\n\nfrom a practical point of view, I don't actually think we have a way of reusing these as the point is that these are out-of-core by definition.\n\nSo I suppose adding `.next()`` would be ok.\n\nSee if anything breaks.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "referenced"
    ],
    "changed_files": 7,
    "additions": 64,
    "deletions": 47,
    "changed_files_list": [
      "pandas/io/common.py",
      "pandas/io/parsers.py",
      "pandas/io/sas.py",
      "pandas/io/stata.py",
      "pandas/io/tests/test_common.py",
      "pandas/io/tests/test_sas.py",
      "pandas/io/tests/test_stata.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12157,
    "reporter": "rafguns",
    "created_at": "2016-01-27T15:33:44+00:00",
    "closed_at": "2016-01-30T19:22:40+00:00",
    "resolver": "chris-b1",
    "resolved_in": "059ffaadc72d6c64f27e88885a0e0c7409b0fb08",
    "resolver_commit_num": 30,
    "title": "read_excel(..., index_col=0, squeeze=True) raises AttributeError",
    "body": "`pd.read_excel` with keyword argument `squeeze=True` raises an AttributeError. This is unexpected, in that:\n- I would expect it to work analogously to `pd.read_csv`, which does not exhibit the same problem\n- it worked fine in a previous version (0.15 I think?)\n\nSteps to reproduce:\n\n\n",
    "labels": [
      "Bug",
      "Regression",
      "IO Excel",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "I was the culprit here -  #10967, looks like `squeeze` is untested on `read_excel`.  Probably just need a little extra logic around that name assignment, I can take a look.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 6,
    "additions": 31,
    "deletions": 7,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/io/excel.py",
      "pandas/io/tests/data/test_squeeze.xls",
      "pandas/io/tests/data/test_squeeze.xlsm",
      "pandas/io/tests/data/test_squeeze.xlsx",
      "pandas/io/tests/test_excel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12160,
    "reporter": "jreback",
    "created_at": "2016-01-27T22:28:31+00:00",
    "closed_at": "2016-07-10T21:39:51+00:00",
    "resolver": "sinhrks",
    "resolved_in": "2a96ab7bd9614be79f349975908b42c676a244ab",
    "resolver_commit_num": 327,
    "title": "DEPR: rename Timestamp.offset internally .freq / deprecate passing offset in favor of freq",
    "body": "xref  #6815/#6813 .freq in Timestamp rather than .offset (internally used) (0.14.0 ???)\n\nThis is a bit non-trivial so pulling out to a separate issue from #6581 \n",
    "labels": [
      "Timeseries",
      "Frequency",
      "Deprecate"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 20,
    "additions": 187,
    "deletions": 142,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/io/packers.py",
      "pandas/io/tests/data/legacy_msgpack/0.18.1/0.18.1_x86_64_darwin_2.7.12.msgpack",
      "pandas/io/tests/data/legacy_msgpack/0.18.1/0.18.1_x86_64_darwin_3.5.2.msgpack",
      "pandas/io/tests/data/legacy_pickle/0.18.1/0.18.1_x86_64_darwin_2.7.12.pickle",
      "pandas/io/tests/data/legacy_pickle/0.18.1/0.18.1_x86_64_darwin_3.5.2.pickle",
      "pandas/io/tests/generate_legacy_storage_files.py",
      "pandas/io/tests/test_packers.py",
      "pandas/io/tests/test_pickle.py",
      "pandas/lib.pxd",
      "pandas/src/inference.pyx",
      "pandas/src/period.pyx",
      "pandas/tests/indexing/test_indexing.py",
      "pandas/tests/series/test_constructors.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_base.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tests/test_tslib.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12169,
    "reporter": "mapa17",
    "created_at": "2016-01-28T17:58:27+00:00",
    "closed_at": "2016-02-10T17:29:34+00:00",
    "resolver": "BranYang",
    "resolved_in": "e9558d3b926e089dcda4ce4ff38f1e980777424b",
    "resolver_commit_num": 1,
    "title": "Resample category data with timedelta index",
    "body": "Hi,\n\nI get a very strange behavior when i try to resample categorical data with and timedelta index, as compared to a datetime index. \n\n\n\nIt seems to me the aggregated result in case of using timedelta as an index for the category is always NaN.\nShould this be?\n\nThx\n\nPS: is there a way to specify the dtype for the aggregated columns?\n",
    "labels": [
      "Bug",
      "Resample",
      "Categorical",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "hmm, does appear a little buggy.\n\nyou shouldn't need to specify the dtype on aggregations they are inferred. Here I think there is an embedded exception which is caught in stead of actuallly computing correctly.\n",
      "I look after #11841  as the timedelta resampling is tested a bit more there (but not enough!)\n",
      "The root cause of this issue is that, when construct Series from a dict with TimedeltaIndex as key, it will treat the value as float64. See pandas/core/series.py, from line 172 to 185\n\n``` python\ntry:\n    if isinstance(index, DatetimeIndex):\n        if len(data):\n            # coerce back to datetime objects for lookup\n            data = _dict_compat(data)\n            data = lib.fast_multiget(data, index.astype('O'),\n                                     default=np.nan)\n        else:\n            data = np.nan\n    elif isinstance(index, PeriodIndex):\n        data = ([data.get(i, nan) for i in index]\n                if data else np.nan)\n    else:\n        data = lib.fast_multiget(data, index.values,\n                                 default=np.nan)\n```\n\nI believe just change `isinstance(index, PeriodIndex):` to `isinstance(index, (PeriodIndex, TimedeltaIndex):` would solve this issue\n\nBefore\n\n``` Python\nIn [5]: fxx = d2.resample('10s', how=lambda x: (x.value_counts().index[0]))\n\nIn [6]: fxx\nOut[6]:\n         Group_obj  Group\n00:00:00         A    NaN\n00:00:10         A    NaN\n```\n\nAfter\n\n``` Python\nIn [5]: fxx = d2.resample('10s', how=lambda x: (x.value_counts().index[0]))\n\nIn [6]: fxx\nOut[6]:\n         Group_obj Group\n00:00:00         A     A\n00:00:10         A     A\n```\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 4,
    "additions": 33,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/series.py",
      "pandas/tests/series/test_constructors.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12170,
    "reporter": "rspeer",
    "created_at": "2016-01-28T19:15:12+00:00",
    "closed_at": "2016-02-10T16:20:49+00:00",
    "resolver": "kawochen",
    "resolved_in": "e82d0931b8542ded78204b1f694008752f757990",
    "resolver_commit_num": 44,
    "title": "DataFrame.to_msgpack unexpectedly defaults to latin-1 encoding",
    "body": "I am using Python 3.\n\nI tried saving a DataFrame with Unicode labels using the `.to_msgpack` method. I didn't specify an encoding, because I assumed it would use UTF-8, which is the default encoding for Python in my locale (en_US.UTF-8) as well as just a sensible encoding to use in general.\n\nInstead, it tried to encode labels in Latin-1, which failed. Latin-1 seems like a strangely antiquated default to use in modern code.\n\nI can work around it by passing the `encoding='utf-8'` option, but it would be helpful if UTF-8 were the default, as it is in other Python I/O.\n\nHere's my version information:\n\n\n",
    "labels": [
      "Unicode",
      "Msgpack",
      "API Design"
    ],
    "comments": [
      "hmm, we have `latin1` as the default. @kawochen do you know where that came from? I am guess from the prior version of code (before your upgrade)\n",
      "I didn't know why either.  the new spec says utf-8 for strings (but I think everyone is keeping the encoding/decoding option for compatibility).\n",
      "hmm, also I we should have `encoding` as a top-level option for `.to_msgpack` and `.read_msgpack`\nits passed thru now, but should be part of the doc-string at least.\n",
      "@kawochen can you do a PR for this?\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced"
    ],
    "changed_files": 4,
    "additions": 27,
    "deletions": 14,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/generic.py",
      "pandas/io/packers.py",
      "pandas/io/tests/test_packers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12171,
    "reporter": "jreback",
    "created_at": "2016-01-28T22:02:05+00:00",
    "closed_at": "2016-02-17T13:20:15+00:00",
    "resolver": "troglotit",
    "resolved_in": "5d1857cf0df7555abf3c173e0053c29a5ccfd392",
    "resolver_commit_num": 0,
    "title": "CLN: change getargspec -> signature (using from pandas.compat)",
    "body": "this eliminates some deprecation warnings\n\n\n",
    "labels": [
      "Difficulty Novice",
      "Compat",
      "Effort Low"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 31,
    "deletions": 8,
    "changed_files_list": [
      "pandas/compat/__init__.py",
      "pandas/tests/series/test_analytics.py",
      "pandas/tests/test_panel.py",
      "pandas/util/decorators.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12176,
    "reporter": "wojdyr",
    "created_at": "2016-01-29T15:15:58+00:00",
    "closed_at": "2016-02-27T14:28:05+00:00",
    "resolver": "mfarrugi",
    "resolved_in": "1d6d7d4667806f1b7bca640ffc4eb368441e2471",
    "resolver_commit_num": 0,
    "title": "confusing ImportError message in `__init__.py`",
    "body": "In some cases the error message could be more clear, as discussed here:\n#commitcomment-15762471\n\nmy example:\n\n\n",
    "labels": [
      "Build",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "yeh, prob need to go thru the deps (`pytz`, `dateutil`) and try importing them and give a nice message.\n`numpy` is already checked. Then if this is hit it would be a good message.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 2,
    "additions": 16,
    "deletions": 3,
    "changed_files_list": [
      "pandas/__init__.py",
      "pandas/compat/numpy_compat.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12180,
    "reporter": "DGrady",
    "created_at": "2016-01-29T18:16:15+00:00",
    "closed_at": "2016-02-01T20:09:16+00:00",
    "resolver": "DGrady",
    "resolved_in": "34d98391f450fc4e083784f164ad0f426710af9e",
    "resolver_commit_num": 0,
    "title": "Series.str.get_dummies fails if one of the categorical variables is called 'name'",
    "body": "This works as expected:\n\n\n\nHowever, if any of the categorical variables is named exactly `'name'`, then there's a problem.\n\n\n\nThis is presumably related to the `pandas/core/strings.py` code in the stacktrace.\n\n\n",
    "labels": [
      "Bug",
      "Strings",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "I think this is a bug in `_wrap_result`. The `getattr(result,'Name',None)` only should be checked if 'name' is in `_metadata` IOW, its a `Series`.\n",
      "@DGrady want to take a crack at it?\n",
      "Absolutely \u2014 I'll take a look at it later today.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 12,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/strings.py",
      "pandas/tests/test_strings.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12182,
    "reporter": "chris-b1",
    "created_at": "2016-01-30T02:17:56+00:00",
    "closed_at": "2016-02-10T18:51:40+00:00",
    "resolver": "chris-b1",
    "resolved_in": "d20767925fc866f1a51c43424f7b70a023a2240e",
    "resolver_commit_num": 31,
    "title": "BUG: _repr_latex is called in jupyter qtconsole",
    "body": "xref #11778\n\n![image](-c6c5-11e5-8ca4-14750f904a8f.png)\n",
    "labels": [
      "Output-Formatting",
      "IO LaTeX",
      "Blocker"
    ],
    "comments": [
      "hmm, is this on master?\n",
      "There are some issues at IPython/Jupyter for this: https://github.com/jupyter/qtconsole/pull/65, https://github.com/jupyter/qtconsole/issues/56 (IRkernel has the same problem for the display of their dataframes)\n\n@chris-b1 Would you be able to test if this is solved in qtconsole master?\n",
      "I think this is a reason to _consider_ reverting #11778\n",
      "cc @nbonnotte \n",
      "Or, another option is making the use of the `_repr_latex_` optional, and defaulting to False. So somebody who wants this feature for nbconvert, can switch it on, but knowing that it will bug in qtconsole.\n",
      "so we could have an option `display.latex.repr=False` maybe?\n",
      "Yeah, this also happens on qtconsole master.  If there was a way to make this [`in_qtconsole`](https://github.com/pydata/pandas/blob/master/pandas/core/common.py#L2743) check work again, that'd be a way to avoid an option.\n\nBut since they've restructured all the ipython/jupyter stuff, I don't know how (if it's possible) to do that.\n",
      "This goes beyond my current use of pandas or jupyter, so I'm not really qualified to say anything.\n\nThat being said, I like the idea of being able to nicely export my notebooks containing dataframes to LaTeX. And, if I understand correctly, this is more a bug of the jupyter qtconsole rather than pandas, right? So, if anything, my current understanding of the situation does not convince me that reverting #11778 is absolutely necessary. But maybe it's just because I use jupyter notebooks, and I feel closer to exporting them to LaTeX rather than using the qtconsole.\n",
      "> And, if I understand correctly, this is more a bug of the jupyter qtconsole rather than pandas, right? So, if anything, my current understanding of the situation does not convince me that reverting #11778 is absolutely necessary. But maybe it's just because I use jupyter notebooks\n\nThe qtconsole is used in eg Spyder, which I suppose quite some pandas users will use. \nTo be clear, ideally we find another solution or workaround so reverting #11778 is not needed. But if not, we cannot ship pandas like this in any case, as it would break the interactive usage for lots of users.\n",
      "A possible idea is to do something similar as what `sympy` does, so that the user has to call a function to activate the latex repr support (`sympy.init_printing()`). \nE.g. a `pd.options.display.init_latex_repr()` (although this being a function is not really consistent with the other options). Calling the function would then add the `_repr_latex_` method to DataFrame on the fly.\n",
      "Would it still be possible to convert a notebook to LaTeX?\n\nAnd a naive question: what prevents [in_qtconsole](https://github.com/pydata/pandas/blob/fc77cafe2a70cc08a81ebf2e855bca096f3bb7e6/pandas/core/common.py#L2765-L2780) from working? Is it because jupyter does not want us to know to which front-end it is connected?\n",
      "> Would it still be possible to convert a notebook to LaTeX?\n\nYes, but you will just have call this function in the beginning of your notebook that you want to convert to latex to activate the latex repr\n\n> And a naive question: what prevents in_qtconsole from working? Is it because jupyter does not want us to know to which front-end it is connected?\n\nAs far as I understand, yes, the kernel does not know in which frontend it is run (notebook or qtconsole or ... are all `ipykernel.zmqshell.ZMQInteractiveShell` ). And if you want to convert a notebook to nbconvert, the latex repr should already be stored inside of the notebook.\n",
      "@jorisvandenbossche so what should we do about this, revert the actual `_repr_latex_` method?\n\nThe other changes are fine.\n",
      "I think the best would be something like the following:\n\nHave a new `pd.options.display.latex_repr` option, which defaults to False, and when setting it to True, this triggers a callback function that adds the `_latex_repr_` method dynamically to DataFrame.\n",
      "It doesn't even need to be a callback, `_repr_latex` can just return `None` (if the option is False) and the qtconsole will fallback.  I'll submit a PR here shortly.\n",
      "Ah, yes, that seems also to work. Even simpler!\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "commented",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 33,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/options.rst",
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/config_init.py",
      "pandas/core/frame.py",
      "pandas/tests/frame/test_repr_info.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12185,
    "reporter": "toobaz",
    "created_at": "2016-01-30T16:45:53+00:00",
    "closed_at": "2016-07-29T00:15:15+00:00",
    "resolver": "toobaz",
    "resolved_in": "5b0d947e8079a438bdac9490efa97227a9145f9f",
    "resolver_commit_num": 15,
    "title": "read_csv() restarts index (if not loaded) at every chunk",
    "body": "See [this comment](#discussion_r51347252). Basically we now have\n\n\n\nbut\n\n\n\nWe want the latter to also have index (0,1).\n",
    "labels": [
      "Bug",
      "API Design",
      "CSV",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [],
    "events": [
      "cross-referenced",
      "renamed",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 5,
    "additions": 60,
    "deletions": 6,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/io/parsers.py",
      "pandas/io/tests/parser/common.py",
      "pandas/io/tests/parser/test_network.py",
      "pandas/io/tests/test_common.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12188,
    "reporter": "edublancas",
    "created_at": "2016-01-31T02:25:51+00:00",
    "closed_at": "2016-05-26T12:54:22+00:00",
    "resolver": "edublancas",
    "resolved_in": "57ea76fb9a1d0b23943c700a6129d37de6df6adc",
    "resolver_commit_num": 0,
    "title": "Confusing interpretation of what DataFrame.join does",
    "body": "From the docs I see that the difference between .merge and .join is that .join operates using indexes by default, but it also lets you use columns,  so I tried to use it for that since it sounds natural when coming from the SQL world.\n\nFrom the [docs](-docs/version/0.17.1/generated/pandas.DataFrame.join.html):\n\n> on : column name, tuple/list of column names, or array-like\n> Column(s) to use for joining, otherwise join on index. If multiples columns given, the passed DataFrame must have a MultiIndex. Can pass an array as the join key if not already contained in the calling DataFrame. Like an Excel VLOOKUP operation\n\nFrom my understanding, if on is absent a join operation is performed on the index, if on is present, it would be reasonable to think that the same operation would be performed.\n\nHaving said that:\n\n\n\nOutput:\n\n\n\nAnd if I add id as index:\n\n\n\nOutput:\n\n\n\nIs that the correct behavior? If yes, I think the documentation is misleading. It took me a lot to find the bug in my code and I ended up using merge since .join works in an unexpected way.\n\nI don't think I'm the [only one](-simple-join-not-working) with this issue, so maybe a change in the documentation would help to clarify how .join works.\n",
    "labels": [
      "Docs",
      "Reshaping",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "xref to #6336 \n",
      "So the above join operation does this\n\n```\nIn [3]: a.join(b, on='id', lsuffix='_x',  rsuffix='_y', how='left')\nOut[3]: \n           address_x  id_x          address_y  id_y\n0   1820 SOME STREET     1    1140 BIG STREET     3\n1  32 ANOTHER STREET     2    20 SMALL STREET     4\n2    1140 BIG STREET     3   1820 SOME STREET     1\n3    20 SMALL STREET     4  32 ANOTHER STREET     2\n4   1090 AVENUE NAME     5                NaN   NaN\n```\n\nthis merge\n\n```\nIn [4]: pd.merge(a, b, left_on='id', right_index=True, suffixes=('_x','_y'), how='left')\nOut[4]: \n           address_x  id_x          address_y  id_y\n0   1820 SOME STREET     1    1140 BIG STREET     3\n1  32 ANOTHER STREET     2    20 SMALL STREET     4\n2    1140 BIG STREET     3   1820 SOME STREET     1\n3    20 SMALL STREET     4  32 ANOTHER STREET     2\n4   1090 AVENUE NAME     5                NaN   NaN\n```\n\nWhereas I think you _think_ it should do this one\n\n```\nIn [5]: pd.merge(a, b, left_on='id', right_on='id', suffixes=('_x','_y'), how='left')\nOut[5]: \n           address_x  id          address_y\n0   1820 SOME STREET   1   1820 SOME STREET\n1  32 ANOTHER STREET   2  32 ANOTHER STREET\n2    1140 BIG STREET   3    1140 BIG STREET\n3    20 SMALL STREET   4    20 SMALL STREET\n4   1090 AVENUE NAME   5   1090 AVENUE NAME\n```\n\nI agree that is a bit confusing a bit, but I think the rationale is that you are joining from left, and the `on` controls what you are joining (on the left). There is no way in the `.join` to control the rhs, which is pre-supposed to be the index.\n\nSo this is what\n\n```\nIn [9]: a.join(b.set_index('id'), on='id', lsuffix='_x',  rsuffix='_y', how='left')\nOut[9]: \n           address_x  id          address_y\n0   1820 SOME STREET   1   1820 SOME STREET\n1  32 ANOTHER STREET   2  32 ANOTHER STREET\n2    1140 BIG STREET   3    1140 BIG STREET\n3    20 SMALL STREET   4    20 SMALL STREET\n4   1090 AVENUE NAME   5   1090 AVENUE NAME\n```\n\nSo I think a doc-update, maybe with some more examples would help?\n",
      "Thanks for the clarification! I agree that a documentation update will be really helpful.\n\nBesides an update in the [join method doc](http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.DataFrame.join.html), a brief mention of this behavior would be useful at the bottom of Database-style DataFrame joining/merging section [here](http://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging).\n",
      "sure - pull requests would be welcome!\n"
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 2,
    "additions": 84,
    "deletions": 13,
    "changed_files_list": [
      "doc/source/merging.rst",
      "pandas/core/frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12196,
    "reporter": "jreback",
    "created_at": "2016-02-01T19:56:57+00:00",
    "closed_at": "2016-02-01T21:20:39+00:00",
    "resolver": "jreback",
    "resolved_in": "f683dc796c495e594ab23da744b326a9bbf795e2",
    "resolver_commit_num": 3865,
    "title": "BUG: numpy master quantile interpolation method",
    "body": "\n",
    "labels": [
      "Bug"
    ],
    "comments": [],
    "events": [
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 1,
    "additions": 9,
    "deletions": 3,
    "changed_files_list": [
      "pandas/tests/frame/test_analytics.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12202,
    "reporter": "m313",
    "created_at": "2016-02-02T10:02:05+00:00",
    "closed_at": "2016-02-12T20:40:12+00:00",
    "resolver": "jreback",
    "resolved_in": "311b9a99a24e6885ed06ed64001d95cb474648c5",
    "resolver_commit_num": 3883,
    "title": "Resampling converts int to float, but only in group by",
    "body": "\n\nCalling resample() on the dataframe does not change the type.\n\n\n\nHowever, when calling resample() in a group by statement, float type is returned!\n\n\n\nWhy is val converted to float in the group by statement?\nI originally posted this on [stack overflow](-converts-int-to-float-but-only-in-group-by).\n\n\n",
    "labels": [
      "Bug",
      "Resample",
      "Testing"
    ],
    "comments": [
      "I don't think this was getting passed thru correctly, and is fixed in the recently merged\n#11841 (in master / 0.18.0 coming soon)\n\nNote the new api\n\njust need confirming tests\n\n```\nIn [13]: df.groupby('group').resample('1D').ffill()\nOut[13]: \n            val\ndate           \n2016-01-03    5\n2016-01-10    6\n2016-01-17    7\n2016-01-24    8\n\nIn [14]: df.groupby('group').resample('1D').ffill().val.dtype\nOut[14]: dtype('int32')\n```\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 2,
    "additions": 19,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12203,
    "reporter": "dacoex",
    "created_at": "2016-02-02T13:24:59+00:00",
    "closed_at": "2016-03-20T15:10:03+00:00",
    "resolver": "gfyoung",
    "resolved_in": "e55875e5753509a1397cd0a50825811926ace477",
    "resolver_commit_num": 6,
    "title": "io.read_csv: ValueError due to long lines",
    "body": "I try to read a csv file were some lines are longer than the rest.\n\nPandas `C engine` throws an error with these lines. \nBut I do not was to skip these as discussed in [ a comment on a similar issue](#issuecomment-30425534).\n\nI prefer to \"cut\" the \"bad columns\" off using usecols. But I get the following errors:\n\n\n\nThrows:\n\n`ValueError: Expected 10 fields in line 100, saw 20`\n\nWhy is the `ValueError` raised although I explicitly defined the columns to use?\n\nReferences:\n- #2886 \n- [Dealing with bad lines]() -- does not apply\n- [Dealing with bad lines II](-unclean-data-csv-using-pandas/)\n- [Handling Variable Number of Columns with Pandas - Python](-variable-number-of-columns-with-pandas-python)\n- [pandas read_csv and filter columns with usecols](-read-csv-and-filter-columns-with-usecols), especially: \n",
    "labels": [
      "CSV",
      "API Design",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "Here's a longer but self-contained example based on [pandas read_csv and filter columns with usecols](http://stackoverflow.com/questions/15017072/pandas-read-csv-and-filter-columns-with-usecols)\n\n```\n\nimport pandas as pd\n#from cSt import StringIO\n#from cStringIO import StringIO\n\nimport io\n\n# works\ncsv = r\"\"\"dummy,date,loc,x\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1\nbar,20090102,b,3\nbar,20090103,b,5\"\"\"\n\ndf = pd.read_csv(io.StringIO(csv),\n        header=0,\n        sep=\",\",\n        index_col=[\"date\", \"loc\"], \n        usecols=[\"date\", \"loc\", \"x\"],\n        parse_dates=[\"date\"])\ndf2 = pd.read_csv(io.StringIO(csv),\n        header=None,\n        sep=\",\",\n        usecols=[0, 2, 3])\n#        parse_dates=0)\n\n# throws value error\n# ValueError: Expected 4 fields in line 5, saw 6\n\ncsv = r\"\"\"dummy,date,loc,x\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1,2,2\nbar,20090102,b,3\nbar,20090103,b,5\"\"\"\n\ndf2 = pd.read_csv(io.StringIO(csv),\n        header=None,\n        sep=\",\",\n        usecols=[0, 2, 3],\n         engine='python')\n#        parse_dates=0)\n\n# works\n\ncsv = r\"\"\"dummy,date,loc,x\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1,2,2\nbar,20090102,b,3\nbar,20090103,b,5\"\"\"\n\nmycols = ['col_1', 'col_2', 'col_3', 'col_4', 'col_5', 'col_6']\ndf3 = pd.read_csv(io.StringIO(csv),\n        header=None,\n        sep=\",\",\n        names=mycols,\n         engine='python')\n#        parse_dates=0)\n\n\n# works but inefficient\n\ncsv = r\"\"\"dummy,date,loc,x\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1,2,2\nbar,20090102,b,3\nbar,20090103,b,5\"\"\"\n\nmycols = list(map(str, range(6)))\ndf4 = pd.read_csv(io.StringIO(csv),\n        header=None,\n        sep=\",\",\n        names=mycols,\n         engine='python')\n#        parse_dates=0)\n\ndf4_cut = df4.iloc[:,:4]\n\n# improved parametrised solution:\n\ncsv = r\"\"\"dummy,date,loc,x\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1,2,2\nbar,20090102,b,3\nbar,20090103,b,5\"\"\"\n\ncol_expected = 4\ncol_saw = 6\nmycols = list(map(str, range(col_saw)))\ndf5 = pd.read_csv(io.StringIO(csv),\n        header=None,\n        sep=\",\",\n        names=mycols,\n         engine='python')\n#        parse_dates=0)\n\ndf5_cut = df5.iloc[:,:col_expected]\n```\n\nSo why does the version where the columns are selected based on `names` work and the version where use cols are given does not work?\n",
      "looks like a dupe of this: https://github.com/pydata/pandas/issues/9755\n\npls confirm\n",
      "I connot confirm. \n\nThe reason: in all examples of #9755 `names` **and** `usecols` were given.\n\nIn my case, it's data without any column names.\n\nSo I would like to be able to specify the columns to be used without using names.\n",
      "ok, i'll mark it. though I suspect its a very similar soln. pull-requests welcome.\n",
      "I noted that `nrows` is even more sensitive\n",
      "The whole problem is that `usecols` is used to derive a subset of the cleanly parsed csv file.\n\nE.g. when reading\n\n```\n\n1,2,3\n1,2,3\n1,2,3\n1,2,3\n1,2,3\n```\n\nusecols[0,1] would aloow to get a df with the col 1 and col 2.\n\nWhat I seek, any others on SO, is to limit the number of columns that are parsed initially.\n\ne.g.\n\n```\n1,2,3\n1,2,3\n1,2,3,4,5,6,7,8,9,0\n1,2,3\n1,2,3,4,5,6,7,8,9,0,,4,5,6,7,8,9,0\n```\n\nIn this case, usecols does not apply and work.\nThe different length of the 2 bad rows confuse teh reader completely.\n\nBut real world data, often from dataloggers, produces such artefacts on random basis... \n"
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "unlabeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 6,
    "additions": 50,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/parser.pyx",
      "pandas/src/parser/tokenizer.c",
      "pandas/src/parser/tokenizer.h"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12211,
    "reporter": "uzihs",
    "created_at": "2016-02-02T20:16:13+00:00",
    "closed_at": "2016-04-30T14:32:55+00:00",
    "resolver": "sinhrks",
    "resolved_in": "db35ff455227d1b705509966740d3d6607faa3d9",
    "resolver_commit_num": 300,
    "title": "str(data_frame) raises an exception when there is a column with both NaT value and Timestamp with timezone info",
    "body": "When you try to print a data frame that has a Timestamp column with both NaT values and Timestamp with timezone, AND the number or rows is more than the display setting - an exception is raised.\n\n\n\nThis is the traceback when running the code above with 0.17.1 (with 0.17.0, the traceback is different, with this error message: AttributeError: 'numpy.ndarray' object has no attribute 'tz_localize')\n\n---\n\nTypeError                                 Traceback (most recent call last)\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/IPython/core/formatters.py in **call**(self, obj)\n    339             method = _safe_get_formatter_method(obj, self.print_method)\n    340             if method is not None:\n--> 341                 return method()\n    342             return None\n    343         else:\n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/frame.py in _repr_html_(self)\n    573             return self.to_html(max_rows=max_rows, max_cols=max_cols,\n    574                                 show_dimensions=show_dimensions,\n--> 575                                 notebook=True)\n    576         else:\n    577             return None\n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/frame.py in to_html(self, buf, columns, col_space, colSpace, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, bold_rows, classes, escape, max_rows, max_cols, show_dimensions, notebook)\n   1529                                            max_rows=max_rows,\n   1530                                            max_cols=max_cols,\n-> 1531                                            show_dimensions=show_dimensions)\n   1532         # TODO: a generic formatter wld b in DataFrameFormatter\n   1533         formatter.to_html(classes=classes, notebook=notebook)\n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/format.py in **init**(self, frame, buf, columns, col_space, header, index, na_rep, formatters, justify, float_format, sparsify, index_names, line_width, max_rows, max_cols, show_dimensions, **kwds)\n    378             self.columns = frame.columns\n    379 \n--> 380         self._chk_truncate()\n    381         self.adj = _get_adjustment()\n    382 \n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/format.py in _chk_truncate(self)\n    444             else:\n    445                 row_num = max_rows_adj // 2\n--> 446                 frame = concat((frame.iloc[:row_num, :], frame.iloc[-row_num:, :]))\n    447             self.tr_row_num = row_num\n    448 \n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/tools/merge.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\n    811                        verify_integrity=verify_integrity,\n    812                        copy=copy)\n--> 813     return op.get_result()\n    814 \n    815 \n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/tools/merge.py in get_result(self)\n    993 \n    994             new_data = concatenate_block_managers(\n--> 995                 mgrs_indexers, self.new_axes, concat_axis=self.axis, copy=self.copy)\n    996             if not self.copy:\n    997                 new_data._consolidate_inplace()\n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/internals.py in concatenate_block_managers(mgrs_indexers, axes, concat_axis, copy)\n   4454                                                 copy=copy),\n   4455                          placement=placement)\n-> 4456               for placement, join_units in concat_plan]\n   4457 \n   4458     return BlockManager(blocks, axes)\n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/internals.py in <listcomp>(.0)\n   4454                                                 copy=copy),\n   4455                          placement=placement)\n-> 4456               for placement, join_units in concat_plan]\n   4457 \n   4458     return BlockManager(blocks, axes)\n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/internals.py in concatenate_join_units(join_units, concat_axis, copy)\n   4551     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\n   4552                                          upcasted_na=upcasted_na)\n-> 4553                  for ju in join_units]\n   4554 \n   4555     if len(to_concat) == 1:\n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/internals.py in <listcomp>(.0)\n   4551     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\n   4552                                          upcasted_na=upcasted_na)\n-> 4553                  for ju in join_units]\n   4554 \n   4555     if len(to_concat) == 1:\n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/internals.py in get_reindexed_values(self, empty_dtype, upcasted_na)\n   4799 \n   4800             if self.is_null and not getattr(self.block,'is_categorical',None):\n-> 4801                 missing_arr = np.empty(self.shape, dtype=empty_dtype)\n   4802                 if np.prod(self.shape):\n   4803                     # NumPy 1.6 workaround: this statement gets strange if all\n\nTypeError: data type not understood\n\n---\n\nTypeError                                 Traceback (most recent call last)\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/IPython/core/formatters.py in **call**(self, obj)\n    695                 type_pprinters=self.type_printers,\n    696                 deferred_pprinters=self.deferred_printers)\n--> 697             printer.pretty(obj)\n    698             printer.flush()\n    699             return stream.getvalue()\n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/IPython/lib/pretty.py in pretty(self, obj)\n    381                             if callable(meth):\n    382                                 return meth(obj, self, cycle)\n--> 383             return _default_pprint(obj, self, cycle)\n    384         finally:\n    385             self.end_group()\n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/IPython/lib/pretty.py in _default_pprint(obj, p, cycle)\n    501     if _safe_getattr(klass, '__repr__', None) not in _baseclass_reprs:\n    502         # A user-provided repr. Find newlines and replace them with p.break_()\n--> 503         _repr_pprint(obj, p, cycle)\n    504         return\n    505     p.begin_group(1, '<')\n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/IPython/lib/pretty.py in _repr_pprint(obj, p, cycle)\n    683     \"\"\"A pprint that just redirects to the normal repr function.\"\"\"\n    684     # Find newlines and replace them with p.break_()\n--> 685     output = repr(obj)\n    686     for idx,output_line in enumerate(output.splitlines()):\n    687         if idx:\n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/base.py in **repr**(self)\n     61         Yields Bytestring in Py2, Unicode String in py3.\n     62         \"\"\"\n---> 63         return str(self)\n     64 \n     65 \n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/base.py in **str**(self)\n     40 \n     41         if compat.PY3:\n---> 42             return self.**unicode**()\n     43         return self.**bytes**()\n     44 \n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/frame.py in **unicode**(self)\n    539             width = None\n    540         self.to_string(buf=buf, max_rows=max_rows, max_cols=max_cols,\n--> 541                        line_width=width, show_dimensions=show_dimensions)\n    542 \n    543         return buf.getvalue()\n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/frame.py in to_string(self, buf, columns, col_space, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, line_width, max_rows, max_cols, show_dimensions)\n   1478                                            max_rows=max_rows,\n   1479                                            max_cols=max_cols,\n-> 1480                                            show_dimensions=show_dimensions)\n   1481         formatter.to_string()\n   1482 \n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/format.py in **init**(self, frame, buf, columns, col_space, header, index, na_rep, formatters, justify, float_format, sparsify, index_names, line_width, max_rows, max_cols, show_dimensions, **kwds)\n    378             self.columns = frame.columns\n    379 \n--> 380         self._chk_truncate()\n    381         self.adj = _get_adjustment()\n    382 \n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/format.py in _chk_truncate(self)\n    444             else:\n    445                 row_num = max_rows_adj // 2\n--> 446                 frame = concat((frame.iloc[:row_num, :], frame.iloc[-row_num:, :]))\n    447             self.tr_row_num = row_num\n    448 \n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/tools/merge.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\n    811                        verify_integrity=verify_integrity,\n    812                        copy=copy)\n--> 813     return op.get_result()\n    814 \n    815 \n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/tools/merge.py in get_result(self)\n    993 \n    994             new_data = concatenate_block_managers(\n--> 995                 mgrs_indexers, self.new_axes, concat_axis=self.axis, copy=self.copy)\n    996             if not self.copy:\n    997                 new_data._consolidate_inplace()\n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/internals.py in concatenate_block_managers(mgrs_indexers, axes, concat_axis, copy)\n   4454                                                 copy=copy),\n   4455                          placement=placement)\n-> 4456               for placement, join_units in concat_plan]\n   4457 \n   4458     return BlockManager(blocks, axes)\n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/internals.py in <listcomp>(.0)\n   4454                                                 copy=copy),\n   4455                          placement=placement)\n-> 4456               for placement, join_units in concat_plan]\n   4457 \n   4458     return BlockManager(blocks, axes)\n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/internals.py in concatenate_join_units(join_units, concat_axis, copy)\n   4551     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\n   4552                                          upcasted_na=upcasted_na)\n-> 4553                  for ju in join_units]\n   4554 \n   4555     if len(to_concat) == 1:\n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/internals.py in <listcomp>(.0)\n   4551     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\n   4552                                          upcasted_na=upcasted_na)\n-> 4553                  for ju in join_units]\n   4554 \n   4555     if len(to_concat) == 1:\n\n/Users/uzi/.virtualenvs/pandas/lib/python3.4/site-packages/pandas/core/internals.py in get_reindexed_values(self, empty_dtype, upcasted_na)\n   4799 \n   4800             if self.is_null and not getattr(self.block,'is_categorical',None):\n-> 4801                 missing_arr = np.empty(self.shape, dtype=empty_dtype)\n   4802                 if np.prod(self.shape):\n   4803                     # NumPy 1.6 workaround: this statement gets strange if all\n\nTypeError: data type not understood\n",
    "labels": [
      "Bug",
      "Reshaping",
      "Timezones",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "Hmm I just fixed this for the Series constructor, but the DataFrame construtor does a different introspection.\n\n```\nIn [14]: Series([pd.Timestamp(datetime.now().replace(tzinfo=UTC))]+[pd.NaT]*5)         \nOut[14]: \n0   2016-02-02 16:53:41.321229+00:00\n1                                NaT\n                  ...               \n4                                NaT\n5                                NaT\ndtype: datetime64[ns, UTC]\n```\n",
      "I see that it's assigned to 0.18.0. But it is not yet resolved in the version that was released last week. Right?\n",
      "no, its assigned to next major release, which is kind of a catch-all to say its an open issue. so this is not resolved.\n",
      "Actually this _does_ look fixed though. #11693, #11755, #12217 were all fixed in 0.18.0. could have been caught there. \n\n```\nIn [9]: test\nOut[9]: \n                                dt  x\n0 2016-03-23 11:11:48.441987+00:00  1\n1                              NaT  1\n2                              NaT  1\n3                              NaT  1\n4                              NaT  1\n5                              NaT  1\n```\n\nWant to do some confirming tests?\n",
      "https://github.com/pydata/pandas/commit/dc6c678fbbd5dbad8e16e179d4e7e25feb3e5642 or referenced issues might be the fixes\n",
      "actually forgot the part with the `max_rows`, nvm. This is still an issue.\n",
      "Right. I tested the example I posted above, with 0.18.0. Same behaviour...\n",
      "the root cause is https://github.com/pydata/pandas/issues/11594 (and I point to a fix there). Its pretty straightforward to fix actually.\n",
      "This is already fixed (maybe by #12195). Adding test.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "commented",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 2,
    "additions": 55,
    "deletions": 13,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/tests/formats/test_format.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12217,
    "reporter": "andyljones",
    "created_at": "2016-02-03T09:22:52+00:00",
    "closed_at": "2016-02-13T01:15:50+00:00",
    "resolver": "jreback",
    "resolved_in": "dc6c678fbbd5dbad8e16e179d4e7e25feb3e5642",
    "resolver_commit_num": 3884,
    "title": "Concatenating two localized datetimes returns NaTs  ",
    "body": "Issue:\n\n\n\nDependencies:\n\n\n",
    "labels": [
      "Bug",
      "Reshaping",
      "Timezones",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "I can replicate with python 2.7.10, windows 10, pandas 0.17.1, numpy 1.9.3.\n",
      "hmm, partially was addressed in #12195 \n\nbut still looks buggy\n"
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "milestoned",
      "demilestoned",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 2,
    "additions": 44,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12221,
    "reporter": "toobaz",
    "created_at": "2016-02-03T15:07:21+00:00",
    "closed_at": "2016-02-11T02:55:17+00:00",
    "resolver": "toobaz",
    "resolved_in": "c805c3b0f6d9dd20b2319e37e0002d413b1e34ca",
    "resolver_commit_num": 9,
    "title": "KeyError Iterating on empty HDFStore",
    "body": "\n\nshould, I think, return `[]`, instead it yields:\n\n\n",
    "labels": [
      "Difficulty Novice",
      "Error Reporting",
      "IO HDF5",
      "Effort Low"
    ],
    "comments": [
      "has nothing to do with an empty store, `HDFStore` is an iterable (e.g. has `__getitem__` and `__len__` defined), but not `__iter__`, so this essentially calls `.get`. I suppose it should just raise a better error message.\n\nyou are probably looking for `.keys()` (though I suppose `__iter__` could just call that)\n",
      "Sorry, I was mislead by `HDFStore().root` being iterable. I will implement the `__iter__` you suggest.\n"
    ],
    "events": [
      "renamed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 13,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/io/pytables.py",
      "pandas/io/tests/test_pytables.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12223,
    "reporter": "s-wakaba",
    "created_at": "2016-02-03T17:01:14+00:00",
    "closed_at": "2017-02-24T20:18:20+00:00",
    "resolver": "Dr-Irv",
    "resolved_in": "595580464a256fb883e8baa5b6e62f2013f0cf1a",
    "resolver_commit_num": 10,
    "title": "multi-index display bug",
    "body": "Thanks for great tool!\n\nWhen printing a long DataFrame with multi-index which are result of  set_index(..., append=True),\ndisplaying index values look strange.\n\n\n\nWhen DataFrames are short (in this case, 10 rows), there is no problem.\nHowever, when DataFrames is long (in this case, 100 rows) and omitted middle-part on printing, some columns of index is not correct. \n\n\n\nresult of pd.show_versions() is as follows.\n\n\n",
    "labels": [
      "Bug",
      "MultiIndex"
    ],
    "comments": [
      "Related to #10461.\n\nThis is caused by `MultiIndex.append` internally used. The 2nd level is overwritten by level named with \"1\".\n\n```\nimport pandas as pd\n\n# OK\nidx1 = pd.MultiIndex.from_tuples([(1, 'A', 1), (1, 'A', 2)])\nidx1.append(idx1)\n# MultiIndex(levels=[[1], [u'A'], [1, 2]],\n#            labels=[[0, 0, 0, 0], [0, 0, 0, 0], [0, 1, 0, 1]])\n\n# NG, index name has number which is different from location\nidx2 = idx1.set_names([0, 'a', 1])\nidx2.append(idx2)\n# MultiIndex(levels=[[1], [1, 2], [1, 2]],\n#            labels=[[0, 0, 0, 0], [0, 1, 0, 1], [0, 1, 0, 1]],\n#            names=[0, u'a', 1])\n```\n\nAs a workaround, do not use numeric names. \n"
    ],
    "events": [
      "labeled",
      "labeled",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned"
    ],
    "changed_files": 10,
    "additions": 46,
    "deletions": 22,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/reshape.py",
      "pandas/formats/format.py",
      "pandas/indexes/base.py",
      "pandas/indexes/multi.py",
      "pandas/io/sql.py",
      "pandas/tests/frame/test_combine_concat.py",
      "pandas/util/doctools.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12232,
    "reporter": "ckingdon95",
    "created_at": "2016-02-04T21:24:54+00:00",
    "closed_at": "2016-02-08T15:18:59+00:00",
    "resolver": "kshedden",
    "resolved_in": "ca4f738114a12d719116117b5d2ceb4f39a8cebb",
    "resolver_commit_num": 13,
    "title": "issue with StataReader for stata files versions 108 and older",
    "body": "I am having an issue with the StataReader class, which is found in stata.py (\"pandas/io/stata.py\").\nI have pandas: 0.17.1.\n\nThe following is the python code I am trying to run:\n\n\n\nwhere fileName is a stata file.\n\nThe following code is part of the _read_old_header method(which starts on line 1184) of the StataReader class in stata.py, which gets called during the initialization of a StataReader object:\n\n\n\nI have no errors when my stata files are newer than version 108, but with files that are version 105, there seems to be a bug in _decode_bytes. The above code passes in self and only one additional argument to _decode_bytes, the string that is returned by path_or_buf.read(1).\n\nHere is the the method _decode_bytes (line 896):\n\n\n\nWhen no third argument is passed in (as is the case when it is called by _read_old_header), the argument \"errors\" is set to None. Here is where the error is thrown. The error is:\n\n\n\nThat is the issue: the decode method of the string class is expecting the second argument to not be a None type, but _decode_bytes passes in errors as None by default. \n",
    "labels": [
      "IO Stata",
      "Compat"
    ],
    "comments": [
      "cc @bashtage @kshedden \n",
      "@ckingdon95  thanks for the detailed report.  We don't have any test files that old, and I cannot create a file that old with the latest version of stata, which is the only one I can access (see link below).  So we might need someone to provide us with a test file to troubleshoot this.  Are there any version 108 files floating around on the web?\n\nhttp://www.stata.com/support/faqs/data-management/save-for-previous-version/\n",
      "I don't think we need `_decode_bytes` there, the column types are all ASCII.\n\n_decode_bytes is not covered by any tests and is only called in one place.\n\nCan someone with a version < 108 file change lines 1204-1208 of stata.py to:\n\n `typlist = [self.OLD_TYPE_MAPPING[self.path_or_buf.read(1)] for i in range(self.nvar)]`\n\nand report back?  If we adopt this we should remove the `_decode_bytes` method.\n",
      "thanks for the reply! the stata files I am trying to read can be found at this website: http://econ.worldbank.org/WBSITE/EXTERNAL/EXTDEC/EXTRESEARCH/EXTLSMS/0,,contentMDK:21544648~pagePK:64168445~piPK:64168309~theSitePK:3358997,00.html\n"
    ],
    "events": [
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "cross-referenced",
      "labeled",
      "milestoned"
    ],
    "changed_files": 4,
    "additions": 53,
    "deletions": 42,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/io/stata.py",
      "pandas/io/tests/data/S4_EDUC1.DTA",
      "pandas/io/tests/test_stata.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12258,
    "reporter": "toobaz",
    "created_at": "2016-02-08T13:41:47+00:00",
    "closed_at": "2016-09-27T10:36:44+00:00",
    "resolver": "pijucha",
    "resolved_in": "b81d444fb324eb637546a8e806554172d182fd52",
    "resolver_commit_num": 6,
    "title": "\"ValueError\" on specific MultiIndex.from_product initialization(s?)",
    "body": "Probably my favourite bug so far:\n\n\n\nThis is with Python 3, doesn't happen with Python 2. I will try to investigate a bit later.\n",
    "labels": [
      "Testing",
      "MultiIndex",
      "Compat",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "I guess these should work. We don't appear to have any tests for empty level construction. \n\nNote these are pretty much useless in practice.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 5,
    "additions": 135,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/categorical.py",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tools/tests/test_util.py",
      "pandas/tools/util.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12259,
    "reporter": "randomgambit",
    "created_at": "2016-02-08T14:45:49+00:00",
    "closed_at": "2016-07-21T10:59:35+00:00",
    "resolver": "bashtage",
    "resolved_in": "0fe5a345f90e4d9029a0bf923245b51c6c6a8322",
    "resolver_commit_num": 26,
    "title": "ValueError: Data type datetime64[ns] not currently understood. Please report an error to the developers.",
    "body": "I do what I am told to do ;-)\n\nI have a big dataframe with the following columns\n\n\n\nand I try to save it using pd.to_stata\n\n`daily_news.to_stata( \"mypath/data.dta\",convert_dates= {'obs_day' : 'td'})`\n\nand I get\n\n\n",
    "labels": [
      "Error Reporting",
      "IO Stata"
    ],
    "comments": [
      "pls `pd.show_versions()` and provide a reproducible example.\n\nIIRC this is simply not supported\n\ncc @kshedden this should be `NotImplementedError` in this case\n",
      "```\npd.show_versions()\n\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 63 Stepping 2, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: en_US\n\npandas: 0.17.1\nnose: 1.3.7\npip: 7.1.2\nsetuptools: 18.5\nCython: 0.23.4\nnumpy: 1.10.1\nscipy: 0.16.0\nstatsmodels: None\nIPython: 4.0.1\nsphinx: 1.3.1\npatsy: 0.4.0\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.4.4\nmatplotlib: 1.5.0\nopenpyxl: 2.2.6\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.7.7\nlxml: 3.4.4\nbs4: 4.4.1\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.9\npymysql: None\npsycopg2: None\nJinja2: None\n\n```\n\nI ll try to get a working example.\nwhat do you mean by IIRC this is simply not supported?\n\nthanks!\n",
      "Try converting the datetime64 to an array of datetimes, some examples here:\n\nhttp://stackoverflow.com/questions/13703720/converting-between-datetime-timestamp-and-datetime64\n",
      "This error is happening because the call `to_stata` is being used on a DF that has datetimes but these have not been included in the `convert_dates` dict. If you included keys for all datetime columns, this command should work as expected.\n\nWill try a PR that will default to `tc` type which is close to datetime64, only using ms rather than ns.\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 124,
    "deletions": 49,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/frame.py",
      "pandas/io/stata.py",
      "pandas/io/tests/test_stata.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12278,
    "reporter": "joshlk",
    "created_at": "2016-02-10T10:36:56+00:00",
    "closed_at": "2016-02-23T14:57:16+00:00",
    "resolver": "Dorozhko-Anton",
    "resolved_in": "6b544de628fa56a62c4834922e7bcc5a22ba00bf",
    "resolver_commit_num": 0,
    "title": "Performance: .unique / .nunique of categorical series slow on large data set",
    "body": "I've noticed that `Series.unique` and `Series.nunique` when used with a categorical series can be slow on large dataset. Presumably its not utilising the shortcuts:\n\n\n\nHeres an example in iPhython:\n\n\n\nIts significantly slower indicating its not using the above shortcut.\n\npd.show_versions()\n\n\n",
    "labels": [
      "Performance",
      "Categorical",
      "Docs",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "pls pd.show_versions()\nand an example \n",
      "Sorry @jreback more details have been added above\n",
      "so, `.unique` on a categorical has a couple of guarantees, namely that it is in the order of appearance, and it only includes values that are actually present, e.g.\n\n```\nIn [3]: s = Series(list('babc')).astype('category',categories=list('abcd'))\n\nIn [4]: s.unique()\nOut[4]: \n[b, a, c]\nCategories (3, object): [b, a, c]\n\nIn [6]: s.cat.categories\nOut[6]: Index([u'a', u'b', u'c', u'd'], dtype='object')\n```\n\nso if you want `.categories`, then just use that. \n",
      "I suppose it is possible to add an option (to all `.unique`), not just this one, to do some sort of `quick` uniqueness like this, but not sure of the utility beyond categoricals.\n",
      "I didn't realise that, I thought they were equivalent. Thanks\n",
      "It would be good to clarify this in the docs I think:\n- [ ] add a note to the docstring of `categories` that this includes all categories and not only those present in the data\n- [ ] add the above example (https://github.com/pydata/pandas/issues/12278#issuecomment-182426808) to the tutorial docs `categorical.rst`\n",
      "ok, let's repurpose this then. @joshlk want to do a PR?\n",
      "Can I get this one?\n",
      "sure\n"
    ],
    "events": [
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "closed",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "reopened",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 1,
    "additions": 17,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/categorical.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12288,
    "reporter": "toobaz",
    "created_at": "2016-02-11T09:24:39+00:00",
    "closed_at": "2016-02-12T14:38:36+00:00",
    "resolver": "toobaz",
    "resolved_in": "1ca094f8b0d9776965210cd4b378af5ed7404e55",
    "resolver_commit_num": 10,
    "title": "RangeIndex copy behaviour differs from other indices",
    "body": "It is my understanding that the `copy` parameter of `RangeIndex` is present virtually only for API compatibility with other indices: however the behaviour for `copy=False` differs from other indices in what follows:\n\n\n\nIn other words, since `RangeIndex` doesn't have any data to copy, the value of the `copy` parameter should be irrelevant. For homogeneity with other indices, we should always act as if `copy=True`. The docs should also be changed to state that `copy` parameter has no effect and is present only for API compatibility. I'll push a PR if you agree.\n",
    "labels": [
      "Bug",
      "Indexing",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "this is a bug in the constructor I think. It should match what `Int64Index` does. \nIt doesn't matter that `RangeIndex` doesn't have data to copy, it still should make a new index.\n",
      "So `Int64Index`,  `Float64Index` all have the same issue.\nThey are not propogating the name of a passed index on construction. That is the issue here.\n`RangeIndex` actually is ok.\n\nThese look ok\n\n```\nIn [6]: a = Index(list('abc'),name='foo')\n\nIn [7]: Index(a)\nOut[7]: Index([u'a', u'b', u'c'], dtype='object', name=u'foo')\n```\n\n```\nIn [20]: i = Index(xrange(3),name='foo')\n\nIn [21]: i\nOut[21]: RangeIndex(start=0, stop=3, step=1, name=u'foo')\n\nIn [22]: pd.RangeIndex(i)\nOut[22]: RangeIndex(start=0, stop=3, step=1, name=u'foo')\n```\n\nWhile Int64Index does not\n\n```\nIn [8]: i = Index(range(3),name='foo')\n\nIn [9]: Index(i)\nOut[9]: Int64Index([0, 1, 2], dtype='int64', name=u'foo')\n\nIn [10]: pd.Int64Index(i)\nOut[10]: Int64Index([0, 1, 2], dtype='int64')\n```\n",
      "I think we have 2 problems here: the one I reported and the one you now point at. If we fix the one you're pointing at, we will still have a difference that `Int64Index(Int64Index(), copy=*)` will create two indices no matter the value of `copy`, while `RangeIndex(RangeIndex(), copy=False)` will not, and this reflects in the impossibility of giving different names. So you're right in `Int64Index` & co. being buggy, but this doesn't have much to say on my PR.\n",
      "Or to put it shortly: [this test](https://github.com/pydata/pandas/pull/12295/files#diff-c3cc00bc22d7c510c58dfe0be4c46abfR141) fails in master. I think it shouldn't. Do you agree? [Y/n]\n",
      "ok, going to merge your fix, pls open a new issue for the `Int64Index/Float64Index` name issues I desrcribe above.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 28,
    "deletions": 11,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/indexes/range.py",
      "pandas/tests/indexes/test_range.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12290,
    "reporter": "ghost",
    "created_at": "2016-02-11T11:13:31+00:00",
    "closed_at": "2016-02-12T13:39:08+00:00",
    "resolver": "kawochen",
    "resolved_in": "273bfb0d0c8425cb36b247e1895bb9a3bf89b83b",
    "resolver_commit_num": 45,
    "title": "Computing time difference with timezone in Series with one row gives wrong result",
    "body": "When there are only one row containing timestamps with timezone in DataFrame, computing time difference between columns gives incorrect result - always '0 days':\n\n\n\nIf there are more than one row, result is correct:\n\n\n\nOne row containing timestamps in naive datetime also gives correct result:\n\n\n\nInstalled versions:\n\n\n",
    "labels": [
      "Bug",
      "Timezones",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "hmm that does look like a bug.\n",
      "Where is \"-\" being implemented in for `Series`?  The `sub()` and `subtract()` methods work perfectly fine.  I suspect it's somewhere hidden <a href=\"https://github.com/pydata/pandas/blob/master/pandas/core/ops.py#L264\">here</a>, but if someone with a better understanding could enlighten, that would be great!\n",
      "@gfyoung that's the right place. you should just step thru.\n",
      "`pandas` is trolling me.\n\nI was able to track the bug down to this line <a href=\"https://github.com/pydata/pandas/blob/master/pandas/core/ops.py#L490\">here</a>.  For some strange reason, the `lvalue` (and also the `rvalue`) become `NaT` which is why the difference becomes zero because the `np.int64` view value is just `np.iinfo(np.int64).min`.  However, my attempts to replicate that error directly were strangely not very successful.\n\nWhat I did was this, using a <u>local</u> installation of the codebase:\n1. Right before this <a href=\"https://github.com/pydata/pandas/blob/master/pandas/core/ops.py#L482\">line</a>, I placed a `raise Exception(lvalues, rvalues)`.  Afterwards, if you walk through the code presented in the issue, you <i>should</i> be able to confirm that they are both `DateTimeIndex` objects with the correct times.\n2. I then removed the `Exception`, and right before this <a href=\"https://github.com/pydata/pandas/blob/master/pandas/core/ops.py#L494\">line</a>, I placed a `raise Exception(lvalues, rvalues)`.  Afterwards, if you walk through the code presented in the issue, you <i>should</i> be able to confirm that both are converted to `NaT`.\n3. I then removed the `Exception`, and right before this <a href=\"https://github.com/pydata/pandas/blob/master/pandas/core/ops.py#L490\">line</a>.  I assigned attributes `old_lvalues` and `old_rvalues` to the `_TimeOp` attribute.\n4. I then ran this code:\n\n``` .Python\nimport operator\nimport pandas as pd\n\ndf = pd.DataFrame(columns=['s', 'f'])\n\ndf.s = [pd.Timestamp('2016-02-08 13:43:14.605000', tz='America/Sao_Paulo')]\ndf.f = [pd.Timestamp('2016-02-10 13:43:14.605000', tz='America/Sao_Paulo')]\n\nt = _TimeOp.maybe_convert_for_time_op(df.s, df.f, '__sub__', operator.sub)\n\nprint t.lvalue, t.rvalue\nprint t.old_lvalues.tz_localize(None), t.old_rvalues.tz_localize(None)\n```\n\nYou should be able to see my confusion.\n",
      "that IS odd. \n",
      "@jreback @polart : Also, the title should be changed, as it's really an issue with `Series` arithmetic operations and has nothing to do actually with `DataFrame`.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 13,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/tests/series/test_operators.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12292,
    "reporter": "bteplygin",
    "created_at": "2016-02-11T12:52:35+00:00",
    "closed_at": "2016-02-27T15:09:05+00:00",
    "resolver": "BranYang",
    "resolved_in": "78d671fbcd18736c3fa8f47aead07b26b895ac4b",
    "resolver_commit_num": 3,
    "title": "read_excel crash with empty Exception",
    "body": "If read one empty column from excel file, read_excel raise Exception without error message.\nSmall test case: content of excel file:\n\n| A | B | C | D |\n| --- | --- | --- | --- |\n|  | 1 | 100 | f |\n|  | 2 | 200 | f |\n|  | 3 | 300 | f |\n|  | 4 | 400 | f |\n\nRun:\n\n\n\nResult:\n\n\n\nPython 2.7.7\npandas 0.17.1\nxlrd 0.9.4\n",
    "labels": [
      "Bug",
      "IO Excel",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "can you programatically create this example as a frame and write it to excel, alternatively post a link to the actual excel file (first is preferred if you can replicate the actual error)\n",
      "For example:\n\n```\nimport pandas as pd\ndf = pd.DataFrame([[\"\", 1, 100], [\"\", 2, 200], [\"\", 3, 300], [\"\", 4, 400]])\ndf.to_excel(\"test_excel.xls\", index=False, header=False)\npd.read_excel(\"test_excel.xls\", parse_cols=[0], header=None)\n```\n",
      "@bteblygin thanks!\n\nPR to fix would be great!\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 75,
    "deletions": 15,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/io/excel.py",
      "pandas/io/tests/test_excel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12303,
    "reporter": "vonosmas",
    "created_at": "2016-02-12T00:43:14+00:00",
    "closed_at": "2020-04-19T21:25:06+00:00",
    "resolver": "jreback",
    "resolved_in": "aeb2c82acb0d1fbdbd6c4fce796e35a161757d1e",
    "resolver_commit_num": 3902,
    "title": "float-cast-overflow: converting NaN to integers in numpy C code triggered by pandas",
    "body": "Converting floating-point value NaN to any integer data type is an undefined behavior in C. However, it actually happens in numpy extension module, which is probably caused by incorrect usage of it from pandas. If the former is built with Clang+UBSan (), there are error reports indicating the issue.\n\nNow, this is somewhat tricky to reproduce, because it involves building NumPy C code with UBSan. The following instructions should work on Ubuntu 14.04\n- Get fresh enough Clang (e.g. 3.8.0rc2 from -releases/3.8.0/rc2)\n- Build NumPy with Clang and float-cast-overflow detection\n\n\n- Fetch latest pandas\n- Export ASan runtime library to provide UBSan implementation, setup runtime flags for sanitizers:\n\n\n- Build pandas\n\n\n- Run tests from the test suite triggering the issue\n\n\n\nIt seems that pandas are using \"np.nan\" to aggressively: by calling astype(<integer_type>) on arrays that can contain NaNs, and in calling fill(np.nan) on np.empty() arrays of integral types.\n",
    "labels": [
      "Testing",
      "Build",
      "Linux"
    ],
    "comments": [
      "can you show an example w/o all of the custom builds. Just stock numpy and latest pandas (or a version).\n",
      "No: you need to recompile C code in NumPy for the error to be reported (otherwise NaN is somehow silently converted to a long under the hood when you're running `pandas/tests/test_groupby.py:TestGroupBy.test_agg_nested_dicts`. I can try to pinpoint the exact place in pandas source code which causes this behavior.\n",
      "so what exactly is the problem, are these crashing for you?\n",
      "They are crashing in a custom setup, where I build everything from source with Clang and `-fsanitize=float-cast-overflow` enabled. I understand it's not the way people usually numpy/pandas (i.e. not what you get from `easy_install`). However, the bug report shows an actual undefined behavior, and the result of casting NaN to int can be different under different compilers/versions, and is known to be different on different architectures.\n",
      "well you shouldn't _ever_ use `easy_install`, that is really old, at the very least use `pip` or much better is using `conda`. \n\nIn any event would need an actual reproducible example on a stock install. As these tests work just fine at a glance. maybe I am missing something here.\n",
      "if you can provide a reproducible example that demonstrates a failure on stock numpy and pandas master, pls reopen.\n",
      "Your call - I totally understand it's hard to deal with the \"issues\" that are exposed in non-standard build configurations.\n\nHowever, undefined behavior is tricky and dangerous - and if we can reliably detect it, it's worth fixing it sooner rather than later, when compilers gets smarter and will break your code in unexpected ways.\n\nI'd argue that converting NaN to integer is never correct, and usually indicates an error in logic - even if the code currently generated in the stock builds happens to work for all the use cases - this is happening by accident. I've installed faulthandler to collect the stack trace of pandas triggering the issue:\n\n  File \"/pandas/pandas/core/common.py\", line 1362 in _possibly_downcast_to_dtype\n  File \"/pandas/pandas/core/groupby.py\", line 715 in _try_cast\n  File \"/pandas/pandas/core/groupby.py\", line 748 in _cython_agg_general\n  File \"/pandas/pandas/core/groupby.py\", line 996 in var\n  File \"/pandas/pandas/core/groupby.py\", line 979 in std\n  File \"/pandas/pandas/core/groupby.py\", line 2526 in aggregate\n  File \"/pandas/pandas/core/groupby.py\", line 2595 in _aggregate_multiple_funcs\n  File \"/pandas/pandas/core/groupby.py\", line 2529 in aggregate\n  File \"/pandas/pandas/core/groupby.py\", line 2595 in _aggregate_multiple_funcs\n  File \"/pandas/pandas/core/groupby.py\", line 2529 in aggregate\n  File \"/pandas/pandas/core/base.py\", line 475 in _agg_1dim\n  File \"/pandas/pandas/core/base.py\", line 492 in _agg\n  File \"/pandas/pandas/core/base.py\", line 501 in _aggregate\n  File \"/pandas/pandas/core/groupby.py\", line 3052 in aggregate\n  File \"/pandas/pandas/core/groupby.py\", line 3535 in aggregate\n\nSo, it seems that _possibly_downcast_to_dtype calls `np.allclose` on NaN, and NaN converted to integral data type. I'd argue that NaN should be special-cased here: as converting NaN to int gives unpredictable results.\n\nIf you don't want to deal with these issues now (while everything \"works\") - fine, let's just leave the bug. If you want to - I can provide help with building/testing code or patches against UB.\n",
      "@vonosmas you would have to show what this is being called. `np.allclose` in that function is NEVER called with anything but bool/int dtype.\n\nAs I said, need a reproducible example.\n",
      "`dtype` is integral, but result is not:\n\n``` python\n$ git diff pandas/core/common.py\n--- a/pandas/core/common.py\n+++ b/pandas/core/common.py\n@@ -1359,6 +1359,7 @@ def _possibly_downcast_to_dtype(result, dtype):\n             # do a test on the first element, if it fails then we are done\n             r = result.ravel()\n             arr = np.array([r[0]])\n+            print arr, dtype\n             if not np.allclose(arr, trans(arr).astype(dtype)):\n                 return result\n$ cat test.py\nimport numpy as np\nfrom pandas import DataFrame\n\nif __name__ == '__main__':\n  df = DataFrame({'A': ['foo', 'bar', 'foo', 'bar',\n                        'foo', 'bar', 'foo', 'foo'],\n                  'B': ['one', 'one', 'two', 'two',\n                        'two', 'two', 'one', 'two'],\n                  'C': np.random.randn(8) + 1.0,\n                  'D': np.arange(8)})\n\n  g = df.groupby(['A', 'B'])\n\n  result = g.agg({'C': {'ra': ['mean', 'std']},\n                  'D': {'rb': ['mean', 'std']}})\n$ python test.py\n[ 1.] int64\n[ nan] int64\n```\n",
      "This should patch it.\n\n```\ndiff --git a/pandas/core/common.py b/pandas/core/common.py\nindex 6165297..dee566a 100644\n--- a/pandas/core/common.py\n+++ b/pandas/core/common.py\n@@ -1359,7 +1359,8 @@ def _possibly_downcast_to_dtype(result, dtype):\n             # do a test on the first element, if it fails then we are done\n             r = result.ravel()\n             arr = np.array([r[0]])\n-            if not np.allclose(arr, trans(arr).astype(dtype)):\n+            if isnull(arr).any() or not np.allclose(arr,\n+                                                    trans(arr).astype(dtype)):\n                 return result\n\n             # a comparable, e.g. a Decimal may slip in here\n```\n",
      "Thank you, confirmed! Another failure (extracted from `pandas/tests/test_groupby.py:TestGroupBy.test_count_cross_type`):\n\n``` python\n$ cat test.py\nimport numpy as np\nfrom pandas import DataFrame\nimport faulthandler\n\nif __name__ == '__main__':\n  faulthandler.enable()\n  vals = np.hstack((np.random.randint(0, 5, (100, 2)), np.random.randint(\n      0, 2, (100, 2))))\n\n  df = DataFrame(vals, columns=['a', 'b', 'c', 'd'])\n  df[df == 2] = np.nan\n```\n\nwhich yields:\n\n```\n$ python test.py\nnumpy/core/src/multiarray/lowlevel_strided_loops.c.src:867:22: runtime error: value nan is outside the range of representable values of type 'long'\nFatal Python error: Aborted\n\nCurrent thread 0x00007f5eec71a7c0 (most recent call first):\n  File \"/pandas/pandas/core/internals.py\", line 4393 in _putmask_smart\n  File \"/pandas/pandas/core/internals.py\", line 809 in putmask\n  File \"/pandas/pandas/core/internals.py\", line 2801 in apply\n  File \"/pandas/pandas/core/internals.py\", line 2824 in putmask\n  File \"/pandas/pandas/pandas/core/generic.py\", line 4406 in where\n  File \"/pandas/pandas/pandas/core/frame.py\", line 2374 in _setitem_frame\n  File \"/pandas/pandas/pandas/core/frame.py\", line 2336 in __setitem__\n  File \"test.py\", line 11 in <module>\n```\n\nI don't know if assignment `df[df == 2] = np.nan` is in fact valid here, or the problem is in the test case. Same applies to the code in `test_cython_group_transform_algos`:\n\n``` python\nactual = np.zeros_like(data, dtype='int64')\nactual.fill(np.nan)\n```\n",
      "ok, will close it with the PR\n",
      "see #12360 should fix all of the issues you have reported..\n\nunfort I cannot really test with these, but at least added some comments\n",
      "Another example, where you might consider removing `fill(np.nan)` calls in `_cython_operation`:\n\n``` python\n$ git diff pandas/core/groupby.diff\n--- a/pandas/core/groupby.py\n+++ b/pandas/core/groupby.py\n@@ -1732,6 +1732,7 @@ class BaseGrouper(object):\n                 result, counts, values, labels, func, is_numeric)\n         elif kind == 'transform':\n             result = np.empty_like(values, dtype=out_dtype)\n+            print out_dtype\n             result.fill(np.nan)\n             # temporary storange for running-total type tranforms\n             accum = np.empty(out_shape, dtype=out_dtype)\n$ cat test.py\nimport numpy as np\nfrom pandas import DataFrame\nimport faulthandler\n\nif __name__ == '__main__':\n  faulthandler.enable()\n\n  df = DataFrame({'int': [1, 1, 1, 1, 2] * 200})\n  labels = np.random.randint(0, 50, size=1000).astype(float)\n  gb_target = dict(by=labels)\n  gb = df.groupby(**gb_target)\n  gb.transform('cumsum', *()).sort_index(axis=1)\n$ python test.py\ni8\nnumpy/core/src/multiarray/lowlevel_strided_loops.c.src:867:22: runtime error: value nan is outside the range of representable values of type 'long'\n\n  File \"/pandas/pandas/core/groupby.py\", line 1735 in _cython_operation\n  File \"/pandas/pandas/core/groupby.py\", line 1777 in transform\n  File \"/pandas/pandas/core/groupby.py\", line 727 in _cython_transform\n  File \"/pandas/pandas/core/groupby.py\", line 1283 in cumsum\n  File \"/pandas/pandas/core/groupby.py\", line 3390 in transform\n  File \"test.py\", line 12 in <module>\n```\n",
      "updated. lmk if any more!\n",
      "these are indeed edge cases where we shouldn't be doing something, but numpy 'allows'  as just casts it to the min int, generally not what we want.\n",
      "@vonosmas thanks for the reports, this is now merged. If you find any more you can post here.\n",
      "Thank you for fixing these! I've rebased and ran the entire test suite (sorry for not doing this earlier). Six test cases indicated errors, I've extracted the code patterns that trigger them (see below):\n\n``` python\n$ cat test.py\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame\nfrom pandas import Series\nfrom pandas import RangeIndex\nfrom pandas.tests.frame.common import TestData\nimport faulthandler\nimport sys\n\nif __name__ == '__main__':\n  faulthandler.enable()\n\n  # Test case 1 (test_groupby_cumprod)\n  # (pandas.tests.test_groupby.TestGroupBy)\n  # Overflow: casting floating-point 2^63 to int64.\n  if sys.argv[1] == '1':\n    df = DataFrame({'key': ['b'] * 100, 'value': 2})\n    actual = df.groupby('key')['value'].cumprod()\n\n  # Test case 2 (test_timedelta_ops_with_missing_values)\n  # (pandas.tseries.tests.test_timedeltas.TestTimedeltas)\n  # Casting NaN to long.\n  if sys.argv[1] == '2':\n    df1 = DataFrame(['00:00:01']).apply(pd.to_timedelta)\n    timedelta_NaT = pd.to_timedelta('NaT')\n    actual = df1 + timedelta_NaT\n\n  # Test case 3 (test_NaT_cast)\n  # (pandas.tseries.tests.test_timeseries.TestSeriesDatetime64)\n  # Casting NaN to long.\n  if sys.argv[1] == '3':\n    s = Series([np.nan]).astype('M8[ns]')\n\n  # Test case 4 (test_constructor_maskedarray)\n  # (pandas.tests.frame.test_constructors.TestDataFrameConstructors)\n  # Casting NaN to long.\n  if sys.argv[1] == '4':\n    mat = np.ma.masked_all((2, 3), dtype=float)\n    df = DataFrame(mat, columns=['A', 'B', 'C'],\n                   index=[1, 2], dtype=np.int64)\n\n  # Test case 5 (test_astype)\n  # (pandas.tests.frame.test_dtypes.TestDataFrameDataTypes)\n  # Overflow: casting large floating-point (>1e14) to int32.\n  if sys.argv[1] == '5':\n    mn = TestData().all_mixed._get_numeric_data().copy()\n    mn['big_float'] = np.array(123456789101112., dtype='float64')\n    casted = mn.astype('int32')\n\n  # Test case 6 (test_binops)\n  # (pandas.tests.indexes.test_range.TestRangeIndex)\n  # Casting \"inf\" to long.\n  if sys.argv[1] == '6':\n    a = RangeIndex(0, 20, 2)\n    b = RangeIndex(-10, 10, 2)\n    result = pow(a, b)\n```\n\nRunning them with float-cast-overflow detection yields:\n\n```\n$ python test.py 1\nnumpy/core/src/multiarray/lowlevel_strided_loops.c.src:867:22: runtime error: value 9.22337e+18 is outside the range of representable values of type 'long'\n  File \"/pandas/pandas/core/common.py\", line 1402 in _possibly_downcast_to_dtype\n  File \"/pandas/pandas/core/groupby.py\", line 716 in _try_cast\n  File \"/pandas/pandas/core/groupby.py\", line 731 in _cython_transform\n  File \"/pandas/pandas/core/groupby.py\", line 1275 in cumprod\n  File \"test.py\", line 18 in <module>\n\n$ python test.py 2\nnumpy/core/src/multiarray/arraytypes.c.src:1020:17: runtime error: value nan is outside the range of representable values of type 'long'\n  File \"/pandas/pandas/core/internals.py\", line 1510 in _try_coerce_result\n  File \"/pandas/pandas/core/internals.py\", line 1086 in get_result\n  File \"/pandas/pandas/core/internals.py\", line 1102 in eval\n  File \"/pandas/pandas/core/internals.py\", line 2814 in apply\n  File \"/pandas/pandas/core/internals.py\", line 2831 in eval\n  File \"/pandas/pandas/core/frame.py\", line 3533 in _combine_const\n  File \"/pandas/pandas/core/ops.py\", line 1089 in f\n  File \"test.py\", line 26 in <module>\n\n$ python test.py 3\nnumpy/core/src/multiarray/arraytypes.c.src:1020:17: runtime error: value nan is outside the range of representable values of type 'long'\n  File \"/pandas/pandas/core/common.py\", line 2631 in _astype_nansafe\n  File \"/pandas/pandas/core/internals.py\", line 465 in _astype\n  File \"/pandas/pandas/core/internals.py\", line 422 in astype\n  File \"/pandas/pandas/core/internals.py\", line 2814 in apply\n  File \"/pandas/pandas/core/internals.py\", line 2855 in astype\n  File \"/pandas/pandas/core/generic.py\", line 2887 in astype\n  File \"test.py\", line 32 in <module>\n\n$ python test.py 4\nnumpy/core/src/multiarray/lowlevel_strided_loops.c.src:867:22: runtime error: value nan is outside the range of representable values of type 'long'\n  File \"/pandas/pandas/core/frame.py\", line 417 in _init_ndarray\n  File \"/pandas/pandas/core/frame.py\", line 240 in __init__\n  File \"test.py\", line 40 in <module>\n\n$ python test.py 5\nnumpy/core/src/multiarray/lowlevel_strided_loops.c.src:867:22: runtime error: value 1.23457e+14 is outside the range of representable values of type 'int'\n  File \"/pandas/pandas/core/common.py\", line 2631 in _astype_nansafe\n  File \"/pandas/pandas/core/internals.py\", line 465 in _astype\n  File \"/pandas/pandas/core/internals.py\", line 422 in astype\n  File \"/pandas/pandas/core/internals.py\", line 2814 in apply\n  File \"/pandas/pandas/core/internals.py\", line 2855 in astype\n  File \"/pandas/pandas/core/generic.py\", line 2887 in astype\n  File \"test.py\", line 48 in <module>\n\n$ python test.py 6\nnumpy/core/src/umath/loops.c.src:918:30: runtime error: value inf is outside the range of representable values of type 'long'\n  File \"/pandas/pandas/indexes/base.py\", line 3152 in _evaluate_numeric_binop\n  File \"test.py\", line 56 in <module>\n```\n",
      "@vonosmas gr8 thanks. I'll take a look.\n",
      "let me reopen so won't forget\n",
      "Looks like this issue is fairly old with a nonstandard build set up. Going to close for now but happy to reopen if we can get report of current behavior"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "closed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "referenced",
      "commented",
      "reopened",
      "commented",
      "milestoned",
      "referenced",
      "cross-referenced",
      "commented",
      "referenced",
      "commented",
      "referenced",
      "commented",
      "commented",
      "referenced",
      "referenced",
      "closed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "reopened",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "unlabeled",
      "cross-referenced",
      "labeled"
    ],
    "changed_files": 4,
    "additions": 45,
    "deletions": 9,
    "changed_files_list": [
      "pandas/core/common.py",
      "pandas/core/groupby.py",
      "pandas/core/internals.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12309,
    "reporter": "toobaz",
    "created_at": "2016-02-12T14:42:19+00:00",
    "closed_at": "2016-06-14T13:53:54+00:00",
    "resolver": "toobaz",
    "resolved_in": "07761c519154a6ed8a9345512476152226499ba6",
    "resolver_commit_num": 13,
    "title": "Int64Index and  Float64Index (and... ?) do not propagate name of passed index",
    "body": "\n\n`Float64Index` as well\nideally move the tests to `Base` to test all index types.\nSee #12288.\n",
    "labels": [
      "Indexing",
      "Difficulty Novice",
      "Compat",
      "Effort Low"
    ],
    "comments": [
      "Related question: are the following behaviours, currently in master, desired?\n- explicitly given datatype is respected in Int64Index but not in Float64Index:\n  \n  ```\n  In [3]: Int64Index(np.arange(3, dtype=np.int32), dtype=np.int32).dtype\n  Out[3]: dtype('int32')\n  \n  In [4]: Float64Index(np.arange(3, dtype=np.float32), dtype=np.float32).dtype\n  Out[4]: dtype('float64')\n  ```\n- data dtype is not respected by default (even when it could be):\n  \n  ```\n  In [5]: Int64Index(np.arange(3, dtype=np.int32)).dtype\n  Out[5]: dtype('int64')\n  ```\n",
      "@toobaz no those are as expected, these by default will coerce to `float64/int64` always (or raise an error if its impossible). we technically allow coercing to a smaller dtype size (e.g. 32bit). \n\nI dont think its ever useful to have a `Int64Index` with a `dtype` of int32 (and break some things), IIRC I added it a while back for compat reasons. You can take this out and see what if anything breaks, and we can simply disallow (and cast as needed).\n",
      "Not sure I understood correctly: OK for coercing to 64 bits both floats and ints by default. But what about a different, explicit, coercion (i.e. `dtype=np.float32`)? I understand you're saying that supporting this may make sense for floats but not for ints, right? (the current behaviour is the opposite)\n",
      "I am saying it exists for both now, but doesn't make sense for either. IIRC I added this for ints, but don't really remember why it was needed. Ideally we should just drop this support as its uncessary and confusing, coercing things like `np.float32` -> `float64` and similarly for ints.\n",
      "OK, so the `dtype` argument would become useless (at least for those two index types), right? Should it be deprecated? Or do we prefer to leave it for compatibility?\n",
      "its for compatibility and assertions (IOW if you pass something and it cannot be coerced then you raise, this already happens now)\n",
      "So for instance in `Int64Index` with `dtype=np.int32`, I should check I can cast `data` to `np.int32`, just for the sake of asserting... and then store as `np.int64` anyway?! Seems a bit awkward.\n",
      "no I would simply cast to `int64`. I suspect some things might break currently if you change this. IIRC might be a platform issue, I don't remember.\n",
      "@toobaz Is this related, or a new issue?\n\n``` python\nIn [8]: s2=pd.Series(2., index=pd.PeriodIndex(start='1995-01-02', end='2016-06-30', freq='B', name='date'))\n\nIn [10]: s1=pd.Series(3., index=pd.PeriodIndex(start='1995-01-02', end='2016-06-03', freq='B', name='date'))\n\nIn [12]: s3=s1*s2\n\nIn [13]: s3\nOut[13]: \n1995-01-02    6.0\n1995-01-03    6.0\n1995-01-04    6.0\n1995-01-05    6.0\n...\n2016-06-28    NaN\n2016-06-29    NaN\n2016-06-30    NaN\nFreq: B, dtype: float64\n\nIn [14]: s3.index.name\n#none\n```\n",
      "@MaximilianR same bug (not that I have perfectly clear the code paths, but it behaves fine in my branch)\n",
      "@toobaz so just add this as a test in your PR\n",
      "@jreback I thought you didn't want that PR to grow too much :-)\n\nSeriously: I don't have clear the code path _leading to_ `PeriodIndex.__new__()` , but then the difference is clearly inside it, so we wouldn't be testing anything \"logically new\". I can add the check for `.name` in a test on the multiplication of series though...\n",
      "oh, right this is about Periods. If it works, just include the addtl test. Periods are a little bit in flux now.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "referenced"
    ],
    "changed_files": 15,
    "additions": 185,
    "deletions": 79,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/indexes/base.py",
      "pandas/indexes/category.py",
      "pandas/indexes/numeric.py",
      "pandas/tests/frame/test_block_internals.py",
      "pandas/tests/indexes/common.py",
      "pandas/tests/indexes/test_base.py",
      "pandas/tests/indexes/test_category.py",
      "pandas/tests/indexes/test_numeric.py",
      "pandas/tests/test_testing.py",
      "pandas/tseries/index.py",
      "pandas/tseries/period.py",
      "pandas/tseries/tdi.py",
      "pandas/tseries/tests/test_timedeltas.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12311,
    "reporter": "jreback",
    "created_at": "2016-02-12T15:52:34+00:00",
    "closed_at": "2016-02-13T02:35:59+00:00",
    "resolver": "jreback",
    "resolved_in": "d24fb34938889efbbe7b008b9c74de578ec604d5",
    "resolver_commit_num": 3882,
    "title": "DEPR: warnings from style deprecation only in PY3",
    "body": "-ci.org/pydata/pandas/jobs/108709963\n\n\n\ncc @TomAugspurger \n\nprob should bump the stacklevel as well\n",
    "labels": [
      "Deprecate"
    ],
    "comments": [
      "this fixes: https://github.com/jreback/pandas/commit/ad78ce21fe0d04952eb8919cd65566d14c96b779\n"
    ],
    "events": [
      "labeled",
      "mentioned",
      "subscribed",
      "milestoned",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 8,
    "deletions": 3,
    "changed_files_list": [
      "pandas/core/config.py",
      "pandas/core/config_init.py",
      "pandas/tests/test_categorical.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12322,
    "reporter": "jreback",
    "created_at": "2016-02-13T17:22:31+00:00",
    "closed_at": "2016-02-15T20:05:56+00:00",
    "resolver": "jreback",
    "resolved_in": "b3588766b24f17498d10e522352788abef80dc1b",
    "resolver_commit_num": 3896,
    "title": "COMPAT: issues w.r.t. deprecating float indexers",
    "body": "from #12246 \n\n@jorisvandenbossche \n\nSome after merge feedback:\n\nI took a look and I also like the logic now as how you explained it in your last post (for label based indexing: if the label evaluates as equal, then it is interpreted as the existing label)\n\nA few things:\n- The whatsnew needs to be updated (as it still says it raises now for all cases, as you started the PR)\n\n\n- (minor) When using `iloc` with a float (so the case it still should raise an error), the error message is not fully correct:\n  \n  \n  \n  This is not 'label' indexing, but 'integer' or 'positional' indexing\n- The behaviour of using a float indexer with `ix` on a non-numerical index has changed:\n  \n  \n  \n  and with 0.16.2\n  \n  \n  \n  The change is logical, as before the float was interpreted as a positional indexer (and for this the warning was raised). But now a float cannot be a positional indexer anymore, so it is interpreted as a new label. \n  To be clear, I think this change is OK, but just wanted to point out a case where this change will not raise an error but alter your results (worth mentioning in the whatsnew docs?)\n",
    "labels": [
      "Compat"
    ],
    "comments": [
      "#12330 should take care of all of these points (I doced what `.ix` does a bit more)\n\n@jorisvandenbossche \n"
    ],
    "events": [
      "labeled",
      "mentioned",
      "subscribed",
      "milestoned",
      "referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 69,
    "deletions": 13,
    "changed_files_list": [
      "doc/source/advanced.rst",
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/indexes/base.py",
      "pandas/tests/test_indexing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12332,
    "reporter": "jcrist",
    "created_at": "2016-02-15T19:01:58+00:00",
    "closed_at": "2016-02-15T20:28:58+00:00",
    "resolver": "jreback",
    "resolved_in": "cac5f8b33fee92395cae4a9b4b469e2130322281",
    "resolver_commit_num": 3897,
    "title": "Resample with OHLC sometimes results in series on pandas release 0.18.0.rc1",
    "body": "\n",
    "labels": [
      "Bug",
      "Resample"
    ],
    "comments": [
      "@jcrist fixed by #12329 \n",
      "this was incorrectly taking an `.asfreq()` path\n",
      "Tested, that PR, fixes all issues for dask test suite. Thanks!\n"
    ],
    "events": [
      "renamed",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "commented"
    ],
    "changed_files": 5,
    "additions": 55,
    "deletions": 23,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12333,
    "reporter": "jreback",
    "created_at": "2016-02-15T21:02:04+00:00",
    "closed_at": "2016-03-08T23:38:19+00:00",
    "resolver": "jreback",
    "resolved_in": "5f7e290b70792b663aca4f5baafd573e48919f50",
    "resolver_commit_num": 3927,
    "title": "API: behaviour of label indexing with floats on integer index",
    "body": "from #12246 \n\n@jorisvandenbossche this looks odd\n\n\n",
    "labels": [
      "Indexing",
      "Compat",
      "API Design"
    ],
    "comments": [
      "I think there is clear agreement about the following:\n- when doing integer positional indexing, using floats are not allowed.  \n  Eg:\n  \n  ```\n  In [22]: s = pd.Series([1,2,3])\n  \n  In [23]: s.iloc[0.0]\n  TypeError: cannot do positional indexing on <class 'pandas.indexes.range.RangeIn\n  dex'> with these indexers [0.0] of <type 'float'>\n  \n  In [24]: s2 = pd.Series([1,2,3], index=['a', 'b', 'c'])\n  \n  In [25]: s2[0.0]\n  TypeError: cannot do label indexing on <class 'pandas.indexes.base.Index'> with\n  these indexers [0.0] of <type 'float'>\n  ```\n  \n  This was deprecated and now errors after Jeff's PR.\n- when doing label indexing on a non-numerical index (this is logical, but just to be complete)\n\nThere is less agreement on when floats should be allowed when doing **label indexing on a numerical but not-float index** (so integer index or RangeIndex) in case the float can be interpreted as an integer:\n- Previously, these were interpreted as integers, but raised a deprecation warning\n- In current master, when setting, the float will be coerced to an integer when possible:\n  \n  ```\n  In [38]: s = pd.Series([1,2,3])\n  \n  In [39]: s[1.0] = 10\n  \n  In [40]: s\n  Out[40]:\n  0     1\n  1    10\n  2     3\n  dtype: int64\n  ```\n- But (also in current master), when getting a value, this still raises, also for coercables floats:\n  \n  ```\n  In [42]: s[1.0]\n  TypeError: cannot do label indexing on <class 'pandas.indexes.range.RangeIndex'>\n  with these indexers [1.0] of <type 'float'>\n  ```\n  \n  giving the strange contradiction between `s[1.0] = 10` and `s[1.0]` raising\n\nI think for setting, the _rule_ is if the label you want to set evaluates as equal to one of the labels in the index, then it works without changing the index (`1 == 1.0` and `1 != '1'`). If it does not evaluates to equal, then it is seen as a new label and the index is expanded.\n\nI would argue that the same rule should apply for _getting_ as for setting. \nOn the other hand, setting is already more flexible (as for example `s[1.5] = 10` is allowed and will upcast the index, while getting with this `s[1.5]` obviously raises).\n\ncc @shoyer @TomAugspurger\n",
      "The issue I have with @jorisvandenbossche last\n\n> I think for setting, the rule is if the label you want to set evaluates as equal to one of the labels in the index, then it works without changing the index (1 == 1.0 and 1 != '1'). If it does not evaluates to equal, then it is seen as a new label and the index is expanded.\n\nas this would _only_ apply to `[], .ix` and NOT `.loc` as `.loc` is strict about labels while the others can do positional AND/OR labels.\n\nBut since this in an integer index we are talking about we have the odd fallback where it is treated like a label (`.ix`), so this would be inconsistent then? (of course I may be misrepresenting things here as fallback is simply confusing).\n",
      "> as this would only apply to [], .ix and NOT .loc as .loc is strict about labels while the others can do positional AND/OR labels.\n\nWe could say that a python dict is also strict about labels, but `{1: 'one'}[1.0]` goes through fine.\n\nI think I'm in agreement with Joris; haven't though how it affects fallback indexing entirely yet.\n",
      "AFAIK, integer indexes don't have any fallback to positional, and the indexer is _always_ interpreted as label. So I don't see how this could mess up with the fallback indexing.\n\n> .loc is strict about labels while the others can do positional AND/OR labels.\n\nI don't see why that is a reason to make `.loc` have different behaviour, as we are speaking here _only_ about label indexing, not positional.\n",
      "You also have this discrepancy:\n\n```\nIn [71]: 1.0 in s.index\nOut[71]: True\n\nIn [72]: s[1.0]\nTypeError: cannot do label indexing on <class 'pandas.indexes.numeric.Int64Index\n'> with these indexers [1.0] of <type 'float'>\n```\n\n(to be honest, it would only be a full discrepancy if this gave a \"KeyError 1.0 not found in index\")\n",
      "ok, so @jorisvandenbossche what do you think _should_ work then. give me an example for setting/getting with the various indexers.\n",
      "I agree with @jorisvandenbossche, as I stated in the last PR:\n- `__setitem__` and `__getitem__` should be entirely consistent\n- We should use equality (like the Python dict) to determine whether or not a key matches\n- If inserting a key that doesn't exist, then the index type is upcast if necessary \n",
      "so @shoyer then essentially you want to have full support back for float indexers? (excluding `.iloc`)\n\nthats what your conditions imply.\n",
      "@jreback Yes, I guess so. If pandas were stricter about not upcasting types (or even better, if we did not support reindexing with `.loc` at all), then it could make sense to make the distinction. But as is, I think we should be OK with float indexers. Obviously float indexers that don't match an integer will still result in KeyError, though\n",
      "I suppose this means we are reverting this as well.\n\n```\nIn [1]: s = Series(range(3))\n\nIn [2]: s\nOut[2]: \n0    0\n1    1\n2    2\ndtype: int64\n\nIn [3]: s[1.0:2]\nTypeError: cannot do slice start value indexing on <class 'pandas.indexes.range.RangeIndex'> with these indexers [1.0] of <type 'float'>\n```\n\nand have it return equiv of `s[1:2]` (and allow setting).\n\nNote that this will only apply to integer-like indices as string index will still raise `TypeError`.\n",
      "> I suppose this means we are reverting this as well.\n> \n> In [3]: s[1.0:2]\n> TypeError: cannot do slice start value indexing on <class 'pandas.indexes.range.RangeIndex'>\n\nNo, that should still raise, as this is _positional_ indexing (alas ..):\n\n```\nIn [84]: s = pd.Series([1,2,3,4], index=range(1,5))\n\nIn [85]: s\nOut[85]:\n1    1\n2    2\n3    3\n4    4\ndtype: int64\n\nIn [86]: s[1:3]\nOut[86]:\n2    2\n3    3\ndtype: int64\n\nIn [87]: s[1.0:3]\nTypeError: cannot do slice start value indexing on <class 'pandas.indexes.numeri\nc.Int64Index'> with these indexers [1.0] of <type 'float'>\n```\n",
      "But you are correct for the case of slicing in `.ix` (and `.loc`). But strangely this does not raise on master for me at the moment:\n\n```\nIn [88]: s.ix[1.0:3]\nOut[88]:\n1    1\n2    2\n3    3\ndtype: int64\n\nIn [89]: pd.__version__\nOut[89]: '0.18.0rc1+10.gcac5f8b'\n```\n",
      "that last has always been allowed.\n",
      "@jreback I don't really understand the confusion between us. My main issue is (here and in https://github.com/pydata/pandas/pull/12370/files#r53281590): I don't see why we should have differences in behaviour between scalar indexing and slicing (with regard to labels being found or not). It should just be:\n- positional: floats are disallowed, only integers\n- label-based: we use equality to see if a label matches\n\nThis last rule is then applicable to `loc`/`ix`/`[]` for scalar indexing (as these are all label based), and to `loc`/`ix` for slicing (as `[]` is positional and not label-based in the case of slicing, and so should follow the positional rule).\n\nWhy would this last one be different for `loc`? \nIf you find a matching label with `s.loc[1.0]`, why should this exact same label not match in `s.loc[1.0:2]` ?\n",
      "the confusion is we had agreement on disallowing floats in indexers except for float indices\n\nwhich is s very simple rule\n\nnow we are allowing them all over the place essentially going back to where we were \n\ncreating another indexing mess where nothing is simple\n",
      "> now we are allowing them all over the place essentially going back to where we were \n> creating another indexing mess where nothing is simple\n\nAnd I am sorry for that, as you are doing the hard work in dealing with the complex indexing code, and then changing behaviour every time is not making it easier\n\n> the confusion is we had agreement on disallowing floats in indexers except for float indices \n> which is s very simple rule\n\nOK, I agree that that is indeed a clear and simple rule. However, I personally think the rule should be how @shoyer formulated it: _\"We should use equality to determine whether or not a key matches\"_. \nI understand that you are maybe in favor of the \"disallow all floats except for FloatIndex\" rule (is that the case?). \nBut if we accept the equality-rule (which I thought you did, as in the merged PR #12246, you applied it for the setitem case), I think this rule should be entirely consistent for both setitem and getitem, for both scalar indexing as slicing, for both `ix` and `loc`.\n\nIt's from that point of vue (accepting the equality-rule), that I don't see why positional based slicing in `loc` should be different.\n",
      "I want to echo @jorisvandenbossche thanks for all your work on this. I'm still too scared to open up `core/indexing.py` :smile: \n\nThe rules of\n- equality for key / label indexing (like python's dict)\n- strict (integer) for location indexing (like where NumPy's going)\n- symmetry between getitem and setitem\n\nare the best. \n",
      "Hmm, the `loc` slicing case is a bit more complicated than I thought ... I didn't think about the case where the float does not match one of the labels, because that is currently allowed in slicing\n\n(Using 0.17.1 here for the examples) Assuming we follow the rules as @shoyer summarised them above (https://github.com/pydata/pandas/issues/12333#issuecomment-184770215), accessing one value with a float that matches should work:\n\n```\nIn [1]: s = Series(range(3))\n\nIn [2]: s\nOut[2]:\n0    0\n1    1\n2    2\ndtype: int64\n\nIn [4]: s.loc[1.0]\nOut[4]: 1\n```\n\nThen I would say, slicing with matching items should also work:\n\n```\nIn [5]: s.loc[1.0:2]\nOut[5]:\n1    1\n2    2\ndtype: int64\n```\n\nBut what to do with slicing with a float that does _not_ match one of the labels in the index? When slicing with integers, the slice values don't need to be contained in the index, eg:\n\n```\nIn [6]: s.loc[1:10]\nOut[6]:\n1    1\n2    2\ndtype: int64\n```\n\nSo should this also work with floats? And currently, this also works with floats with a fractional part, eg:\n\n```\nIn [7]: s.loc[0.5:2.5]\nOut[7]:\n1    1\n2    2\ndtype: int64\n```\n\nSo in the **slicing** case, there is no equality required (only an ability to do searchsorted?), in the case of a monotonic index. So the defined rules do not really cover this case ..\n",
      "Slicing with floats in on an integer index is actually (now I remember) deliberately implemented, and even extended to work with decreasing monotonic indexes by @shoyer (https://github.com/pydata/pandas/pull/8680) even after the initial deprecation for float indexers was put in place.\n\nThis also did not raise a deprecation warning:\n\n```\nIn [1]: pd.__version__\nOut[1]: u'0.17.1'\n\nIn [2]: s = Series(range(3))\n\nIn [3]: s.loc[1.0:2.5]\nOut[3]:\n1    1\n2    2\ndtype: int64\n```\n\nwhile the above now raises in master. So @jreback even if we decide that we don't want this behaviour any longer, we should at least first deprecate it IMO.\n",
      "I don't mind fixing it to be the right API, but I don't agree with @shoyer at all here. The indexing should strictly depend on the index type. I don't see why a float should be coereced to integer if they happen to be equal.\n\nI heare preaching all the time about how data shouldn't determine indexing. Well isn't that exactly what you are doing here?\n\nWe should not allow float indexer/slicers at all except with `Float64Index`, full-stop. This is a natural and simple rule. Everything else leads to confusion, special cases and whatnot. \n\nA python dictionary and numpy are just too simple for this case. They don't have to simultaneously deal with different TYPES of indexers and they only care about strict label matching (dicts) or positional (numpy).\n\nTo be honest I don't care what the rules are for `.ix`, whatever goes is fine, its already pretty screwed up. However for `.loc` we have some very strict rules. Every case above you mentioned is bending them some more, pretty soon this just be another `.ix`.\n\nPandas must deal with both.\n\nI think this is getting pretty lost in the fact that a user should have really well laid out rules for what is expected of indexing. They by definition HAVE to be simple. They are already way way too complex and special cased.\n",
      "This discussion reminds me of the one we had back in #8613. \n\nWe ended resolving this by _not_ making a distinction between floats and integers for label based indexing: #9566\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "mentioned",
      "subscribed",
      "milestoned",
      "labeled",
      "renamed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 18,
    "additions": 1260,
    "deletions": 1120,
    "changed_files_list": [
      "doc/source/advanced.rst",
      "doc/source/indexing.rst",
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/frame.py",
      "pandas/core/indexing.py",
      "pandas/indexes/base.py",
      "pandas/indexes/multi.py",
      "pandas/indexes/numeric.py",
      "pandas/indexes/range.py",
      "pandas/tests/frame/test_indexing.py",
      "pandas/tests/indexing/__init__.py",
      "pandas/tests/indexing/test_categorical.py",
      "pandas/tests/indexing/test_floats.py",
      "pandas/tests/indexing/test_indexing.py",
      "pandas/tseries/base.py",
      "pandas/tseries/index.py",
      "pandas/tseries/period.py",
      "pandas/tseries/tdi.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12334,
    "reporter": "jreback",
    "created_at": "2016-02-15T21:52:43+00:00",
    "closed_at": "2016-02-17T01:12:08+00:00",
    "resolver": "jreback",
    "resolved_in": "286d304cdb15a349a416afd1b809c0e952a6d3c6",
    "resolver_commit_num": 3900,
    "title": "API: consistency is return results of aggregation on SeriesGroupby",
    "body": "xref 12329\n\nFor consistency, the proposal is to make `[4]` have a multi-level index `('C','sum'),('C','std')` as its not a rename (like `[5]`). This previously raised in 0.17.1\n\n\n",
    "labels": [
      "Groupby",
      "API Design"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 32,
    "deletions": 12,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12336,
    "reporter": "toobaz",
    "created_at": "2016-02-15T23:08:46+00:00",
    "closed_at": "2016-02-18T13:15:08+00:00",
    "resolver": "rinoc",
    "resolved_in": "672fb146980c104f7b01b835f3839dac9d8cee1e",
    "resolver_commit_num": 1,
    "title": "Initializing category with single value raises AttributeError",
    "body": "\n\n(Not a particularly brilliant use of categories, but they can then be concatenated with others.)\n",
    "labels": [
      "Bug",
      "Categorical",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "yeah looks buggy.\n",
      "obviously the expected result\n\n```\nIn [2]: pd.Series(0, index=range(3)).astype(\"category\")\nOut[2]: \n0    0\n1    0\n2    0\ndtype: category\nCategories (1, int64): [0]\n```\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 11,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/series.py",
      "pandas/tests/series/test_constructors.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12338,
    "reporter": "dhomeier",
    "created_at": "2016-02-16T00:26:27+00:00",
    "closed_at": "2016-02-16T01:47:48+00:00",
    "resolver": "jreback",
    "resolved_in": "0813a3359a7a946ee5f9246a5d49a933ff130f8e",
    "resolver_commit_num": 3898,
    "title": "TST: errors in test_constructor_compound_dtypes (pandas.tests.frame.test_block_internals.TestDataFrameBlockInternals)",
    "body": "Getting these errors in the 0.18.0rc1 test suite on Mac OS X 10.10 (numpy 0.11.0b3):\n\nwith Python 2.7.11:\n\n\n\nwith Python 3.4.4 and 3.5.1:\n\n\n",
    "labels": [
      "Testing",
      "Can't Repro"
    ],
    "comments": [
      "I cannot repro this (with the exact versions  you are using)\n\nI have `PYTHONIOENCODING=UTF-8` set. (which is typical for a PY3 env)\n\nactually this var doesn't matter\n",
      "I have seen this happen on windows, IOW the dateparsing routines fail and raise a different error than we are expecting.\n\ncan you show versions of pytz, dateutil\n\n```\n(numpy_dev3)bash-3.2$ conda list pytz\n# packages in environment at /Users/jreback/miniconda/envs/numpy_dev3:\n#\npytz                      2015.7                   py35_0  \n(numpy_dev3)bash-3.2$ conda list dateutil       \n# packages in environment at /Users/jreback/miniconda/envs/numpy_dev3:\n#\npython-dateutil           2.4.2                    py35_0  \n```\n",
      "A bit behind, yes:\n\n```\n> fink list pytz dateutil\nInformation about 9337 packages read in 4 seconds.\n i   dateutil-py27                    2.2-3                    Extended date and time calculations\n i   dateutil-py34                    2.2-3                    Extended date and time calculations\n i   dateutil-py35                    2.2-3                    Extended date and time calculations\n i   pytz-py27                        2014.2-1                 World timezone definitions database\n i   pytz-py34                        2014.2-1                 World timezone definitions database\n i   pytz-py35                        2014.2-1                 World timezone definitions database\n```\n",
      "ok, that is it. `dateutil==2.2` (via pip) reproduces.\n",
      "Wish I had a better test, but this fixes. We are not testing this. As have to modify the ci to explicity pip install `dateutil=2.2` (as its not on conda, as IIRC it was a completely buggy/bogus version :<). Its not even tagged in the repo, really weird\n\n```\ndiff --git a/pandas/core/common.py b/pandas/core/common.py\nindex 70c02c5..0a6a309 100644\n--- a/pandas/core/common.py\n+++ b/pandas/core/common.py\n@@ -1681,7 +1681,7 @@ def _possibly_cast_to_datetime(value, dtype, errors='raise'):\n                                 errors=errors).tz_localize(dtype.tz)\n                         elif is_timedelta64:\n                             value = to_timedelta(value, errors=errors)._values\n-                    except (AttributeError, ValueError):\n+                    except (AttributeError, ValueError, TypeError):\n                         pass\n\n         # coerce datetimelike to object\n```\n",
      "Tested the patch here as well, and also the original source against `dateutil-2.4.2`, both passing.\n",
      "@dhomeier gr8 thanks!\n"
    ],
    "events": [
      "commented",
      "commented",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 2,
    "deletions": 4,
    "changed_files_list": [
      "ci/requirements-3.4.build",
      "ci/requirements-3.4.pip",
      "ci/requirements-3.4.run",
      "pandas/core/common.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12344,
    "reporter": "hantusk",
    "created_at": "2016-02-16T10:36:18+00:00",
    "closed_at": "2016-03-06T15:28:42+00:00",
    "resolver": "gfyoung",
    "resolved_in": "a174898dc3d2dff6c71cb43fb105793113a0c3ff",
    "resolver_commit_num": 5,
    "title": "DataFrame.fillna corrupts columns with duplicated names",
    "body": "\n",
    "labels": [
      "Bug",
      "Indexing",
      "Missing-data",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "hmm, that appears to be the case. pull-requests to investigate are welcome. Duplicate column support _should_ work, though not as complete as more standard unique support.\n",
      "@jreback : Where would the potentially offending code be located?\n",
      "core/generic \n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 18,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/indexing.py",
      "pandas/tests/frame/test_nonunique_indexes.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12352,
    "reporter": "wavexx",
    "created_at": "2016-02-16T16:34:52+00:00",
    "closed_at": "2016-02-17T13:22:18+00:00",
    "resolver": "jreback",
    "resolved_in": "f1aad46c451baa71c902068d4969467f352ba232",
    "resolver_commit_num": 3903,
    "title": "nunique + TimeGrouper error",
    "body": "This used to work in the past:\n\n\n\nbut now I get the obscure:\n\n\n",
    "labels": [
      "Bug",
      "Resample",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "The logically equivalent:\n\n```\ntmp.groupby(pd.TimeGrouper('D')).ID.apply(lambda x: x.nunique())\n```\n\nworks as intended.\n",
      "hmm, does look buggy.\n",
      "this last worked in 0.16.2, and failed in 0.17.0 (and continues in 0.18.0),\n",
      "Thanks for investigating the exact breaking point. I'm currently revisiting some code that I wrote for python2.7 with pandas 0.16.\\* and now I'm porting to python3 and 0.17.1 (currently Debian unstable).\n",
      "yeah there were some fixes related to this, but this one didn't take.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "milestoned",
      "demilestoned",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 39,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/groupby.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12358,
    "reporter": "wavexx",
    "created_at": "2016-02-16T23:44:34+00:00",
    "closed_at": "2016-02-17T14:29:42+00:00",
    "resolver": "jreback",
    "resolved_in": "69baf4c30a8b404f8626728e89a4c5fd5b606a44",
    "resolver_commit_num": 3904,
    "title": "set_index(DatetimeIndex) unexpectedly shifts tz-aware datetime",
    "body": "This is another issue I've found in code that used to work:\n\n\n\nwrites:\n\n\n\nIt's unclear to me why the time is shifted. If we take a pd.DatetimeIndex which is not directly contained in the df, it works as it should:\n\n\n\n\n",
    "labels": [
      "Timezones",
      "Bug"
    ],
    "comments": [
      "a couple of things:\n\n1) your syntax is incorrect (yes this did work, but it is completely misleading), as its not clear that you actually mean to localize\n\nso construct the index like this. IOW. you have to say, hey this a local UTC time, THEN convert it.\n\n```\ntm = pd.DatetimeIndex(pd.to_datetime([\"2014-01-01 10:10:10\"])).tz_localize('UTC').tz_convert('Europe/Rome')\n\nIn [4]: tm\nOut[4]: DatetimeIndex(['2014-01-01 11:10:10+01:00'], dtype='datetime64[ns, Europe/Rome]', freq=None)\n```\n\n2) `df.set_index(df.tm, inplace=True)`\n\nThis is a nonsensical operation, what do you think this should do?\n\nyou probably mean\n`df.index = df.tm`\n\nYou are effectively setting the index with a 'key' from they array; this technically works as you only have 1 element (otherwise it would raise). but as I said doesn't make any sense.\n\n```\nIn [30]: df.set_index?\nSignature: df.set_index(keys, drop=True, append=False, inplace=False, verify_integrity=False)\nDocstring:\nSet the DataFrame index (row labels) using one or more existing\ncolumns. By default yields a new object.\n\nParameters\n----------\nkeys : column label or list of column labels / arrays\ndrop : boolean, default True\n    Delete columns to be used as the new index\nappend : boolean, default False\n    Whether to append columns to existing index\ninplace : boolean, default False\n    Modify the DataFrame in place (do not create a new object)\nverify_integrity : boolean, default False\n    Check the new index for duplicates. Otherwise defer the check until\n    necessary. Setting to False will improve the performance of this\n    method\n\nExamples\n--------\n>>> indexed_df = df.set_index(['A', 'B'])\n>>> indexed_df2 = df.set_index(['A', [0, 1, 2, 0, 1, 2]])\n>>> indexed_df3 = df.set_index([[0, 1, 2, 0, 1, 2]])\n\nReturns\n-------\ndataframe : DataFrame\nFile:      ~/pandas/pandas/core/frame.py\nType:      instancemethod\n```\n",
      "It seems clear enough to me that if I know the tz of the series, there's no point to \"localize\" it later.\nIn fact, I always start from UTC. pd.to_datetime has an utc keyword which I would have expected to make the DatetimeIndex UTC _and_ tz-aware, which would be what I need 99% of the time, but it doesn't (what's the point of this argument is still unclear to me!?).\n\nAs for setting the index, yes, it's dodgy. It's a reduced test-case from some convoluted code.\nHowever, why does it shift time? I see no reason why in this explicit case it should.\n",
      "passing it rather than explicity localizing leads to a lot of ambiguity, what should I doing here?\n\n```\nIn [1]: DatetimeIndex(['2014-01-01 11:10:10+01:00'],tz='UTC')\nOut[1]: DatetimeIndex(['2014-01-01 10:10:10+00:00'], dtype='datetime64[ns, UTC]', freq=None)\n```\n\nas to your second point, it is converted to a numpy array, thus the tz is lost. the first arg only accepts a list or np.array NOT a Series, excactly for this reason.\n",
      "tz_localize() converts the timezone, I explicitly don't want it to do any conversion as my dates do not contain any.\n\nIn fact, if I could bug you one more time about this, what's the more efficient way to start from a unix timestamp (obviously in UTC) and get to a localized series?\n",
      "NO `tz_localize`, SETS the timezone, `tz_convert` converts it!\n\nHere's some examples.\n\nYou CAN use the `utc=True` flag on `pd.to_datetime`; this WILL return it localized to UTC. (just don't do this directly with `DatetimeIndex`. All will be well if you use `pd.to_datetime` for all conversion needs, then operate on the resulting objects\n\n```\nIn [2]: v = Timestamp('20130101').value\n\nIn [3]: v\nOut[3]: 1356998400000000000\n\nIn [4]: pd.to_datetime(v,unit='ns')\nOut[4]: Timestamp('2013-01-01 00:00:00')\n\nIn [5]: pd.to_datetime(v/1000000,unit='ms')\nOut[5]: Timestamp('2013-01-01 00:00:00')\n\nIn [6]: pd.to_datetime(v/1000000,unit='ms').tz_localize('UTC')\nOut[6]: Timestamp('2013-01-01 00:00:00+0000', tz='UTC')\n\nIn [7]: pd.to_datetime(v/1000000,unit='ms',utc=True)\nOut[7]: Timestamp('2013-01-01 00:00:00+0000', tz='UTC')\n\nIn [8]: pd.to_datetime(v/1000000,unit='ms').tz_localize('UTC')\nOut[8]: Timestamp('2013-01-01 00:00:00+0000', tz='UTC')\n\nIn [9]: Series(pd.to_datetime(v/1000000,unit='ms').tz_localize('UTC'))\nOut[9]: \n0   2013-01-01 00:00:00+00:00\ndtype: datetime64[ns, UTC]\n\nIn [10]: Series(pd.to_datetime(v/1000000,unit='ms')).dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\nOut[10]: \n0   2012-12-31 19:00:00-05:00\ndtype: datetime64[ns, US/Eastern]\n```\n",
      "On Wed, Feb 17 2016, Jeff Reback notifications@github.com wrote:\n\n> You CAN use the utc=True flag on pd.to_datetime; this WILL return it localized\n> to UTC. (just don't do this directly with DatetimeIndex. All will be well if\n> you use pd.to_datetime for all conversion needs, then operate on the resulting\n> objects\n\nOk, this made things a little bit clearer regarding the tz.\nPoint understood.\n\nI'm still not super-happy about the set_index behavior. I've given it\nsome extra-though, but I don't see where and why the tz would be lost.\n\nWhere exactly this conversion happens?\n\nimport pandas as pd\ntm = pd.DatetimeIndex(pd.to_datetime([\"2014-01-01 10:10:10\"]), tz='UTC').tz_convert('Europe/Rome')\ndf = pd.DataFrame({'tm': tm})\nprint(df.set_index(tm).index[0].hour)\nprint(pd.DatetimeIndex(pd.Series(df.tm))[0].hour)\nprint(df.set_index(df.tm).index[0].hour)\n\n=> 11 11 10\n\nIgnore the fact that I could assign to index for a moment.\n\nI'm supplying a type to set_index that should be equivalent to the\nfirst or second print statement.\n",
      "looks like a bug after all!\n\nfixed by #12365 \n",
      "On Wed, Feb 17 2016, Jeff Reback notifications@github.com wrote:\n\n> looks like a bug after all!\n> \n> fixed by #12365\n\nSorry for being pedantic!\n",
      "no, persistence is good! you got me to actually step thru and see what was happening. always better to test.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "commented",
      "closed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "referenced",
      "labeled",
      "unlabeled",
      "milestoned",
      "cross-referenced",
      "reopened",
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 13,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/frame.py",
      "pandas/tests/frame/test_alter_axes.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12363,
    "reporter": "jreback",
    "created_at": "2016-02-17T02:48:12+00:00",
    "closed_at": "2016-04-26T15:03:02+00:00",
    "resolver": "jreback",
    "resolved_in": "699424027fb657192541bcd0c3d9f9b7d26f2300",
    "resolver_commit_num": 3993,
    "title": "BUG: inconsistent name for returned Series in groupby",
    "body": "I would think `[7]` and `[8]` would be identical. Note this is true for all ops I have looked at, so it is a general issue.\n\n\n",
    "labels": [
      "Bug",
      "Groupby",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "xref #8093 \n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 12,
    "additions": 719,
    "deletions": 132,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/base.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/missing.py",
      "pandas/core/window.py",
      "pandas/indexes/base.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_window.py",
      "pandas/tools/merge.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12373,
    "reporter": "jennolsen84",
    "created_at": "2016-02-17T23:00:05+00:00",
    "closed_at": "2016-03-06T00:45:44+00:00",
    "resolver": "BranYang",
    "resolved_in": "3d70be7fabc51b6dd0b6dca4519b76d5e469966f",
    "resolver_commit_num": 4,
    "title": "BUG: rolling functions raise ValueError on float32 data",
    "body": "pd version: v0.18.0rc1\n\nminimal example:\n\n\n\nstack trace:\n\n\n",
    "labels": [
      "Bug",
      "Stats",
      "Dtypes"
    ],
    "comments": [
      "I think I forgot the casting (these should really have dtype specific functions, e.g. float32/float64), but that's your other issue.\n\n```\n> /Users/jreback/pandas/pandas/core/window.py(460)func()\n    458                 def func(arg, window, min_periods=None):\n    459                     minp = check_minp(min_periods, window)\n--> 460                     return cfunc(arg, window, minp, **kwargs)\n    461 \n    462             # calculation function\n\nipdb> p arg\narray([ 0.,  1.,  2.,  3.,  4.], dtype=float32)\nipdb> p cfunc\n<built-in function roll_max>\nipdb> p cfunc(com._ensure_float64(arg), window, minp, **kwargs)\narray([ nan,  nan,   2.,   3.,   4.])\n```\n\nwant to do a PR to fix?\n"
    ],
    "events": [
      "renamed",
      "renamed",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 203,
    "deletions": 7,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/window.py",
      "pandas/tests/test_window.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12386,
    "reporter": "jreback",
    "created_at": "2016-02-18T21:54:16+00:00",
    "closed_at": "2016-03-02T12:46:00+00:00",
    "resolver": "paul-reiners",
    "resolved_in": "eba78032e927d2680850c88190f3bafe18cac6ba",
    "resolver_commit_num": 0,
    "title": "CLN: remove pandas.util.testing.choice",
    "body": "xref #12384 \n\nso I think this was for numpy < 1.7 compat.\n\nwe should remove `pandas.util.testing.choice`\n#L173\n\nthere might be some latent doc references\n",
    "labels": [
      "Clean",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "cc @carroux \n",
      "```\npandas/tests/frame/test_query_eval.py\n99:        a = tm.choice(['red', 'green'], size=10)\n100:        b = tm.choice(['eggs', 'ham'], size=10)\n152:        a = tm.choice(['red', 'green'], size=10)\n153:        b = tm.choice(['eggs', 'ham'], size=10)\n246:        a = tm.choice(['red', 'green'], size=10)\n978:        a = Series(tm.choice(list('abcde'), 20))\n\npandas/tests/test_graphics_others.py\n638:            gender = tm.choice(['male', 'female'], size=n)\n712:            gender_int = tm.choice([0, 1], size=n)\n\npandas/tests/test_graphics.py\n63:            gender = tm.choice(['Male', 'Female'], size=n)\n64:            classroom = tm.choice(['A', 'B', 'C'], size=n)\n3864:            gender = tm.choice(['male', 'female'], size=n)\n\npandas/tools/tests/test_merge.py\n239:            df = DataFrame({'a': tm.choice(['m', 'f'], size=3),\n241:            df2 = DataFrame({'a': tm.choice(['m', 'f'], size=10),\n248:            df = DataFrame({'a': tm.choice(['m', 'f'], size=3),\n251:            df2 = DataFrame({'a': tm.choice(['m', 'f'], size=10),\n257:            df = DataFrame({'a': tm.choice(['m', 'f'], size=3),\n259:            df2 = DataFrame({'a': tm.choice(['m', 'f'], size=10),\n\nvb_suite/groupby.py\n146:obj = tm.choice(list('ab'), size=n).astype(object)\n```\n",
      "I think I have this finished.  See #12490.\n",
      "This is fixed in pull request #12505.  Tests have passed, it's all in one commit, and whatsnew has been added.\n"
    ],
    "events": [
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "milestoned",
      "demilestoned",
      "commented",
      "cross-referenced",
      "commented",
      "milestoned",
      "demilestoned",
      "referenced",
      "cross-referenced",
      "commented"
    ],
    "changed_files": 7,
    "additions": 22,
    "deletions": 30,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/tests/frame/test_query_eval.py",
      "pandas/tests/test_graphics.py",
      "pandas/tests/test_graphics_others.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/util/testing.py",
      "vb_suite/groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12388,
    "reporter": "jennolsen84",
    "created_at": "2016-02-19T01:44:40+00:00",
    "closed_at": "2016-05-30T14:39:34+00:00",
    "resolver": "jennolsen84",
    "resolved_in": "cc1025a62019215a0fa38a891e07e6ca6ba656f1",
    "resolver_commit_num": 0,
    "title": "pd.eval division operation upcasts float32 to float64 ",
    "body": "The current behavior is inconsistent with normal python division of two `DataFrame`s (see code sample).\n\nPandas upcasts both terms to 64-bit floats when it detects a division, see:\n\n#L453\n\nI think numexpr can handle different types too, and upcast automatically, though I am not 100% sure.  I can submit a PR, but how do you recommend fixing this?  Something like the following?\n\n\n\nThe downside is that if someone does `2 + df`, they'll probably still end up upcasting it.  But this proposal is still better than what we have today\n\nI might re-write the above using `filter` too, but at this time I just wanted to discuss the general approach\n#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Dtypes",
      "Numeric"
    ],
    "comments": [
      "```\nIn operations implying a scalar and an array, the normal rules of casting are used in Numexpr, in contrast with NumPy, where array types takes priority. For example, if 'a' is an array of type `float32` and 'b' is an scalar of type `float64` (or Python `float` type, which is equivalent), then 'a*b' returns a `float64` in Numexpr, but a `float32` in NumPy (i.e. array operands take priority in determining the result type). If you need to keep the result a `float32`, be sure you use a `float32` scalar too.\n```\n\n(this is different that what you are saying, but should prob handle non-the-less). I would do this test/casting in `_cast_inplace` itself.\n",
      "`numpy` behavior seems to make more sense.  \n\n```\npd.eval('3.5 / float32array')\n```\n\nis much easier to write than:\n\n```\ns = np.float32('3.5')\npd.eval('s / float32array')\n```\n\nAlso, if someone that didn't read the `numexpr` docs super carefully, they would've missed the little detail.\n\nTherefore, should we mimic `numpy` behavior?\n\nAs for `_cast_inplace`, should we modify the signature?  After the changes, it would be much more specialized function.  It looks like it is only used once, so we have that going for us.\n",
      "Thought about it some more\n\nWe could look at the whole expression, and come up with an output datatype:\n\n```\nIf all array elements in an expression are floats32 and ints:\nthen\n    output type = float32\nelse:\n    output type = float64\n```\n\nThis still has corner cases like adding two int32 arrays will result in float64.  It is unclear what the solution of adding two int32 arrays should be: If the numbers are small, then int32 array as an output array is OK, but if the numbers are big you need int64 arrays.  A way around this would be to let the user specify an `out` parameter.  We could do extra checks to warn the user in case there are incompatiblities, like if two float64s are being added, but the output type is float32, etc.\n\nSo, the proposal now becomes:\n1.  Add `out` parameter to let user specify the destination of the datatype.  must be ndarray or a pandas object (so either has `.dtype` or `.values.dtype`)\n2.  Choose an output array dtype to be one of `{float64, float32}`, depending on datatypes of arrays in the expression.  `float32` is chosen if all arrays in the expression have dtypes of float32 or any of the ints, otherwise `float64` is chosen.\n3.  Warn if `out` is specified, and is `float32` array, but input contains `float64` array.\n",
      "I don't recall why we are casting in the first place. I would ideally like to defer this entirey to the engine. \n@chris-b1 @cpcloud any recall?\n\nif not, then would be ok with passing a `dtype=` argument for casting and default to the minimum casting needed (though this just adds another layer of indirection but I guess needs to be done).\n",
      "Should we go with `numpy` casting behavior (instead of `numexpr`)?  `numpy` behavior is consistent `pandas` when `numexpr` is not used.\n\nSo, what we'd have to do here is to down-cast constants from float64 to float32, if and only if all arrays are float32s.  E.g., `numpy` and `pandas` will use float64 as output dtype when int32 arrays are multiplied with float32 constant.  So, it seems like float32 array case is the main thing we have to worry about.\n\ne.g.\n\n```\nIn [1]: import pandas as pd\nIn [2]: import numpy as np\nIn [3]: pd.Series(np.arange(5, dtype=np.float32)) * 2.0\nOut[3]: \n0    0\n1    2\n2    4\n3    6\n4    8\ndtype: float32\n\nIn [11]: a = pd.Series(np.arange(5, dtype=np.int32)) * np.float32(1.1)\nIn [12]: a\nOut[12]: \n0    0.0\n1    1.1\n2    2.2\n3    3.3\n4    4.4\ndtype: float64\n\nIn [13]: np.arange(5, dtype=np.int32) * np.float32(1.1)\nOut[13]: array([ 0.        ,  1.10000002,  2.20000005,  3.30000007,  4.4000001 ])\nIn [14]: z = np.arange(5, dtype=np.int32) * np.float32(1.1)\nIn [15]: z.dtype\nOut[15]: dtype('float64')\n\n```\n",
      "I think you have to upcast by default, the only way I wouldn't would be if the users indicated (with `dtype=`) that its ok to proceed and then I would simply cast things to the passed dtype so the underlying wouldn't then upcast.\n",
      "but wouldn't this result in inconsistent behavior between normal pandas binary operations (like `s * 2.0`, which does not upcast s if it is a float32 series) and `pd.eval('s * 2.0')`, which will end up upcasting?\n",
      "@jennolsen84 hmm. that is a good point. just trying to avoid pandas do _any_ casting here. What if we remove that and just let the engine do it? (I don't really recall why this is special cased here). Or if we are forced to do it, then I guess you are right would have to do a lowest-common denonimator cast (maybe use `np.find_common_type`\n",
      "how about this as a start?  https://github.com/jennolsen84/pandas/commit/c82819fe483bb7dd218e94caabc4cd806b488275\n\nI manually tested it, and the behavior is now consistent with non-numexpr related code.  I am trying to avoid casting un-necessarily as you recommended, and letting the lower-level libraries take care of a lot of things.\n\nI did run the nosetests, and they all pass on existing tests.\n\nIf the commit looks good to you, I can add in some tests, add to docs, etc. and submit a PR.\n",
      "@jreback can you please take another look at the commit?  I addressed your comment, and I am not sure if you missed it.\n",
      "@jennolsen84 yeh just getting back to this.\n\nyour soln seems fine. However I still don't understand _why_ it is necessary to upcast (and only for division); what does numexpr do (if you don't upcast)? is it wrong?\n",
      "We're casting to float32 in all ops (not just division).\n\nThe division thing was another case where `pandas` was casting to `float`(64), so I had to make a change there as well.\n\nThe reason why the cast happens at all is for some reason `numexpr` would cast a scalar 64 bit float \\* array 32 bit float to 64-bit floats.  I am not sure why.  This is inconsistent with `numpy`, and un-necessarily slower and takes up more RAM.\n\nI will submit a PR (with whatsnew and tests)\n",
      "thanks @jennolsen84 why don't you submit and we'll go from there\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "milestoned"
    ],
    "changed_files": 4,
    "additions": 57,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/computation/expr.py",
      "pandas/computation/ops.py",
      "pandas/computation/tests/test_eval.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12393,
    "reporter": "phil20686",
    "created_at": "2016-02-19T11:53:45+00:00",
    "closed_at": "2016-02-26T02:13:43+00:00",
    "resolver": "MasonGallo",
    "resolved_in": "e1cec52695f29976538cd0f3d0b28f32b03b53c5",
    "resolver_commit_num": 0,
    "title": "Confusing behaviour of df.empty",
    "body": "This is as much a documentation issue as anything else. Basically it seems confusing that df.empty != df.dropna().empty I.e. that a a dataframe consiting entirely of na is not treated as empty. Obviously this is a bit of an edge case, but it caused a bunch of failures for me when used eith pd.read_sql methods, as database tables will often have columns that are not available for partiicular entities, and so can return an entire series of na. \n\nIt seems to me that in all cases df.empty should be the same as df.dropna().empty, but I understand that opinions might differ on this point, but at least the behaviour should be clearly documented.\n",
    "labels": [
      "Docs",
      "Reshaping",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "Can you show an example, the following is works. I agree certainly could update the documentation (with some examples and such)\n\n```\nIn [8]: df = DataFrame({'A' : [np.nan]})\n\nIn [9]: df.empty\nOut[9]: False\n\nIn [10]: df.dropna().empty\nOut[10]: True\n```\n",
      "That is exactly the behavior I was questioning, I think out[9] should be True. It seems to me that a dataframe containing nothing by na cells is \"empty\" according to most definitions.....\n\nPhil\n",
      "_I_ certainly expected that df.empty would be true if a data frame contained nothing but na cells.\n",
      "no, that is not a normal definition of empty which is 0-len. nulls are real values which are placeholders. The key here is that you actually have a valid index. Changing this would involve the definition dependent on the data itself which is not a good thing. welcome to have a doc update with some examples though.\n",
      "Well its certainly not the case that df.empty is the same as len(df.index==0) e.g. \n\n```\ndf = pd.DataFrame([], index=[0,1,2])\nprint df.empty #True\nprint len(df.index==0) #False\n```\n\nAlso\n\n```\ndf = pd.DataFrame([], index=[0,1,2], columns=['A','B'])\ndf.empty #False\n```\n",
      "So not only do I dispute that len=0 is the semantic definition of empty, that doesn't appear to be the implementation anyway.\n",
      "Also stuff like: \n\n```\ndf = pd.DataFrame([], index=[0,1,2], columns=['A','B'])\ndf.set_value(1, 'A', 17)\ndf['B'].empty # False\n```\n\nwhich just seems plain wrong. If I have a column that I have never added any data to, it should not return false when asked if its empty. I guess under the hood when you specify an index and columns it autofills the dataframe somehow, and that results in this behaviour, but the definition of empty should really play nicely with the default dataframe constructor in these examples imo. Else its just confusing.\n",
      "its very simple. its empty only if all axes are len 0\n",
      "It sounds like you're looking for some other collection of `Series`. A `DataFrame` is a tabular collection, and it makes sense to look at the shape. \n",
      "@phil20686 You can eg use:\n\n```\nIn [15]: df['B'].isnull().all()\nOut[15]: True\n```\n",
      "@jrebeck my example shows that that is not the implemented behavior:\n\n```\ndf = pd.DataFrame([], index=[0,1,2])\nprint df.empty #True \nsum(df.shape) == 0 #false\ndf = pd.DataFrame([], columns=['A','B'])\nprint df.empty # True\nsum(df.shape) == 0 #false\n```\n\nI really find it super weird that pre-allocation should result in empty=False, e.g. if you concatenate an empty series with a non empty dataframe it will get preallocated and then extracting it means empty has changed from True to False. This seems very strange to me, in some abstract sense series C is the same object, but merely moving it around has changed its properties.\n\n```\ndf = pd.DataFrame([], index=[0,1,2], columns=['A','B'])\ndf.set_value(1, 'A', 17)\nseries = pd.Series(name=\"C\")\nprint series.empty\ndf2 = pd.concat([df,series], axis=1)\nprint df2[\"C\"].empty\n```\n\nAnyway, my main point is that this behavior should be documented, because its quite counter-intuitive, not to argue about definitions of empty.\n",
      "@phil20686 every one of those results is correct, what exactly is counter intuitive here?\n\nthere isn't any 'pre-allocation' at all. You have indices. If the indicies are 0 in any way (could be 1 dim or not) then you are empty, otherwise you are not.\n\nWhat exactly are you using `.empty`? To be honest I have rarely needed this. Most operations in pandas just work regardless if things are empty or not. I think the docs are fairly clear on [this](http://pandas.pydata.org/pandas-docs/stable/basics.html#boolean-reductions).\n",
      "Um. I had a series that had .empty = True, I concat it with a dataframe, and then I extract the series, and then magically .empty=False? Even though at no time has the user added data to it?\n\nSimilarly, you can create a dataframe with either an index or columns but no values and its \"empty\", but if it has both and no values its \"non-empty\". \n\nYou don't think that is counter intuitive behavior? \n\nAnyway, I think most people would assume that empty == contains no data. That clearly isn't the case as it looks like its the same as \n\n```\nnot any(df.shape)\n```\n\nAnyway, my main point was that the documentation of Dataframe.empty should note these behaviors. \n",
      "@phil20686 one of the highlites of pandas is that it aligns data. When you put in a series it was empty, however, the concat realigned the Series to the other values in the DataFrame\n\n```\nIn [19]: df2\nOut[19]: \n     A    B   C\n0  NaN  NaN NaN\n1   17  NaN NaN\n2  NaN  NaN NaN\n\nIn [20]: df2.columns\nOut[20]: Index([u'A', u'B', u'C'], dtype='object')\n\nIn [21]: df2['C']\nOut[21]: \n0   NaN\n1   NaN\n2   NaN\nName: C, dtype: float64\n```\n\nthen [21] is clearly NOT empty; yes it is all null. Which is a MUCH more common operation. `.empty` is a very blunt instrument and not really used much in practice for this very reason. It is correct as far as it goes.\n",
      "@phil20686 In trying to clear some things up, I think we have to make a distinction between two points:\n- The definition of `empty` is clear**: it returns True if the length of one or all of the axes is 0 (you could see it as \"len(index) x len(columns) == 0\" for a dataframe). It is by defintion _not_ about having all NaNs or not. It is quite possible you find this not the best definition, but taking this definition as a starting point, all the return values in the examples you showed are consistent and as expected.\n- What _is_ maybe more surprising in some cases, leading to the unintuitive behaviour you described, is the way pandas fills Series/DataFrames with NaNs (and once filled with NaNs, it is not empty anymore). Pandas will fill a DataFrame with NaNs once it has both an index and columns. And there are indeed operation (eg concat) where index/columns can be added, leading to filling with NaNs. \n\nI just want to point out that I think the confusion you get has another root cause than the `empty` method. \nAnd the current `empty` method is just not the method you are looking for I think. It would be more something like `allnan` or `allnull`, which you can obtain with `isnull().all()`\n\n** _I don't say it is 'clear' in the docs, I mean in implementation_\n\nBut indeed, the docs of empty can certainly point that out. Do you want to do a PR to specify that this is not about NaNs ?\n",
      "I agree that the docs of empty could point this out (I actually had a student just ask me about this). Since it's been a few days, I can do a quick PR.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 1,
    "additions": 37,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/generic.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12396,
    "reporter": "multiloc",
    "created_at": "2016-02-19T19:26:24+00:00",
    "closed_at": "2018-05-13T13:17:49+00:00",
    "resolver": "paul-mannino",
    "resolved_in": "1dcddba2200b89cffe97ae7a32a34cdec3a7c8fb",
    "resolver_commit_num": 2,
    "title": "Concatenation of DFs with all NaT columns and TZ-aware ones breaks",
    "body": "Concatenating DFs that have columns with all NaTs and TZ-aware ones breaks as of 0.17.1:\n\n\n\nPossibly related to #11693, #11705 and the  #11456 family. However, this doesn't appear to be caused by the TZ-aware vs. non-TZ aware problems referenced there. \nVersions:\n\n\n",
    "labels": [
      "Reshaping",
      "Timezones",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "Also checked this against the 0.18.0.rc1 release candidate, still breaking there with the same error.\n",
      "yeh this is a slightly different issue, though pretty straightforward if you are interested in a PR.\n",
      "I'll give it a shot tomorrow\n",
      "See #12403\n",
      "I just faced this bug. Will it be fixed?",
      "if you would like to submit a PR this would move it along",
      "I also just encountered this bug in 0.20.3"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "demilestoned",
      "milestoned",
      "referenced",
      "commented",
      "commented",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 167,
    "deletions": 6,
    "changed_files_list": [
      "doc/source/whatsnew/v0.23.0.txt",
      "pandas/core/indexes/base.py",
      "pandas/core/internals.py",
      "pandas/tests/reshape/test_concat.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12397,
    "reporter": "relativistic",
    "created_at": "2016-02-19T21:44:56+00:00",
    "closed_at": "2017-01-25T21:20:37+00:00",
    "resolver": "jreback",
    "resolved_in": "7277459fa9531d987f0d626eda7aa9af9e457420",
    "resolver_commit_num": 4191,
    "title": "Add to_dataframe() method to MultiIndex",
    "body": "I find the `Index.to_series` method is a convenient way to allow indices to act as columns of a dataframe where desired. However, the behavior of `MultiIndex.to_series`, which gives a `Series` of tuples, is less useful.\n\nWould it be convenient to provide a `to_dataframe` method for index classes? This would be a natural extension of the utility of `to_series`, and more useful for `MultiIndex` objects I would think.\n\nI'm   something equivalent to:\n\n> \n",
    "labels": [
      "API Design",
      "MultiIndex"
    ],
    "comments": [
      "can you give an actual usecase?\n\nnormally you would simply do `.reset_index()`\n",
      "I just searched for this. I have a DF filled with boolean values (an adjacency matrix). I want to store the pairs of labels for which the value is True. I start by doing a simple\n\n```\ndf = df.unstack()\ndf = df[df == True]\n```\n\nnow I would like to do\n\n```\ndf.to_dataframe().to_csv(path)\n```\n\ninstead I need to do\n\n```\ndf.reset_index()[['indexname1', 'indexname2']].to_csv(path)\n```\n\n(admittedly, not a huge problem, but on the other hand `to_series()` itself could easily be replaced by `.reset_index()['indexname']` I think; `to_dataframe` seems to me at least as useful)\n",
      "Sorry, missed the notification with @jreback 's comment.\n\nOne of the nice things about `to_series()` is that it maintains the original index, which is useful if you need to interact with the original dataframe. `reset_index()` does not do this. To get what I want with `reset_index()`, I would need to do something like:\n\n```\nindex_names = df.index.names\ndf_indx = df.reset_index()[index_names]\ndf_indx.index = df.index\n```\n\nThis is a bit verbose, but admittedly, not bad. The main problem is that this is fragile, and will not work on all DataFrames. It would fail if:\n-  any string  in `index_names` has a duplicate in `df.columns` .\n- The index is unnamed\n\nAny one of these problems is relatively easy to get around, if you know a priori the construction of the dataframe. However, its a bit more effort if you want to write a general-purpose function that is guarantied to work in all conditions. And even in the case where you have some idea of your input, it would simply be nice if pandas could deal with this for you so you can focus  data analysis, rather than how to wrangle it into the right format.\n",
      "@relativistic again a fully worked out example here would be instructive. `.reset_index()` DOES preserve the index as its now in a column(s). So not sure what you mean.\n",
      "@jreback : Okay, sure. I'll try to pull together an example some evening soon. Can't show the code I was working on unfortunately, so I'll have to come up with another example.\n",
      "here is an external impl (e.g. could prob be faster / better if we did this inside of a ``.to_dataframe()``) method\r\n\r\n```\r\nIn [24]: i = pd.MultiIndex.from_product([np.arange(1000),np.arange(1000)],names=['one','two'])\r\n\r\nIn [25]: len(i)\r\nOut[25]: 1000000\r\n\r\nIn [26]: %timeit DataFrame(i.tolist(), columns=i.names)\r\n1 loop, best of 3: 612 ms per loop\r\n\r\nIn [27]: %timeit DataFrame([], index=i).reset_index()\r\n10 loops, best of 3: 38.5 ms per loop\r\n```",
      "Just for reference: at the time, I ended up doing:\r\n\r\n```\r\nIn [4]: %timeit pd.DataFrame({i.names[n] or n : i.levels[n][i.labels[n]]\r\n                              for n in range(len(i.names))})\r\n100 loops, best of 3: 16.4 ms per loop\r\n```\r\nwhich performs slightly better than (same as above, just to compare on the same CPU):\r\n```\r\nIn [5]: %timeit pd.DataFrame([], index=i).reset_index()\r\n10 loops, best of 3: 21.5 ms per loop\r\n```",
      "@toobaz that's only true for a relatively small frame"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 63,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/api.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/indexes/multi.py",
      "pandas/tests/indexes/test_multi.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12401,
    "reporter": "jreback",
    "created_at": "2016-02-20T18:23:15+00:00",
    "closed_at": "2017-10-04T11:07:59+00:00",
    "resolver": "jreback",
    "resolved_in": "48d0460ab9acbee223bae1be699344f8fd232224",
    "resolver_commit_num": 4521,
    "title": "DEPR: filter & select",
    "body": "do we need label selectors? we should for sure just have a single method for this. maybe call it `query_labels`? to be consistent with `.query` as the workhorse for data selection.\r\n\r\n- [x] ``.select`` (#17633)\r\n- [ ] ``.filter``\r\n\r\nxref #6599 \r\n",
    "labels": [
      "Indexing",
      "API Design",
      "Deprecate",
      "Needs Discussion"
    ],
    "comments": [
      "I personally find `filter` a useful function (at least I have used it to good purpose in my own work) to *select* certain columns. See also the examples added in https://github.com/pandas-dev/pandas/pull/12399. Although it should rather be called `select` ...\r\n\r\nLess sure about `select`. That seems less useful, certainly now `loc` accepts a function. ",
      "I think I have revised my thoughts here.\r\n\r\nwe should promote (in the doc / the-one-way-to-do-it), ``.select`` as the main *label* filtering function, and deprecate ``.filter`` (which ATM serve the same purpose). Maybe needs some API tweaks. \r\n\r\n``.filter`` is traditionally a *data* selection / filtering function.",
      "They are quite different at the moment:\r\n\r\n- `filter`:\r\n  - acts on columns by default (for dataframe)\r\n  - can select based on list, or simple 'like'/more advanced regex\r\n- `select`:\r\n  - acts on index by default\r\n  - selects based on function applied to index labels",
      "further: ``.filter`` uses ``.select`` for regex matching in its implementation.",
      "further: we use ``.filter()`` in ``.groupby()`` to allow a filter for group inclusion (boolean return)",
      "I have found `DataFrame.filter` to be useful, especially with `like` or `regex`. I have never used `DataFrame.select`, which feels very non-idiomatic to me.\r\n\r\nSo I would be happy to deprecate `select`. It's also highly confusing how `GroupBy.filter` works like `DataFrame.select`, not `.filter`.",
      ">  It's also highly confusing how `GroupBy.filter` works like `DataFrame.select`, not `.filter`.\r\n\r\nI agree this is highly confusing. Is renaming one of those out of the question? `filter` is a common name for a higher-order function which filters elements based on the result of a Boolean-valued function that was passed in, exactly like `GroupBy.filter`, so that seems like an appropriate name for what is currently `DataFrame.select`. There's also Python's builtin `filter` function.\r\n\r\nAnother option might be merging the functionality of `select` and `filter` under one name, so it supports both list-like and function arguments.",
      "so the problem as highlited by @jorisvandenbossche is that ``.select`` acts on the index (which is what ``groubpy.filter`` and boolean selection does). so it is a highly confusing name.\r\n\r\n``.filter`` is also a confusing name as it acts on the labels of columns.\r\n\r\nWe need a combined functionaility of the current ``DataFrame.select/filter`` (IOW to select *labels* from an axis and should accept a list-like, scalar and callable, like most other functions)\r\n\r\nsignature should be something like this (default for most functions is axis=0)\r\n\r\n```\r\ndef select_labels(arraylike or scalar or callable, axis=0, regex=False)\r\n```\r\n\r\nnow as to what to do:\r\n\r\n- ``select_labels`` I think is a nice name (open to suggestions), though other systems (spark & sql), use ``.select`` to mean label/column selection.\r\n- deprecate ``.select`` in favor of ``.select_labels``\r\n- deprecate ``.filter`` in favor of ``select_labels``\r\n\r\n@dkasak interested in taking this on?",
      "I would suggest simply deprecating/removing `select` without making a replacement. Indexing is a fine alternative.\r\n\r\n`DataFrame.filter()` is useful. I wish it were called `select` instead, both because that matches SQL and `filter` suggests filtering rows with a boolean expression (like `filter` in dplyr or Ibis), but I don't think changing the name is worth the hassle. \r\n\r\nIn general, I think we should avoid making small changes in the API for the basic grammar of data manipulation in pandas, unless we rethink things more broadly for a larger, breaking change (e.g., in pandas2).",
      ">  but I don't think changing the name is worth the hassle\r\n\r\nsure it is - pandas is going to exist for 1.x for quite some time \r\n\r\nbetter to make changes to the right spelling sooner rather than later \r\n\r\nI am all for deprecating filter and calling it select (or select_labels)\r\n",
      "I don't have time to handle this at the moment, but I may be interested in doing it when time permits if it hasn't been done already by then.\r\n\r\nFWIW, upon some thought, I still think changing the name of `.filter` to `.select*` would be best. I don't feel strongly about `.select` vs `.select_labels`. I generally prefer shorter names, but the added verbosity here might make things clearer. Calling it `.select` has the benefit that only one name is deprecated, not two.\r\n\r\nI'm not so sure about dropping the current behaviour of `.select` entirely because I have a use case which I'm not sure how to implement without it (and without resorting to things like `.reset_index()` to regain the ability to select by using a function).\r\n\r\nIn particular, I have a `MultiIndex` with 2 levels, each of which has elements of type `str`. In other words, each index value is conceptually a pair of strings. Currently I'm doing something like\r\n\r\n    df.select(lambda x: condition1(x[0]) and condition2(x[1]))\r\n\r\nand similar to select particular rows. How could this be implemented without current `.select` functionality?\r\n\r\n",
      "can u show a complete example of how using select "
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "demilestoned",
      "milestoned",
      "referenced",
      "referenced"
    ],
    "changed_files": 10,
    "additions": 179,
    "deletions": 53,
    "changed_files_list": [
      "doc/source/whatsnew/v0.21.0.txt",
      "pandas/core/common.py",
      "pandas/core/generic.py",
      "pandas/core/indexing.py",
      "pandas/tests/frame/test_alter_axes.py",
      "pandas/tests/frame/test_axis_select_reindex.py",
      "pandas/tests/frame/test_mutate_columns.py",
      "pandas/tests/groupby/test_groupby.py",
      "pandas/tests/series/test_indexing.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12406,
    "reporter": "jreback",
    "created_at": "2016-02-21T15:17:47+00:00",
    "closed_at": "2016-02-22T16:44:03+00:00",
    "resolver": "jreback",
    "resolved_in": "2415d8a48b69043b6bcde4f837c20462a10130b4",
    "resolver_commit_num": 3910,
    "title": "COMPAT: numpy master changes, floordiv & getitem(nan)",
    "body": "-ci.org/pydata/pandas/jobs/110280302\n\nThese indicate an indexing change with `nan` that now returns `TypeError` rather than `IndexError`, ok makes sense\n\n\n\nSo our comparison in this test changed. So we need to make this change for 0.18.0 as well [here](#L1002), where these should be `NaN` and not `inf`.\n\n\n\n\n",
    "labels": [
      "Compat",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [],
    "events": [
      "labeled",
      "milestoned",
      "cross-referenced",
      "labeled",
      "labeled",
      "renamed",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 2,
    "additions": 28,
    "deletions": 10,
    "changed_files_list": [
      "pandas/src/sparse.pyx",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12409,
    "reporter": "multiloc",
    "created_at": "2016-02-21T18:51:05+00:00",
    "closed_at": "2016-02-23T14:54:21+00:00",
    "resolver": "multiloc",
    "resolved_in": "e45e3b42e42c756d52005988e26203602b992d5e",
    "resolver_commit_num": 0,
    "title": "date_range breaks with tz-aware start/end dates and closed intervals in 0.18.0.rc1",
    "body": "The following works on 0.17.1 but breaks on the release candidate 0.18.0.rc1\n\n\n\nOn 0.18.0.rc1:\n\n\n",
    "labels": [
      "Regression",
      "Timezones"
    ],
    "comments": [
      "The fix looks simple, I'll post a PR in a bit\n",
      "gr8!\n"
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 32,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_daterange.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12411,
    "reporter": "AbeHandler",
    "created_at": "2016-02-22T02:13:44+00:00",
    "closed_at": "2016-03-23T17:56:45+00:00",
    "resolver": "jreback",
    "resolved_in": "85f8cf74b733a6782730f17b4a68fa4b028f2013",
    "resolver_commit_num": 3953,
    "title": "Unexpected output using brackets vs. parents in dataframe.loc",
    "body": "I have a dataframe, `a_dataframe`. I want to access the value at index=a, column=0. I can do this successfully with `a_dataframe.loc[\"a\", \"0\"]`. However, `a_dataframe.loc(\"a\", \"0\")` returns an object! Note: parens, no brackets.\n\nI am new to Pandas, so this was very unexpected behavior in the API. I would think the API should throw an exception or issue a warning. This is a very, very subtle difference in syntax and I think it makes it very easy for new users to get confused.\n#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n   true\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.10.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 14.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.16.2\nnose: 1.3.7\nCython: 0.23.2\nnumpy: 1.8.0rc1\nscipy: 0.13.0b1\nstatsmodels: 0.6.1\nIPython: 4.0.0\nsphinx: 1.3.1\npatsy: 0.4.1\ndateutil: 2.2\npytz: 2013.7\nbottleneck: None\ntables: None\nnumexpr: 2.4.6\nmatplotlib: 1.3.1\nopenpyxl: 2.2.0-b1\nxlrd: 0.9.4\nxlwt: None\nxlsxwriter: None\nlxml: 3.5.0\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.8\npymysql: None\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\n",
    "labels": [
      "Indexing",
      "Error Reporting",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "This is a standard Python and pandas way of indexing, there is much documentation : http://pandas.pydata.org/pandas-docs/stable/indexing.htm\n",
      "Is that something we can put an exception on? Or is that being used internally?\n",
      "no\n",
      "To back that up, here are some failures you get when you remove the `__call__` method from `_NDFrameIndexer`, albeit being called from the tests:\n\n```\n======================================================================\nERROR: test_loc_arguments (pandas.tests.test_indexing.TestIndexing)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/workspace/pandas/pandas/tests/test_indexing.py\", line 2474, in test_loc_arguments\n    result = df.loc(axis=0)['A1':'A3', :, ['C1', 'C3']]\nTypeError: '_LocIndexer' object is not callable\n\n======================================================================\nERROR: test_per_axis_per_level_doc_examples (pandas.tests.test_indexing.TestIndexing)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/workspace/pandas/pandas/tests/test_indexing.py\", line 2459, in test_per_axis_per_level_doc_examples\n    df.loc(axis=0)[:, :, ['C1', 'C3']] = -10\nTypeError: '_LocIndexer' object is not callable\n\n======================================================================\nERROR: test_per_axis_per_level_setitem (pandas.tests.test_indexing.TestIndexing)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/workspace/pandas/pandas/tests/test_indexing.py\", line 2538, in test_per_axis_per_level_setitem\n    df.loc(axis=0)[:, :] = 100\nTypeError: '_LocIndexer' object is not callable\n\n```\n",
      "The `__call__` method takes `*args, **kwargs`. It is not possible to only remove the `args` (it seems they are not used)? This would already raise a error message in this case I think?\n",
      "Or, if `*args` is never used by the internals, the presence of an `*args` could give an error suggesting user is confusing parentheses and square brackets. \n",
      "It's not clear that the internals are using `__call__` rather than the tests, if anyone wants to dig around further...\n",
      "@MaximilianR The use-case you see in the tests that use call like `df.loc(axis=0)[:, :, ['C1', 'C3']]` is AFAIK perfectly allowed as user code (although not really well documented. EDIT: it is mentioned here: http://pandas.pydata.org/pandas-docs/stable/advanced.html#using-slicers)\n",
      "yeh, this needs to stay like it is. We may need to pass more arguments\n\ne.g.\n\n`df.loc(strict=False)[....]` for strict-reindexing vs raising behavior and such\n",
      "@jorisvandenbossche thanks\n",
      "@jreback yes, the kwargs are certainly needed, but the args as well?\n",
      "@jorisvandenbossche yes I suppose we could raise on non-zero len *args, might provide some kind of an error check. ok will reopen\n",
      "@AbeHandler want to do a pull-request?\n",
      "> yes I suppose we could raise on non-zero len *args, might provide some kind of an error check\n\nCould also just remove the `*args` kw, although less helpful error message  \n",
      "@MaximilianR right I think in this case, since we are going out-of-the-way to be helpful, a nice error message pointing to the correct syntax would be useful.\n",
      "@jreback happy to do a pull request! \n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "closed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "reopened",
      "labeled",
      "unlabeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 7,
    "additions": 127,
    "deletions": 23,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/indexing.py",
      "pandas/core/internals.py",
      "pandas/tests/indexing/test_indexing.py",
      "pandas/tests/test_format.py",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12438,
    "reporter": "kawochen",
    "created_at": "2016-02-24T14:43:01+00:00",
    "closed_at": "2017-03-22T13:58:04+00:00",
    "resolver": "jreback",
    "resolved_in": "79581ffe6fb73089dfa8394c2f4e44677acfe1ce",
    "resolver_commit_num": 4313,
    "title": "BLD: Travis dedup via webhooks",
    "body": "Came across this just now \nIt sounds like it cancels old builds on a PR.  It works via github webhooks.\n",
    "labels": [
      "CI"
    ],
    "comments": [
      "oh that looks nice. ok i'll try this when I have some time.\n",
      "ok I setup this webhook, let's monitor and see if this works\n",
      "@jreback I have seen builds cancelled for out-of-date (repushed) commits when working https://github.com/pydata/pandas/pull/12833\n",
      "that was me :) sometimes I cancel the dups\n",
      "For inspiration, there is a script in this PR on conda-forge to do something like this: https://github.com/conda-forge/staged-recipes/pull/2257",
      "that's nice will steal!"
    ],
    "events": [
      "commented",
      "milestoned",
      "labeled",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 102,
    "deletions": 21,
    "changed_files_list": [
      ".travis.yml",
      "ci/install_travis.sh",
      "ci/travis_fast_finish.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12448,
    "reporter": "jorisvandenbossche",
    "created_at": "2016-02-25T23:47:06+00:00",
    "closed_at": "2016-03-08T23:39:51+00:00",
    "resolver": "jreback",
    "resolved_in": "14cf67fbb25c1695fd126ab034af2218d04c154c",
    "resolver_commit_num": 3928,
    "title": "Breaking examples due to resample refactor",
    "body": "While using master a bit, I discovered some more cases where the new resample API breaks things:\n- Plotting. `.plot` is a dedicated groupby/resample method (which adds each group individually to the plot), while I think it is a very common idiom to quickly resample your timeseries and plot it with (old API) eg `s.resample('D').plot()`. \n  Example with master:\n  \n  \n  \n  ![figure_1](-dbb4-11e5-80ad-87e663ef5de8.png)\n  \n  while previously it would just have given you one continuous line. \n  This one can be solved I think by special casing `plot` for `resample` (not have it a special groupby-like method, but let it warn and pass the the `resample().mean()` result to `Series.plot()` like the 'deprecated_valids')\n- When you previously called a method on the `resample` result that is also a valid Resampler method now. Eg `s.resample(freq).min()` would previously have given you the \"minimum daily average\" while now it will give you the \"minimum per day\". \n  This one is more difficult/impossible to solve I think? As you could detect that case if you know it is old code, but cannot distinguish it from perfectly valid code with the new API. If we can't solve it, I think it deserves some mention in the whatsnew explanation.\n- Using `resample` on a `groupby` object (xref #12202). Using the example of that issue, with 0.17.1 you get:\n  \n  \n  \n  while with master you get:\n  \n  \n  \n  which will give you different results/error with further operations on that. Also, this case does not raise any FutureWarning (which should, as the user should adapt the code to `groupby().resample('D').ffill()`)\n",
    "labels": [
      "API Design",
      "Resample"
    ],
    "comments": [
      "ok\n\n1) just need to define `.plot(...)` on the `Resampler` to actually call `.mean().plot(...)` (and have a nice warning message)\n\nsimilar / better than this\n\n```\nIn [5]: s.resample('15min',how='sum').plot()\n/Users/jreback/miniconda/bin/ipython:1: FutureWarning: how in .resample() is deprecated\nthe new syntax is .resample(...).sum()\n  #!/bin/bash /Users/jreback/miniconda/bin/python.app\nOut[5]: <matplotlib.axes._subplots.AxesSubplot at 0x11bd82450>\n```\n\n3) need to provide warnings here as well (this is handled by the `.groupby(...).resample(...)` which actually calls things, but hits a different path\n",
      "2) I think you are right, have to doc this. It should break code loudly though as the previous API would return a scalar, this will return a Series.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 8,
    "additions": 245,
    "deletions": 81,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/tests/test_graphics.py",
      "pandas/tests/test_groupby.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12453,
    "reporter": "ghost",
    "created_at": "2016-02-26T12:33:57+00:00",
    "closed_at": "2016-06-06T23:20:34+00:00",
    "resolver": "brandys11",
    "resolved_in": "67b72e3cbbaeb89a5b9c780b2fe1c8d5eaa9c505",
    "resolver_commit_num": 0,
    "title": "BUG: excel export merge margin ",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n|  | SALARY |  |  |  | DAYS |  |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| YEAR |  | 2013 | 2014 |  | All | 2013 | 2014 |  | All |\n| MONTH |  | 12 | 1 | 2 |  | 12 | 1 | 2 |  |\n| JOB | NAME |  |  |  |  |  |  |  |  |\n| Employ | Mary | 23 | 200 | 190 | 413 | 5 | 15.0 | 5.0 | 8.333333 |\n| Worker | Bob | 17 | 210 | 80 | 307 | 3 | 10.5 | 8.0 | 8.000000 |\n| All |  | 40 | 410 | 270 | 720 | 4 | 12.0 | 6.5 | 8.142857 |\n\nBut when exporting to Excel, the Month 2, on SALARY and DAYS, merges with the next column and All above.\n\n2014 .......... | All ...........|\n1 ..... | 2 ....................... |\n15 ... | 5 ..... | 8.333333 | \n\nWhen exporting to HTML it's ok with the header but, if the value cell has no value then shows cuts the columns.\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Windows\nOS-release: 2012ServerR2\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 47 Stepping 2, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.17.1\nnose: None\npip: 8.0.2\nsetuptools: 20.1.1\nCython: None\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: None\nIPython: None\nsphinx: None\npatsy: None\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 0.9.4\nxlwt: None\nxlsxwriter: 0.8.4\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\nJinja2: None\nNone\n\n![image](-dc85-11e5-960e-d6fe6ac85ee8.png)\n",
    "labels": [
      "IO Excel",
      "Bug",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "try `merge_cells=False`\n",
      "cc @chris-b1 \n",
      "merge_cells affects all the cells and not just the \"margins\".\n\n![image](https://cloud.githubusercontent.com/assets/976878/13352739/4c86f01e-dc87-11e5-9c87-2a89a9377abd.png)\n",
      "Something going awry in the logic [here](https://github.com/pydata/pandas/blob/master/pandas/core/format.py#L1786) when there's an empty level, I'll take a look.\n",
      "Here a picture of when exporting the same dataframe to html:\n\n![image](https://cloud.githubusercontent.com/assets/976878/13372797/9b70b790-dd4b-11e5-9fc7-c8fc0ba76e26.png)\n",
      "I have just opened PR for this. The only problem could be, that empty name is changed to [Unnamed x_level_y](https://github.com/pydata/pandas/blob/master/pandas/io/parsers.py#L1848) when reading back form excel.\n",
      "I think the expected output above is wrong\n\n```\nIn [11]: df\nOut[11]: \n            SALARY                      DAYS                     \nYEAR          2013   2014           All 2013  2014            All\nMONTH           12      1      2          12     1    2          \nJOB    NAME                                                      \nEmploy Mary   23.0  200.0  190.0  413.0  5.0  15.0  5.0  8.333333\nWorker Bob    17.0  210.0   80.0  307.0  3.0  10.5  8.0  8.000000\nAll           40.0  410.0  270.0  720.0  4.0  12.0  6.5  8.142857\n```\n\nThis is correct (as pandas prints it in the console)\n\n```\nIn [12]: df.index\nOut[12]: \nMultiIndex(levels=[[u'All', u'Employ', u'Worker'], [u'', u'Bob', u'Mary']],\n           labels=[[1, 2, 0], [2, 1, 0]],\n           names=[u'JOB', u'NAME'])\n\nIn [13]: df.columns\nOut[13]: \nMultiIndex(levels=[[u'DAYS', u'SALARY'], [2013, 2014, u'All'], [1, 2, 12, u'']],\n           labels=[[1, 1, 1, 1, 0, 0, 0, 0], [0, 1, 1, 2, 0, 1, 1, 2], [2, 0, 1, 3, 2, 0, 1, 3]],\n           names=[None, u'YEAR', u'MONTH'])\n```\n",
      "Have you been referring to my PR? If yes, is it ok to change behavior so that empty index names would stay empty instead \"Unnamed: %d_level_%d\" as it is right now?\n",
      "yes I referred to the PR. My point is that the example in the issue is wrong.\n",
      "Sorry, but I am still not sure if I fully understand what you are saying. The html table in first comment looks exactly as yours. And the one in the comment from 27th February is wrong, although I get correct output when calling df.to_html(). \n\nRunning it after applying my PR would yield:\n\n```\nIn [11]: excel = pd.read_excel(file, header=[0,1,2], index_col=[0,1])\n\nIn [12]: excel\nOut[12]: \n            DAYS                               SALARY            \nYEAR        2013  2014                     All   2013 2014        All\nMONTH         12     1    2 Unnamed: 5_level_2     12    1    2   Unnamed: 9_level_2\nJOB    NAME                                                       \nEmploy Mary    5  15.0  5.0           8.333333     23  200  190   413\nWorker Bob     3  10.5  8.0           8.000000     17  210   80   307\nAll    Bob     4  12.0  6.5           8.142857     40  410  270   720\n\nIn [13]: excel.index\nOut[13]: \nMultiIndex(levels=[['All', 'Employ', 'Worker'], ['Bob', 'Mary']],\n           labels=[[1, 2, 0], [1, 0, 0]],\n           names=['JOB', 'NAME'])\n\nIn [14]: excel.columns\nOut[14]: \nMultiIndex(levels=[['DAYS', 'SALARY'], [2013, 2014, 'All'], [1, 2, 12, 'Unnamed: 5_level_2', 'Unnamed: 9_level_2']],\n           labels=[[0, 0, 0, 0, 1, 1, 1, 1], [0, 1, 1, 2, 0, 1, 1, 2], [2, 0, 1, 3, 2, 0, 1, 4]],\n           names=[None, 'YEAR', 'MONTH'])\n\n```\n\nThe problem is that \"Unnamed\" cells are not merged and we get more Indexes on the 3rd level. I have tried to empty strings insted of \"Unnamed\" cells. Some tests have failed -> [test_unnamed_columns](https://github.com/pydata/pandas/blob/master/pandas/io/tests/parser/common.py#L230) as the column names have to be unique. We could count the number of empty cells and use the \"Unnamed cells\" only if there is more of them or be Ok with having \"Unnamed\" cells all the time. \n\nDo you have any other sugestions, or should I try implementing one of mentioned?\n",
      "@brandys11 The tables are not the same, salary is misplaced in the top table. The top row is 1 column off. \n\nAs far as your PR, you have something wrong. You should't have any unnamed columns. This conversation should move to the PR itself, not the issue.\n",
      "Sorry, but I'm not fully understanding. Is it a bug or am I doing something wrong?\n"
    ],
    "events": [
      "commented",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "commented",
      "renamed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 96,
    "deletions": 23,
    "changed_files_list": [
      "pandas/formats/format.py",
      "pandas/io/excel.py",
      "pandas/io/tests/test_excel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12467,
    "reporter": "tsdlovell",
    "created_at": "2016-02-26T19:33:34+00:00",
    "closed_at": "2016-04-06T13:00:10+00:00",
    "resolver": "tsdlovell",
    "resolved_in": "e04f3438c362777fc2fea24994caf388639214d8",
    "resolver_commit_num": 0,
    "title": "Concat of tz-aware and tz-unaware dataframes fails",
    "body": "This snippet\n\n\n\ncauses\n\n> .../pandas/tseries/common.py in _concat_compat(to_concat, axis)\n>     282         if 'datetime' in typs or 'object' in typs:\n>     283             to_concat = [convert_to_pydatetime(x, axis) for x in to_concat]\n> --> 284             return np.concatenate(to_concat, axis=axis)\n>     285 \n>     286         # we require ALL of the same tz for datetimetz\n> \n> ValueError: all the input arrays must have same number of dimensions\n\nWe would expect it to return something like this\n\n> pd.DataFrame(dict(time=pd.Series([pd.Timestamp('2015-01-01', tz=None), pd.Timestamp('2015-01-01', tz='UTC')], dtype=object)))\n> Out[18]: \n>                         time\n> 0        2015-01-01 00:00:00\n> 1  2015-01-01 00:00:00+00:00\n\noutput of `pd.show_versions()`\n### INSTALLED VERSIONS\n\ncommit: fe584e7829dc4dd9fa585a7fbf5fd47fc6f4b057\npython: 2.7.11.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.13.0-53-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.0rc1+47.gfe584e7\nnose: 1.3.7\npip: 8.0.3\nsetuptools: 20.1.1\nCython: 0.23.4\nnumpy: 1.10.4\nscipy: None\nstatsmodels: None\nxarray: None\nIPython: 4.1.1\nsphinx: 1.3.5\npatsy: None\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 0.9.4\nsqlalchemy: 0.9.4\npymysql: None\npsycopg2: None\njinja2: 2.8\n",
    "labels": [
      "Bug",
      "Reshaping",
      "Timezones"
    ],
    "comments": [
      "can you rebase/update\n",
      "rebased, looking into the test failures\n"
    ],
    "events": [
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 27,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/tests/frame/test_combine_concat.py",
      "pandas/tseries/common.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12473,
    "reporter": "AlJohri",
    "created_at": "2016-02-26T21:04:14+00:00",
    "closed_at": "2016-04-18T17:17:44+00:00",
    "resolver": "sinhrks",
    "resolved_in": "4c84f2dd9113bfe940bf4ddb9f0dfdcdaf466188",
    "resolver_commit_num": 276,
    "title": "Pandas datetime64 series no longer has map function when localized",
    "body": "Create test DF.\n\n\n\n\n\nThis call to `map` works fine:\n\n\n\n**Localizing the datetime64 causes it to no longer have the map function**\n\n\n\nDoesn't work:\n\n\n#### Error Message\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Bug",
      "Dtypes",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "Thanks for the report. It looks a bug when `dtype._values` doesn't return `ndarray`. PR is appreciated.\n\n```\ns = pd.Series(pd.date_range('2011-01-01', '2011-01-02', freq='H').tz_localize('US/Eastern'))\ns.dtype\n# datetime64[ns, US/Eastern]\n\ns.map(lambda x: x)\n# TypeError: Argument 'arr' has incorrect type (expected numpy.ndarray, got DatetimeIndex)\n```\n\n```\ns = pd.Series([1, 1, 2, 3], dtype='category')\ns.dtype\n# category\n\ns.map(lambda x: x)\n# TypeError: Argument 'arr' has incorrect type (expected numpy.ndarray, got Categorical)\n```\n",
      "similar to #11757\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files": 13,
    "additions": 237,
    "deletions": 32,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/categorical.py",
      "pandas/core/common.py",
      "pandas/core/frame.py",
      "pandas/core/internals.py",
      "pandas/core/series.py",
      "pandas/indexes/base.py",
      "pandas/indexes/category.py",
      "pandas/tests/indexes/test_category.py",
      "pandas/tests/series/test_analytics.py",
      "pandas/tests/series/test_apply.py",
      "pandas/tests/test_categorical.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12486,
    "reporter": "jreback",
    "created_at": "2016-02-27T20:24:10+00:00",
    "closed_at": "2016-04-26T15:03:02+00:00",
    "resolver": "jreback",
    "resolved_in": "699424027fb657192541bcd0c3d9f9b7d26f2300",
    "resolver_commit_num": 3993,
    "title": "API: groupby.resample *maybe* can return a deferred operation",
    "body": "xref #12448 / #12449 \n\nand on [SO](-0-18-changes-to-resample-how-to-upsample-with-groupby/36294398#36294398)\n\n\n\nThis replicates 0.17.1 (something slightly off with it including the grouper column)\n\n\n\nA pure asfreq operation\n\n\n\nWould be nice for this to work\n\n\n",
    "labels": [
      "Groupby",
      "API Design",
      "Resample"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "renamed",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 12,
    "additions": 719,
    "deletions": 132,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/base.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/missing.py",
      "pandas/core/window.py",
      "pandas/indexes/base.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_window.py",
      "pandas/tools/merge.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12489,
    "reporter": "dsm054",
    "created_at": "2016-02-28T17:40:29+00:00",
    "closed_at": "2016-03-03T12:11:02+00:00",
    "resolver": "jreback",
    "resolved_in": "84781b4b9e328d7f7dc91e934ab69e071f7a8f55",
    "resolver_commit_num": 3920,
    "title": "blacklist numexpr 2.4.4",
    "body": "Under some circumstances, numexpr 2.4.4 can lead to flaky output (see #12023 and links therein).  This happened again once on SO in the last few days, and it's a suspect in another case.\n\n[UPDATE: the suspect was convicted of the second crime as well!]\n\nWe should just refuse to work with 2.4.4, rather than leave the impression pandas is unstable, and require a later version as soon as we can.  There are apparently still enough people out there with up-to-date pandas and out-of-date numexpr that a warning could probably help someone. Accordingly, after some discussion on gitter, it sounds like we should\n\n(1) add a warning to the docs\n(2) centralize the version check (right now both computation/eval and computation/expressions check the numexpr version)\n(3) only allow numexpr if >= 2.1 and not == 2.4.4; otherwise, set _USE_NUMEXPR to False\n(4) in the warning we raise, recommend upgrading from 2.4.4 because we can't easily prevent other dependencies from using it\n(5) add a test to make sure that the presence of 2.4.4 disables _USE_NUMEXPR (say, by adding it to the requirements-3.4_SLOW).\n",
    "labels": [
      "Compat"
    ],
    "comments": [],
    "events": [
      "labeled",
      "milestoned",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 14,
    "additions": 126,
    "deletions": 57,
    "changed_files_list": [
      "ci/requirements-3.4_SLOW.run",
      "ci/requirements-3.5.run",
      "doc/source/install.rst",
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/computation/__init__.py",
      "pandas/computation/eval.py",
      "pandas/computation/expressions.py",
      "pandas/computation/tests/test_compat.py",
      "pandas/computation/tests/test_eval.py",
      "pandas/core/datetools.py",
      "pandas/core/format.py",
      "pandas/io/common.py",
      "pandas/util/clipboard.py",
      "pandas/util/print_versions.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12493,
    "reporter": "VelizarVESSELINOV",
    "created_at": "2016-02-29T09:13:19+00:00",
    "closed_at": "2016-04-13T01:30:28+00:00",
    "resolver": "gfyoung",
    "resolved_in": "827745dcf0cec55e8b071acc54b0aa5e610e640e",
    "resolver_commit_num": 3,
    "title": "usecols is not respected when the file is empty",
    "body": "xref #9755\n\nIf the file is empty, it is logical to compensate with extra columns.\n#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\nCurrent output:\n\n\n\nExpected output:\n\n\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.3.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.17.1\nnose: None\npip: 7.1.2\nsetuptools: 18.3.2\nCython: None\nnumpy: 1.10.1\nscipy: 0.16.1\nstatsmodels: None\nIPython: 4.0.1\nsphinx: None\npatsy: None\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.5.0\nopenpyxl: 2.3.2\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: 4.4.1\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\nJinja2: 2.8\nNone\n",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "CSV",
      "Effort Low"
    ],
    "comments": [
      "I agree, since the following is true. IOW an empty with `names` only produces an empty frame with the names; `usecols` just sub-selects.\n\n```\nIn [19]: read_csv(StringIO(''), names=name_list, header=None)\nOut[19]: \nEmpty DataFrame\nColumns: [Dummy, X, Dummy_2]\nIndex: []\n```\n",
      "FYI, if you try running that using `engine='python'`, you get a `StopIteration` error when `s = StringIO('')`.  Is that a bug as well?\n",
      "I'll answer my own question: yes.  If the C engine can do it, the Python one should be able to do it as well.\n",
      "Thanks, much appreciated :)\n"
    ],
    "events": [
      "renamed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 8,
    "additions": 171,
    "deletions": 43,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/io/common.py",
      "pandas/io/excel.py",
      "pandas/io/html.py",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_html.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/parser.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12503,
    "reporter": "jreback",
    "created_at": "2016-03-01T13:31:42+00:00",
    "closed_at": "2016-07-13T11:52:44+00:00",
    "resolver": "jreback",
    "resolved_in": "7dd4091458d9117e57d2ad9ce3126855bd00108c",
    "resolver_commit_num": 4054,
    "title": "CLN & REORG core/common.py",
    "body": "some ideas on reorging.\n- [X] rename `pandas/util/misc.py` to `pandas/util/tools.py` or `pandas/util/util.py` ()\n  - [X] move `core/generic.py/_validate_kwargs` (and new `validate_args`) here ()\n  - [x] move the `various python utilities` section from `core/common` (#13147)\n- [x] create `pandas/types/` (#12804)\n  - [x] move `pandas/core/dtypes.py` here\n- [x] create `pandas/types/types.py` (#12804) \n  - [x] move `bind_method`, `create_pandas_abc_types` here (from `core/common.py`)\n  - [x] move the `is_...._dtype` to here (#13147)\n- [x] move `core/common.py/concat*` to new sub-package `types/concat` (#12844)\n- [x] move `core/common.py/take*` stuff to `algorithms` (may need to create a sub-package for this) (#12804)\n- [x] create new sub-package `pandas/formats` (#12804)\n  - [x] move `core/common.py/printing utilities` to new sub-package `pandas/formats/printing.py` (#12804)\n  - [x] move `core/format.py` here as well (maybe split later) (#12804)\n",
    "labels": [
      "Clean"
    ],
    "comments": [
      "cc @wesm \n",
      "+1 for the reorganization.  Will move the `arg` and `kwarg` validators into their own separate module in my own PR (#12413).\n",
      "+1 on anything that helps separate implementation matters, especially anything we're contemplating replacing / refactoring, from the public user API. \n"
    ],
    "events": [
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 118,
    "additions": 4944,
    "deletions": 4134,
    "changed_files_list": [
      "ci/lint.sh",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/__init__.py",
      "pandas/api/__init__.py",
      "pandas/api/tests/__init__.py",
      "pandas/api/tests/test_api.py",
      "pandas/api/types/__init__.py",
      "pandas/compat/numpy/function.py",
      "pandas/computation/ops.py",
      "pandas/computation/pytables.py",
      "pandas/computation/tests/test_eval.py",
      "pandas/core/algorithms.py",
      "pandas/core/api.py",
      "pandas/core/base.py",
      "pandas/core/categorical.py",
      "pandas/core/common.py",
      "pandas/core/config_init.py",
      "pandas/core/convert.py",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/indexing.py",
      "pandas/core/internals.py",
      "pandas/core/missing.py",
      "pandas/core/nanops.py",
      "pandas/core/ops.py",
      "pandas/core/panel.py",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/core/strings.py",
      "pandas/core/window.py",
      "pandas/formats/format.py",
      "pandas/formats/printing.py",
      "pandas/formats/style.py",
      "pandas/indexes/base.py",
      "pandas/indexes/category.py",
      "pandas/indexes/multi.py",
      "pandas/indexes/numeric.py",
      "pandas/indexes/range.py",
      "pandas/io/common.py",
      "pandas/io/data.py",
      "pandas/io/excel.py",
      "pandas/io/html.py",
      "pandas/io/packers.py",
      "pandas/io/parsers.py",
      "pandas/io/pickle.py",
      "pandas/io/pytables.py",
      "pandas/io/sql.py",
      "pandas/io/stata.py",
      "pandas/io/tests/test_sql.py",
      "pandas/io/tests/test_stata.py",
      "pandas/sparse/array.py",
      "pandas/sparse/frame.py",
      "pandas/sparse/list.py",
      "pandas/sparse/panel.py",
      "pandas/sparse/series.py",
      "pandas/src/testing.pyx",
      "pandas/stats/moments.py",
      "pandas/stats/ols.py",
      "pandas/tests/frame/test_apply.py",
      "pandas/tests/frame/test_constructors.py",
      "pandas/tests/frame/test_dtypes.py",
      "pandas/tests/frame/test_indexing.py",
      "pandas/tests/indexing/test_indexing.py",
      "pandas/tests/series/test_constructors.py",
      "pandas/tests/series/test_datetime_values.py",
      "pandas/tests/series/test_indexing.py",
      "pandas/tests/series/test_quantile.py",
      "pandas/tests/test_base.py",
      "pandas/tests/test_categorical.py",
      "pandas/tests/test_common.py",
      "pandas/tests/test_generic.py",
      "pandas/tests/test_graphics.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_infer_and_convert.py",
      "pandas/tests/test_lib.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tests/test_nanops.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_panel4d.py",
      "pandas/tests/test_strings.py",
      "pandas/tests/types/test_cast.py",
      "pandas/tests/types/test_common.py",
      "pandas/tests/types/test_dtypes.py",
      "pandas/tests/types/test_generic.py",
      "pandas/tests/types/test_inference.py",
      "pandas/tests/types/test_io.py",
      "pandas/tests/types/test_missing.py",
      "pandas/tests/types/test_types.py",
      "pandas/tools/merge.py",
      "pandas/tools/pivot.py",
      "pandas/tools/plotting.py",
      "pandas/tools/tile.py",
      "pandas/tools/util.py",
      "pandas/tseries/base.py",
      "pandas/tseries/common.py",
      "pandas/tseries/converter.py",
      "pandas/tseries/frequencies.py",
      "pandas/tseries/index.py",
      "pandas/tseries/offsets.py",
      "pandas/tseries/period.py",
      "pandas/tseries/tdi.py",
      "pandas/tseries/tests/test_bin_groupby.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_resample.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tests/test_timezones.py",
      "pandas/tseries/timedeltas.py",
      "pandas/tseries/tools.py",
      "pandas/tseries/util.py",
      "pandas/types/api.py",
      "pandas/types/cast.py",
      "pandas/types/common.py",
      "pandas/types/concat.py",
      "pandas/types/inference.py",
      "pandas/types/missing.py",
      "pandas/util/testing.py",
      "pandas/util/validators.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12527,
    "reporter": "itcarroll",
    "created_at": "2016-03-04T17:33:44+00:00",
    "closed_at": "2016-04-20T00:04:17+00:00",
    "resolver": "gliptak",
    "resolved_in": "1320ef75dacd832874dc1895fabef4d59586a117",
    "resolver_commit_num": 22,
    "title": "When .loc returns IndexError rather than KeyError",
    "body": "#### Updated\n\nI have a MultiIndex'd DataFrame that returns a KeyError for one integer and an IndexError for a different integer, neither integer is in the first level of the index. This only occurs when attempting to access a scalar value, a slice always give KeyError. The behavior does not occur on a cut down version of the (>200M when pickled) data frame, or I would attach a working example. Can send the file if needed though.\n\n\n#### Expected Output\n\n\n\nThe expected behavior occurs for nearly all integers I try that are not in the first level of the index. How could special integers give an IndexError?\n#### output of `pd.show_versions()`\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.3.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.17.0\nnose: 1.3.7\npip: 8.0.2\nsetuptools: 19.4\nCython: 0.23.4\nnumpy: 1.10.4\nscipy: 0.16.1\nstatsmodels: None\nIPython: 4.0.0\nsphinx: None\npatsy: 0.4.0\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: 2.4.6\nmatplotlib: 1.5.0\nopenpyxl: None\nxlrd: 0.9.4\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: 4.4.1\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.11\npymysql: None\npsycopg2: None\n",
    "labels": [
      "Bug",
      "Indexing",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "@itcarroll thanks for the report! It's helpful to include examples that are fully reproducible. _Sometimes_ there's something unique about the file that's causing the bug, but most of the time it's something else. Fortunately this time it looks like it's just the length of the DataFrame:\n\n``` python\n\n# KeyError\ndf = pd.DataFrame(1, index=pd.MultiIndex.from_product([[1, 2], range(499999)]), columns=['dest']); df.loc[(3, 0), 'dest']\n\n# IndexError\ndf = pd.DataFrame(1, index=pd.MultiIndex.from_product([[1, 2], range(500000)]), columns=['dest']); df.loc[(3, 0), 'dest']\n```\n",
      "Seem to be hitting (in `pandas/index.pyx`):\n\n```\n# Don't populate hash tables in monotonic indexes larger than this\n_SIZE_CUTOFF = 1000000\n```\n"
    ],
    "events": [
      "renamed",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "labeled",
      "cross-referenced"
    ],
    "changed_files": 4,
    "additions": 21,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/index.pyx",
      "pandas/indexes/multi.py",
      "pandas/tests/indexes/test_multi.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12533,
    "reporter": "sinhrks",
    "created_at": "2016-03-05T15:45:26+00:00",
    "closed_at": "2016-04-29T17:05:48+00:00",
    "resolver": "sinhrks",
    "resolved_in": "7bbd031104ee161b2fb79ba6f5732910661f94f8",
    "resolver_commit_num": 297,
    "title": "Allow .where to accept callable as condition",
    "body": "#### Code Sample, a copy-pastable example if possible\n\nAllow `.where` and `.mask` to accept `callable` as cond. This is useful if `DataFrame` is changed during method chaining.\n\n\n#### Expected Output\n\nShould be the same as:\n\n\n",
    "labels": [
      "Indexing",
      "API Design"
    ],
    "comments": [
      "yeah I link to the master issue for this #11485; `.query`, `.where`, and indexers should accept callables\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "cross-referenced",
      "commented",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files": 13,
    "additions": 588,
    "deletions": 23,
    "changed_files_list": [
      "doc/source/indexing.rst",
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/common.py",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/indexing.py",
      "pandas/core/panel.py",
      "pandas/core/series.py",
      "pandas/tests/frame/test_indexing.py",
      "pandas/tests/frame/test_query_eval.py",
      "pandas/tests/indexing/test_callable.py",
      "pandas/tests/series/test_indexing.py",
      "pandas/tests/test_panel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12543,
    "reporter": "evectant",
    "created_at": "2016-03-06T17:40:02+00:00",
    "closed_at": "2016-03-06T22:42:23+00:00",
    "resolver": "evectant",
    "resolved_in": "a58ad4f094e0bed3c3730fbb280b6583828e01d5",
    "resolver_commit_num": 1,
    "title": "BLD: Math not rendered in Travis-built docs",
    "body": "Travis-built docs do not appear to have math rendered correctly. See -docs.github.io/pandas-docs-travis/computation.html#exponentially-weighted-windows for one example.\n",
    "labels": [
      "Docs"
    ],
    "comments": [
      "Apparently due to: `WARNING: LaTeX command 'latex' cannot be run (needed for math display), check the pngmath_latex setting`.\n",
      "Yes, that is what I thought would be the reason, but it is strange that this warning is raised, as I explicitly added install of latex just for this.\n\nBut anyway, we maybe better switch to mathjax\n",
      "Thanks for the report BTW!\n"
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 1,
    "additions": 1,
    "deletions": 1,
    "changed_files_list": [
      "ci/build_docs.sh"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12553,
    "reporter": "mfixman",
    "created_at": "2016-03-07T14:58:17+00:00",
    "closed_at": "2016-12-04T17:42:07+00:00",
    "resolver": "mroeschke",
    "resolved_in": "c0e13d1bccd4a783486eba8cc769db48a7875de8",
    "resolver_commit_num": 2,
    "title": "SeriesGroupby.nunique raises an IndexError on empty Series",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Bug",
      "Groupby",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "ok, pull-requests are welcome to fix\n",
      "Is this supposed to return 0 or an empty Series? I am fixing it right now but it seems awkward returning 0, while other calls of  the nunique() return Series objects.\n",
      "equiv to this, an empty Series should be returned. You should only do that null checking (the line where it errors), if the series has len\n\n```\nIn [1]: s = Series([])\n\nIn [2]: s.groupby(s.index).sum()\nOut[2]: Series([], dtype: float64)\n```\n",
      "Thanks, made a pull request:\nhttps://github.com/pydata/pandas/pull/12557\nEdit: Output:\n\n```\n>>> reload(pandas)\n<module 'pandas' from 'pandas/__init__.pyc'>\n>>> pandas.Series().groupby(level = 0).nunique()\nSeries([], dtype: int64)\n>>> pandas.Series([1],[1]).groupby(level = 0).nunique()\n1    1\ndtype: int64\n>>>\n```\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 14,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.2.txt",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12554,
    "reporter": "gregory-marton",
    "created_at": "2016-03-07T17:18:17+00:00",
    "closed_at": "2016-04-27T14:28:37+00:00",
    "resolver": "Komnomnomnom",
    "resolved_in": "8001e15e3eef726180d1c93c7ae454d659b9bcd9",
    "resolver_commit_num": 42,
    "title": "to_json raises 'Unhandled numpy dtype 15' with complex values.",
    "body": "#### Code Sample, a copy-pastable example if possible\n\nimport pandas\ndf = pandas.DataFrame({'a': [complex(3, -1)]})\ndf.to_json()\n#### Expected Output\n\n'{\"a\":{\"0\":3-1j}}'\n#### Actual output:\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "IO JSON",
      "Complex",
      "Needs Discussion",
      "Dtypes"
    ],
    "comments": [
      "I think the fundamental problem here is that JSON has no native way to represent complex values. I suppose we could encode complex values in some way here (e.g., by converting to strings or dictionaries with keys real/imaginary), but I'm not sure how useful that would be.\n",
      "this is not tested, and AFAIK not in the JSON standard. What would JSON output look like for this?\n",
      "Given that json is, after all, _javascript_ object notation, one choice might be to implicitly rely on a javascript library that handles complex numbers. Three candidates are [math.js](http://mathjs.org/),  [numbers.js](https://github.com/numbers/numbers.js) and [numericjs](https://github.com/sloisel/numeric/). \n\nmath.js and numbers.js are Apache licensed. numericjs is MIT licensed.\n\nnumbers.js depends on node.js. I did not find lists of dependencies for math.js. I did find a statement that numericjs has no dependencies.\n\nmath.js names complex values as Complex objects, but with a lowercase math.complex as a factory ([documentation](https://github.com/josdejong/mathjs/blob/master/docs/datatypes/complex_numbers.md)):\n\n```\nvar a = math.complex(2, 3);     // Complex 2 + 3i\na.re;                           // Number 2\na.im;                           // Number 3\n\nvar b = math.complex('4 - 2i'); // Complex 4 - 2i\nb.re = 5;                       // Number 5\nb;                              // Complex 5 - 2i\n```\n\nnumbers.js names complex values as Complex objects ([test file](https://github.com/numbers/numbers.js/blob/master/test/complex.test.js)):\n\n```\n    var A = new Complex(3, 4);\n```\n\nThe documentation is sparse. \n\nnumericjs names complex values as x,y coordinates ([documentation](https://github.com/sloisel/numeric/blob/master/src/documentation.html)):\n\n```\nIN> z = new numeric.T(3,4);\nOUT> {x: 3, y: 4}\n```\n\nOn cursory examination of the repos, math.js seems the most mature.\n\nSo my proposal would be to actually expect the output:\n'{\"a\":{\"0\":math.complex(3, -1)}}'\n\nOnce math.js has been imported, then that json becomes interpretable in javascript, but that's a side concern for pandas.\n\nIt also seems relatively clear for human consumption, and relatively easy to implement in python deserialization. The major downside I see is that any naive interpreter (e.g. a json validator) will fail.\n\nGiven that there is a downside, perhaps this could be implemented as an option to to_json, with a default of turning these values into strings, perhaps with a warning? E.g. \n\n```\n'{\"a\":{\"0\":\"math.complex(3, -1)\"}}'\nWarning: Serializing numeric complex value as a string. Consider using the complex=True option.\n```\n\nor \n\n```\n'{\"a\":{\"0\":\"3-1j\"}}'\n```\n\nThe first of those would be easier to DTRT and robustly re-serialize as a complex value anyway.\n",
      "(grumble... why does github put \"Close and comment\" next to \"Comment\", instead of e.g. putting \"Preview\" there, and putting \"Close and Comment\" on the other side or whatever?) Sorry.\n",
      "@gregory-marton I think you are confusing the `JSON` spec and `javascript`. These are not the same thing. JSON is a platform independent way of representing things. See the spec [here](https://en.wikipedia.org/wiki/JSON). \n",
      "see #12213 for some related commentary.\n",
      "@jreback not confusing; intentionally confounding as a way of getting to a readable and sensible standard proposal.\n",
      "In particular, especially in the case of stringifying the value, notating it as \"math.complex(3, -1)\" makes the intent very clear, and confusion with something else (e.g. a version string) unlikely. I look to javascript on a historical basis, and because using an existing library's notation might aid buy-in.\n",
      "@gregory-marton Even in JavaScript, it's [a bad idea](http://stackoverflow.com/questions/1843343/json-parse-vs-eval) to use `eval` rather than `JSON.parse`. No matter how we handle complex values, `DataFrame.to_json` needs to always generate valid JSON (or error).\n",
      "@gregory-marton  Now that I read your reply more carefully, I see that you do propose using strings. That seems fine to me. I would definitely prefer `\"3 - 1j\"` to `math.complex(3, -1)` because the former can be more easily parsed in different languages (e.g., in Python).\n",
      "@shoyer fair enough. Non-string proposal withdrawn.\n",
      "@shoyer the downside I see with using just '3-1j' or '3 - 1j' is that those are not crazy strings to have in other contexts. Consider a product code or version string. That gets worse when you allow '3.2-1.j' etc. It would be safer to automatically make the decision to parse \"math.complex(3, -1)\" as a complex value in python than it would be to parse \"3-1j\" that way.\n\nAs for requiring spaces, I know that python is the language of significant whitespace, but that's not a sort of whitespace that I would imagine standing out to a casual reader as required.\n",
      "@gregory-marton well that's the issue with JSON its type-less and you have to infer things. You can have a look at how / if there are already any standards for how to represent complex numbers (e.g. for example in dates there are 2 competing ones, an integer epoch and as an ISO string).\n",
      "@jreback Good point! I should have looked. [math.js's complex numbers documentation](http://mathjs.org/docs/datatypes/complex_numbers.html) has a json section. They encode complex values as\n\n```\n{mathjs: 'Complex', re: number, im: number}\n```\n\nThat certainly seems like an option, as dictionaries are not allowed in DataFrames (right?)\n\nSo then the desired output would be\n\n```\n'{\"a\":{\"0\":{\"mathjs\":\"Complex\",\"re\":3,\"im\":-1}}}'\n```\n",
      "```\nIn [6]: df = pandas.DataFrame({'a': [complex(3, -1)]})\n\nIn [11]: df['a'] = df['a'].apply(lambda x: {'mathjs' : 'Complex', 're' : x.real, 'im' : x.imag})\n\nIn [12]: df.to_json()\nOut[12]: '{\"a\":{\"0\":{\"mathjs\":\"Complex\",\"im\":-1.0,\"re\":3.0}}}'\n```\n",
      "@jreback well, there goes that idea, then. Other suggestions? \n\nOr is that a special case pandas would be willing to live with?\n",
      "well _you_ can certainly do this. \n\nI think the default should be to stringify (like we do for everything else that cannot be handled). The issue here is that `.to_json()` knows its a numpy array, but doesn't know what to do with it. e.g. its sensible to:\n\n```\nIn [13]: str(complex(3, -1))\nOut[13]: '(3-1j)'\n```\n\nfor each element.\n",
      "So for json.dumps, I can provide a cls=... to specify how I want stuff converted if there is no default. That's analogous to the default_handler option to `.to_json`\n\nSo I'm trying to write an encoder that will dtrt without my having to apply the above transformation separately. So my first guess is:\n\n```\nimport json\nclass MyEncoder(json.JSONEncoder):\n  # disable method-hidden because https://github.com/PyCQA/pylint/issues/414\n  def default(self, obj): # pylint: disable=method-hidden\n    try:\n      if hasattr(obj, 'to_json'):\n        if 'default_handler' in obj.to_json.__code__.co_varnames:\n          return obj.to_json(default_handler=MyEncoder)\n        else:\n          return obj.to_json()\n      if hasattr(obj, 'toJSON'):\n        return obj.toJSON()\n      if hasattr(obj,'to_dict'):\n        return obj.to_dict()\n      if isinstance(obj, complex):\n        return {'mathjs' : 'Complex', 're' : x.real, 'im' : x.imag}\n        # See discussion at https://github.com/pydata/pandas/issues/12554\n      return json.JSONEncoder.default(self, obj)\n    except Exception as e:\n      return repr(e)\n\nfrom pandas import DataFrame\ndf = DataFrame({'a': [1, 2.3, complex(4, -5)],\n                'b': [float('nan'), None, 'N/A']})\n\nassert ('{\"a\":{\"0\":1.0,\"1\":2.3,\"2\":{\"mathjs\":\"Complex\",\"re\":4,\"im\":-5}},'\n        '\"b\":{\"0\":null,\"1\":null,\"2\":\"N\\/A\"}}') == df.to_json(default_handler=MyEncoder)\n```\n\nBut I still get \n\n```\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-17-5adab7ea3faf> in <module>()\n      1 assert ('{\"a\":{\"0\":1.0,\"1\":2.3,\"2\":{\"mathjs\":\"Complex\",\"re\":4,\"im\":-5}},'\n----> 2         '\"b\":{\"0\":null,\"1\":null,\"2\":\"N\\/A\"}}') == df.to_json(default_handler=MyEncoder)\n\n.../lib/python2.7/site-packages/pandas/core/generic.pyc in to_json(self, path_or_buf, orient, date_format, double_precision, force_ascii, date_unit, default_handler)\n    892             force_ascii=force_ascii,\n    893             date_unit=date_unit,\n--> 894             default_handler=default_handler)\n    895 \n    896     def to_hdf(self, path_or_buf, key, **kwargs):\n\n.../lib/python2.7/site-packages/pandas/io/json.pyc in to_json(path_or_buf, obj, orient, date_format, double_precision, force_ascii, date_unit, default_handler)\n     33             obj, orient=orient, date_format=date_format,\n     34             double_precision=double_precision, ensure_ascii=force_ascii,\n---> 35             date_unit=date_unit, default_handler=default_handler).write()\n     36     else:\n     37         raise NotImplementedError(\"'obj' should be a Series or a DataFrame\")\n\n.../lib/python2.7/site-packages/pandas/io/json.pyc in write(self)\n     76             date_unit=self.date_unit,\n     77             iso_dates=self.date_format == 'iso',\n---> 78             default_handler=self.default_handler)\n     79 \n     80 \n\nRuntimeError: Unhandled numpy dtype 15\n```\n\nThoughts on next steps? I understand applying the function manually is a workaround. But it would be nice if default_handler could apply when a numpy dtype happens to be unhandled.\n\nBut also, I'm somewhat new to this, and may not have gotten my guess right.\nThanks in advance!\n",
      "this is a bit tricky. The problem is that since this is a numpy dtype it goes down a certain path (in the c-code), then can't figure out what to do and raises an error (and the default-handler is not called). So that's a bug, that when fixed will allow the default_handler to succeed. \n",
      "Should I file that bug separately, or will this one serve?\n",
      "Perhaps I should file that with [numpy](https://github.com/numpy/numpy/issues), rather than pandas?\n",
      "has nothing to do with numpy, code is [here](https://github.com/pydata/pandas/blob/master/pandas/src/ujson/python/objToJSON.c)\n",
      "Perhaps it should omit raising the RuntimeError, just return JT_INVALID, and it should be [write](https://github.com/pydata/pandas/blob/master/pandas/io/json.py#L71)'s responsibility to check the return code and invoke the default_handler if available, which should raise its own error, or if there is no default_handler, to raise a RuntimeError then? I think I could write that patch.\n",
      "yep exactly. rather it should return a code that allows it to fall out of that loop, then treat it like an iterable (not 100% sure if that would work, but give it a try). Once the iterable has it it should call the defaultHandler\n\nI misread a bit of what you pointed to. This should ALL be in the c-code. That's where everything is handled.\n",
      "Thank you for misunderstanding me well.\n\nJust removing the RuntimeError creates interesting json:\n\n```\n'\"{\\\\\"a\\\\\":{\\\\\"0\\\\\":,\\\\\"1\\\\\":,\\\\\"2\\\\\":},\\\\\"b\\\\\":{\\\\\"0\\\\\":null,\\\\\"1\\\\\":null,\\\\\"2\\\\\":\\\\\"N\\\\\\\\/A\\\\\"}}\"'\n```\n\nThat's a json string representing a python string that contains a stringified dictionary, with the first column replaced by something very strange.\n\nBut of course one needs to do more. This is as far as I got today: \n\n```\n--- a/pandas/io/tests/test_json/test_pandas.py\n+++ b/pandas/io/tests/test_json/test_pandas.py\n@@ -815,11 +815,38 @@ DataFrame\\\\.index values are different \\\\(100\\\\.0 %\\\\)\n\n     def test_default_handler(self):\n         value = object()\n-        frame = DataFrame({'a': ['a', value]})\n-        expected = frame.applymap(str)\n+        frame = DataFrame({'a': [7, value]})\n+        expected = DataFrame({'a': [7, str(value)]})\n         result = pd.read_json(frame.to_json(default_handler=str))\n         assert_frame_equal(expected, result, check_index_type=False)\n\n+    def test_default_handler_with_json_dumps(self):\n+        import json\n+        class PlausibleHandler(json.JSONEncoder):\n+            # disable method-hidden because\n+            # https://github.com/PyCQA/pylint/issues/414\n+            def default(self, obj): # pylint: disable=method-hidden\n+                try:\n+                    if hasattr(obj, 'to_json'):\n+                        optargs = obj.to_json.__code__.co_varnames\n+                        if 'default_handler' in optargs:\n+                            return obj.to_json(default_handler=PlausibleHandler)\n+                        else:\n+                            return obj.to_json()\n+                    if isinstance(obj, complex):\n+                        return {'mathjs' : 'Complex',\n+                                're' : x.real, 'im' : x.imag}\n+                        # https://github.com/pydata/pandas/issues/12554\n+                    return json.JSONEncoder.default(self, obj)\n+                except Exception as e:\n+                    return repr(e)\n+        df_list = [ 9, DataFrame({'a': [1, 2.3, complex(4, -5)],\n+                                  'b': [float('nan'), None, 'N/A']})]\n+        expected = ('[9,{\"a\":{\"0\":1.0,\"1\":2.3,'\n+                    '\"2\":{\"mathjs\":\"Complex\",\"re\":4,\"im\":-5}},'\n+                    '\"b\":{\"0\":null,\"1\":null,\"2\":\"N\\/A\"}}]')\n+        self.assertEqual(expected, json.dumps(df_list, cls=PlausibleHandler))\n+\n     def test_default_handler_raises(self):\n         def my_handler_raises(obj):\n             raise TypeError(\"raisin\")\ndiff --git a/pandas/src/ujson/python/objToJSON.c b/pandas/src/ujson/python/objToJSON.c\nindex dcb509b..cbabb11 100644\n--- a/pandas/src/ujson/python/objToJSON.c\n+++ b/pandas/src/ujson/python/objToJSON.c\n@@ -544,12 +544,8 @@ static int NpyTypeToJSONType(PyObject* obj, JSONTypeContext* tc, int npyType, vo\n     return *((npy_bool *) value) == NPY_TRUE ? JT_TRUE : JT_FALSE;\n   }\n\n-  PRINTMARK();\n-  PyErr_Format (\n-      PyExc_RuntimeError,\n-      \"Unhandled numpy dtype %d\",\n-      npyType);\n-  return JT_INVALID;\n+  PRINTMARK();  // GREMIO\n+  return JT_OBJECT;\n }\n\n\n@@ -1834,11 +1830,15 @@ void Object_beginTypeContext (JSOBJ _obj, JSONTypeContext *tc)\n\n   if (enc->npyType >= 0)\n   {\n-    PRINTMARK();\n-    tc->prv = &(enc->basicTypeContext);\n-    tc->type = NpyTypeToJSONType(obj, tc, enc->npyType, enc->npyValue);\n-    enc->npyType = -1;\n-    return;\n+    int jt_type = NpyTypeToJSONType(obj, tc, enc->npyType, enc->npyValue);\n+    if (jt_type != JT_INVALID) // GREMIO\n+    {\n+      PRINTMARK();\n+      tc->prv = &(enc->basicTypeContext);\n+      tc->type = jt_type;\n+      enc->npyType = -1;\n+      return;\n+    }\n   }\n\n   if (PyBool_Check(obj))\n```\n\nI verified that the test fails the way it should without changes.\n\nWith the shown set of changes, it gives me a segfault, which I would have started debugging, but [el capitan is stupid](http://stackoverflow.com/questions/33162757/how-to-install-gdb-debugger-in-mac-osx-el-capitan). Tomorrow I'll try on a different machine where debugging isn't quite such a pita. In the meantime, I'd love some early feedback.\n",
      "cc @Komnomnomnom\n",
      "So where I got with this today is that I have the default handler getting called for each element of the bad-dtype column. Unfortunately it's getting called with the whole DataFrame, instead of with the individual cell value. Even then, I'd like the dispatch only to go to the defaultHandler if the type is not one of the ones I can handle internally, so that remains to be done too. If one of you knows how I can get the individual value as a PyObject instead of getting the whole df, that'd be a great hint.\n\n[patch.txt](https://github.com/pydata/pandas/files/168472/patch.txt)\n",
      "I feel like what I want is:\n\n```\nPyObject *PyArray_GETITEM(PyArrayObject* arr, void* itemptr)\nGet a Python object from the ndarray, arr, at the location pointed to by itemptr. Return NULL on failure.\n```\n\nbut I'm not sure where to get my hands on the relevant PyArrayObject instead of the whole dataframe.\n",
      "I left in a huff last time after having the following conversation with gdb:\n\n```\n(gdb) p enc->outputFormat\n$19 = 4 \n(gdb) n\n2282          pc->iterNext = PdBlock_iterNext;\n(gdb) p enc->outputFormat\n$20 = 4 \n(gdb) n\n2283          pc->iterGetName = PdBlock_iterGetName;\n(gdb) p enc->outputFormat\n$21 = 1207959552\n(gdb) p pc\n$24 = (TypeContext *) 0x7ffffffe99a0\n(gdb) p &enc\n$25 = (PyObjectEncoder **) 0x7ffffffe99b0\n(gdb) p &(pc->iterNext)\n$28 = (JSPFN_ITERNEXT *) 0x7ffffffe99b0\n```\n\nI have otherwise made no significant further progress. I'm going to stop working on this for awhile. If someone wants to jump in with some explanation of the right way to approach the desired behavior, I might take another shot, but stuff like the above is just ... discouraging.\n\nOut of curiosity, how current and how strong are the use cases that drive the decision to write this in C? In what client contexts are serialization and deserialization the most performance-sensitive tasks, more important than flexibility (and perhaps correctness, if the above is an indication)? Is there a considered design behind the structure of the C code (long functions, use of goto, non-use of [boost](http://www.boost.org/doc/libs/1_60_0/libs/python/doc/html/tutorial/tutorial/object.html)), and if so, where can I read about it?\n",
      "@gregory-marton that's why I cc @Komnomnomnom . He is the original author. The style is typical for a c-style when its pretty low level like this. This is how numpy does lots of things, the gotos are almost all for error handling. Further you can't use much newer type things (e.g. boost). These things I believe all compile under c99.\n\nAs far as perf. This is of course why its written in c. In fact this is a pretty big bottleneck in general for JSON. \n\nAs far as correctness, this is simply an unsupported feature (as are a couple of other things). The `default_handler` is supposed to work for this and that's the bug.\n",
      "Correctness: the surprising overlap of memory addresses is not something I've diagnosed, and it may in fact be related to one of my attempts to implement the requested feature, and it may even be working as intended. But cryptic behavior of that sort may make it very easy indeed to introduce bugs, if in fact the interaction doesn't point to one already. \n\nSome of the gotos appear to be for ITERABLES processing, which I suspect could be delineated more clearly, and which is precisely where my understanding fell down. One could clarify the code without sacrificing performance by breaking up large functions into more digestible pieces with clearer flow and more naming to guide a new developer regarding intent.\n\nRe: perf and language, I didn't realize that C99 support ([and perhaps beyond](http://docs.scipy.org/doc/numpy/reference/c-api.coremath.html)) was an explicit pandas goal, sorry. I don't want to derail this ticket with further discussion. Thanks.\n\nThank you for cc'ing @Komnomnomnom. I look forward to any input.\n",
      "@gregory-marton the c code is the ujson library highly customised / subverted to iterate through numpy and pandas objects. Because it's trying to deal directly with c datatypes whilst sticking to the ujson logic things get a little... tricky.\n\nDealing with the numpy C data types directly at the top of that function is deliberate and it's unwise to let them be handled further down the function. That said it should be straightforward to dispatch to the default handler for unsupported dtypes. Happy to take a look if you're fed up with it, mind if I nab your test code?\n",
      "Please do! I put my work-in-progress, including tests, into the patchfile at the bottom of comment https://github.com/pydata/pandas/issues/12554#issuecomment-195185737\n",
      "@gregory-marton just added some JSON fixes (#12802, #12878) which should take care of this once merged. \n\nAlso fyi have a look at `pandas.io.json.dumps`. It should serialise most standard, numpy or pandas objects and you can avoid having to hack around with the standard `json` lib, e.g.\n\n``` python\nIn [31]: lst = [ 9, DataFrame({'a': [1, 2.3, (4, -5)], 'b': [float('nan'), None, 'N/A']}), np.array([1,2,3])]\n\nIn [32]: pandas.io.json.dumps(lst)\nOut[32]: '[9,{\"a\":{\"0\":1,\"1\":2.3,\"2\":[4,-5]},\"b\":{\"0\":null,\"1\":null,\"2\":\"N\\\\/A\"}},[1,2,3]]'\n```\n\nHopefully this will be exposed at the top level at some point (#9147)\n"
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "closed",
      "reopened",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned"
    ],
    "changed_files": 4,
    "additions": 91,
    "deletions": 45,
    "changed_files_list": [
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/io/tests/json/test_pandas.py",
      "pandas/src/ujson/python/objToJSON.c"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12558,
    "reporter": "nickeubank",
    "created_at": "2016-03-08T03:42:59+00:00",
    "closed_at": "2016-03-12T17:46:32+00:00",
    "resolver": "kawochen",
    "resolved_in": "f9e50690418672b34cfd314e5f60132eeafa7727",
    "resolver_commit_num": 46,
    "title": "BUG?: value_counts(normalize=True) normalizes over all observations including NaN.",
    "body": "By default, `value_counts` ignores missing values. To get them displayed, one must add the option `dropna=False`. But when one normalizes, they are entering into the denominator even when `dropna=True`. \n#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\npd.show_versions()\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.4.4.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.3.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.17.1\nnose: 1.3.7\npip: 8.1.0\nsetuptools: 20.2.2\nCython: 0.23.4\nnumpy: 1.10.4\nscipy: 0.16.1\nstatsmodels: None\nIPython: 4.0.1\nsphinx: 1.3.1\npatsy: 0.4.0\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.4.4\nmatplotlib: 1.4.3\nopenpyxl: 2.2.6\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.7.7\nlxml: 3.4.4\nbs4: 4.4.1\nhtml5lib: 0.999\n None\napiclient: None\nsqlalchemy: 1.0.9\npymysql: None\npsycopg2: None\nJinja2: 2.8\n",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "Numeric",
      "Effort Low"
    ],
    "comments": [
      "yeh, looks like its not respecting `dropna`\n\n```\nIn [3]: s.value_counts(normalize=True,dropna=False)\nOut[3]: \nNaN     0.500000\n 3.0    0.166667\n 2.0    0.166667\n 1.0    0.166667\ndtype: float64\n\nIn [4]: s.value_counts(normalize=True,dropna=True)\nOut[4]: \n3.0    0.166667\n2.0    0.166667\n1.0    0.166667\ndtype: float64\n```\n"
    ],
    "events": [
      "renamed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 4,
    "additions": 32,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/algorithms.py",
      "pandas/core/groupby.py",
      "pandas/tests/test_algos.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12564,
    "reporter": "pganssle",
    "created_at": "2016-03-08T22:29:11+00:00",
    "closed_at": "2016-04-03T14:06:00+00:00",
    "resolver": "facaiy",
    "resolved_in": "ad8ade8d8bfe87d7d82e7cbecfc1dd88c54d77a1",
    "resolver_commit_num": 0,
    "title": "Categorical equality check raises ValueError in DataFrame",
    "body": "Apparently there's an issue when comparing the equality of a scalar value against a categorical column as part of a DataFrame. In the example below, I'm checking against `-np.inf`, but comparing to a string or integer gives the same results.\n\nThis raises `ValueError: Wrong number of dimensions`.\n#### Code Sample\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Bug",
      "Indexing",
      "Difficulty Novice",
      "Categorical",
      "Effort Low"
    ],
    "comments": [
      "here's a simpler example. Yeah I suppose this should work. Note that comparing a vs a DataFrame is not typically useful. You almost always compare against a Series (and then inddex).\n\npull-requests are welcome.\n\n```\nIn [23]: df = DataFrame({'A' : ['foo','bar','baz']})\n\nIn [24]: df['B'] = df['A'].astype('category')\n\nIn [25]: df['A'] == 'foo'\nOut[25]: \n0     True\n1    False\n2    False\nName: A, dtype: bool\n\nIn [26]: df['B'] == 'foo'\nOut[26]: \n0     True\n1    False\n2    False\nName: B, dtype: bool\n\nIn [27]: df[['A']] == 'foo'\nOut[27]: \n       A\n0   True\n1  False\n2  False\n\nIn [28]: df[['B']] == 'foo'\nValueError: Wrong number of dimensions\n```\n",
      "My use case, if you want to know, was that I just wanted to discard rows with `np.inf` in any column (though you could imagine the same thing with 0 or something). I find it easier and more readable to just broadcast the comparison across the entire DataFrame, even though logically speaking I know that the comparison really only needs to be applied to the float columns.\n",
      "``` python\n>>> df[['A']].ndim\n2\n>>> df[['A']]._data.blocks[0].values\narray([['foo', 'bar', 'baz']], dtype=object)\n>>> df[['A']]._data.blocks[0].values.ndim\n2\n>>> df[['B']].ndim\n2\n>>> df[['B']]._data.blocks[0].values\n[foo, bar, baz]\nCategories (3, object): [bar, baz, foo]\n>>> df[['B']]._data.blocks[0].values.ndim\n1\n```\n\ninteresting, \nI think that's why pandas raises `ValueError: Wrong number of dimensions.`\nright?\n",
      "@ningchi that is only a manifestation of the issue, not the cause. `CategoricalBlocks` only hold a 1-d structure. The comparison path goes thru `core/internals/Block/eval`.\n\nyou can prob get away with changing this:\n\n`transf = (lambda x: x.T) if is_transposed else (lambda x: x)`\n\nto something like\n\n```\ndef transf(x):\n     transposer = lambda x: x.T if is_transposed: lambda x: x\n     return lambda x: _block_shape(transposer(x), ndim=self.ndim)\n```\n",
      "Thanks, @jreback\n\nbecause `CategoricalBlocks` only hold a 1-d structure, I have no idea how to extend its `ndim`, except using `to_dense()` like `NonConsolidatableMixIn.get_values`.\n\nOn the another hand, is it strange to convert `self.dim` to `self.value.dim`?  as we expect a Dataframe, rather than a Series.\n",
      "did you try the code above?\n",
      "@jreback Sorry, I didn't test your suggestion completely. Many thanks.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 29,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/internals.py",
      "pandas/tests/indexing/test_categorical.py",
      "pandas/tests/test_categorical.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12572,
    "reporter": "ctyler-wapiti",
    "created_at": "2016-03-09T16:57:09+00:00",
    "closed_at": "2016-03-11T01:39:04+00:00",
    "resolver": "tonypartheniou",
    "resolved_in": "71b7237f233824deae249b26bf98fdb6d9ae26a3",
    "resolver_commit_num": 1,
    "title": "SignedJwtAssertionCredentials has been removed from oauth2client version 2.00 breaking read_gbq",
    "body": "In pandas/io/gbq.py there is the function:\n\n`def _test_google_api_imports():\n\n\n\nThis will fail for users with oauth2client version higher than 2.0.0 since SignedJwtAssertionCredentials  has been removed from the module. \n\nGoogle suggests other ways of handling service account credentials. See:\n#v200\n",
    "labels": [
      "Google I/O"
    ],
    "comments": [
      "can you `pd.show_versions()`\n",
      "cc @tworec \ncc @parthea \ncc @andrewryno\ncc @andrioni \ncc @jacobschaer\n",
      "> > > pd.show_versions()\n\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Windows\nOS-release: 8.1\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.18.0rc1\nnose: 1.3.7\npip: 8.1.0\nsetuptools: 20.2.2\nCython: None\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: None\nxarray: None\nIPython: None\nsphinx: None\npatsy: None\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: None\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: 0.9.2\napiclient: 1.3.1\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\n"
    ],
    "events": [
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "labeled",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "milestoned",
      "referenced",
      "referenced"
    ],
    "changed_files": 7,
    "additions": 74,
    "deletions": 12,
    "changed_files_list": [
      "ci/requirements-2.7.pip",
      "ci/requirements-3.4.pip",
      "doc/source/install.rst",
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.18.0.txt",
      "pandas/io/gbq.py",
      "pandas/io/tests/test_gbq.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12574,
    "reporter": "chris-b1",
    "created_at": "2016-03-09T18:12:26+00:00",
    "closed_at": "2016-03-16T13:59:42+00:00",
    "resolver": "drewfustin",
    "resolved_in": "a5670f2043251b4c9bd7f80573f7983722b4b2a8",
    "resolver_commit_num": 0,
    "title": "BUG: Series with Categorical and dtype",
    "body": "xref #discussion_r55561598\n\n\n",
    "labels": [
      "Bug",
      "Reshaping",
      "Difficulty Novice",
      "Categorical",
      "Effort Low"
    ],
    "comments": [
      "@jreback I'm new to contributing to OS, let alone pandas. I think this is the fix that you're referencing. Let me know if not and I'll fix.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 18,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/series.py",
      "pandas/tests/series/test_constructors.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12577,
    "reporter": "nickeubank",
    "created_at": "2016-03-09T21:57:10+00:00",
    "closed_at": "2016-03-16T13:17:21+00:00",
    "resolver": "OXPHOS",
    "resolved_in": "f7faee0865d682c81f07134570b863e7e1d75f85",
    "resolver_commit_num": 0,
    "title": "BUG: Crosstab margins ignoring dropna",
    "body": "`crosstab` also has a bug -- it counts np.nan in margin totals even when `dropna=True`.\n\nAppears independent of #12569 and #4003\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.4.4.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.3.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.17.1\nnose: 1.3.7\npip: 8.1.0\nsetuptools: 20.2.2\nCython: 0.23.4\nnumpy: 1.10.4\nscipy: 0.16.1\nstatsmodels: None\nIPython: 4.0.1\nsphinx: 1.3.1\npatsy: 0.4.0\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.4.4\nmatplotlib: 1.4.3\nopenpyxl: 2.2.6\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.7.7\nlxml: 3.4.4\nbs4: 4.4.1\nhtml5lib: 0.999\n None\napiclient: None\nsqlalchemy: 1.0.9\npymysql: None\npsycopg2: None\nJinja2: 2.8\n",
    "labels": [
      "Bug",
      "Missing-data",
      "Reshaping",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "Shouldn't the expected output be?\n\n```\nOut[233]: \nb    3  4  All\na             \n1.0  1  0    1\n2.0  1  3    4\nAll  2  3    5\n```\n",
      "@sakulkar how is that different from issue report? Looks like we're saying same thing. \n",
      "I edited the top @nickeubank (it was incorrect)\n",
      "Oops thanks\n"
    ],
    "events": [
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented"
    ],
    "changed_files": 3,
    "additions": 68,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/tools/pivot.py",
      "pandas/tools/tests/test_pivot.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12591,
    "reporter": "xflr6",
    "created_at": "2016-03-11T09:58:24+00:00",
    "closed_at": "2016-03-17T13:48:14+00:00",
    "resolver": "xflr6",
    "resolved_in": "1893ffd3e97303a77b30ee05ab4e0033caa80b6a",
    "resolver_commit_num": 1,
    "title": "API: rename Index.sym_diff to Index.symmetric_difference",
    "body": "Following builtin `set`, I would expect the method name to be `symmetric_difference`.\n\nHow about deprecating `sym_diff` in favour of this (see also #6016, #8226)?\n",
    "labels": [
      "API Design",
      "Difficulty Novice",
      "Effort Low",
      "Deprecate"
    ],
    "comments": [
      "@xflr6, well said, We should have included `sym_diff` -> `symmetric_difference` along with the `diff` -> `difference` change.\n\nWould you be interested in submitting a pull requests to deprecate `sym_diff` in favor of `symmetric_difference`?\n",
      "Sounds manageable, sure.\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "unlabeled",
      "milestoned",
      "commented",
      "labeled",
      "cross-referenced"
    ],
    "changed_files": 6,
    "additions": 28,
    "deletions": 20,
    "changed_files_list": [
      "doc/source/api.rst",
      "doc/source/indexing.rst",
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/indexes/base.py",
      "pandas/tests/indexes/common.py",
      "pandas/tests/indexes/test_base.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12599,
    "reporter": "TulyOpt",
    "created_at": "2016-03-11T20:13:35+00:00",
    "closed_at": "2016-05-23T13:05:43+00:00",
    "resolver": "gliptak",
    "resolved_in": "f8a11ddccd98c4da3366294cc7ffd2d82ffb4106",
    "resolver_commit_num": 26,
    "title": "Error : Invalid type promotion",
    "body": "#### Code Sample\n\nimport pandas as pd\n\ndef main():\n\n\n\nif **name** == \"**main**\":\n    main()\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Windows\nOS-release: 8.1\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 61 Stepping 4, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.17.1\nnose: 1.3.7\npip: 8.0.2\nsetuptools: 19.6.2\nCython: 0.23.4\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: 0.6.1\nIPython: 4.0.3\nsphinx: 1.3.1\npatsy: 0.4.0\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.4.6\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.8.4\nlxml: 3.5.0\nbs4: 4.4.1\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.11\npymysql: None\npsycopg2: None\nJinja2: 2.8\n",
    "labels": [
      "Bug",
      "Indexing",
      "Dtypes",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "yeah, prob not the most friendly. Note that you should not be doing this generally. A Series is a single dtyped object.\n\npull-requests to fix welcome\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 46,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/core/indexing.py",
      "pandas/tests/series/test_indexing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12610,
    "reporter": "chinskiy",
    "created_at": "2016-03-13T17:11:38+00:00",
    "closed_at": "2016-03-25T13:30:15+00:00",
    "resolver": "chinskiy",
    "resolved_in": "26c7d8d7e613890d680a4d4c77464c806facde27",
    "resolver_commit_num": 0,
    "title": "Bug: Pandas Series name attribute can be array",
    "body": "#### Code Sample\n\n\n#### Expected Output\n\nget error when result_df try to be created with array name attribute,\nor give error when try to create Series object with array argument in name attribute.\n#### output of `pd.show_versions()`\n\n---\n\ncommit: None\npython: 2.7.6.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.2.0-30-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_GB.UTF-8\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.1.0\nsetuptools: 20.2.2\nCython: 0.23.4\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.1.2\nsphinx: 1.3.6\npatsy: 0.4.1\ndateutil: 2.5.0\npytz: 2015.7\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.5.1\nopenpyxl: 2.3.3\nxlrd: 0.9.4\nxlwt: None\nxlsxwriter: 0.8.4\nlxml: 3.5.0\nbs4: 4.4.1\nhtml5lib: 1.0b8\n 0.9.2\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\njinja2: 2.8\nboto: None\n",
    "labels": [
      "Bug",
      "Compat",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "there was an old issue somewhat related: https://github.com/pydata/pandas/pull/9193\n\nbut this _should_ be caught. want to investigate and see where this is happening?\n",
      "Okay, I'll try.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 7,
    "additions": 41,
    "deletions": 7,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/generic.py",
      "pandas/core/series.py",
      "pandas/tests/series/test_alter_axes.py",
      "pandas/tests/series/test_constructors.py",
      "pandas/tests/series/test_io.py",
      "pandas/tests/series/test_repr.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12617,
    "reporter": "pwaller",
    "created_at": "2016-03-14T08:17:11+00:00",
    "closed_at": "2016-04-10T14:26:28+00:00",
    "resolver": "sinhrks",
    "resolved_in": "0a2b7235eb728244556230b36a35f7d43b0dd6f3",
    "resolver_commit_num": 282,
    "title": "BUG: Index.str.partition gives ValueError while trying to compute index name (expand=False)",
    "body": "`Index.str.partition`'s behaviour changed in 3ab35b40274276e4f9b40918a7e260f1abb941c1 in a backwards incompatible way which doesn't seem intentional.\n#### Code Sample, a copy-pastable example if possible\n\n\n\n\n#### Expected Output\n\n\n#### Versions\n\nI bisected the problem to 3ab35b40274276e4f9b40918a7e260f1abb941c1 (cc @sinhrks). The problem is not present in 0.17.1 and is present in 0.18.0rc2. \n\nNote: when I use a `name=\"foo\"`, it happens to work because it has the correct length, the index name ends up being `[\"f\", \"o\", \"o\"]`, so it's possible this is tested somewhere and happens to work even though it's not correct for a large number of use cases.\n",
    "labels": [
      "Bug",
      "Strings",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "Thanks, and sorry for the inconvenience. I'll check soon.\n",
      "Tthis is not a regression. The `name` argument you were using in 0.17.1 didn't do anything (a `MultiIndex` does not have a `name`, only `names` for the levels), \n\nSo the result (w/o specifying name) is correct.\n\nThe `expand=False` case has a bug in it.\n\n```\nIn [50]: pandas.Index([\"a,b\", \"c,d\"], expand=True).str.partition(\",\")\nOut[50]: \nMultiIndex(levels=[[u'a', u'c'], [u','], [u'b', u'd']],\n           labels=[[0, 1], [0, 0], [0, 1]])\n\nIn [51]: pandas.Index([\"a,b\", \"c,d\"], expand=True).str.partition(\",\").names\nOut[51]: FrozenList([None, None, None])\n\nIn [52]: pd.__version__\nOut[52]: '0.18.0+7.g17a5982'\n```\n"
    ],
    "events": [
      "mentioned",
      "subscribed",
      "renamed",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "renamed",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 59,
    "deletions": 6,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/strings.py",
      "pandas/tests/test_strings.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12620,
    "reporter": "sinhrks",
    "created_at": "2016-03-14T10:56:01+00:00",
    "closed_at": "2016-03-17T14:17:46+00:00",
    "resolver": "sinhrks",
    "resolved_in": "eeb81f69a8fd4008fb2fb8e7a748ee1a8db38b55",
    "resolver_commit_num": 260,
    "title": "BUG: Concat with tz-aware and timedelta raises AttributeError",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\ncurrent master.\n\nIt is caused by the below line, we must check `typs` length must be 1, otherwise the result should be `object` dtype.\n\n#L278\n",
    "labels": [
      "Bug",
      "Reshaping",
      "Timedelta",
      "Timezones",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "hmm, I think these might be caused by the set operations; they require ONLY the specified dtypes, so I don't think this should even hit this paricular ``_compat_concat`, should be handled at a slightly higher level.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 4,
    "additions": 32,
    "deletions": 23,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/common.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tseries/common.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12622,
    "reporter": "wesm",
    "created_at": "2016-03-14T15:35:02+00:00",
    "closed_at": "2016-03-25T13:25:14+00:00",
    "resolver": "kawochen",
    "resolved_in": "33659fd9d24aa0210781b763bb13c45eae31d5ca",
    "resolver_commit_num": 47,
    "title": "BUG: pandas.Timestamp misbehaves in deeply nested pprint on Python 3",
    "body": "I'm not sure if we can fix this in pandas, but I ran into this recently:\n\n\n",
    "labels": [
      "Output-Formatting"
    ],
    "comments": [
      "not useful.......but\n\n```\nIn [4]: pprint.saferepr(d)\nOut[4]: \"{'bar': [{'what': {'footttttttttttttttttttttttttttttttttt': Timestamp('2011-01-01 00:00:00')}}, {'what': {'footttttttttttttttttttttttttttttttttt': Timestamp('2011-01-01 00:00:00')}}, {'what': {'footttttttttttttttttttttttttttttttttt': Timestamp('2011-01-01 00:00:00')}}, {'what': {'footttttttttttttttttttttttttttttttttt': Timestamp('2011-01-01 00:00:00')}}, {'what': {'footttttttttttttttttttttttttttttttttt': Timestamp('2011-01-01 00:00:00')}}, {'what': {'footttttttttttttttttttttttttttttttttt': Timestamp('2011-01-01 00:00:00')}}, {'what': {'footttttttttttttttttttttttttttttttttt': Timestamp('2011-01-01 00:00:00')}}, {'what': {'footttttttttttttttttttttttttttttttttt': Timestamp('2011-01-01 00:00:00')}}, {'what': {'footttttttttttttttttttttttttttttttttt': Timestamp('2011-01-01 00:00:00')}}, {'what': {'footttttttttttttttttttttttttttttttttt': Timestamp('2011-01-01 00:00:00')}}], 'foo': 1}\"\n```\n",
      "I think if we could define `_Timestamp` to actually use `__slots__`. http://stackoverflow.com/questions/10401935/python-method-wrapper-type\n\nbut not really sure how cython does this.\n"
    ],
    "events": [
      "cross-referenced",
      "commented",
      "labeled",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 65,
    "deletions": 45,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/tseries/tests/test_tslib.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12623,
    "reporter": "TomAugspurger",
    "created_at": "2016-03-14T19:38:08+00:00",
    "closed_at": "2016-03-17T14:06:48+00:00",
    "resolver": "TomAugspurger",
    "resolved_in": "b0d1e2a54653ee41ffd4659b7519a821466a369c",
    "resolver_commit_num": 34,
    "title": "BUG: Series.rename treats Series as list-like instead of mapping",
    "body": "\n\nProblem is [here](#L2336) using an `isinstance(,MutableMapping)` instead of `com.is_dict_like`. [`rename_axis`](#L705) should be fine, but will add tests for both.\n",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "Indexes are NOT dict_like though (Series ARE). so something else happening.\n\n```\nIn [3]: pandas.core.common.is_dict_like(pd.RangeIndex(10))\nOut[3]: False\n\nIn [4]: pandas.core.common.is_dict_like(pd.Index([1,2,3]))\nOut[4]: False\n```\n",
      "Ah.... my bad. I thought I didn't reproduce it with a non-RangeIndexed series, but I must have been mistaken.\n\n``` python\nIn [52]: s.rename(pd.Series({'a': 'A', 'b': 'B'}))\nOut[52]:\na    0\nb    1\nName: [A, B], dtype: int64\n```\n\nI'll update.\n",
      "somewhat related to #12610 (IOW setting the `.name` with a list should just fail), but you don't need to explicity check for this (well, you do but for a different reason, namely that you are doing something different with it)\n",
      "Updated, can't believe I missed this. Will have a fix tonight.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "renamed",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 55,
    "deletions": 22,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/generic.py",
      "pandas/core/series.py",
      "pandas/tests/series/test_alter_axes.py",
      "pandas/tests/test_generic.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12631,
    "reporter": "sinhrks",
    "created_at": "2016-03-15T10:33:30+00:00",
    "closed_at": "2016-03-31T13:38:14+00:00",
    "resolver": "sinhrks",
    "resolved_in": "9b90016c0690cdda961d57afeeefd0bf88926701",
    "resolver_commit_num": 267,
    "title": "API: Index.take inconsistently handle fill_value",
    "body": "Normal `Index` ignores `fill_value` (this is desribed in docstring)\n\n\n\nBut `DatetimeIndex` does (used in reshape ops at least). \n\n\n\nOtherwise `PeriodIndex` doesn't.\n\n\n\nSo I'd like to discuss:\n1. All Index should handle `fill_value`?\n2. Only `PeriodIndex` should handle `fill_value` to be compat with other datetime-likes.\n",
    "labels": [
      "Indexing",
      "API Design",
      "Period"
    ],
    "comments": [
      "this should be supported for any indexes which support NA (IOW not `RangeIndex/Int64Index`). So for sure `PeriodIndex` (I think `TimedeltaIndex` already works for this). You may need to override `Int64Index` and fix `Index` to make this work.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 14,
    "additions": 476,
    "deletions": 47,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/indexes/base.py",
      "pandas/indexes/category.py",
      "pandas/indexes/multi.py",
      "pandas/tests/indexes/test_base.py",
      "pandas/tests/indexes/test_category.py",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tests/indexes/test_numeric.py",
      "pandas/tests/indexes/test_range.py",
      "pandas/tseries/base.py",
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_timedeltas.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12642,
    "reporter": "jreback",
    "created_at": "2016-03-16T13:19:48+00:00",
    "closed_at": "2016-04-17T14:31:40+00:00",
    "resolver": "OXPHOS",
    "resolved_in": "3bed097b56db84891c891009c6bf57bb512040e5",
    "resolver_commit_num": 2,
    "title": "BUG: crosstab with margins=True and dropna=False",
    "body": "xref #12614 \n\n\n",
    "labels": [
      "Bug",
      "Missing-data",
      "Reshaping",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "also let's tests with a multi-index\n",
      "cc @OXPHOS \n",
      "Thanks. I like that test.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 65,
    "deletions": 5,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/tools/pivot.py",
      "pandas/tools/tests/test_pivot.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12644,
    "reporter": "gfyoung",
    "created_at": "2016-03-16T15:22:15+00:00",
    "closed_at": "2016-05-01T15:11:59+00:00",
    "resolver": "gfyoung",
    "resolved_in": "23eb483d17ce41c0fe41b0bfa72c90df82151598",
    "resolver_commit_num": 16,
    "title": "Compatibility issues with numpy's fromnumeric.py",
    "body": "A recent spate of issues/PR's stemming from calling functions defined in numpy's `fromnumeric.py` module <a href=\"\">here</a> that have identically-named but differently implemented methods/functions in `pandas` is indicative of a much larger compatibility issue between the two libraries with this module.  A thorough overview of all of the functions from the `fromnumeric.py` module and cross-referencing them to implementations in `pandas` is needed to avoid similar issues.\n\nRelevant PRs:\n#12413 (issue: #12238)\n#12603 (issue: #12600)\n#12638\n\n<a href=\"\">#7325</a> (from `numpy`)\n",
    "labels": [
      "API Design",
      "Compat"
    ],
    "comments": [
      "As mentioned in #12600, I'll tackle this as a follow-up once these PR's are landed.\n",
      "as discussed we basically have 2 classes of issues:\n- like `sorter` a seemingly innocuous argument that numpy needs, but pandas does not. So soln is now to pass thru, with no checks (and its still a named argument, currently passed via position from numpy). we should note in the doc-string this behavior.\n- like `.round,.idxmax,stat functions`. mainly the `out` argument which is not needed (and confusing to pandas). soln is to allow `**kwargs`, but check them for invalid args (to avoid misspellings and such). and raise if this particular arg is not `None` (in this case `out`).\n",
      "cc @mortada @jorisvandenbossche @shoyer @wesm \n",
      "IMHO we should not be striving to make pandas API compatible with NumPy (except offering an `__array__` API, of course), but we should avoid unnecessary / common conflicts if possible\n",
      "@wesm: Agreed.  I think in this case though trying to \"align\" the API with `numpy`'s makes sense because it should be perfectly legal for example to call either `np.searchsorted` or `Series.searchsorted` without Python blowing up on the user.\n",
      "@gfyoung numpy's behavior is a bug really, in that it shouldn't just call `_wrap_it` and assume everything is a sub-class (like it does). I know you are fixing that, so this is really for compat.\n",
      "@jreback : Right, I guess \"align\" gives the connotation that `pandas` is doing something wrong, when we're really just trying to \"accommodate\" `numpy`'s buggy API.\n",
      "Well, now `pandas` is not alone.  `numpy`'s close cousin `scipy` has these exact same compatibility issues <a href=\"https://github.com/scipy/scipy/issues/5987\">too</a> as I filed just now.\n",
      "My `numpy` PR has been merged.  So now (hopefully) we can just worry about backwards compatibility.\n",
      "maybe add `order` arg from `np.argsort` as well (IOW we could remove from pandas)\n",
      "prob could start with seeing which functions call `_wrap_it` on the numpy side here.\n",
      "A massive PR (#12810) addressing this issue is finally up.  There were _a lot_ more incompatibilities than I had expected.  Hopefully this PR should address almost if not all of them.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced"
    ],
    "changed_files": 50,
    "additions": 1820,
    "deletions": 269,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/__init__.py",
      "pandas/compat/numpy/__init__.py",
      "pandas/compat/numpy/function.py",
      "pandas/compat/pickle_compat.py",
      "pandas/core/base.py",
      "pandas/core/categorical.py",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/ops.py",
      "pandas/core/panel.py",
      "pandas/core/series.py",
      "pandas/indexes/base.py",
      "pandas/indexes/category.py",
      "pandas/indexes/multi.py",
      "pandas/indexes/range.py",
      "pandas/io/tests/test_date_converters.py",
      "pandas/io/tests/test_gbq.py",
      "pandas/sparse/array.py",
      "pandas/sparse/frame.py",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_array.py",
      "pandas/sparse/tests/test_frame.py",
      "pandas/sparse/tests/test_series.py",
      "pandas/tests/frame/test_analytics.py",
      "pandas/tests/frame/test_operators.py",
      "pandas/tests/indexes/common.py",
      "pandas/tests/indexes/test_base.py",
      "pandas/tests/indexes/test_category.py",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tests/series/test_analytics.py",
      "pandas/tests/test_algos.py",
      "pandas/tests/test_base.py",
      "pandas/tests/test_categorical.py",
      "pandas/tests/test_generic.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_util.py",
      "pandas/tseries/base.py",
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_base.py",
      "pandas/tseries/tests/test_converter.py",
      "pandas/tseries/tests/test_offsets.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tests/test_tslib.py",
      "pandas/tslib.pyx",
      "pandas/types/generic.py",
      "pandas/util/validators.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12647,
    "reporter": "raderaj",
    "created_at": "2016-03-16T20:44:32+00:00",
    "closed_at": "2016-04-22T15:09:57+00:00",
    "resolver": "kshedden",
    "resolved_in": "33683cc0be98b1b7c6708d214588d99101eaad04",
    "resolver_commit_num": 15,
    "title": "TypeError with pd.read_sas() (pandas 0.18.0)",
    "body": "#### Code Sample, a copy-pastable example if possible\n\ndf = pd.read_sas('infilename.sas7bdat')\n#### Expected Output\n\nto read a sas7bdat file into a pandas data frame.\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.1.0\nsetuptools: 20.2.2\nCython: 0.23.4\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.0.3\nsphinx: 1.3.5\npatsy: 0.4.0\ndateutil: 2.5.0\npytz: 2016.1\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.4.6\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.8.4\nlxml: 3.5.0\nbs4: 4.4.1\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.11\npymysql: None\npsycopg2: None\njinja2: 2.8\n## boto: 2.39.0\n### Error seen\n\nTypeError                                 Traceback (most recent call last)\n<ipython-input-9-3046a8f15981> in <module>()\n----> 1 pd.read_sas('mydatainfo.sas7bdat')\n\nC:\\Anaconda\\lib\\site-packages\\pandas\\io\\sas\\sasreader.py in read_sas(filepath_or_buffer, format, index, encoding, chunksize, iterator)\n     52         reader = SAS7BDATReader(filepath_or_buffer, index=index,\n     53                                 encoding=encoding,\n---> 54                                 chunksize=chunksize)\n     55     else:\n     56         raise ValueError('unknown SAS format')\n\nC:\\Anaconda\\lib\\site-packages\\pandas\\io\\sas\\sas7bdat.py in **init**(self, path_or_buf, index, convert_dates, blank_missing, chunksize, encoding)\n    234             self._path_or_buf = open(self._path_or_buf, 'rb')\n    235 \n--> 236         self._get_properties()\n    237         self._parse_metadata()\n    238 \n\nC:\\Anaconda\\lib\\site-packages\\pandas\\io\\sas\\sas7bdat.py in _get_properties(self)\n    333             self.os_name = buf.rstrip(b'\\x00 ').decode()\n    334         else:\n--> 335             buf = self._path_or_buf.read(_os_maker_offset, _os_maker_length)\n    336             self.os_name = buf.rstrip(b'\\x00 ').decode()\n    337 \n\nTypeError: read() takes at most 1 argument (2 given)\n\n---\n## Tracking down the error:\n\nIt looks like line 335 should either be set up to read a single length from the _path_or_buf bytestream something like:\n`buf = self._path_or_buf.read(_os_maker_length)` \nor replace it with something like this\n`buf = self._read_bytes(_os_maker_offset, _os_maker_length)`\n\n---\n\n*\\* note i tried this under python3.5 and got the same error.\n",
    "labels": [
      "IO SAS",
      "Bug"
    ],
    "comments": [
      "can you post a link to the file?\n",
      "cc @kshedden  \n",
      "Thanks for the report.  I think using read_bytes would resolve this.  But we don't have any test files that reach the `else` clause of this branch.  The code here traces back to line 1450 of Jared Hobbs' code linked below (but I introduced the bug when refactoring):\n\nhttps://bitbucket.org/jaredhobbs/sas7bdat/src/da1faa90d0b15c2c97a2a8eb86c91c58081bdd86/sas7bdat.py?fileviewer=file-view-default\n\nIf you could provide us with a test file that reaches `else` here, or give us a tip on where to find one that would be great.  I am generating all the test files on a linux/intel box with a recent version of SAS. \n",
      "@kshedden thanks. I found another small file that caused this error; but sas7bdat files are 'unsupported' by github issues. Should I create a branch and upload there?\n",
      "fix (and test case linked in comments) in #12658\n",
      "So this means the sas_read() method doesn't work at all on sas7bdat files, right?\n",
      "well @itzybitzy there is a linked issue which will close this. will be part of 0.18.1. IIRC there was issues with some encodings, but you can review the issues if you'd like. Of course any fixes welcome as well!\n",
      "The current release works on many sas7bdat files, there are around 20 in\nthe test suite that all work fine.  There was a minor bug that will be\nfixed for 0.18.1.\n\nOn Wed, Mar 30, 2016 at 3:31 PM, itzybitzy notifications@github.com wrote:\n\n> So this means the sas_read() method doesn't work at all on sas7bdat files,\n> right?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/pydata/pandas/issues/12647#issuecomment-203597085\n",
      "Thanks @kshedden. I still don't know much about how GitHub works and I'm not sure where to find the test suite. But I was asking because I was able to get the same error with a very simple data set, generated like this (in SAS):\n\n```\nlibname tmp 'c:\\temp';  \ndata tmp.test;\n    do i=1 to 100;\n        x=rannor(0);\n        output;\n    end;\nrun;\n```\n\nI'm wondering if this is causing `pd.read_sas()` to fail for the same reason, or if perhaps there's another bug somewhere.\n",
      "The SAS7BDAT file specification is not public, so we have no idea what\nissues might come up down the line.  The problems I am aware of are due to\nsome very minor differences in the headers, which encode meta-data about\nthe file and the machine used to generate the file.  We have fixed these in\na branch that should become part of 0.18.1.\n\nWe are also working on improving performance and hope to have a much faster\nversion of this ready soon.\n\nOn Thu, Mar 31, 2016 at 12:48 PM, itzybitzy notifications@github.com\nwrote:\n\n> Thanks @kshedden https://github.com/kshedden. I still don't know much\n> about how GitHub works and I'm not sure where to find the test suite. But I\n> was asking because I was able to get the same error with a very simple data\n> set, generated like this (in SAS):\n> \n> libname tmp 'c:\\temp';\n> data tmp.test;\n>     do i=1 to 100;\n>         x=rannor(0);\n>         output;\n>     end;\n> run;\n> \n> I'm wondering if this is causing pd.read_sas() to fail for the same\n> reason, or if perhaps there's another bug somewhere.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/pydata/pandas/issues/12647#issuecomment-204016922\n",
      "@itzybitzy if you'd like to save that file with SAS and provide it on a link, we could use it as an example (if its not already covered by the fixes that @kshedden have done).\n",
      "@jreback Here's the sas data set. (I saved it as a zip so I could upload it here.)\n\n[test.zip](https://github.com/pydata/pandas/files/198314/test.zip)\n"
    ],
    "events": [
      "commented",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "milestoned",
      "labeled",
      "referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 12,
    "additions": 2111,
    "deletions": 432,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/io/sas/sas7bdat.py",
      "pandas/io/sas/sas_constants.py",
      "pandas/io/sas/saslib.pyx",
      "pandas/io/tests/sas/data/airline.csv",
      "pandas/io/tests/sas/data/airline.sas7bdat",
      "pandas/io/tests/sas/data/productsales.csv",
      "pandas/io/tests/sas/data/productsales.sas7bdat",
      "pandas/io/tests/sas/data/test_12659.csv",
      "pandas/io/tests/sas/data/test_12659.sas7bdat",
      "pandas/io/tests/sas/test_sas7bdat.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12651,
    "reporter": "jreback",
    "created_at": "2016-03-17T02:42:05+00:00",
    "closed_at": "2016-12-14T14:54:21+00:00",
    "resolver": "adrian-stepien",
    "resolved_in": "43928d49171750c8827f1c6e02c416c0f50fdbeb",
    "resolver_commit_num": 0,
    "title": "DOC: improve cross-links in docs between .expanding and .cum*",
    "body": "xref #12648 \n- cross-link doc-strings for .cum\\* to the .expanding().\\* (for sum,prod,max,min, which are only supported).\n- show an example (in computation.rst wher eexpanding is shown? or basics.rst (where cum\\* are shown)) how these are related (w.r.t. NaN filling).\n\n\n",
    "labels": [
      "Docs",
      "Numeric",
      "Algos",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "@jreback I looked into this but am confused as to how to implement the cross-linking in the docstrings. The expanding function for dataframes and series don't have a doc page for .sum, .product, etc. As I understand it this is just chaining the .expanding method with the .sum method, so there shouldn't be a separate doc page right?\n\nAs an alternative we could just cross-link .expanding to all of the .cum\\* and vice-versa. But I'm not sure if that's what you were going for. \n",
      "just show an example as indicated above\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "cross-referenced",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "milestoned",
      "demilestoned",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 52,
    "deletions": 17,
    "changed_files_list": [
      "doc/source/basics.rst",
      "doc/source/computation.rst",
      "pandas/core/generic.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12654,
    "reporter": "ywhcuhk",
    "created_at": "2016-03-17T04:39:45+00:00",
    "closed_at": "2016-04-22T15:09:57+00:00",
    "resolver": "kshedden",
    "resolved_in": "33683cc0be98b1b7c6708d214588d99101eaad04",
    "resolver_commit_num": 15,
    "title": "Read SAS  (sas7bdat)",
    "body": "Very excited to have this new feature in Pandas. I have a few comments to share:\n1. `pd.read_sas()` doesn't read SAS date variable correctly (this is noted in the doc). Dates are read as `numpy.float64`. Note in SAS, dates are recorded as numbers relative to 1960-1-1. It would be helpful to allow some sort of arguments to parse the date variable correctly. \n2. Moreover, SAS has some special missing variables such as `.B` or `.R`. I wonder how are these cases treated? \n3. Not nearly as fast as `read_csv()`. To read a 700MB SAS data. The time is\n\n\n\nThe time for the same CSV file (I covered the same file to CSV using SAS) is \n\n\n",
    "labels": [
      "Performance",
      "IO SAS",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "To improve performance, the path to follow might be the one indicated in #10517.\n",
      "cc @kshedden \n\nthis is mostly in python ATM. You always write for correctness first, profile, then if necessary use things like cython in critical sections. pull-requests are welcome for an improved implm.\n",
      "Thanks for the feedback.  A few comments:\n- Performance is a major issue.  The pandas version is currently not usable for my own use case (files with billions of rows).  FWIW I have a golang version that is about 20x faster (https://github.com/kshedden/datareader) which I use for big files.  Most of the slowness in `pandas.read_sas` is in `process_byte_array_with_data` (already in cython but probably not optimally set up).  I have a local version with improvements that is about 30% faster, but I was looking for a much bigger improvement.  \n- The other open sourced SAS7BDAT readers that I know of , including the one from wizard, don't support compression properly or at all, e.g.: https://github.com/WizardMac/ReadStat/issues/21  I put a lot of time into getting the compression support to work.\n- Date support needs some work, but this should be relatively easy to fix.  There are several different date formats in SAS.  We detect and autoconvert some but not all of them.\n- I don't have documentation for missing value codes, if you can point me to it (i.e. which float values correspond to which codes) I should be able to add it.\n- I just noticed that wizard supports sas7bcat (categorical data format codes), I will look into porting this over at some point.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "demilestoned",
      "cross-referenced"
    ],
    "changed_files": 12,
    "additions": 2111,
    "deletions": 432,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/io/sas/sas7bdat.py",
      "pandas/io/sas/sas_constants.py",
      "pandas/io/sas/saslib.pyx",
      "pandas/io/tests/sas/data/airline.csv",
      "pandas/io/tests/sas/data/airline.sas7bdat",
      "pandas/io/tests/sas/data/productsales.csv",
      "pandas/io/tests/sas/data/productsales.sas7bdat",
      "pandas/io/tests/sas/data/test_12659.csv",
      "pandas/io/tests/sas/data/test_12659.sas7bdat",
      "pandas/io/tests/sas/test_sas7bdat.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12655,
    "reporter": "mangecoeur",
    "created_at": "2016-03-17T11:41:50+00:00",
    "closed_at": "2016-04-01T13:02:56+00:00",
    "resolver": "onesandzeroes",
    "resolved_in": "e61241c67e65c973446b283b264ee8cddc86b4dc",
    "resolver_commit_num": 14,
    "title": "COMPAT: read_excel not supporting pathlib.Path",
    "body": "According to the 0.17.1 release notes:\n\n> pd.read_\\* functions can now also accept pathlib.Path, or py._path.local.LocalPath objects for the filepath_or_buffer argument. (#11033)\n\nDoing so with read_excel gives `ValueError: Must explicitly set engine if not passing in buffer or path for io.`\n\nSee also #11773\n#### output of `pd.show_versions()`\n\npython: 3.5.1.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.3.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_GB.UTF-8\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.1.0\nsetuptools: 20.2.2\nCython: 0.23.4\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: 0.6.1\nxarray: 0.7.1\nIPython: 4.1.2\nsphinx: 1.3.5\npatsy: 0.4.0\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.4.6\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.8.4\nlxml: 3.5.0\nbs4: 4.4.1\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.11\npymysql: None\npsycopg2: 2.6.1 (dt dec pq3 ext)\njinja2: 2.8\nboto: 2.39.\n",
    "labels": [
      "IO Excel",
      "Compat",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "that was not a fully specified release note. certainly tests/fixes welcome. This is pretty straightforward to do.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "renamed",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 49,
    "deletions": 14,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/io/excel.py",
      "pandas/io/tests/test_excel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12659,
    "reporter": "gdementen",
    "created_at": "2016-03-17T13:50:50+00:00",
    "closed_at": "2016-04-22T15:09:57+00:00",
    "resolver": "kshedden",
    "resolved_in": "33683cc0be98b1b7c6708d214588d99101eaad04",
    "resolver_commit_num": 15,
    "title": "BUG: Read SAS fails",
    "body": "cc: @kshedden\n#### Code Sample, a copy-pastable example if possible\n\n\n\nDo you want a PR with this test case?\n#### Expected Output\n\npandas reads the file without crashing :)\nI tracked the problem to _process_columntext_subheader. It seems like there is an encoding issue...\n\nYou might need to apply  PR #12658 first to see this issue.\n\nFWIW, here is an AWFUL HACK to make pandas read the file \"almost\" correctly (I get column names as bytes instead of str in this case).\n\n\n#### output of `pd.show_versions()`\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Windows\nOS-release: 10\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.1.0\nsetuptools: 20.2.2\nCython: None\nnumpy: 1.10.4\nscipy: None\nstatsmodels: None\nxarray: None\nIPython: 4.1.2\nsphinx: 1.3.1\npatsy: None\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.4.6\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 0.9.4\nxlwt: None\nxlsxwriter: 0.8.4\nlxml: 3.5.0\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\n",
    "labels": [
      "Bug",
      "Difficulty Intermediate",
      "Effort Low",
      "IO SAS"
    ],
    "comments": [
      "cc @kshedden  \n",
      "I had the same issue on Ubuntu 15.10 running Python 2.7.10 \n"
    ],
    "events": [
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 12,
    "additions": 2111,
    "deletions": 432,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/io/sas/sas7bdat.py",
      "pandas/io/sas/sas_constants.py",
      "pandas/io/sas/saslib.pyx",
      "pandas/io/tests/sas/data/airline.csv",
      "pandas/io/tests/sas/data/airline.sas7bdat",
      "pandas/io/tests/sas/data/productsales.csv",
      "pandas/io/tests/sas/data/productsales.sas7bdat",
      "pandas/io/tests/sas/data/test_12659.csv",
      "pandas/io/tests/sas/data/test_12659.sas7bdat",
      "pandas/io/tests/sas/test_sas7bdat.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12663,
    "reporter": "agartland",
    "created_at": "2016-03-17T20:16:28+00:00",
    "closed_at": "2016-03-23T17:36:38+00:00",
    "resolver": "agartland",
    "resolved_in": "286e7a366ea23d364ab0834725f7a1d51f880d04",
    "resolver_commit_num": 0,
    "title": "Copy method does not make truly deep copies of dtype object arrays",
    "body": "The `copy` method of `pd.Series` and `pd.DataFrame` has a parameter `deep` which claims to [Make a deep copy, i.e. also copy data](-docs/stable/generated/pandas.Series.copy.html). The example below seems to show that this isn't a truly deep copy (as in `from copy import deepcopy`) I can't seem to find the implementation in the source. I am wondering if this behavior below is expected, if something different should be done for `dtype=object` to make it a truly deep copy, or if we could at least add a note to the documentation that notes this behavior?\n\nThanks!\n\n\n",
    "labels": [
      "Usage Question",
      "Compat",
      "Docs",
      "Difficulty Intermediate"
    ],
    "comments": [
      "this is not supported. pandas objects are stored in numpy arrays (generally) which if they have `object` dtype are simply pointers to python objects. deep-copying them does not imply that numpy array is deep-copied itself (I don't even know if that is actually supported). Deep refers to the indexes themselves being copied.\n\nIt would be expensive to do this. Not really even sure of a usecase for it; generally the actual data are scalar type data (e.g. float, int, string), not actual python objects themselves. \n\nThis is an anti-pattern to store python objects here. I suppose you could add a note to the doc-string.\n",
      "Thanks, that's helpful, I understand why the copy method behaves as it does. I'm wondering if it could be made more clear in the documentation since the word \"deep\" seems at least slightly ambiguous here.\n\nI typically use a DataFrame for numbers, but sometimes I like to have a column that holds some other kind of meta-data in an object. This way I get all the benefits of indexing and mergeing and can keep the meta-data associated with the data (even if this effectively disables many of numpy's nice features and efficiencies)\n",
      "@agartland sure, a doc-string update would be fine. you can even put your example there to make it clear (maybe slim it down a bit).\n",
      "I added a note in the copy method docstring that should help. I didn't know where/how to add an example but here's a slimmed down version if you think its useful:\n\n```\nimport copy\n\na = pd.Series([1, 'a', [4,5,6]])\nb = a.copy(deep=True)\nc = copy.deepcopy(a)\n\nprint a\n\n\"\"\"Changes to the copy.deepcopy don't affect the original.\"\"\"\nc.loc[2].append(0)\nprint a.loc[2], b.loc[2], c.loc[2]\n\n\"\"\"Changes to the a.copy(deep=True) are reflected in the original.\"\"\"\nb.loc[2].append(-9)\nprint a.loc[2], b.loc[2], c.loc[2]\n```\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "labeled",
      "labeled",
      "cross-referenced",
      "commented",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 1,
    "additions": 8,
    "deletions": 2,
    "changed_files_list": [
      "pandas/core/generic.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12669,
    "reporter": "anntzer",
    "created_at": "2016-03-19T04:54:25+00:00",
    "closed_at": "2016-04-26T23:40:37+00:00",
    "resolver": "jreback",
    "resolved_in": "753b7f2035fce826bb2259f72067c8b978f6721f",
    "resolver_commit_num": 3998,
    "title": "ERR: better error message on invalid window when using .rolling",
    "body": "xref #12765. Let's pick up this as a test case and give a more meaningful error message. e.g. for `Window` the `win_type` is missing, so this should be the error.\n\n\n\nNote that `pd.DataFrame(np.arange(10)).rolling(2.).median()` works fine.\n\npandas 0.18.0.\n",
    "labels": [
      "Reshaping",
      "Difficulty Novice",
      "Error Reporting",
      "Effort Low"
    ],
    "comments": [
      "yeah looks like the validation is only in place for a `Window` and not the `Rolling` and sub-classes. It should be an int, or raise a nicer message.\n\npull-requests welcome.\n",
      "I would prefer requiring an int raising with an informative `TypeError` in this case.\n",
      "I'm also in favor of requiring ints too.\n",
      "Dose this look all right? I don't think we need ' whatsnew entry'. Do we? \n",
      "@pt247 pls comment on PRs directly - always need a whatsnew entry\n",
      "The input validation is inconsistent in `window.py`. I was proposing to reuse ``_prep_window` to unify behaviour. Could somebody familiar with the code offer guidance (instead of spot fixes)? Thanks\n",
      "@gliptak these DO have different validations, as `Window` can accept an integer or an integer array, while `Rolling` can only accept an integer. This does not require a big change.\n",
      "I closed https://github.com/pydata/pandas/issues/12714, https://github.com/pydata/pandas/pull/12718 is closer to the approach described.\n",
      "ok, that's fine.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "cross-referenced",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 144,
    "deletions": 29,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/window.py",
      "pandas/tests/test_window.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12678,
    "reporter": "jreback",
    "created_at": "2016-03-20T20:46:05+00:00",
    "closed_at": "2016-04-06T19:17:26+00:00",
    "resolver": "gfyoung",
    "resolved_in": "c6c201e27c7ed57a823ec7261340dfeec1e0226a",
    "resolver_commit_num": 7,
    "title": "ERR: dont' allow ambiguous usecols",
    "body": " and comment: #discussion_r56771668\n\n`usecols=['a',1]` should raise\n",
    "labels": [
      "Error Reporting",
      "CSV"
    ],
    "comments": [
      "@jreback : I'll tackle this one as a follow-up in terms of fixing documentation and enforcement.  In the meantime, I'll remove the tests I added that had the ambiguous columns.\n",
      "Shouldn't we also just raise on duplicates in `usecols` ? eg `['a', 'a']`\n",
      "@jorisvandenbossche : `usecols` is a set, so you won't ever have such a problem.\n",
      "#11822 and https://github.com/pydata/pandas/issues/11823 converts it to a list.\n\nthough if we allow only non-duplicated and make it a set operation against `names`...\n\nthough current doc-string say its `array-like`\n",
      "@gfyoung aha, I didn't know that, but then adding tests for the behaviour for duplicates values in `usecols` as we are doing in https://github.com/pydata/pandas/pull/11882 makes no sense?\n",
      "what we could do is:\n- don't allow mixed-integers (e.g. 'a', 1) as these are ambiguous\n- still use a set for `usecols`, so duplicates are gone, but handle this as a set-selection operation against `.names/columns` (IOW if names is passed or a header is read in). If there are duplicates there its ok, this `usecols` just sub-selects. Duplicate handling will be solely there.\n",
      "@jreback : What do you mean by \"duplicate\" handling?  Is that what #11823 will be doing?\n",
      "This is on master.\n\n```\nIn [12]: pd.read_csv(StringIO(\"\"\"1,2,3\"\"\"), engine='c', header=None, \n   ....:             names=['a', 'b', 'a'], usecols=['a','a'])\nOut[12]: \n   a  a\n0  1  1\n\nIn [13]: pd.read_csv(StringIO(\"\"\"1,2,3\"\"\"), engine='python', header=None, \n            names=['a', 'b', 'a'], usecols=['a','a'])\nOut[13]: \n   a\n0  1\n```\n\nbut I think that these should BOTH output\n\n```\n   a   a\n0  1   3\n```\n\nIOW, the `usecols` is just a filter as a set (so its `'a'` in this case).\n\nThen names takes over and you get the 0th and 2nd columns (that are named 'a')\n",
      "cc @sxwang \n",
      "@jreback : AFAICT, such behaviour will be fixed in #11882 right?\n",
      "@jreback I agree\n\n@gfyoung Well the PR for that issue is https://github.com/pydata/pandas/pull/11882, and currently there it is not yet this behaviour (but still in reviewing phase)\n",
      "Okay, but in terms of allocation, that issue should be tackled there, and I could just handle the enforcing non-ambiguous `usecols`, right?\n",
      "@gfyoung yes, that's right!\n",
      "yep that sounds right. let's restrict this issue to ambiguous errors\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 166,
    "deletions": 27,
    "changed_files_list": [
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12687,
    "reporter": "jreback",
    "created_at": "2016-03-22T14:28:20+00:00",
    "closed_at": "2016-05-01T15:11:59+00:00",
    "resolver": "gfyoung",
    "resolved_in": "23eb483d17ce41c0fe41b0bfa72c90df82151598",
    "resolver_commit_num": 16,
    "title": "COMPAT: breakage with just merged: https://github.com/numpy/numpy/pull/7325",
    "body": "see [here](-ci.org/pydata/pandas/jobs/117710492)\nxref #12603 \n#12644\n\n[numpy issue 7325]()\n\ncc @gfyoung \ncc @njsmith\n",
    "labels": [
      "Compat"
    ],
    "comments": [
      "@gfyoung I think we just need to _also_ accept `**kwargs` (and validate)\n",
      "@jreback : The test needs to be modified to check which version of `numpy` we are using.  That exception will only be thrown if we're not using `numpy-dev` (on `numpy-dev`, no exceptions will be thrown because the `DataFrame` or `Series` gets converted to a `numpy` array).  What's the best the way to check `numpy` version?\n",
      "I don't think its a test failure, rather need to accept `**kwargs` AS WELL as `*args` (similar to how we do in the stat functions). The signature is consistent across all versions of numpy. I don't want to see specific version tests, except in very specific cases.\n\n```\nIn [1]: pd.Series.round?\nSignature: pd.Series.round(self, decimals=0, *args)\n```\n",
      "Fair enough.  I'll tackle this issue in a PR that will also fix #12644.\n",
      "@gfyoung if you have a chance on this one. annoying when master fails :< (even though its the allowed failures, that's just to make it finish faster :)\n",
      "@jreback : No worries.  As I said above, I'll fix it in a PR that will address all of the `fromnumeric.py` compat issues.\n",
      "@gfyoung ok, great.\n",
      "@gfyoung if you are looking for something to do.....I want to fix the failing build on travis (numpy compat build) for this (can leave open for more general things).\n",
      "@jreback : Alas, the fix is more complicated than just adding `**kwargs`.  I didn't consider in <a href=\"https://github.com/numpy/numpy/pull/7325\">#7325</a> the fact that a function can raise `TypeError` that isn't necessarily a signature compatibility issue, like when you try to take the sum of a `Series` with `Categorical` data.\n\nxref <a href=\"https://github.com/numpy/numpy/pull/7491\">#7491</a> in `numpy` (this PR will fix the issue on the `numpy` end)\n",
      "My massive PR #12810 will address this breakage on the `pandas` side now.  The combination of my `numpy` PR and this one should fully fix the problem.\n",
      "Does this fix the pandas breakage without any work on the pandas side? I'm somewhat annoyed that there was a problem.\n\nOOPS: wrong PR ;)\n"
    ],
    "events": [
      "labeled",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced"
    ],
    "changed_files": 50,
    "additions": 1820,
    "deletions": 269,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/__init__.py",
      "pandas/compat/numpy/__init__.py",
      "pandas/compat/numpy/function.py",
      "pandas/compat/pickle_compat.py",
      "pandas/core/base.py",
      "pandas/core/categorical.py",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/ops.py",
      "pandas/core/panel.py",
      "pandas/core/series.py",
      "pandas/indexes/base.py",
      "pandas/indexes/category.py",
      "pandas/indexes/multi.py",
      "pandas/indexes/range.py",
      "pandas/io/tests/test_date_converters.py",
      "pandas/io/tests/test_gbq.py",
      "pandas/sparse/array.py",
      "pandas/sparse/frame.py",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_array.py",
      "pandas/sparse/tests/test_frame.py",
      "pandas/sparse/tests/test_series.py",
      "pandas/tests/frame/test_analytics.py",
      "pandas/tests/frame/test_operators.py",
      "pandas/tests/indexes/common.py",
      "pandas/tests/indexes/test_base.py",
      "pandas/tests/indexes/test_category.py",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tests/series/test_analytics.py",
      "pandas/tests/test_algos.py",
      "pandas/tests/test_base.py",
      "pandas/tests/test_categorical.py",
      "pandas/tests/test_generic.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_util.py",
      "pandas/tseries/base.py",
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_base.py",
      "pandas/tseries/tests/test_converter.py",
      "pandas/tseries/tests/test_offsets.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tests/test_tslib.py",
      "pandas/tslib.pyx",
      "pandas/types/generic.py",
      "pandas/util/validators.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12688,
    "reporter": "jreback",
    "created_at": "2016-03-22T14:35:26+00:00",
    "closed_at": "2016-12-13T23:11:38+00:00",
    "resolver": "dhimmel",
    "resolved_in": "37614485a9740df1c55e7f0da2d32216e2561af1",
    "resolver_commit_num": 2,
    "title": "CLN: consolidate compression inference as much as possible",
    "body": "we have 3 copies of the compression inference routines in `parser.py, parser.py, and io/common.py`. See how much we can reduce this (_some_ is necessary as sometimes we need to actually return a file handle or need to read in and re-wrap).\n\nxref #12175 \n",
    "labels": [
      "CSV",
      "Difficulty Intermediate",
      "Effort Low",
      "Clean"
    ],
    "comments": [
      "cc @lababidi \n",
      "@gfyoung if you are looking for something to do :>\n\nI think we can blow away `_wrap_compresed` in parsers/io/parsers.py and incorporate any such functionaility in `io/common.py/_get_handle`\n\nthere is also compression handling in `parser.pyx` but I think this might be harder to remove (in favor of `_get_handle`, though maybe if we pass an argument might work).\n\nxref #13317 \n",
      "Haha.  Thanks @jreback for the mention.  I'll take a closer look when I have the time. :smile:\n",
      "@jreback I got the following error when attempting to test:\n\n``````\nTypeError: assert_almost_equal() got an unexpected keyword argument 'check_dtype'```\n``````\n",
      "you need to rebuild extensions\n\n`python setup.py build_ext --inplace`\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 7,
    "additions": 166,
    "deletions": 217,
    "changed_files_list": [
      "pandas/formats/format.py",
      "pandas/io/common.py",
      "pandas/io/json.py",
      "pandas/io/parsers.py",
      "pandas/io/s3.py",
      "pandas/io/tests/parser/compression.py",
      "pandas/io/tests/parser/test_network.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12695,
    "reporter": "AnkurDedania",
    "created_at": "2016-03-22T18:41:01+00:00",
    "closed_at": "2016-04-10T14:22:42+00:00",
    "resolver": "sinhrks",
    "resolved_in": "c03f5456676eb349deefb1ed7ec52f9dc3419ee5",
    "resolver_commit_num": 281,
    "title": "RangeIndex not generated with pd.concat with ignore_index as True",
    "body": "With version 0.18.0 introducing RangeIndex, should RangeIndex be applied for concat when ignore_index is set to True?\n\n`df = pd.concat([df_a, df_b], ignore_index=True)`\n\ndf.index is an instance of Int64Index, not RangeIndex currently in 0.18.0\n",
    "labels": [
      "Enhancement",
      "Indexing",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "yes this should be here as well. pull-requests welcome. \n",
      "make sure tests are of the form: `assert_frame_equal(resut, expected, check_index_type=True)` (otherwise defaults to an 'equiv' comparison, which doesnt distinguish between Int64 and equiv RangeIndex.\n\nIn fact that is how to find differences in the test suite (to look for cases where we could/should be returning a `RangeIndex` but are currently not).\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 91,
    "deletions": 31,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12701,
    "reporter": "emsems",
    "created_at": "2016-03-23T14:33:44+00:00",
    "closed_at": "2016-04-03T17:52:22+00:00",
    "resolver": "jonaslb",
    "resolved_in": "9f68a9639d9b7323e830350ecefc133a6197c6e5",
    "resolver_commit_num": 1,
    "title": "DataFrame.drop() does nothing for non-unique MultiIndex when attempting to drop from a level with DatetimeIndex",
    "body": "Hi,\nthere seems to be a bug in DataFrame.drop() when the DataFrame has a non-unique MultiIndex and one of the levels is a DatetimeIndex (labels of which I would like to pass to the drop-method)\n#### Code Sample\n\n\n#### Expected Output\n\n\n#### Current Output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Bug",
      "Indexing",
      "Difficulty Novice",
      "MultiIndex",
      "Effort Low"
    ],
    "comments": [
      "yeah this is broken, needs something like for the non-unique multi-index case, rather than how it is doing it now [here](https://github.com/pydata/pandas/blob/master/pandas/core/generic.py#L1884)\n\n```\nIn [21]: df.loc[idx[:,~df.index.get_level_values('tstamp').isin([ts])], :]\nOut[21]: \n                         a   b   c\nid tstamp                         \n0  2016-03-23 12:00:00   0   1   2\n   2016-03-23 12:00:00   3   4   5\n1  2016-03-23 13:00:00   6   7   8\n   2016-03-23 13:00:00   9  10  11\n   2016-03-23 14:00:00  12  13  14\n2  2016-03-23 14:00:00  15  16  17\n3  2016-03-23 15:00:00  18  19  20\n5  2016-03-23 17:00:00  27  28  29\n```\n\nSo the indexer should be\n`~axis.get_level_values(leel).isin(labels]`\n\nobviously not well tested :<\n\npull-requests welcomed!\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 23,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/generic.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12706,
    "reporter": "jreback",
    "created_at": "2016-03-23T21:44:52+00:00",
    "closed_at": "2016-03-26T00:32:57+00:00",
    "resolver": "tdhopper",
    "resolved_in": "5870731f32ae569e01e3c0a8972cdd2c6e0301f8",
    "resolver_commit_num": 0,
    "title": "ERR: validation options that accept callables",
    "body": "#issuecomment-200554983\n\n\n\nThis should raise\n\n\n",
    "labels": [
      "Difficulty Novice",
      "Error Reporting",
      "Effort Low"
    ],
    "comments": [
      "the issue is that in `core/config_init.py` `float_format` doesn't have a validator.\n",
      "Right: [here](https://github.com/pydata/pandas/blob/eb87db9e3dffe7febb575b82bf26065e807c028b/pandas/core/config_init.py#L282). \n\nIs [`callable`](https://docs.python.org/2/library/functions.html#callable) sufficient?\n",
      "you could prob just create a `is_callable = callable` [here](https://github.com/pydata/pandas/blob/master/pandas/core/config.py#L800)\n\nand `validator=is_callable` would work I think\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 42,
    "deletions": 6,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/config.py",
      "pandas/core/config_init.py",
      "pandas/tests/test_config.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12723,
    "reporter": "jcrist",
    "created_at": "2016-03-28T15:58:32+00:00",
    "closed_at": "2016-04-17T14:13:08+00:00",
    "resolver": "facaiy",
    "resolved_in": "0734df0cfa0d050bdb2a810db1c6c4a1465f28e3",
    "resolver_commit_num": 1,
    "title": "Unexpected behavior with binary operators and fill_value",
    "body": "From  `fill_value` doesn't seem to apply if the argument to a binary operator is a constant, but works fine for other arguments:\n\n\n\nFrom the docstring I'd expect it to be equivalent to `df2.fillna(0).add(2)`. If this is intended behavior, then the docstring should be updated to clarify this.\n",
    "labels": [
      "Bug",
      "Missing-data",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "I'd like to take a try.\n",
      "It breaks [here](https://github.com/pydata/pandas/blob/602883a34a818f47f90651de99d786773b72d4a7/pandas/core/ops.py#L1089).\n\n`return self._combine_const(other, na_op)` miss `fill_value` parameter.\n\nSo, we can fix it before the method or in the method of DataFrame, \nwhich method is better? \n",
      "The fix is not complete, I find that the unexpected behavior exists also in Series.\n"
    ],
    "events": [
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "renamed"
    ],
    "changed_files": 6,
    "additions": 56,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/ops.py",
      "pandas/sparse/tests/test_frame.py",
      "pandas/sparse/tests/test_series.py",
      "pandas/tests/frame/test_missing.py",
      "pandas/tests/series/test_missing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12725,
    "reporter": "emmawillemsma",
    "created_at": "2016-03-28T22:28:00+00:00",
    "closed_at": "2016-03-31T13:15:40+00:00",
    "resolver": "onesandzeroes",
    "resolved_in": "b6959c87db64121210aa1bc28c02563dbccfee53",
    "resolver_commit_num": 15,
    "title": "Loffset doesn't work when resampling with count()",
    "body": "The label offset `loffset` has no effect when resampling a Series using `.count()`:\n\n\n\n\n",
    "labels": [
      "Difficulty Novice",
      "Regression",
      "Resample",
      "Effort Low"
    ],
    "comments": [
      "prob a bug in the transfer of the impl and not being fully tested -  pull requests to fix welcome! (should be easy fix)\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 42,
    "deletions": 8,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12727,
    "reporter": "has2k1",
    "created_at": "2016-03-29T04:14:23+00:00",
    "closed_at": "2016-03-31T13:24:07+00:00",
    "resolver": "has2k1",
    "resolved_in": "0d58446503abc0f4299df8644c7d64163664a885",
    "resolver_commit_num": 0,
    "title": "pd.timedelta has smaller than expected range",
    "body": "The pandas timedelta class has less than expected minimum and maximum timedeltas. The range is `[-99999, 99999]`  days instead of [`-999999999, 999999999]` days\n#### Code Sample\n\n\n#### Expected Output\n\n\n\nThe problem is illustrated at points `2` (silent overflow) and `4`(overflow error)\n#### output of `pd.show_versions()`\n\n\n\nI get similar results on python `3.5.1`.\n",
    "labels": [
      "Timedelta",
      "Compat"
    ],
    "comments": [
      "Unlike in numpy, pandas.Timdelta always uses nanosecond precision, so anything more than [about 100000 days](https://www.google.com/search?q=2%5E64+nanoseconds+in+years&rlz=1C9BKJA_enUS592US592&oq=2%5E64+nano&aqs=chrome.1.69i57j0.9678j0j7&hl=en-US&sourceid=chrome-mobile&ie=UTF-8#hl=en-US&q=2%5E63+nanoseconds+in+days) cannot be represented. Something does seem to be going wrong with the numeric overflow, though. \n",
      "@has2k1 I guess. This is by definition as to the implementation as @shoyer comments, and in-line with the dateranges represented by `Timestamps`.\n",
      "I suppose a doc-section along the lines of: http://pandas.pydata.org/pandas-docs/stable/gotchas.html#gotchas-timestamp-limits would be fine. Maybe we should move these to the `Timeseries/Timedelta` section though. @jorisvandenbossche ?\n\n```\nIn [8]: pd.Timedelta(np.iinfo(np.int64).max)\nOut[8]: Timedelta('106751 days 23:47:16.854775')\n\nIn [9]: pd.Timedelta(np.iinfo(np.int64).min)\nOut[9]: NaT\n\nIn [10]: pd.Timedelta(np.iinfo(np.int64).min+1)\nOut[10]: Timedelta('-106752 days +00:12:43.145224')\n```\n",
      "@jreback, yes that would be helpful. In PR #12728 I'll include a section to that end in the\n `timedelta` documentation.\n"
    ],
    "events": [
      "renamed",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned"
    ],
    "changed_files": 6,
    "additions": 65,
    "deletions": 22,
    "changed_files_list": [
      "doc/source/gotchas.rst",
      "doc/source/timedeltas.rst",
      "doc/source/timeseries.rst",
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/tseries/tests/test_timedeltas.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12729,
    "reporter": "matthew-brett",
    "created_at": "2016-03-29T09:30:07+00:00",
    "closed_at": "2016-04-05T13:37:52+00:00",
    "resolver": "jreback",
    "resolved_in": "fded94274effa2d592e48ec2079ba3b78e5bb232",
    "resolver_commit_num": 3965,
    "title": "Test errors for numpy 1.11 / pandas 0.18.0",
    "body": "See: -ci.org/matthew-brett/manylinux-testing/jobs/119188668#L3214\n\n\n\nNo such error testing against numpy 1.10.4 : -ci.org/matthew-brett/manylinux-testing/jobs/119039450#L523\n",
    "labels": [
      "Compat"
    ],
    "comments": [
      "This is at odds with the wheels built directly by the numpy on master. see [here](https://travis-ci.org/pydata/pandas/jobs/118605953). 2 known failures on pandas side ATM due to some compat issues. This is master numpy / master pandas.\n\nThe error you are seeing is rather old IIRC.\n",
      "We have this changing from an `IndexError` in numpy < 1.11 to a `TypeError` in 1.11. Was this changed reverted before 1.11?\n@charris @shoyer \n\n```\nIn [32]: arr = np.arange(0,2).astype('M8[ns]')\n\nIn [33]: arr\nOut[33]: \narray(['1969-12-31T19:00:00.000000000-0500',\n       '1969-12-31T19:00:00.000000001-0500'], dtype='datetime64[ns]')\n\nIn [34]: arr[slice('2014',None,None)]\nIndexError: invalid slice\n\nIn [35]: np.__version__\nOut[35]: '1.10.4'\n```\n\n```\n>>> np.__version__\n'1.11.0b3'\n>>> arr = np.arange(0,2).astype('M8[ns]')\n>>> arr\narray(['1970-01-01T00:00:00.000000000', '1970-01-01T00:00:00.000000001'], dtype='datetime64[ns]')\n>>> arr[slice('2014',None,None)]\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nIndexError: failed to coerce slice entry of type str to integer\n```\n\n```\n>>> np.__version__\n'1.12.0.dev0+75c5af3'\n>>> arr = np.arange(0,2).astype('M8[ns]')\n>>> arr[slice('2014',None,None)]\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: slice indices must be integers or None or have an __index__ method\n```\n",
      "I don't see where you tested with 1.11.0, the reversion was in 1.11.0rc1\n\n```\ncommit 466d60d364dfc979ccffa58b5c8212137e6b2a27\nAuthor: Charles Harris <charlesr.harris@gmail.com>\nDate:   Sun Feb 14 11:49:29 2016 -0700\n\n    Revert #6271 from charris/change-deprecated-indexes-to-error\n\n    This reverts commit 0b0206c1617841ed2e5324de752ee8ede2cce791,\n    reversing changes made to 438cdd3d75a0bb606e7ab7f96e59744c9b78d748.\n\n    This reverts all the changes of indexing deprecation warnings to\n    errors that were done prior to branching Numpy 1.11.x. It is easier\n    to make a clean reversion here than to pick out those bits involving\n    integer indexing specifically. As the aim of this reversion is to\n    give downstream projects a bit more time to adapt I don't think the\n    slight overkill is worth worrying about, especially if downstream\n    projects start paying close attention to deprecations.\n\n    The 1.11.0-notes also need updating, but I will submit a separate\n    PR against master for that.\n\n    Closes #7162.\n```\n",
      "it's the same (forgot to pull tags)\n\nok so you reverted this change - will update on our side\n"
    ],
    "events": [
      "commented",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "milestoned",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 6,
    "additions": 28,
    "deletions": 28,
    "changed_files_list": [
      "ci/requirements-3.5_OSX.build",
      "ci/requirements-3.5_OSX.run",
      "pandas/compat/numpy_compat.py",
      "pandas/io/pytables.py",
      "pandas/io/tests/test_pytables.py",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12737,
    "reporter": "jreback",
    "created_at": "2016-03-29T19:46:54+00:00",
    "closed_at": "2016-05-18T13:19:39+00:00",
    "resolver": "chris-b1",
    "resolved_in": "009d1df85ec6e6f80cace1d949bb7cdc8d35df7c",
    "resolver_commit_num": 32,
    "title": "PERF: DataFrame groupby with fast transform",
    "body": "from [SO](-to-speed-up-the-replacement-of-missing-values-for-each-groupby-group-in-pand)\n\n\n\nThis is then iterating over groups. Last I can see this was changed is: [here](). My recollection is that this was ONLY supposed to hit in a special case, and the general case is simply a repeat based on the indices. \n\nThis seems to be hitting in all cases makes transform back to super SLOW.\n",
    "labels": [
      "Prio-high",
      "Groupby",
      "Performance",
      "Difficulty Intermediate",
      "Effort Medium",
      "Enhancement"
    ],
    "comments": [
      "cc @ajcr \ncc @chris-b1 \ncc @evanpw \n",
      "So we have a fast path for Series transforms, but not for DataFrame transforms.\n\n```\nIn [17]: result1 = g.transform('first')\n\nIn [18]: result2 = pd.concat([g.B.transform('first'), g.C.transform('first')], keys=['B','C'], axis=1)\n\nIn [19]: result1.equals(result2)\nOut[19]: True\n\nIn [20]: %timeit g.transform('first')\n10 loops, best of 3: 170 ms per loop\n\nIn [21]: %timeit pd.concat([g.B.transform('first'), g.C.transform('first')], keys=['B','C'], axis=1)\n1000 loops, best of 3: 2 ms per loop\n```\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "renamed",
      "labeled",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 5,
    "additions": 82,
    "deletions": 31,
    "changed_files_list": [
      "asv_bench/benchmarks/groupby.py",
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/core/groupby.py",
      "pandas/tests/test_categorical.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12738,
    "reporter": "lminer",
    "created_at": "2016-03-29T23:21:04+00:00",
    "closed_at": "2016-04-26T15:03:02+00:00",
    "resolver": "jreback",
    "resolved_in": "699424027fb657192541bcd0c3d9f9b7d26f2300",
    "resolver_commit_num": 3993,
    "title": "Allow .expanding() to be called on groupby object",
    "body": "Right now when I want to have expanding operate at the level of a group I have to apply a convenience function like so:\n\n\n\nWould be nice to be able to call a method directly on a groupby object:\n\n\n",
    "labels": [
      "Enhancement",
      "Groupby",
      "Reshaping"
    ],
    "comments": [
      "related to #12486 \n\nyes this should be possible - have to manage the dispatching correctly\n"
    ],
    "events": [
      "commented",
      "referenced",
      "renamed",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 12,
    "additions": 719,
    "deletions": 132,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/base.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/missing.py",
      "pandas/core/window.py",
      "pandas/indexes/base.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_window.py",
      "pandas/tools/merge.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12741,
    "reporter": "sinhrks",
    "created_at": "2016-03-30T01:49:08+00:00",
    "closed_at": "2016-04-03T17:03:18+00:00",
    "resolver": "sinhrks",
    "resolved_in": "36a8bd9cb71638abba3f2d97558b7d342d7ff273",
    "resolver_commit_num": 265,
    "title": "CLN: Move i8_boxer logic to BlockManager",
    "body": "Based on the discussion in #12532. Move boxing logic to `Block/BlockManager` from `common.py`.\n\nAdd either `.asobject` property (compat with `Index`) or `.get_object_values()` to `BlockManager/Block`. \n",
    "labels": [
      "Internals",
      "Clean",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "labeled",
      "labeled",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 13,
    "additions": 942,
    "deletions": 627,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/common.py",
      "pandas/core/frame.py",
      "pandas/core/internals.py",
      "pandas/core/series.py",
      "pandas/tests/frame/test_analytics.py",
      "pandas/tests/frame/test_apply.py",
      "pandas/tests/frame/test_quantile.py",
      "pandas/tests/series/test_analytics.py",
      "pandas/tests/series/test_apply.py",
      "pandas/tests/series/test_misc_api.py",
      "pandas/tests/series/test_quantile.py",
      "pandas/tseries/base.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12749,
    "reporter": "sciunto",
    "created_at": "2016-03-30T15:59:37+00:00",
    "closed_at": "2016-04-11T23:57:05+00:00",
    "resolver": "jreback",
    "resolved_in": "504ad4641e5b3a87b655da5dd31840de12495e97",
    "resolver_commit_num": 3974,
    "title": "COMPAT: show warning when using numexpr not installed and defaulted",
    "body": "cc @mrocklin\n\nHi,\n\nContext:\n\nRight now, pandas treats numexpr as an optional dependency. I'm a packager for archlinux, and recently, I got a feedback on the dask package saying that numexpr was missing as a dependency of this package. -dask/\nHowever, dask does not explicitly on  numexpr. The reason is detailed below. \n\nAnalysis:\n\nIn `pandas.computations`, `eval()` takes an optional argument `engine='numexpr'`.\nIf numexpr is not install, then any call with default arguments will raise an exception importError from the function `_check_engine` in pandas/computations/eval.py.\n\n`eval()` is called (at least) from query, that's why we are in trouble if we run dask's test without numexpr.\n\nRFC:\n\nHere is the question: is numexpr really an optional dependency since it's the default argument?\nI would say no, but comments are open :)\nFrom the dask devs point of view, they do not have to mark numexpr as a dependency because they do not use it explicitly. To me, they can expect that default arguments from pandas work out of the box.\nFrom the pandas packager (not me), pandas says that numexpr is optional, treated as optional. No problem here too, he followed the guidelines.\n\nI see two options:\n- pandas changes the default backend\n- or pandas adds numexpr as a true dependency.\n",
    "labels": [
      "Compat",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "this was fixed in 0.18.0. previously if you tried to use `.eval` and `numexpr` wasn't installed it would bork. \n",
      "xref https://github.com/pydata/pandas/pull/12511\n",
      "The problem is obviously not fixed (or not correctly) for the case I describe because it happens with pandas 0.18.0. It's annoying to see a post closed without discussion. :(\n",
      "Note that I also read the code in master before opening this issue.\n",
      "@sciunto so what is the reproducible example then. are you _sure_ you are using 0.18.0\n",
      "also you didn't read our issue submission guidelines. I don't see `pd.show_versions()`\n",
      "I suppose you are doing this?\n\n```\nIn [1]: pd.show_versions()\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 14.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: en_US.UTF-8\nLANG: en_US.UTF-8\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.0.3\nsetuptools: 20.2.2\nCython: 0.23.4\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: 0.6.1\nxarray: 0.7.2\nIPython: 4.1.1\nsphinx: 1.3.1\npatsy: 0.4.0\ndateutil: 2.5.2\npytz: 2016.1\nblosc: 1.2.8\nbottleneck: 1.0.0\ntables: None\nnumexpr: None\nmatplotlib: 1.5.0\nopenpyxl: 2.2.6\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.7.7\nlxml: 3.4.4\nbs4: 4.3.2\nhtml5lib: 0.999\nhttplib2: 0.9.2\napiclient: 1.4.1\nsqlalchemy: 1.0.11\npymysql: 0.6.6.None\npsycopg2: 2.6.1 (dt dec pq3 ext)\njinja2: 2.8\nboto: 2.39.0\n```\n\n```\nIn [2]: df = DataFrame({'A' : [1,2,3]})\n\nIn [3]: df.query('A>0')\nImportError: 'numexpr' is not installed or an unsupported version. Cannot use engine='numexpr' for query/eval if 'numexpr' is not installed\n```\n\n```\nIn [4]: df.query('A>0',engine='python')\nOut[4]: \n   A\n0  1\n1  2\n2  3\n```\n",
      "I guess that could be more friendly. Though _maybe_ should show a `PerformanceWarning` that you are trying to use a function that is intended for a highly recommended dep.\n",
      "Yes I'm sure... \n\n```\nyaourt -Ss python-pandas\ncommunity/python-pandas 0.18.0-1 [installed]\n    Cross-section and time series data analysis toolkit\n```\n\n```\n=================================================================================== FAILURES ====================================================================================\n__________________________________________________________________________________ test_query ___________________________________________________________________________________\n\n    def test_query():\n        df = pd.DataFrame({'x': [1, 2, 3, 4], 'y': [5, 6, 7, 8]})\n        a = dd.from_pandas(df, npartitions=2)\n>       q = a.query('x**2 > y')\n\ndask/dataframe/tests/test_dataframe.py:1280:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ndask/dataframe/core.py:1467: in query\n    dummy = self._pd.query(expr, **kwargs)\n/usr/lib/python3.5/site-packages/pandas/core/frame.py:2140: in query\n    res = self.eval(expr, **kwargs)\n/usr/lib/python3.5/site-packages/pandas/core/frame.py:2209: in eval\n    return _eval(expr, inplace=inplace, **kwargs)\n/usr/lib/python3.5/site-packages/pandas/computation/eval.py:233: in eval\n    _check_engine(engine)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nengine = 'numexpr'\n\n    def _check_engine(engine):\n        \"\"\"Make sure a valid engine is passed.\n\n        Parameters\n        ----------\n        engine : str\n\n        Raises\n        ------\n        KeyError\n          * If an invalid engine is passed\n        ImportError\n          * If numexpr was requested but doesn't exist\n        \"\"\"\n        if engine not in _engines:\n            raise KeyError('Invalid engine {0!r} passed, valid engines are'\n                           ' {1}'.format(engine, list(_engines.keys())))\n\n        # TODO: validate this in a more general way (thinking of future engines\n        # that won't necessarily be import-able)\n        # Could potentially be done on engine instantiation\n        if engine == 'numexpr':\n            if not _NUMEXPR_INSTALLED:\n>               raise ImportError(\"'numexpr' is not installed or an \"\n                                  \"unsupported version. Cannot use \"\n                                  \"engine='numexpr' for query/eval \"\n                                  \"if 'numexpr' is not installed\")\nE               ImportError: 'numexpr' is not installed or an unsupported version. Cannot use engine='numexpr' for query/eval if 'numexpr' is not installed\n\n/usr/lib/python3.5/site-packages/pandas/computation/eval.py:39: ImportError\n______________________________________________________________________________ test_to_hdf_kwargs _______________________________________________________________________________\n\n    def test_to_hdf_kwargs():\n        df = pd.DataFrame({'A': ['a', 'aaaa']})\n        ddf = dd.from_pandas(df, npartitions=2)\n>       ddf.to_hdf('tst.h5', 'foo4', format='table', min_itemsize=4)\n\ndask/dataframe/tests/test_io.py:1028:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ndask/dataframe/core.py:532: in to_hdf\n    fletcher32, get=get, **kwargs)\ndask/dataframe/io.py:655: in to_hdf\n    get=get, **dask_kwargs)\ndask/base.py:43: in _get\n    return get(dsk2, keys, **kwargs)\ndask/async.py:516: in get_sync\n    raise_on_exception=True, **kwargs)\ndask/async.py:462: in get_async\n    fire_task()\ndask/async.py:458: in fire_task\n    get_id, raise_on_exception])\ndask/async.py:508: in apply_sync\n    return func(*args, **kwds)\ndask/async.py:264: in execute_task\n    result = _execute_task(task, data)\ndask/async.py:245: in _execute_task\n    args2 = [_execute_task(a, cache) for a in args]\ndask/async.py:245: in <listcomp>\n    args2 = [_execute_task(a, cache) for a in args]\ndask/async.py:246: in _execute_task\n    return func(*args2)\ndask/compatibility.py:26: in apply\n    return func(*args, **kwargs)\n/usr/lib/python3.5/site-packages/pandas/core/generic.py:1096: in to_hdf\n    return pytables.to_hdf(path_or_buf, key, self, **kwargs)\n/usr/lib/python3.5/site-packages/pandas/io/pytables.py:259: in to_hdf\n    complib=complib) as store:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <[RecursionError(\"maximum recursion depth exceeded\") raised in repr()] HDFStore object at 0x7f7298752198>, path = 'tst.h5', mode = 'a', complevel = 0, complib = None\nfletcher32 = False, kwargs = {}\n\n    def __init__(self, path, mode=None, complevel=None, complib=None,\n                 fletcher32=False, **kwargs):\n        try:\n            import tables  # noqa\n        except ImportError as ex:  # pragma: no cover\n            raise ImportError('HDFStore requires PyTables, \"{ex}\" problem '\n>                             'importing'.format(ex=str(ex)))\nE           ImportError: HDFStore requires PyTables, \"No module named 'tables'\" problem importing\n\n/usr/lib/python3.5/site-packages/pandas/io/pytables.py:389: ImportError\n========================================================= 2 failed, 846 passed, 21 skipped, 5 xfailed in 375.39 seconds =========================================================\n==> ERREUR\u00a0: Une erreur s\u2019est produite dans check().\n    Abandon...\n\n```\n\nPlease, read carefully my first post.\n",
      "well, you can submit a pull-request if you would like.\n\nI DID read your first post. we have 1600 issues.\n"
    ],
    "events": [
      "mentioned",
      "subscribed",
      "commented",
      "closed",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "reopened",
      "commented",
      "commented",
      "milestoned",
      "labeled",
      "labeled",
      "renamed",
      "milestoned",
      "demilestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 5,
    "additions": 83,
    "deletions": 20,
    "changed_files_list": [
      ".travis.yml",
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/computation/eval.py",
      "pandas/tests/frame/test_query_eval.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12759,
    "reporter": "sinhrks",
    "created_at": "2016-03-31T14:30:22+00:00",
    "closed_at": "2016-07-15T00:30:34+00:00",
    "resolver": "sinhrks",
    "resolved_in": "0a70b5fef3ae2363fea040ea47dd52247811c8c8",
    "resolver_commit_num": 335,
    "title": "API: Change Period('NAT') to return NaT, not Period instance",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n`Timedelta` returns `NaT` when input can be regarded as `NaT`. Otherwise `Period` returns its special representation (This was done in #7485 prior to `Timedelta`).\n\n\n#### Expected Output\n\n\n\nThe fix should affects:\n- [ ] `Period` and `PeriodIndex`, `Series` ops\n  - [ ] add, sub, comp\n- [x] `PeriodIndex` creation from list-like which contains `Period` and `NaT` (#13430)\n- [ ] `PeriodIndex` boxing\n- [ ] `.to_period`\n- [ ]  `__contains__` any NaT-like(`pd.NaT, None, float('nan'), np.nan`)  (#13582)\n#### output of `pd.show_versions()`\n\nCurrent master.\n",
    "labels": [
      "Missing-data",
      "API Design",
      "Period",
      "Clean"
    ],
    "comments": [
      "yep, looks good.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced"
    ],
    "changed_files": 8,
    "additions": 407,
    "deletions": 302,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/src/period.pyx",
      "pandas/tests/indexes/test_datetimelike.py",
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_base.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_tslib.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12766,
    "reporter": "denfromufa",
    "created_at": "2016-04-01T15:45:32+00:00",
    "closed_at": "2016-12-16T23:26:00+00:00",
    "resolver": "nateyoder",
    "resolved_in": "6f4e36a0f8c3638fe5dfe7bf68af079a2f034d00",
    "resolver_commit_num": 0,
    "title": "API: Index.map should return Index rather than array",
    "body": "\n\n\n\n\n\n\n\nI think `map` should accept `inplace`\n\n\n",
    "labels": [
      "API Design",
      "Compat",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "`.map` return an array, not an `Index`. So if that is fixed then this would work. \n\n@denfromufa pr's welcome.\n",
      "`inplace` is not acceptable for an immutable object.\n",
      "Not sure if it's that easy, but seemed like a good start to contribute something.\nPutting in a PR, I just turn it into `Index` before returning?\n",
      "Go for it! :)\n\nYes it should be that simple!\n\nOn Monday, April 4, 2016, Joerg Rings notifications@github.com wrote:\n\n> Not sure if it's that easy, but seemed like a good start to contribute\n> something.\n> Putting in a PR, I just turn it into Index before returning?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/pydata/pandas/issues/12766#issuecomment-205583011\n"
    ],
    "events": [
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "renamed",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 13,
    "additions": 188,
    "deletions": 61,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/categorical.py",
      "pandas/indexes/base.py",
      "pandas/indexes/category.py",
      "pandas/tests/indexes/test_base.py",
      "pandas/tests/indexes/test_category.py",
      "pandas/tests/series/test_apply.py",
      "pandas/tests/test_categorical.py",
      "pandas/tseries/base.py",
      "pandas/tseries/tests/test_converter.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_timedeltas.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12768,
    "reporter": "sebov",
    "created_at": "2016-04-01T15:53:52+00:00",
    "closed_at": "2016-04-03T14:33:08+00:00",
    "resolver": "jonaslb",
    "resolved_in": "64977f185747016e6939307b4f1a8258edcd7f89",
    "resolver_commit_num": 0,
    "title": "BUG: filter (with dropna=False) when there are no groups fulfilling the condition",
    "body": "For a DataFrame I want to preserve rows that belong to groups that fulfil specific condition and replace other rows with NaN. I have used a combination of 'groupby' and 'filter' (with dropna=False). In a special case when there are no groups fulfilling the condition an exception occured.\n\n\n\nThe problem I have identified is in the _apply_filter method of _GroupBy class (core/groupby.py) -- line with \"mask[indices.astype(int)] = True\" throws because in my case indices is equal to []; shouldn't it be \"indices = np.array([])\" instead of \"indices = []\" in the case when len(indices) == 0\n\n\n#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Bug",
      "Groupby",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "yep looks buggy.\n",
      "pull-requests welcome\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 16,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12769,
    "reporter": "max-sixty",
    "created_at": "2016-04-01T22:49:34+00:00",
    "closed_at": "2016-04-03T20:35:52+00:00",
    "resolver": "max-sixty",
    "resolved_in": "05b218922c31733056ca12efc3ffd0030e57be8d",
    "resolver_commit_num": 6,
    "title": "BUG: Resample loses PeriodIndex name",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n`Out[21]: '0.18.0'`\n",
    "labels": [
      "Bug",
      "Indexing",
      "Period",
      "Difficulty Novice",
      "Effort Low",
      "Resample"
    ],
    "comments": [
      "all these types of things are already fixed in #12743 . So will note this. actually. I take that back. something not getting propogated.\n"
    ],
    "events": [
      "renamed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "labeled",
      "cross-referenced"
    ],
    "changed_files": 4,
    "additions": 117,
    "deletions": 110,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12770,
    "reporter": "max-sixty",
    "created_at": "2016-04-01T23:07:50+00:00",
    "closed_at": "2016-04-13T01:12:56+00:00",
    "resolver": "max-sixty",
    "resolved_in": "77be872f4fcef90c648f714df2abafb422c7332a",
    "resolver_commit_num": 7,
    "title": "ERR: Resample pad on existing freq causes recursion error",
    "body": "Not a big bug, but not ideal behavior:\n\n\n#### Expected Output\n\nRaise on the initial resample? Return itself? Deliberately raise on `pad`?\n#### output of `pd.show_versions()`\n\n18.0\n",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "Resample",
      "Effort Low",
      "Period"
    ],
    "comments": [
      "this is a 1-line fix.\n\n```\nelif ax.freq == self.freq:\n    return self\n```\n\nin `_downsample`.\n\nmay need testing with both period & datetime\n",
      "any workaround right now?\n",
      "change the index to a `DateimeIndex` (you can anchor at `how='start' or 'end'`\n\n```\nIn [8]: series.index = series.index.to_timestamp()\n\nIn [9]: series\nOut[9]: \ndate\n2000-01-01    0\n2000-02-01    1\n2000-03-01    2\n2000-04-01    3\n2000-05-01    4\n2000-06-01    5\n2000-07-01    6\n2000-08-01    7\n2000-09-01    8\n2000-10-01    9\nFreq: MS, dtype: int64\n\nIn [10]: series.resample('M').first()\nOut[10]: \ndate\n2000-01-31    0\n2000-02-29    1\n2000-03-31    2\n2000-04-30    3\n2000-05-31    4\n2000-06-30    5\n2000-07-31    6\n2000-08-31    7\n2000-09-30    8\n2000-10-31    9\nFreq: M, dtype: int64\n\nIn [11]: series.resample('M').pad()\nOut[11]: \ndate\n2000-01-31    0\n2000-02-29    1\n2000-03-31    2\n2000-04-30    3\n2000-05-31    4\n2000-06-30    5\n2000-07-31    6\n2000-08-31    7\n2000-09-30    8\n2000-10-31    9\nFreq: M, dtype: int64\n```\n"
    ],
    "events": [
      "renamed",
      "renamed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "labeled",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 111,
    "deletions": 61,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12772,
    "reporter": "sinhrks",
    "created_at": "2016-04-02T02:56:04+00:00",
    "closed_at": "2016-04-03T17:03:18+00:00",
    "resolver": "sinhrks",
    "resolved_in": "36a8bd9cb71638abba3f2d97558b7d342d7ff273",
    "resolver_commit_num": 265,
    "title": "BUG: Series.quantile dtype / nan handling issue",
    "body": "Found some issues when working on #12572. Both being fixed.\n#### Code Sample, a copy-pastable example if possible\n##### 1. DataFrame may coerce to float even if interpolation\n\n\n##### 2. May return scalar even if its input is a list-like\n\n\n#### output of `pd.show_versions()`\n\nCurrent master.\n",
    "labels": [
      "Bug",
      "Numeric"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 13,
    "additions": 942,
    "deletions": 627,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/common.py",
      "pandas/core/frame.py",
      "pandas/core/internals.py",
      "pandas/core/series.py",
      "pandas/tests/frame/test_analytics.py",
      "pandas/tests/frame/test_apply.py",
      "pandas/tests/frame/test_quantile.py",
      "pandas/tests/series/test_analytics.py",
      "pandas/tests/series/test_apply.py",
      "pandas/tests/series/test_misc_api.py",
      "pandas/tests/series/test_quantile.py",
      "pandas/tseries/base.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12774,
    "reporter": "max-sixty",
    "created_at": "2016-04-02T05:03:08+00:00",
    "closed_at": "2016-04-13T01:12:56+00:00",
    "resolver": "max-sixty",
    "resolved_in": "77be872f4fcef90c648f714df2abafb422c7332a",
    "resolver_commit_num": 7,
    "title": "BUG: Count on resampled PeriodIndex fails",
    "body": "\n\nExpected:\n\n\n\nOn pandas 0.18\n",
    "labels": [
      "Bug",
      "Period",
      "Resample",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "yeah things much less tested with `PeriodIndex`, and some convoluted logic exists w.r.t. to the `kind` arg. Which we should prob just remove. If you want to `.resample` with a `PeriodIndex` just convert it. \n",
      "would appreciate a PR that goes thru all the available functions for periods.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 111,
    "deletions": 61,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12775,
    "reporter": "strnam",
    "created_at": "2016-04-02T07:46:34+00:00",
    "closed_at": "2016-04-22T15:20:13+00:00",
    "resolver": "gfyoung",
    "resolved_in": "5688d2771a00675bb530fc2d61fb8e356d8e134d",
    "resolver_commit_num": 12,
    "title": "read_csv return wrong dataframe when setting skiprows. ",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n\n\nIt should skip '1,\"line 11\\nline 12\",2' instead skip    '1,\"line 11' \n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.10.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.2.3-300.fc23.x86_64\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.1.1\nsetuptools: 18.0.1\nCython: None\nnumpy: 1.11.0\nscipy: 0.14.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 3.2.1\nsphinx: 1.2.3\npatsy: 0.4.1\ndateutil: 2.5.2\npytz: 2016.3\nblosc: None\nbottleneck: 0.6.0\ntables: 3.2.2\nnumexpr: 2.4.6\nmatplotlib: 1.4.3\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\n",
    "labels": [
      "Bug",
      "CSV",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "very tricky as you have embedded newlines within quoted fields. I guess the skip_lines is not accounting for the quoted fields (and ignoring them).\n",
      "this is related to #10911 \n",
      "cc @mdmueller\ncc @selasley\ncc @evanpw\n",
      "Thank you. That problem appear in my real project when other people give me a large csv file contain text column (text from news website). Because the file is big so I just want to read a part of file using skiprows parameter, but it don't work as I expect.   \n",
      "Using the python engine instead of the faster c engine works for the data given above\n\n```\nIn [4]: pd.read_csv(StringIO(data), skiprows=[1], engine='python')\nOut[4]: \n   id              text  num_lines\n0   2  line 21\\nline 22          2\n1   3           line 31          1\n```\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 129,
    "deletions": 20,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/parser/tokenizer.c",
      "pandas/src/parser/tokenizer.h"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12784,
    "reporter": "jreback",
    "created_at": "2016-04-03T15:38:21+00:00",
    "closed_at": "2016-04-03T18:15:01+00:00",
    "resolver": "jreback",
    "resolved_in": "ee4c2c7b0905d22dc12a481e98ed68a916c82ba4",
    "resolver_commit_num": 3961,
    "title": "COMPAT: sparse issue on compat build",
    "body": "xref #12779 \n\n-ci.org/pydata/pandas/jobs/120431675\n\nI suspect because of `numpy=1.7.1`\n\n\n\non win64, python 3.5, numpy ==1.10.4\nThis is prob a take issue as well, I think these should _always_ be `int32` (the block locations).\n\n\n",
    "labels": [
      "Sparse",
      "Compat",
      "Testing"
    ],
    "comments": [
      "@sinhrks \n",
      "2nd fixed here: https://github.com/jreback/pandas/commit/a6b1a221aa675fd2d0d0916f099af4cc3e60352f\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "renamed",
      "commented",
      "referenced",
      "referenced"
    ],
    "changed_files": 1,
    "additions": 8,
    "deletions": 3,
    "changed_files_list": [
      "pandas/sparse/tests/test_array.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12785,
    "reporter": "gfyoung",
    "created_at": "2016-04-03T16:47:24+00:00",
    "closed_at": "2016-04-14T01:17:55+00:00",
    "resolver": "gfyoung",
    "resolved_in": "2ea06019d64366b4028459fcc26c040beaf94f11",
    "resolver_commit_num": 8,
    "title": "DOC: Categorical sort_values and sort Documentation",
    "body": "In `categorical.py`, we enforce the fact that `self` must be ordered when calling `min` or `max`.  However, `self` can be unordered when calling `sort_values`.  This doesn't make sense in my mind, for if you can sort the values for unordered `self`, then I can then find a minimum value of `self`.  The same comment applies to `argsort` as well.\n",
    "labels": [
      "Difficulty Novice",
      "Error Reporting",
      "Categorical",
      "Effort Low"
    ],
    "comments": [
      "prob an overight, should be using `check_for_ordered(..)` internally there.\n",
      "Given how straightforward the PR is, would it be for `v0.18.1` or `v0.19.0`?\n",
      "its a bug, why would it not be 0.18.1?\n",
      "Just wanted to double check. :smile: \n",
      "Are we sure this is a bug? \nWe have had a lot of discussions about the API, and certainly also about the ordered implication, min() raising etc. But I can't really remember if we discussed this specific issue.\n\nIn any case, I think it can be quite annoying if you cannot sort values (eg to just group them). \nAnd as a comparison, R does allow to sort, but raises for `min()` in case of unordered factors.\n",
      "you could be right @jorisvandenbossche, I don't really remember.\n\ncc @janschulz \n",
      "@jorisvandenbossche : Is calling `.as_ordered()` that problematic?  How about my explanation above?\n\n@Everyone : Regardless of whether this is a bug or not, something will have to give.  The documentation insists that `argsort` and `sort` can only be called on ordered `Categorical` objects, but the internals do not match.\n",
      "That was a consious descision sometime after implementing categoricals (and I just saw that the docstring was not changed: https://github.com/pydata/pandas/blob/master/pandas/core/categorical.py#L205)\n\ncommit is here: https://github.com/pydata/pandas/commit/87fec5b527dcbb2dd99592e2d7e298cda5ac225d \n",
      "https://github.com/pydata/pandas/pull/9347 and #9611 \n",
      "Yep, I think it started here: https://github.com/pydata/pandas/pull/9611#issuecomment-77928273\n",
      "But in the end, it was changed here: https://github.com/pydata/pandas/pull/9622 (@janschulz you can see that in the commit (below the commit message))\n",
      "Perhaps refactoring `groupby` would be too difficult, but IMHO don't agree that the convenience is worth compromising the overall logical consistency of the interface.  I suspect there should exist some way to refactor groupby so that the categorical columns can be ordered internally.\n",
      "What are people's thoughts on this?  Should we stick with the current functionality, which is consistent with R but seems to be logically inconsistent, or should it be enforced?\n",
      "Personally, I would leave it as is. \nI also think it makes 'some' sense to be able to sort categoricals. To make the analogy, if you have colored cubes, you can 'sort' these to form groups of all cubes of the same color, but you cannot give an 'order' to them. Of course we use the `sort` method for both 'sort' and 'order' meaning in other cases, which makes it a bit confusing.\n\nAnyway, if we do the above, the docs of `sort_values` should be updated. \nAnd `argsort` does currently not even work on a categorical series:\n\n```\nIn [2]: s = pd.Series(['a', 'b', 'a', 'c'], dtype='category')\n\nIn [5]: s.argsort()\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-5-bb845bcfbc4f> in <module>()\n----> 1 s.argsort()\n\nc:\\users\\vdbosscj\\scipy\\pandas-joris\\pandas\\core\\series.py in argsort(self, axis\n, kind, order)\n   1848         else:\n   1849             return self._constructor(\n-> 1850                 np.argsort(values, kind=kind), index=self.index,\n   1851                 dtype='int64').__finalize__(self)\n   1852\n\nC:\\Anaconda\\envs\\devel\\lib\\site-packages\\numpy\\core\\fromnumeric.pyc in argsort(a\n, axis, kind, order)\n    906     except AttributeError:\n    907         return _wrapit(a, 'argsort', axis, kind, order)\n--> 908     return argsort(axis, kind, order)\n    909\n    910\n\nTypeError: argsort() takes at most 2 arguments (4 given)\n```\n\nAnother numpy compat issue .. :-)\n",
      "@jorisvandenbossche : Don't worry about the `numpy` compat issue.  I've got that one covered. :smile: But regarding the semantics of what `sort` means, okay, that is beginning to make more sense in my head.  I agree at the very least that the documentation should make that clearer because that meaning wasn't the first one that came to my head.\n",
      "<b>@Everyone</b>: I think if it is just a semantics thing, it's just a DOC-change PR then (not an API one as I had originally titled it)?\n",
      "yes why don't u update doc string and docs as needed to be more clear \n",
      "I have generalized this issue to just questions about `Categorical.sort` and `Categorical.sort_values` since this is another question about semantics and wording.  In the documentation, the wording does not make any mention of `na_position` being dependent on `ascending = False` (i.e. it is only respected provided `ascending = False`).  Just curious, why is that the case?\n",
      "@gfyoung Because having good docs is hard and needs many eyes. \nHowever, for me the question in this case is more: why is `na_position` only implemented for `ascending=False`?\n",
      "@jorisvandenbossche : Ah, that's what I actually meant.  Sorry, the \"it\" was a little vague!  My question is: why is `na_position` even dependent on `ascending` at all?\n",
      "@gfyoung `na_position` has to do with the nan sorting, IOW, are `nans` first or last. So it is tied to `ascending` by-definition. This is all doced for Series & Index I believe, many not for `Categorical`. this is why we use `shared_docs` as much as possible (though not in this case because it wasn't done, though it _should_ be done)\n",
      "@jreback : Does `first` and `last` mean that `nan` is the \"largest\" or \"smallest\" element in the `_codes`?  In that case, I can understand the dependence.  Nevertheless, it's not being respected when `ascending = True`, so something has to give here.  The documentation at the very least has to be updated to reflect that dependence.\n\n``` python\n>>> c = pd.Categorical([np.nan, 2, 2, np.nan, 5])\n>>> c.sort_values(ascending=True, na_position='first')\n# expected : [2.0, 2.0, 5.0, NaN, NaN]\n[NaN, NaN, 2.0, 2.0, 5.0]\nCategories: (2, int64): [2, 5]\n>>> c.sort_values(ascending=True, na_position='last')\n[NaN, NaN, 2.0, 2.0, 5.0]\nCategories: (2, int64): [2, 5]\n```\n",
      "```\nIn [6]: s = Series([np.nan, 2, 2, np.nan, 5])\n\nIn [7]: s.sort_values(ascending=True, na_position='first')\nOut[7]: \n0    NaN\n3    NaN\n1    2.0\n2    2.0\n4    5.0\ndtype: float64\n\nIn [8]: s.sort_values(ascending=True, na_position='last')\nOut[8]: \n1    2.0\n2    2.0\n4    5.0\n0    NaN\n3    NaN\ndtype: float64\n\nIn [9]: s.sort_values(ascending=False, na_position='first')\nOut[9]: \n0    NaN\n3    NaN\n4    5.0\n2    2.0\n1    2.0\ndtype: float64\n\nIn [10]: s.sort_values(ascending=False, na_position='last')\nOut[10]: \n4    5.0\n2    2.0\n1    2.0\n0    NaN\n3    NaN\ndtype: float64\n```\n",
      "@jreback : Ah, okay, so it makes sense to align with the `Series` API, but observe that `na_position` is completely independent of `ascending`.\n",
      "yep, misspoke, its independent, but would like to tie `Series,Index,Categorical` doc-strings at the least together\n",
      "@jreback : IMO the `Categorical` documentation should be separate because the meaning of `sort` is not the same as the other two per the discussion above.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "renamed",
      "renamed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 129,
    "deletions": 99,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/categorical.py",
      "pandas/tests/test_categorical.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12792,
    "reporter": "jreback",
    "created_at": "2016-04-04T15:15:55+00:00",
    "closed_at": "2016-04-05T13:37:52+00:00",
    "resolver": "jreback",
    "resolved_in": "fded94274effa2d592e48ec2079ba3b78e5bb232",
    "resolver_commit_num": 3965,
    "title": "COMPAT: some numpy 1.11 warnings",
    "body": "we are not fully testing with numpy 1.11 with all of our dependencies as they are not quite available in conda yet. But you can update an existing env and it will work. Some warnings in current master.\n\n\n\nThe errors are fixed in #12736 \n",
    "labels": [
      "Compat",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 6,
    "additions": 28,
    "deletions": 28,
    "changed_files_list": [
      "ci/requirements-3.5_OSX.build",
      "ci/requirements-3.5_OSX.run",
      "pandas/compat/numpy_compat.py",
      "pandas/io/pytables.py",
      "pandas/io/tests/test_pytables.py",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12794,
    "reporter": "gfyoung",
    "created_at": "2016-04-04T15:45:27+00:00",
    "closed_at": "2016-04-28T14:04:31+00:00",
    "resolver": "gfyoung",
    "resolved_in": "b13ddd57f6f88f75d987dd84d208a2329f1d94bb",
    "resolver_commit_num": 14,
    "title": "API, DOC: SparseArray Interface and Documentation is Confusing",
    "body": "1) `SparseArray` inherits from `np.ndarray`, but it doesn't seem like the instance itself can take on more than one dimension?  Documentation doesn't make that super clear IMO.\n\n2) If having 1-D sparse arrays is the intention, then I think there should be stronger checks against multi-dimensional `data` inputs.  For example, if I pass in a multi-dimensional `ndarray` into `SparseArray`, I get a not so helpful error message that it does not have an `sp_index`.\n",
    "labels": [
      "Sparse",
      "Docs"
    ],
    "comments": [
      "`SparseArray` is an implementation detail. Its not user facing.\n",
      "What do you mean by not \"user-facing\"?\n",
      "exactly what I mean. Its a detail that is hidden away from the user. Its the `.values` of the `SparseSeries`. In theory it could be swapped for another implementation (e.g. from scipy or whatever).  Its not needed by the user and is a sub-class of a numpy-array. So if anything you can simply update the docs to reflect this.\n",
      "Doc already says 1D though.\n",
      "I couldn't find anything that explicitly says that in `array.py` AFAICT\n",
      "@gfyoung you can certainly document it more if you want. \n",
      "@jreback : Sure thing.  It was just another thing I noticed as I have been squashing `fromnumeric.py` compat issues (same with the `Categorical` issue).\n",
      "http://pandas.pydata.org/pandas-docs/stable/sparse.html#sparsearray\n",
      "@kawochen : Fair enough, but I think the documentation internally should nevertheless reflect that as well.  Perhaps it may not be for user eyes, but for developer eyes, more info is better.\n",
      "I agree.\n",
      "@jreback : Made the doc changes in my massive PR #12810 as I was addressing incompatibilities in `pandas.sparse`, so I think this issue can allotted to `v0.18.1`.\n",
      "@gfyoung can you split this one off.\n",
      "@jreback : My computer has been acting a little weird recently, which is why I have been somewhat inactive (answering on phone ATM).  I'll get to this ASAP once I get my computer checked out.\n"
    ],
    "events": [
      "renamed",
      "commented",
      "labeled",
      "commented",
      "commented",
      "labeled",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 2,
    "additions": 9,
    "deletions": 3,
    "changed_files_list": [
      "pandas/sparse/array.py",
      "pandas/sparse/tests/test_array.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12797,
    "reporter": "sinhrks",
    "created_at": "2016-04-04T22:10:28+00:00",
    "closed_at": "2016-04-09T14:44:21+00:00",
    "resolver": "sinhrks",
    "resolved_in": "a23a136f859711478da64a5db6a56a8436135286",
    "resolver_commit_num": 278,
    "title": "BUG: Sparse incorrectly handle fill_value",
    "body": "Sparse looks to handle `missing (NaN)` and `fill_value` confusingly. Based on the doc, I understand `fill_value` is a user-specified value to be omitted in the sparse internal repr. `fill_value` may be different from missing (NaN).\n#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\nCurrent master.\n\nThe fix itself looks straightforward, but it breaks some tests use dubious comparison.\n- #L1730\n",
    "labels": [
      "Bug",
      "Missing-data",
      "Sparse"
    ],
    "comments": [
      "hmm, I think its using `np.nan` as the missing value indicator. Which is right. THEN you fill using the `fill_value` those locations. not the other way around.\n",
      "@jreback I may misunderstand, but `fill_value` will be a missing value indicator if provided (`np.nan` is included in `SparseIndex` indices).\n\n```\npd.SparseArray([1, np.nan, 0, 3, np.nan], fill_value=0)\n[1.0, nan, 0, 3.0, nan]\nFill: 0\nIntIndex\nIndices: array([0, 1, 3, 4], dtype=int32)\n```\n\nThus I feel it is natural to `.to_dense` returns `np.nan` as it is, not `fill_value`.\n",
      "in your example the 0 (2nd element) is the missing one. \n\n```\nIn [5]: pd.SparseArray([1, np.nan, 0, 3, np.nan], fill_value=0).to_dense()\nOut[5]: array([ 1.,  0.,  0.,  3.,  0.])\n```\n\nahh so you think this should be\n`Out[5]: array([ 1.,  np.nan,  0.,  3.,  np.nan])`\n\nyes that is prob right.\n",
      "Ah sorry, added `Expected Output` section.\n",
      "yep that looks right.\n\nyeh I that comparison tests equates `NaN` to missing value, when in fact the `fill_value` are the missing ones.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 10,
    "additions": 332,
    "deletions": 190,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/internals.py",
      "pandas/indexes/base.py",
      "pandas/sparse/array.py",
      "pandas/sparse/frame.py",
      "pandas/sparse/tests/test_array.py",
      "pandas/sparse/tests/test_frame.py",
      "pandas/sparse/tests/test_indexing.py",
      "pandas/sparse/tests/test_panel.py",
      "pandas/sparse/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12809,
    "reporter": "randomgambit",
    "created_at": "2016-04-05T23:54:25+00:00",
    "closed_at": "2016-04-22T15:09:57+00:00",
    "resolver": "kshedden",
    "resolved_in": "33683cc0be98b1b7c6708d214588d99101eaad04",
    "resolver_commit_num": 15,
    "title": "Pandas read_sas error: 'ascii' codec can't decode byte 0xd8 in position 0: ordinal not in range(128)",
    "body": "Hello everybody,\n\nI am using **Pandas 0.18** to open a `sas7bdat` dataset \n\nI simply use:\n\n\n\nand I get the following error\n\n\n\nIf I use\n\n\n\nI get\n\n\n\nOther `sas7bdat` files in my folder are handled just fine by Pandas. \n\nWhen I open the file in SAS I see that the column names are very long and span several lines, but otherwise the files look just fine.\n\nThere are not so many possible options in `read_sas`... what should I do? Is this a bug in `read_sas`? \n\nMany thanks!\n",
    "labels": [
      "IO SAS",
      "Unicode"
    ],
    "comments": [
      "Are you able to share that file, or a similar file with non-senstitive data that raises the same error?\n",
      "well this is the problem.. I cant. but I can do my best to run tests on my side, or do stuff in sas, or whatever you need to sort out the problem\n",
      "You said the lines were long and span several lines. Can you make a dummy file with long names (just random strings like `AAAAA....` might work) and a bit of fake data? (I don't have a copy of SAS). \n\nActually, this might be a dupe of https://github.com/pydata/pandas/issues/12659 Can you try reading the file [linked there](https://github.com/pydata/pandas/blob/8857008178eb90b59ca67874507ef860b3faf88f/pandas/io/tests/sas/data/test17.sas7bdat) and see if the same error is raised?\n",
      "when I try to read that file, I get `TypeError: read() takes at most 1 argument (2 given)`\n",
      "Just making sure, did you click on the `raw` link to download?\n\nhttps://github.com/pydata/pandas/raw/8857008178eb90b59ca67874507ef860b3faf88f/pandas/io/tests/sas/data/test17.sas7bdat\n",
      "yes, I dowloaded the file test17.sas7bdat. This is not the error expected?\n",
      "I checked the details of the file in **SAS**. Apparently it is encoded in latin 1 western.\nSo I tried `read_sas('myfile.sas7bdat', encoding='latin-1')` but I get the same error \n\n-- ascii codec cant decode byte etc..\n",
      "Sorry, I was mistaken about the error message. Looks like this is a different issue.\n",
      "something strange is that even if I specify some encoding, I still get some error relative to the ascii codec. Can that be a cause of the error?\n",
      "the encoding of my sas file is more precisely latin1 western ISO. Created in linux. (but I use pandas on windows)\n",
      "Can you drop into the debugger after it raises the error? `%debug` if your in IPython. Then you can see what's going on.\n\nThe docstring says `encoding` is just for decoding string columns (the actual values), so perhaps it isn't being applied to decoding the column names.\n",
      "aha! ok lemme try the debugger\n",
      "```\n\n> c:\\users\\me\\appdata\\local\\continuum\\anaconda2\\lib\\site-packages\\pandas\\io\\sas\\sas7bdat.py(529)_process_columntext_subheader()\n    527         buf = self._read_bytes(offset, text_block_size)\n    528         self.column_names_strings.append(\n--> 529             buf[0:text_block_size].rstrip(b\"\\x00 \").decode())\n    530 \n    531         if len(self.column_names_strings) == 1:\n\n```\n",
      "this is what I get. then the debugger seems to wait for instructions\n",
      "Ahh, that looks promising though. Does `buf[0:text_block_size].rstrip(b\"\\x00 \").decode('latin1')` work there?\n\nAlthough, that might not go well with the bit stripping there...\n",
      "what do you mean? what should I do?\nsorry I never user the debugger..\n",
      "OK Tom, I found a fix.\n\nJust check the encoding of your sas file (right click, properties, details) and set the encoding.\n\nimport sys\nreload(sys)\nsys.setdefaultencoding(\"latin-1\")\n\nthe question I have is thus: why specifying the encoding in the read_sas function does nothing?\n",
      "I believe the `encoding` parameter is just used to decode text data in the actual DataFrame itself, and not the metadata like column headers. Does that sound correct @kshedden ?\n",
      "According to the docs below, depending on the setting of the VALIDVARNAME\noption, variable names may be either restricted to ASCII, or may be\narbitrary bytes to be decoded somehow:\n\nhttp://support.sas.com/documentation/cdl/en/lrcon/68089/HTML/default/viewer.htm#p18cdcs4v5wd2dn1q0x296d3qek6.htm\n\nI'm not sure if this VALIDVARNAME (which I have never heard of before) is\nin the file somewhere, or is an option that you specify within the\nsession.  In any case, it appears that the column names may need to be\ndecoded.\n\nAlso relevant:\n\nhttp://support.sas.com/documentation/cdl/en/nlsref/61893/HTML/default/viewer.htm#a002601944.htm\n\nOn Wed, Apr 6, 2016 at 8:27 AM, Tom Augspurger notifications@github.com\nwrote:\n\n> I believe the encoding parameter is just used to decode text data in the\n> actual DataFrame itself, and not the metadata like column headers. Does\n> that sound correct @kshedden https://github.com/kshedden ?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/pydata/pandas/issues/12809#issuecomment-206347584\n",
      "yes, makes sense although I dont have any control over the creation of these sas files.\n",
      "I'm working on a PR https://github.com/pydata/pandas/pull/12656 and will\ntry to work this into it.\n\nI haven't had much time lately but will try to get to this next week.\n\nKerby\n\nOn Wed, Apr 6, 2016 at 9:02 AM, randomgambit notifications@github.com\nwrote:\n\n> yes, makes sense although I dont have any control over the creation of\n> these sas files.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/pydata/pandas/issues/12809#issuecomment-206359674\n",
      "@randomgambit, can you try this branch against your SAS file:\n\nhttps://github.com/kshedden/pandas/tree/sas7bdat_perf\n\nI hope it fixes your problem.\n"
    ],
    "events": [
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "labeled"
    ],
    "changed_files": 12,
    "additions": 2111,
    "deletions": 432,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/io/sas/sas7bdat.py",
      "pandas/io/sas/sas_constants.py",
      "pandas/io/sas/saslib.pyx",
      "pandas/io/tests/sas/data/airline.csv",
      "pandas/io/tests/sas/data/airline.sas7bdat",
      "pandas/io/tests/sas/data/productsales.csv",
      "pandas/io/tests/sas/data/productsales.sas7bdat",
      "pandas/io/tests/sas/data/test_12659.csv",
      "pandas/io/tests/sas/data/test_12659.sas7bdat",
      "pandas/io/tests/sas/test_sas7bdat.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12811,
    "reporter": "gfyoung",
    "created_at": "2016-04-06T13:16:42+00:00",
    "closed_at": "2016-05-20T13:19:33+00:00",
    "resolver": "gfyoung",
    "resolved_in": "fecb2ca8559ceee5ce1e4ecd48c7e8a7560d4ce0",
    "resolver_commit_num": 19,
    "title": "fromnumeric.py compatibility with GroupBy, Window, and tslib functions",
    "body": "In #12810, it was decided that compatibility for `groupby` (including `resample.py`) and `window` functions would be left for a separate PR / discussion, which seems reasonable given how massive #12810 already is.  This issue serves a reminder to tackle this after landing #12810, as it seems like this can be easily addressed afterwards.\n",
    "labels": [
      "Compat"
    ],
    "comments": [
      "@jreback : Adding timestamps and timedeltas as well to this issue given my question about `tslib.pyx` (i.e. what sort of compatibility should we give to methods with `numpy` counterparts?)\n",
      "With my initial `fromnumeric.py` PR merged, it seems like a good idea to revisit this.  The major files that I think merit examination for `numpy` compatibility are:\n\n`pandas/core/window/window.py`\n`pandas/tseries/resample.py`\n`pandas/core/groupby.py`\n`pandas/tslib.pyx`\n",
      "how so, I don't really care to be compat with numpy for anything beyond very basic stuff. pls show an example. \n",
      "Examples:\n\n`tslib.round(self, freq)` vs. `np.round(a, decimals=0, out=None)`\n\n`window.max(self, how=None, **kwargs)` vs. `np.max(a, axis=None, out=None, keepdims=False)`\n\n`resample.var(self, ddof=1)` vs. `np.var(a, axis=None, out=None, ddof=0, keepdims=False)`\n\n`groupby.mean(self)` vs. `np.mean(a, axis=None, out=None, keepdims=False)`\n\nAll I was thinking of doing was putting validation calls in the implementation, similar to what was done in my previous PR and nothing more than that.  I'm also perfectly fine leaving them as is since `numpy` decoupling is also one of our objectives with `pandas`.\n",
      "```\nIn [1]: df = DataFrame({'A' : [1,2,1], 'B' : [1,2,3]})\n\nIn [2]: g = df.groupby('A')\n\nIn [3]: g.mean()\nOut[3]: \n   B\nA   \n1  2\n2  2\n\nIn [4]: np.mean(g)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-4-fdcd38489fcb> in <module>()\n----> 1 np.mean(g)\n\n/Users/jreback/miniconda/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc in mean(a, axis, dtype, out, keepdims)\n   2878         try:\n   2879             mean = a.mean\n-> 2880             return mean(axis=axis, dtype=dtype, out=out)\n   2881         except AttributeError:\n   2882             pass\n\nTypeError: mean() got an unexpected keyword argument 'axis'\n```\n",
      "ok that doesn't seem unreasonable\n",
      "Do we actually _want_ something like `np.mean(g)` to work? \nA groupby object is not an array-like such as a Series. IMO we shouldn't put effort in enabling such usage\n",
      "@jorisvandenbossche : I'll leave that for you to debate with @jreback .  To reiterate, I am perfectly fine either way.  This is not as serious a compatibility issue as the previous one I raised in #12644.\n"
    ],
    "events": [
      "cross-referenced",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "commented",
      "renamed",
      "commented",
      "commented",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 12,
    "additions": 316,
    "deletions": 49,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/compat/numpy/function.py",
      "pandas/core/common.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/window.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_window.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py",
      "pandas/tseries/tests/test_tslib.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12814,
    "reporter": "sbuser",
    "created_at": "2016-04-06T16:03:22+00:00",
    "closed_at": "2016-07-19T01:52:00+00:00",
    "resolver": "pijucha",
    "resolved_in": "b225cacb1d2a34e3c4041533a0590133098756fa",
    "resolver_commit_num": 3,
    "title": "BUG: merging with mixed types objects in py3 when unorderable",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n\nWhatever my data looks like, a copy, minus some rows, merged (on indexes) with a copy should function without error, no?\n#### Expected Output\n\n> \n\n`\\Anaconda3\\lib\\site-packages\\pandas\\tools\\merge.py\", line 535, in _get_join_indexers\n    llab, rlab, shape = map(list, zip(* map(fkeys, left_keys, right_keys)))\nTypeError: type object argument after * must be a sequence, not map`\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 30 Stepping 5, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.1.0\nsetuptools: 20.2.2\nCython: 0.23.4\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.0.3\nsphinx: 1.3.1\npatsy: 0.4.0\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.4.6\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.8.4\nlxml: 3.5.0\nbs4: 4.4.1\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.11\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.39.0\nNone\n",
    "labels": [
      "Reshaping",
      "Dtypes",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "pls provide a minimal but complete copy-pastable example\n",
      "Weirdly the following trivial example does work:\n\n```\nimport pandas as pd\n\nd = {'col1': 'foo', 'col2': 'bar'}\ndf = pd.DataFrame(data=d, index=[1, 2, 3, str(1)])\n\ndf1 = df.copy(deep=True)\nprint(df1)\n\ndf2 = df.copy(deep=True)\nprint(df2)\n\ndf2 = df2[(df2.index != 1)] #take a subset\nprint(df2)\n\ncommon = pd.merge(df1, df2, left_index=True, right_index=True)\nprint(common)\n```\n\nSo the problem must be with my data somehow. What's the best way to get my data into a trivial example? If I copy out strings I potentially lose datatypes and so forth. Will a pickled version of a few rows work?\n",
      "sure if you have a reproducible and you don't mind sharing then that would work.\n",
      "This is rather tricky to reproduce, but I had the same issue. Here is a minimal example that triggers it for me:\n\n```\nimport pandas as pd\nfrom math import nan\na = pd.DataFrame({'a': [1, 2, 3]}, index=[1, 2, 'a'])\nb = pd.DataFrame({'b': [2, 3, 4]}, index=[1, nan, nan])\na.join(b)\n```\n\nI had to try a lot of combinations to nail it down, and it seems that the following conditions are needed to trigger this:\n- Exactly one of the indices is of type `object`, the other one is of type `float` - that's why one index contains a string in the example. (If both are object or both are float then it does not produce an error)\n- One index contains at least two `nan` values\n- The values are irrelevant, this is specifically about indices\n\nCheers,\nAdam\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.2.0-36-generic\nmachine: x86_64\nprocessor: \nbyteorder: little\nLC_ALL: en_US.UTF-8\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: None\npip: 8.1.1\nsetuptools: 22.0.0\nCython: 0.24\nnumpy: 1.11.0\nscipy: 0.17.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.2.0\nsphinx: None\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: 2.6.0\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: 0.9.4\nxlwt: None\nxlsxwriter: 0.8.9\nlxml: None\nbs4: 4.4.1\nhtml5lib: None\nhttplib2: 0.9.2\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\npandas_datareader: None\n```\n",
      "you realized that doing `from math import nan` is completely useless as numpy is the definer of `nan` (they are the same), but that is completely non-idiomatic and just plain confusing.\n\nThe issue is mixed object indexes, not a good idea to have mixed types like this in the index ever (or in a column for that matter).\n\nYes this does trigger an error. If you want to have a look, go for it.\n\n```\nipdb> p list(map(fkeys, left_keys, right_keys))\n*** TypeError: unorderable types: str() > int()\n```\n",
      "xref #13432  which is the same unsortable condition.\n\ncc @pijucha \n"
    ],
    "events": [
      "commented",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 11,
    "additions": 583,
    "deletions": 63,
    "changed_files_list": [
      "asv_bench/benchmarks/index_object.py",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/algorithms.py",
      "pandas/indexes/base.py",
      "pandas/tests/indexes/common.py",
      "pandas/tests/indexes/test_base.py",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tests/test_algos.py",
      "pandas/tests/test_groupby.py",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_join.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12817,
    "reporter": "wcbeard",
    "created_at": "2016-04-06T19:53:51+00:00",
    "closed_at": "2016-07-20T21:23:30+00:00",
    "resolver": "sinhrks",
    "resolved_in": "016b35276eea344b861147dfff2d4ff8ae52aadc",
    "resolver_commit_num": 343,
    "title": "ENH: Faster hashing of Period objects",
    "body": "I've noticed that a lot of manipulations using Periods are pretty slow. It [looks like it's hashing](#L789) the tuple of the `ordinal` and `freq` attributes. I'm not sure what the mapping is between `freq` and `freqstr` is, but if `freqstr` can stand in for `freq`, it looks like hashing the string gives a decent speedup. Subclassing `Period` with this change speeds up operations like `drop_duplicates` a lot on my machine.\n\n\n\nDoes this change seem reasonable?\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.3.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.0\nnose: None\npip: 8.1.1\nsetuptools: 20.3\nCython: 0.23.5\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: None\nxarray: None\nIPython: 3.2.3\nsphinx: None\npatsy: None\ndateutil: 2.5.2\npytz: 2016.3\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: 2.5.1\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: 0.9.4\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\n",
    "labels": [
      "Performance",
      "Dtypes",
      "Period"
    ],
    "comments": [
      "The root cause is `Period` is internally stored as `object`. xref #7964\n",
      "A cleaner way to do this might be to update [`DateOffset.__hash__`](https://github.com/pydata/pandas/blob/48f39af49a9ae76c2c7d8f519df6069e7af00755/pandas/tseries/offsets.py#L375) to use `freqstr` or `rule_code` instead of `_params()`.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 5,
    "additions": 152,
    "deletions": 29,
    "changed_files_list": [
      "asv_bench/benchmarks/period.py",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/src/period.pyx",
      "pandas/tseries/tests/test_base.py",
      "pandas/tseries/tests/test_period.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12821,
    "reporter": "lvphj",
    "created_at": "2016-04-07T12:24:32+00:00",
    "closed_at": "2016-08-06T22:57:12+00:00",
    "resolver": "facaiy",
    "resolved_in": "63e8f689a13650bda01281ef257f9266e6394881",
    "resolver_commit_num": 2,
    "title": "agg() function on groupby dataframe changes dtype of datetime64[ns] column to float64 if all items in a single group are NaT",
    "body": "The example below shows two variations of a dataframe which contains a date column set to datetime64[ns] format.\n\nIn the first example, there is a single missing (NaT) date. After groupby and agg(), the dtypes of all the columns in the aggregated dataframe are the same as the original dataframe, as expected (and as desired).\n\nHowever, in the second example, there are several missing dates, arranged so that all the dates in one  group are NaT. After the same groupby and agg() procedures, the dtype of the date column is changed to float64. This is undesired behaviour in my situation and I believe it is a bug.\n#### Code Sample, a copy-pastable example if possible\n\n\n## Expected Output\n\nThe expected output would be for the dtypes in the dataframe after aggregation to be the same as those in the original dataframe.\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.4.1.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.3.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_GB.UTF-8\n\npandas: 0.17.0\nnose: 1.3.7\npip: 1.5.6\nsetuptools: 3.6\nCython: None\nnumpy: 1.10.1\nscipy: None\nstatsmodels: None\nIPython: 4.0.0\nsphinx: None\npatsy: None\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.4.3\nopenpyxl: 2.3.0\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\n",
    "labels": [
      "Bug",
      "Timeseries",
      "Groupby",
      "Missing-data",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "So this works for a frame aggregation, but doesn't for the `.agg`, ok a bug.\n\npull-requests welcome.\n\n```\nIn [32]:  phjTempDF.sort_values(['gender','age','id']).groupby(['gender','age']).first()\nOut[32]: \n                            date  id\ngender age                          \nfemale old                   NaT   2\n       young 2015-12-05 14:19:00  11\nmale   old   2015-12-04 01:00:00   4\n       young 2015-02-04 02:34:00   1\n\nIn [33]: phjTempDF.sort_values(['gender','age','id']).groupby(['gender','age']).agg({'date': 'first','id': 'first'})\nOut[33]: \n                      date  id\ngender age                    \nfemale old             NaN   2\n       young  1.449325e+18  11\nmale   old    1.449191e+18   4\n       young  1.423017e+18   1\n```\n",
      "I'd like to take a look.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 6,
    "additions": 68,
    "deletions": 5,
    "changed_files_list": [
      ".gitignore",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/nanops.py",
      "pandas/tests/frame/test_timeseries.py",
      "pandas/tests/test_groupby.py",
      "pandas/types/cast.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12824,
    "reporter": "ruoyu0088",
    "created_at": "2016-04-08T07:50:09+00:00",
    "closed_at": "2016-05-20T14:09:29+00:00",
    "resolver": "adneu",
    "resolved_in": "cc25040798e016fbcdbba45a927deabedd84ea37",
    "resolver_commit_num": 1,
    "title": "groupby().apply() returns different result depends on the first result is None or not.",
    "body": "The apply document says that it can:\n\n> apply can act as a reducer, transformer, or filter function, depending on exactly what is passed to apply. So depending on the path taken, and exactly what you are grouping. Thus the grouped columns(s) may be included in the output as well as set the indices. \n\nSo, I think it may discard the group if the callback function returns None. Here is two exmaple that works and not works:\n\n\n\nthe output is as following, the first one returns a DataFrame, the second one returns a Series with DataFrames inside:\n\n\n\nThe problem is that `_wrap_applied_output()` in `groupby.py` checks the first element to determine the concat method:\n\n\n\nI want to know, does `groupby().apply()` support discarding groups?\n",
    "labels": [
      "Bug",
      "Groupby",
      "Reshaping",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "yeah that prob should look at the non-Nones to infer what is going on. This is done in the other `_wrap_applied_output` for `NDFrame` so you can copy the same pattern.\n\nwant to do a pull-request?\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 51,
    "deletions": 18,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12839,
    "reporter": "sinhrks",
    "created_at": "2016-04-09T18:15:06+00:00",
    "closed_at": "2016-07-06T21:47:54+00:00",
    "resolver": "adneu",
    "resolved_in": "cc0a188addb46f7b4986dce32947e66295f1bb3b",
    "resolver_commit_num": 2,
    "title": "GroupBy.nth includes group key inconsistently",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n`nth` doesn't inlcude group key as the same as `first` and `last`.\n\n\n\nHowever, calling `head` makes the behavior change. Looks to be caused by `_set_selection_from_grouper` caches its selection.\n\n\n#### Expected Output\n\nalways as below.\n\n\n#### output of `pd.show_versions()`\n\ncurrent master.\n",
    "labels": [
      "Bug",
      "Groupby"
    ],
    "comments": [
      "this is fixed in #11039  (but it IS a change)\n",
      "also some related issues there #7569\n",
      "Thx, haven't noticed. This is also related to #7453 (last `nth` issue).\n",
      "We can enable `nth` tests locates on `test_resample` after both this and #12840 are fixed.\n",
      "This isn't fixed by #11039. Reopen.\n",
      "```\nIn [1]: df = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [1, 2, 3, 4, 5],\n   ...:                    'G': [1, 1, 2, 2, 1]})\n\nIn [2]: \n\nIn [2]: g = df.groupby('G')\n\nIn [3]: g.nth(1)\nOut[3]: \n   A  B\nG      \n1  2  2\n2  4  4\n\nIn [4]: g.head()\nOut[4]: \n   A  B\n0  1  1\n1  2  2\n2  3  3\n3  4  4\n4  5  5\n```\n",
      "Call `head` before `nth`.\n\n```\ng = df.groupby('G')\ng.head()\ng.nth(1)\n#    A  B  G\n# G         \n# 1  2  2  1\n# 2  4  4  2\n```\n",
      "@sinhrks ahh, so something is changing state. ok!\n",
      "On a related note, which version of `g.head()` below is expected?\n\n```\nIn [2]: g = df.groupby('G')\n\nIn [3]: g.head()\nOut[3]: \n   A  B  G\n0  1  1  1\n1  2  2  1\n2  3  3  2\n3  4  4  2\n4  5  5  1\n\nIn [4]: g = df.groupby('G')\n\nIn [5]: g.nth(1)\nOut[5]: \n   A  B\nG      \n1  2  2\n2  4  4\n\nIn [6]: g.head()\nOut[6]: \n   A  B\n0  1  1\n1  2  2\n2  3  3\n3  4  4\n4  5  5\n```\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "closed",
      "commented",
      "reopened",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 57,
    "deletions": 11,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12850,
    "reporter": "jreback",
    "created_at": "2016-04-10T16:05:55+00:00",
    "closed_at": "2016-04-10T19:09:46+00:00",
    "resolver": "jreback",
    "resolved_in": "51ac0224a99d6b9b13d72715f8d4f123ef8ce1d9",
    "resolver_commit_num": 3973,
    "title": "TST/BUG: integer coercion bool with int on windows",
    "body": "xref #12841\n\nthis is a test in `test_coercion.py, L361`\n\n\n\non windows the scalar is not coerced to int64 (as it is on other platforms). IIRC `.where` already has quite complicated logic to do this. How simple is this to fix?\n\ncc @sinhrks \n",
    "labels": [
      "Dtypes",
      "Windows"
    ],
    "comments": [
      "Maybe it is caused by `NumPy`? I don't think everything must be fixed on `pandas` side, but need to clarify what the current behaviors are.\n",
      "oh it IS caused by numpy as that defaults to `int32` when creating integers on windows. However pandas coerces everything internally to int64. So just need to investigate if this is by a bug in the logic somewhere. Or we can just change the test. I'll take a look.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "mentioned",
      "subscribed",
      "milestoned",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 48,
    "deletions": 1,
    "changed_files_list": [
      "pandas/core/common.py",
      "pandas/core/internals.py",
      "pandas/tests/test_common.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12855,
    "reporter": "gfyoung",
    "created_at": "2016-04-10T22:33:40+00:00",
    "closed_at": "2016-12-15T11:28:26+00:00",
    "resolver": "gfyoung",
    "resolved_in": "33e11ade7d1c5753ba3538e539e0917db73b257e",
    "resolver_commit_num": 107,
    "title": "API: Sparse Return Types",
    "body": "The documentation of `SparseArray.cumsum` says it returns a `Series`, but that is not the case:\n\n\n\nGiven that `to_dense` now returns `self.values` and ignores `fill_value`, this current behaviour in `cumsum` seems illogical and should probably just return `SparseArray` regardless.  At the very least though, I think a documentation fix is in order.\n\nEDIT: there is a similar issue with `SparseSeries` that should also be addressed\n",
    "labels": [
      "Docs",
      "Sparse",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "these were all JUST fixed by @sinhrks if you would like to make a master documentation issue, pls do. but leave it as this one. \n",
      "@jreback : Yes, I am aware of what @sinhrks did, as this is against the `master` branch.  He did not make any changes to `cumsum` in his PR.\n",
      "ok, then if you find more things. just add them at the top, DONT't create more issues about this\n",
      "though this is actually not a documentation issue at all. but a functional one.\n",
      "@jreback : well at the very least, the documentation should be changed to match?\n",
      "@gfyoung ABOSOLUTEY NOT. Its an issue, you don't just change the documentation you fix the problem!\n",
      "I'm confused.  We're currently providing incorrect information in the documentation...I meant this should be a temporary measure.\n",
      "@gfyoung how are you confused? its an api issue, changing the documentation (which is only inherited) is pointless. you can certainly propose a fix for the issue. fixing documentation is a band-aid which is not even worth doing.\n",
      "@jreback : If you had just said, \"it's not worthwhile to change the documentation as a stopgap measure even though it is incorrect,\" instead of your last two comments, that would have cleared up my confusion.\n",
      "@gfyoung thanks for the catch. Can it be included in #12810? Lmk if any fix is needed on my side.\n",
      "@sinhrks : https://github.com/pydata/pandas/pull/12810#discussion-diff-59023114\n",
      "Looking at this issue again, I must say I find this `SparseArray.cumsum` API quite confusing.  Isn't the point of this function to provide a cumulative sum as we slowly progress through the array using the `sum` function defined for `SparseArray` i.e.:\r\n\r\n``` python\r\n>>> s = SparseArray([1, 2, 2, np.nan, 2, 2, np.nan], fill_value=2)\r\n>>> s.sum()\r\n9.0\r\n>>> s.cumsum()\r\narray([1., 3., 5., nan, nan, nan, nan])\r\n```\r\n\r\nI would have expected the following:\r\n\r\n``` python\r\narray([1., 3., 5., nan, nan, 7.0, 9.0, nan])\r\n```\r\n\r\nEDIT: Same is true for `SparseSeries`\r\n\r\n@jreback , @jorisvandenbossche , @sinhrks : Thoughts?\r\n",
      "@gfyoung +1. We should make an agreement that ``SparseArray`` is compat with ``pandas`` internals rather than ``numpy``. \r\n\r\n```\r\npd.Series([1, 2, 2, np.nan, 2, 2, np.nan]).cumsum()\r\n# 0    1.0\r\n# 1    3.0\r\n# 2    5.0\r\n# 3    NaN\r\n# 4    7.0\r\n# 5    9.0\r\n# 6    NaN\r\n# dtype: float64\r\n\r\nnp.cumsum([1, 2, 2, np.nan, 2, 2, np.nan])\r\n# array([  1.,   3.,   5.,  nan,  nan,  nan,  nan])\r\n```",
      "@sinhrks : I was about to implement a fix, but what is the meaning of `fill_value` exactly?  Because I thought it should have an impact on aggregation behaviour:\r\n~~~python\r\n>>> data = np.arange(10)\r\n>>> s = SparseArray(data, fill_value=2)\r\n>>> s.sum()\r\n45\r\n>>> s = SparseArray(data, fill_value=np.nan)\r\n>>> s.sum()\r\n45\r\n~~~\r\nIIUC, `fill_value=2` implies that any data point with \"2\" as the value is considered a null / missing data point, meaning that the first sum should be 43?",
      "``fill_value`` is unrelated to normal missing values. It is to specify values omitted in sparse repr. This shouldn't affect to dense / aggregated results.\r\n\r\n```\r\n# this omits np.nan, holds [1, 1] as sparse values with its locations\r\npd.SparseArray([np.nan, np.nan, 1, 1], fill_value=np.nan)\r\n# [nan, nan, 1.0, 1.0]\r\n# Fill: nan\r\n# IntIndex\r\n# Indices: array([2, 3])\r\n\r\n# this omits 1, holds [nan, nan] as sparse values with its locations\r\npd.SparseArray([np.nan, np.nan, 1, 1], fill_value=1)\r\n# [nan, nan, 1, 1]\r\n# Fill: 1\r\n# IntIndex\r\n# Indices: array([0, 1])\r\n```",
      "@sinhrks : Okay, so the sums should be the same?",
      "Yes, should be the same."
    ],
    "events": [
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "renamed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "renamed",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "renamed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 84,
    "deletions": 52,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/sparse/array.py",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_array.py",
      "pandas/sparse/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12866,
    "reporter": "stellasia",
    "created_at": "2016-04-11T17:03:37+00:00",
    "closed_at": "2016-04-18T17:22:25+00:00",
    "resolver": "jreback",
    "resolved_in": "bac68d6bb9ea22e9e744fe709d1270f257d71e0f",
    "resolver_commit_num": 3980,
    "title": "read_json changes dtype (int => float)",
    "body": "Hi there!\n\nThis might be a well-known problem but could not find a track/explaination about it. When reading a json to create a pandas object (Series or DataFrame), the index dtype is changed from int to float.\n\nThe  [doc](-docs/stable/io.html#data-conversion) only mentions the inverse trans-typing:\n\n> a column that was float data will be converted to integer if it can be done safely, e.g. a column of 1.\n\nIs this behaviour expected as well? Is the only solution giving the `read_json` a `dtype` argument? \n#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\nI would like to recover the integer index for the new `Series` : \n\n\n#### Output of `pd.show_versions()`\n\n\n\nThanks!\n",
    "labels": [
      "Bug",
      "Dtypes",
      "IO JSON"
    ],
    "comments": [
      "both other orients recover this, I don't know why `orient='index'` (the default does not).\ncc @Komnomnomnom\n\n```\nIn [7]: pd.read_json(s.to_json(orient='split'),typ='series',orient='split')\nOut[7]: \n0    11\n1    12\n2    13\ndtype: int64\n\nIn [9]: pd.read_json(s.to_json(orient='records'),typ='series',orient='records')\nOut[9]: \n0    11\n1    12\n2    13\ndtype: int64\n```\n",
      "Indeed, `orient='split'` is working fine (even better for my needs). Thanks.\n\nI would be interested in understanding the reason of the different behaviour for `orient='index'` if it is known.\n",
      "The default `orient='index'` converts the pandas object to a JSON object which must have string keys. Therefore when decoded the index values are all strings. They are then (by default) promoted to a float dtype [here](https://github.com/pydata/pandas/blob/master/pandas/io/json.py#L334)\n\n`orient='records'` doesn't apply here as it doesn't serialise the `index` and `orient='split'` serialises the `index`, `columns` and `values` into separate arrays so the `index` values are not encoded to JSON strings and their type is preserved.\n\nSo this is as expected unless the default of converting object types to float when deserialising is changed. Could attempt `int` first and then `float`? (esp for index)\n",
      "@Komnomnomnom right, do we know its an part of an index at that point? (I don't remember).\n\nactually `pd.to_numeric(data)` will do the right thing here (e.g. convert to int if possible, then float otherwise), and raise if not convertible.\n\n```\nIn [64]: pd.to_numeric(['a','1',2])\nValueError: Unable to parse string\n\nIn [65]: pd.to_numeric(['0','1',2])\nOut[65]: array([0, 1, 2])\n\nIn [66]: pd.to_numeric(['0','1',2.0])\nOut[66]: array([ 0.,  1.,  2.])\n\nIn [67]: pd.to_numeric(['0','1.5',2.0])\nOut[67]: array([ 0. ,  1.5,  2. ])\n```\n",
      "Yes it is happening after deserialisation (the c code) has finished.\n\nIt actually does try to convert to `int` further along but it runs into `TypeError: Setting <class 'pandas.core.index.Float64Index'> dtype to anything other than float64 or object is not supported`\n\n``` python\nIn [61]: from pandas import Index\n\nIn [62]: data = Index([u'0', u'1', u'2'], dtype='object')\n\nIn [63]: data.astype('float64').astype('int64')\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-63-7bbb22a22b83> in <module>()\n----> 1 data.astype('float64').astype('int64')\n\n/home/kieran/.virtualenvs/py27/lib/python2.7/site-packages/pandas/core/index.pyc in astype(self, dtype)\n   3828             raise TypeError('Setting %s dtype to anything other than '\n   3829                             'float64 or object is not supported' %\n-> 3830                             self.__class__)\n   3831         return Index(self._values, name=self.name, dtype=dtype)\n   3832 \n\nTypeError: Setting <class 'pandas.core.index.Float64Index'> dtype to anything other than float64 or object is not supported\n```\n",
      "@Komnomnomnom sorry, that is a bug. let me create another issue for that.\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 51,
    "deletions": 50,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/indexes/numeric.py",
      "pandas/io/tests/test_json/test_pandas.py",
      "pandas/tests/indexes/test_numeric.py",
      "pandas/tests/test_common.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12868,
    "reporter": "jduncavage",
    "created_at": "2016-04-11T19:56:41+00:00",
    "closed_at": "2016-04-13T01:12:56+00:00",
    "resolver": "max-sixty",
    "resolved_in": "77be872f4fcef90c648f714df2abafb422c7332a",
    "resolver_commit_num": 7,
    "title": "BUG: Resample on empty Period-Indexed Series returns Datetime-Indexed Series",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n`\nIn [7]: pd.Series([], pd.period_range(start='2000', periods=0)).resample('W').mean().index\nOut[7]: DatetimeIndex([], dtype='datetime64[ns]', freq='W-SUN')\n`\n#### Expected Output\n\n`\nPeriodIndex([], dtype='int64', freq='W-SUN')\n`\n\nOn pandas: 0.18.0\n",
    "labels": [
      "Bug",
      "Period",
      "Difficulty Intermediate",
      "Effort Low",
      "Resample"
    ],
    "comments": [
      "@jreback Lots of issues with PeriodIndex & resample. ref: https://github.com/pydata/pandas/issues/12774 & https://github.com/pydata/pandas/issues/12770\n\nI didn't really follow the resample refactor so I don't know the genesis of these. I had a look through the current code but can't find easy fixes. Looks like a lot is reliant on the `kind` attribute, but it's not completely consistent (i.e. a resampled PeriodIndex at times changes `kind` from `period` to `timestamp`, etc).\n\nDo you have any ideas for an overarching strategy for these? \nFor example, should we just convert every `PeriodIndex` to a `DatetimeIndex`, resample it, and then convert back? \n",
      "@MaximilianR I commented on this in #12774 . The logic from < 0.18.0 is exactly the same. The problem is the period is not systematically tested and there are quite a few edge cases. Further compounding this is the `kind` kw, which I think we could just totally eliminate. Now the obstacle is that its often easier to change the DTI to a PI, resample then change it back. The code paths are there, just the `kind` thing really perverts the logic.\n\nSo what we need is a fair amount of systematic testing. Then remove the `kind` (well you don't actually remove it, but deprecate it and we'll remove it later).\n",
      "I'll make a master Period Resample issue I think.\n",
      "> Now the obstacle is that its often easier to change the DTI to a PI, resample then change it back.\n\nOK - can I confirm that you think this is the best path forward, for all PeriodIndex resampling?\n\n> The logic from < 0.18.0 is exactly the same. \n\nOK, these are regressions though (from our unit tests failing on our upgrade)\n",
      "if u have failing tests then they must be things that are not tested in pandas - nothing regressed AFAICT\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "labeled",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 111,
    "deletions": 61,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12870,
    "reporter": "swkoh",
    "created_at": "2016-04-11T21:17:18+00:00",
    "closed_at": "2016-04-16T01:44:27+00:00",
    "resolver": "OXPHOS",
    "resolved_in": "6c692aee6d411d67997c4fadd023a1d4b7f91c12",
    "resolver_commit_num": 1,
    "title": "read_excel with names keyword argument not working",
    "body": "With pandas 0.18,  pd.read_excel() with name keyworkd does not work anymore.  (pandas 0.17 works fine.) pandas 0.18 has a change to add names as a keyword argument in excel.py at line 76.\n#### Code Sample, a copy-pastable example if possible\n\nfilename = 'test.xlsx'\ncolumn_names = ['field1', 'field2']\ndf = pd.read_excel(filename, index_col=None,  names=column_names)\n#### Expected Output\n\ncolumn_names associated with keyword argument names is supposed to pass down to kwds at C:\\Python27\\Lib\\site-packages\\pandas\\io\\excel.py:177.\n\nkwds is supposed to carry the arguments from application (in this case, column_names above) but it does not get passed. \n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.11.final.0\npython-bits: 32\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.18.0\nnose: None\npip: 8.1.1\nsetuptools: 18.3.2\nCython: 0.23.5\nnumpy: 1.11.0\nscipy: None\nstatsmodels: None\nxarray: None\nIPython: 4.0.0\nsphinx: None\npatsy: None\ndateutil: 2.5.2\npytz: 2016.3\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: 2.5\nmatplotlib: 1.5.1\nopenpyxl: 2.2.2\nxlrd: 0.9.3\nxlwt: 1.0.0\nxlsxwriter: 0.7.3\nlxml: None\nbs4: 4.3.2\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\n",
    "labels": [
      "IO Excel",
      "Bug",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "xref #11874 ; the `names` param was documented as it was in the signature but not the doc-string. But it appears to not be passed thru.\n\n```\nIn [26]: df = DataFrame({'A' : [1,2,3], 'B' : ['foo','bar','baz']})\n\nIn [27]: df.to_excel('test.xlsx')\n\nIn [28]: pd.read_excel('test.xlsx')\nOut[28]: \n   A    B\n0  1  foo\n1  2  bar\n2  3  baz\n\nIn [29]: pd.read_excel('test.xlsx',names=['A'])\nOut[29]: \n   A    B\n0  1  foo\n1  2  bar\n2  3  baz\n```\n",
      "cc @jorisvandenbossche \n"
    ],
    "events": [
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 33,
    "deletions": 7,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/io/excel.py",
      "pandas/io/tests/test_excel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12879,
    "reporter": "randomgambit",
    "created_at": "2016-04-12T12:12:41+00:00",
    "closed_at": "2016-08-08T16:10:54+00:00",
    "resolver": "bkandel-picwell",
    "resolved_in": "b7abef4949fb1ba7fd1004feba4f47ace7004282",
    "resolver_commit_num": 0,
    "title": "DOC: OrderedDict example in groupby aggregation",
    "body": "Hello,\n\nIt's me, again. ;-) I would like to submit to your attention another possible improvement your could implement. \n\nLet me start by saying that 90% of the time, data processing in my field involves some form of groupby at the time frequency, panel frequency, etc. \n\nSo aggregating data efficiently is definitely something important and very useful. \n\nI clearly prefer pandas' `groupby` over stata `collapse` (or others) because it is so much faster.\nHowever, a key functionality seems to be missing in Pandas. Usually, people want to apply functions to several columns, and be able to rename the output. In stata, you would write something like\n\n`collapse (firstnm) jreback=pandas, by(time)`\n\nto create a variable named `jreback`, that contains the first non missing value of the column `pandas` for every group in `time`.\n\nIn Pandas, a similar process seem unnecessarily complex.\n\nI can only use the syntax `group=df.groupby('group').agg({'A':'mean', 'B':['mean','sum']})`\nwhich has a **major disadvantage**\n- **no predictability over the sorting order of the columns**. That is, there is no guarantee that in group, the first column will be A and the second one will be B. I need to `group.column.tolist()` manually to understand which column corresponds to what. That is clearly not efficient (or maybe I am missing something here)\n\nIt would be therefore useful to add an option `column_names` that allows the user to chose columns names at the `agg` level, with the guarantee that the first name correspond to the first column and so on. For instance, in the example above I could specify `col_names=['mean_A','mean_B','this is a sum']`\n\nWhat do you think?\n",
    "labels": [
      "Usage Question",
      "Groupby",
      "Docs",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "@randomgambit there is this\n\n``` python\nIn [60]: df = pd.DataFrame({'A': [1, 2, 3], 'B': ['a', 'a', 'b'], 'C': [3, 4, 5]})\n\nIn [61]: df\nOut[61]:\n   A  B  C\n0  1  a  3\n1  2  a  4\n2  3  b  5\n\nIn [62]: df.groupby('B').agg({'A': {'mean1': 'mean', 'med1': 'median'}, 'C': {'mean2': 'mean', 'med2': 'median'}})\nOut[62]:\n      C          A\n  mean2 med2 mean1 med1\nB\na   3.5  3.5   1.5  1.5\nb   5.0  5.0   3.0  3.0\n```\n\nSo you pass a dict like `{original_column: {'output_name': aggfunc}}`. As you see this is unordered since python's dicts are undordered. So I'd suggest using an OrderedDict if order is important\n\n``` python\n# from collections import OrderedDict\nIn [99]: agg_funcs = OrderedDict([('A', OrderedDict([('mean1', 'mean'), ('med1', 'median')])), ('C', OrderedDict([('mean2', 'mean'), ('med2', 'median')]))])\n\nIn [100]: df.groupby('B').agg(agg_funcs)\nOut[100]:\n      A          C\n  mean1 med1 mean2 med2\nB\na   1.5  1.5   3.5  3.5\nb   3.0  3.0   5.0  5.0\n```\n",
      "excellent! I suggest you add this to the cookbook or to the tutorial. I know many people that are confused by this simple renaming thing. Thanks\n",
      "just a follow up: if I remember well this only works with `agg`, right? what about `transform`?\n",
      "you could put a section at the end of http://pandas-docs.github.io/pandas-docs-travis/groupby.html#aggregation\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "labeled",
      "renamed",
      "milestoned",
      "labeled",
      "labeled",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 1,
    "additions": 12,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/groupby.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12881,
    "reporter": "jreback",
    "created_at": "2016-04-12T13:56:13+00:00",
    "closed_at": "2016-04-18T17:22:25+00:00",
    "resolver": "jreback",
    "resolved_in": "bac68d6bb9ea22e9e744fe709d1270f257d71e0f",
    "resolver_commit_num": 3980,
    "title": "BUG: .astype of Float64Index",
    "body": "xref #12866 \n\n\n\nthis should work\n",
    "labels": [
      "Bug",
      "Dtypes",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "cc @Komnomnomnom \n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 51,
    "deletions": 50,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/indexes/numeric.py",
      "pandas/io/tests/test_json/test_pandas.py",
      "pandas/tests/indexes/test_numeric.py",
      "pandas/tests/test_common.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12896,
    "reporter": "tntdynamight",
    "created_at": "2016-04-14T01:53:33+00:00",
    "closed_at": "2016-05-14T12:04:34+00:00",
    "resolver": "kawochen",
    "resolved_in": "2de2884a7e7abf64f9967f6d8bc05a2d45f59bb4",
    "resolver_commit_num": 48,
    "title": "Multiindex slicing df.loc[idx[dim1,dim2,dim3],:] not working right in some cases",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Output\n\n\n\n\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.16.0-59-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.1.1\nsetuptools: 20.2.2\nCython: 0.23.4\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.0.3\nsphinx: 1.3.5\npatsy: 0.4.0\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.4.6\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.8.4\nlxml: 3.5.0\nbs4: 4.4.1\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.11\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.39.0\n",
    "labels": [
      "Bug",
      "Indexing",
      "MultiIndex",
      "Difficulty Advanced",
      "Effort Low"
    ],
    "comments": [
      "I believe `idx = pd.IndexSlice`\n",
      "hmm, something funny going on.\n\nThis gives something odd as well.\n\n```\nIn [33]: df.loc[idx[:, :, -1000:-950], :].head()\nOut[33]: \n                dat1      dat2      dat3\nCS 0 -1000 -1.306527  1.658131 -0.118164\n     -999  -0.680178  0.666383 -0.460720\n     -998  -1.334258 -1.346718  0.693773\n     -997  -0.159573 -0.133702  1.077744\n     -996  -1.126826 -0.730678 -0.384880\n\nIn [34]: df.loc[idx[:, :, -1000:-50], :].head()\nOut[34]: \n                dat1      dat2      dat3\nCS 0 -1100  1.764052  0.400157  0.978738\n     -1099  2.240893  1.867558 -0.977278\n     -1098  0.950088 -0.151357 -0.103219\n     -1097  0.410599  0.144044  1.454274\n     -1096  0.761038  0.121675  0.443863\n```\n",
      "Any idea what this could be? It seems to be related to the size of the index - how could that be? \n\nCorrect with an index of 1500:\n\n``` python\nIn [240]: l=1500\n\nIn [241]:  ints = (pd.np.random.rand(l)*1e6).round().astype('int')\n\nIn [242]: index=pd.MultiIndex.from_arrays([list('abc')*(l//3), ints])\n\n\nIn [243]:  series=pd.Series(np.random.rand(l), index=index)\n\nIn [244]:  series.sort_index().loc[(slice(None), slice(1e5))]\nOut[244]: \na  2360     0.501724\n   5253     0.892526\n   10122    0.158961\n   15737    0.927828\n...\n   94452    0.460249\n   96376    0.248980\n   97572    0.514986\n   99746    0.719964\ndtype: float64\n```\n\nIncorrect with an index of 15000:\n\n``` python\nIn [245]: l=15000\n\nIn [246]:  ints = (pd.np.random.rand(l)*1e6).round().astype('int')\n\nIn [247]: index=pd.MultiIndex.from_arrays([list('abc')*(l//3), ints])\n\nIn [248]: series=pd.Series(np.random.rand(l), index=index)\n\n\nIn [249]: series.sort_index().loc[(slice(None), slice(1e5))]\nOut[249]: \na  409       0.317578\n   582       0.526421\n   584       0.620082\n   838       0.139467\n...\n   859804    0.510514\n   947555    0.951258\ndtype: float64\n\n```\n",
      "Here? https://github.com/pydata/pandas/blob/master/pandas/index.pyx#L305-L305\n",
      "more like here: https://github.com/pydata/pandas/blob/master/pandas/indexes/multi.py#L1734\n",
      "Yup. But there mus be somewhere where the behavior is different depending on the size of the index?\n",
      "sounds interesting. I'll give this a go\n",
      "Looks solved, thanks for the effort everyone! I guess you will close this upon merge, or I will once it's through. Cheers!\n"
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "commented"
    ],
    "changed_files": 3,
    "additions": 15,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/indexes/multi.py",
      "pandas/tests/indexing/test_indexing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12902,
    "reporter": "elinoreroebber",
    "created_at": "2016-04-14T22:21:21+00:00",
    "closed_at": "2016-04-18T17:10:08+00:00",
    "resolver": "adneu",
    "resolved_in": "a6d76981efc94ae4dcb0ea917aff624d9b497e37",
    "resolver_commit_num": 0,
    "title": "BUG: Complex types are coerced to float when summing along level of MultiIndex",
    "body": "When trying to sum complex numbers along one level of a MultiIndex, I get a `ComplexWarning` and the resulting datatypes are `float64` with imaginary components discarded.  Summing without specifying the level or by unstacking the MultiIndex works as expected.  \n\nBased on the error message, I assume that groupby is being called internally, and that the bug is actually there.  I'm using pandas version 17.1.\n#### Code Sample\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 14.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_CA.UTF-8\n\npandas: 0.17.1\nnose: 1.3.7\npip: 7.1.2\nsetuptools: 19.2\nCython: 0.23.5\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: None\nIPython: 4.1.2\nsphinx: 1.4\npatsy: None\ndateutil: 2.5.1\npytz: 2016.3\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.5.1\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\nJinja2: None\n",
    "labels": [
      "Bug",
      "Groupby",
      "Complex",
      "Effort Low",
      "Difficulty Novice"
    ],
    "comments": [
      "This is about [here](https://github.com/pydata/pandas/blob/master/pandas/core/groupby.py#L1746)\n\npull-requests are welcome. For now we need to coerce this to `object` (rather than treat this as a float)\n\nThis does it. just need some tests and such.\n\n```\ndiff --git a/pandas/core/groupby.py b/pandas/core/groupby.py\nindex 6996254..e2a4482 100644\n--- a/pandas/core/groupby.py\n+++ b/pandas/core/groupby.py\n@@ -1747,7 +1747,7 @@ class BaseGrouper(object):\n             values = _algos.ensure_float64(values)\n         elif com.is_integer_dtype(values):\n             values = values.astype('int64', copy=False)\n-        elif is_numeric:\n+        elif is_numeric and not com.is_complex_dtype(values):\n             values = _algos.ensure_float64(values)\n         else:\n             values = values.astype(object)\n```\n",
      "The bug also occurs when using a Series and invoking `groupby` with `level=0` (so it is not specific to MultiIndex, only indirectly via `groupby`, as suggested).\n\n```\nIn [1]: series = pd.Series(data=np.arange(4)*(1+2j),index=[0,0,1,1])\nIn [2]: print series\n0        0j\n0    (1+2j)\n1    (2+4j)\n1    (3+6j)\ndtype: complex128\n\nIn [3]: series.groupby(level=0).sum()\nOut[3]: \n0    1.0\n1    5.0\ndtype: float64\n```\n\nExpected:\n\n```\nOut[3]: \n0     (1+2j)\n1    (5+10j)\ndtype: complex128\n```\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "labeled",
      "unlabeled",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 14,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12903,
    "reporter": "rs2",
    "created_at": "2016-04-15T11:34:14+00:00",
    "closed_at": "2016-04-26T19:21:19+00:00",
    "resolver": "rs2",
    "resolved_in": "7fbc600e6f11ab256dc873175b971ca06d5ce456",
    "resolver_commit_num": 0,
    "title": "Import statements in period.pyx significantly impact performance",
    "body": "The following imports in pandas/src/period.pyx significantly impact performance when dealing with multiple Period objects. A quick win, guys. \n\n\n\nJust profile the code below and observe the number of times `_find_and_load` gets called:\n\n\n\n is the commit that has introduced the problem.\n\nI will submit a pull request that rectifies the incorrect fix to the circular dependency. \n",
    "labels": [
      "Performance",
      "Period"
    ],
    "comments": [
      "Thanks for the catch. Moving some frequency related codes to `offset.pyx` is one idea (#11214).\n",
      "so this is the same issue behind #11831 \n",
      "@jreback Correct. I'll submit a pull request after lunch. Tested the fix\nlocally and ran unit tests. It's all good.\nOn 15 Apr 2016 10:21 p.m., \"Sinhrks\" notifications@github.com wrote:\n\n> Thanks for the catch. Moving some frequency related codes to offset.pyx is\n> one idea (#11214 https://github.com/pydata/pandas/issues/11214).\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/pydata/pandas/issues/12903#issuecomment-210649958\n",
      "See https://github.com/pydata/pandas/pull/12909\n\nOn Sat, Apr 16, 2016 at 11:26 AM, Paul A rootsumsquared@gmail.com wrote:\n\n> @jreback Correct. I'll submit a pull request after lunch. Tested the fix\n> locally and ran unit tests. It's all good.\n> On 15 Apr 2016 10:21 p.m., \"Sinhrks\" notifications@github.com wrote:\n> \n> > Thanks for the catch. Moving some frequency related codes to offset.pyx\n> > is one idea (#11214 https://github.com/pydata/pandas/issues/11214).\n> > \n> > \u2014\n> > You are receiving this because you authored the thread.\n> > Reply to this email directly or view it on GitHub\n> > https://github.com/pydata/pandas/issues/12903#issuecomment-210649958\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "commented",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 5,
    "additions": 56,
    "deletions": 53,
    "changed_files_list": [
      "asv_bench/benchmarks/period.py",
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/src/period.pyx",
      "pandas/tseries/frequencies.py",
      "pandas/tseries/tests/test_tslib.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12907,
    "reporter": "educhana",
    "created_at": "2016-04-16T11:48:55+00:00",
    "closed_at": "2016-05-27T00:15:29+00:00",
    "resolver": "camilocot",
    "resolved_in": "0f1666d8adfa8e121a935309b1d7ca6effec813c",
    "resolver_commit_num": 0,
    "title": "read_html: support \"decimal\" argument for parsing numbers, like read_csv",
    "body": "read_csv has support for declaring the decimal and thousands separator.\n\nread_html is missing the 'decimal' parameter. it'd be useful and more consistent to accept it too.\n\nExample:\n\n\n",
    "labels": [
      "Enhancement",
      "Difficulty Novice",
      "IO HTML",
      "Effort Low"
    ],
    "comments": [
      "this is a dupe of #8200 but we'll close that issue. This should be straightforward and just need to pass this thru to the parser. Pull-requests are welcome.\n",
      "I don't think that `TextParser` supports passing the decimal keyword in python. The functionality probably needs to be added to PythonParser\n",
      "This defaults to the c-engine, so I don't really see a problem. If it actually is, then you could defer to the python engine if the `decimal` kw is not-None.\n\nhttps://github.com/pydata/pandas/issues/12933 to add the `decimal` option (I don't know if we have that particular issue).\n\nAlternatively, you could post-process.\n\n```\nIn [7]: s = Series(['1,2','2,3'])\n\nIn [8]: s\nOut[8]: \n0    1,2\n1    2,3\ndtype: object\n\nIn [9]: pd.to_numeric(s.str.replace(',','.'),errors='coerce')\nOut[9]: \n0    1.2\n1    2.3\ndtype: float64\n```\n"
    ],
    "events": [
      "commented",
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 49,
    "deletions": 12,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/io/html.py",
      "pandas/io/tests/test_html.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12911,
    "reporter": "jseabold",
    "created_at": "2016-04-17T14:17:59+00:00",
    "closed_at": "2016-04-18T15:26:14+00:00",
    "resolver": "mdboom",
    "resolved_in": "ab4ac1209c4b7517e8579e6e75fa176d59ca8d6c",
    "resolver_commit_num": 0,
    "title": "pandas logo has a latex typo",
    "body": "Someone pointed this out on twitter (sorry, forgot who), but it you should be `y_{it} =` not `y_it` as it looks like it might be now. The `t` should also be subscript.\n",
    "labels": [
      "Community"
    ],
    "comments": [
      "cc @mdboom\n",
      "I just put up a gist with the matplotlib script I used to create the (new) logo, with the corrected LaTeX expression:\n\nhttps://gist.github.com/mdboom/06578d2edbb21807d0e216c8553bdf82\n\nIf you have a good suggestion about where in the pandas tree this should live permanently, I can make a PR with this stuff.\n",
      "thanks @mdboom why don't you do a PR with this in `pandas/doc/logo` (new dir). If you can put the generated logo there as well. thanks.\n",
      "Sure: see #12919.\n"
    ],
    "events": [
      "renamed",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "cross-referenced",
      "commented"
    ],
    "changed_files": 4,
    "additions": 924,
    "deletions": 1,
    "changed_files_list": [
      "README.md",
      "doc/logo/pandas_logo.png",
      "doc/logo/pandas_logo.py",
      "doc/logo/pandas_logo.svg"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12912,
    "reporter": "gfyoung",
    "created_at": "2016-04-17T14:35:20+00:00",
    "closed_at": "2016-04-21T21:10:15+00:00",
    "resolver": "gfyoung",
    "resolved_in": "b3b166a154b1d011f964bfd3ab87e9d16e8dd5d3",
    "resolver_commit_num": 11,
    "title": "DOC, BUG: Clarify and Standardize Whitespace Delimiter Behaviour with Custom Line Terminator",
    "body": "\n\nexpected:\n\n\n\nNote that this bug is only for the C engine, as the Python engine does not yet support `delim_whitespace`.\n",
    "labels": [
      "CSV",
      "Difficulty Intermediate",
      "Effort Low",
      "Bug",
      "Docs"
    ],
    "comments": [
      "this should just raise an error I think. It doesn't make sense, these are 2 conflicting options.\nas `delimwhitespace` -> `\\n` termination implicity.\n",
      "1) Where in the documentation does it say that?  AFAICT, there is no documentation for `delim_whitespace` in `read_csv`.\n\n2) If `delim_whitespace=True` and custom `lineterminator` conflict with each other, why does this work and give me what I was expecting:\n\n``` python\n>>> from pandas import read_csv\n>>> from pandas.compat import StringIO\n>>> data = \"\"\"a b c~1 2 3~4 5 6~7 8 9\"\"\"\n>>> df = read_csv(StringIO(data), lineterminator='~', delimiter=' ')\n>>> df\n    a    b    c\n0   1    2    3\n1   4    5    6\n2   7    8    9\n```\n",
      "ok, I guess accepting `lineterminator` is prob ok, `delim_whitespace` is just a shortcut for setting `sep`\n",
      "also docs could be updated as well.\n",
      "Yep, that's what I was thinking of doing as well, as there aren't any for `delim_whitespace` ATM AFAICT.\n",
      "there is a tiny comment in `io.rst`, but need to add to doc-string and expand a little\n",
      "Ah, okay.  I'll add use that documentation in `io.rst` as a starting point then.\n",
      "@gfyoung regarding your example of `read_csv(StringIO(data), lineterminator='~', delimiter=' ')` that works. But the equivalent of using `delim_whitespace=True` would rather be `delimiter='\\s+'` I think, and this fails as well in combination with the custom lineterminator\n",
      "@jorisvandenbossche : Whitespace in `tokenizer.c` is defined as `' '` or `'\\t'` as evidenced <a href=\"https://github.com/pydata/pandas/blob/master/pandas/src/parser/tokenizer.c#L694\">here</a>, so your regex expression would not apply I believe.  In any case, I still think you can use a custom line terminator as that is separate from delimiter.\n",
      "I don't know about the c internals of the parser code, but in any case here: https://github.com/pydata/pandas/blob/master/pandas/io/parsers.py#L679 the `'\\s+'` option is set to `delim_whitespace=True`, so both are equivalent (although I thought it was set the other way around)\n",
      "To be clear, I am not saying that this is not a bug, I was just pointing out that your example of `delimiter=' '` is not exactly equivalent, or at least not taking the same code path. \nBut to the user, this of course seems equivalent, and ideally would behave the same with regard to `lineterminator`\n",
      "@jorisvandenbossche : I did understand that they were not taking the same code path, though I did not see that override there that you pointed out.  Nevertheless, I think we can agree then that the documentation definitely needs some explanation as what goes on with `delim_whitespace`.\n",
      "@gfyoung Certainly!\n"
    ],
    "events": [
      "commented",
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "renamed",
      "renamed",
      "commented",
      "labeled",
      "commented",
      "labeled",
      "unlabeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 5,
    "additions": 218,
    "deletions": 769,
    "changed_files_list": [
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/io/parsers.py",
      "pandas/io/tests/test_parsers.py",
      "pandas/src/parser/tokenizer.c"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12922,
    "reporter": "batterseapower",
    "created_at": "2016-04-19T07:10:14+00:00",
    "closed_at": "2016-06-16T12:32:24+00:00",
    "resolver": "gfyoung",
    "resolved_in": "d814f433940031029f1ddf0d9abdecdf4ad31dac",
    "resolver_commit_num": 38,
    "title": "DataFrame.to_csv(quoting=csv.QUOTE_NONNUMERIC) quotes numeric values",
    "body": "#### Failing test\n\n\n\nThe issue is that the floats are being output wrapped with quotes, even though I requested QUOTE_NONNUMERIC.\n\nThe problem is that `pandas.core.internals.FloatBlock.to_native_types` (and by extension `pandas.formats.format.FloatArrayFormatter.get_result_as_array`) unconditionally formats the float array to a str array, which is then passed unchanged to the `csv` module and hence will be wrapped in quotes by that code.\n\nI'm not 100% sure but the fix may be to have `FloatBlock.to_native_types` check if quoting is set, and if so to skip using the `FloatArrayFormatter`? I say this because `pandas.indexes.base.Index._format_native_types` already has a special case along these lines. This does seem a bit dirty though!\n\nHere is an awful monkeypatch that works around the problem:\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Bug",
      "Output-Formatting",
      "CSV",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "This is probably only minimally tested now. The data is written thru the csv writer which gets passed the quoting. I think that needs to be turned off as we do all quoting formatting before passing it to the writer (maybe not ALL, and that's the rub, some cases maybe relying on the csv writer actually quoting things).\nAll things are passed as formatted strings (e.g. we do this for floats for example to provide NaN formatting, specific format strings and such).\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 101,
    "deletions": 29,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/core/internals.py",
      "pandas/formats/format.py",
      "pandas/tests/frame/test_to_csv.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12926,
    "reporter": "jorisvandenbossche",
    "created_at": "2016-04-19T12:52:21+00:00",
    "closed_at": "2016-04-20T01:03:18+00:00",
    "resolver": "jreback",
    "resolved_in": "6d2d6db45efb1d36ff4dbccbb17278710b1984d8",
    "resolver_commit_num": 3982,
    "title": "BUG: resample().asfreq() looses end period with TimedeltaIndex",
    "body": "\n\nThe two examples should give the same result. \nTried it with a DatetimeIndex, and there I don't see this problem.\n\ncc @jreback \n",
    "labels": [
      "Bug",
      "Timedelta",
      "Resample"
    ],
    "comments": [
      "Simple test with DatetimeIndex does work:\n\n```\nIn [28]: df.index = df.index + pd.Timestamp('2016-01-01')\n\nIn [29]: df\nOut[29]:\n                     0\n2016-01-01 00:00:00  1\n2016-01-01 00:03:00  3\n\nIn [30]: df.resample('1T').asfreq()\nOut[30]:\n                       0\n2016-01-01 00:00:00  1.0\n2016-01-01 00:01:00  NaN\n2016-01-01 00:02:00  NaN\n2016-01-01 00:03:00  3.0\n```\n",
      "hmm, ok, logic *should8 be the same, but prob not well tested.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "mentioned",
      "subscribed",
      "milestoned",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 199,
    "deletions": 29,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/tseries/frequencies.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_frequencies.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12927,
    "reporter": "yarikoptic",
    "created_at": "2016-04-19T18:29:20+00:00",
    "closed_at": "2016-04-30T23:15:55+00:00",
    "resolver": "jreback",
    "resolved_in": "3e9c3200fd173f29c090dfa52c87ceeeac4a01d3",
    "resolver_commit_num": 4011,
    "title": "2 tests failures on 32bit:  test_searchsorted_sorter and test_sparse_max_row",
    "body": "While building a fresh snapshot (6c692ae) for debians ran into the following consistent one on debian stable and testing (built fine on 64bit, but failed in 32bit userspace):\n\n\n\nI will replicate in interactive env atm, but thought that may be you have dealt with it already or smth so I wouldn't ;-)\n",
    "labels": [
      "Testing",
      "Compat"
    ],
    "comments": [
      "what is your numpy version here? we are still working thru a bunch of numpy issues of numpy > 1.11\n",
      "cc @sinhrks for the 2nd. though we only test on 32-bit vms very occasionally.\n",
      "On Tue, 19 Apr 2016, Jeff Reback wrote:\n\n>    what is your numpy version here? we are still working thru a bunch of\n>    numpy issues of numpy > 1.11\n\ne.g. Debian stable has good old  1.8.2\n\nhere is more information about sorter argument\n\n1483  \n1484        @Substitution(klass='Series', value='v')\n1485        @Appender(base._shared_docs['searchsorted'])\n1486        def searchsorted(self, v, side='left', sorter=None):\n1487            return self._values.searchsorted(Series(v)._values,\n1488 ->                                          side=side, sorter=sorter)\n1489  \n1490        # -------------------------------------------------------------------\n1491        # Combination\n1492  \n1493        def append(self, to_append, verify_integrity=False):\n(Pdb) p sorter\n0    1\n1    2\n2    0\ndtype: int64\n(Pdb) p type(sorter)\n<class 'pandas.core.series.Series'>\n\nand here is easier to digest (explicit dtype listed) for the 2nd one\n\n(Pdb) p result\n'0    1.0\\n1    NaN\\n2    NaN\\n3    3.0\\n4    NaN\\ndtype: float64\\nBlockIndex\\nBlock locations: array([0, 3])\\nBlock lengths: array([1, 1])'\n(Pdb) p exp\n'0    1.0\\n1    NaN\\n2    NaN\\n3    3.0\\n4    NaN\\ndtype: float64\\nBlockIndex\\nBlock locations: array([0, 3], dtype=int32)\\nBlock lengths: array([1, 1], dtype=int32)'\n",
      "Is 2nd case has been fixed by #12936? (thanks, @jreback)\n"
    ],
    "events": [
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "referenced",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files": 7,
    "additions": 91,
    "deletions": 10,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/series.py",
      "pandas/indexes/api.py",
      "pandas/indexes/base.py",
      "pandas/tests/formats/test_format.py",
      "pandas/tests/indexes/test_base.py",
      "pandas/tests/indexing/test_coercion.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12934,
    "reporter": "brandon-rhodes",
    "created_at": "2016-04-20T14:43:49+00:00",
    "closed_at": "2016-04-22T17:03:29+00:00",
    "resolver": "brandon-rhodes",
    "resolved_in": "ed324e8692002db49e8d587a9bb73d28d8776ffe",
    "resolver_commit_num": 0,
    "title": "Add parameter defaults for swaplevel",
    "body": "I will be happy to write up a pull request, but first wanted to gauge the sanity of my suggestion:\n\nI think that `swaplevel()` deserves default values for its parameters, just like its friends like `stack()` and `unstack()` and `sortlevel()` that also all take an initial `level` argument. I suggest:\n\n\n\nThis provides the greatest symmetry with the other methods that operate on levels: they all, if no level is specified, operate on the innermost level as their default.\n\nIn the very common case where there are only two levels to the multi-index anyway, this would reduce this frequent operation to simply `.swaplevel()` or `.swaplevel(axis='columns')` without, I don't think, any more loss of readability than when `stack()` or `unstack()` fail to specify the level upon which they are operating.\n",
    "labels": [
      "Reshaping",
      "API Design",
      "MultiIndex",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "@brandon-rhodes yes this does seem ok\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "labeled",
      "labeled",
      "cross-referenced"
    ],
    "changed_files": 7,
    "additions": 57,
    "deletions": 18,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/series.py",
      "pandas/indexes/multi.py",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12941,
    "reporter": "rrbarbosa",
    "created_at": "2016-04-21T09:06:24+00:00",
    "closed_at": "2016-08-06T22:57:12+00:00",
    "resolver": "facaiy",
    "resolved_in": "63e8f689a13650bda01281ef257f9266e6394881",
    "resolver_commit_num": 2,
    "title": "Operations on NaT returning float instead of datetime64[ns]",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\nAll results above return a `float64` series instead of a `datetime64[ns]` series.\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Bug",
      "Timeseries",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "SO: http://stackoverflow.com/questions/36620256/why-is-pandas-converting-datetimes-to-float-in-aggregate-function#\n",
      "The min/max should return NaT, but the `sum` should fail I think for datetime64\n",
      "so `.sum(..)` is correct, in the the underlying operation raises a `TypeError`, which is then caught and `numeric_only=True` is passed so you end up with `.sum()` of an empty frame and get 0.0.\n\nThe other 2 are not being masked correctly inside `nanops._maybe_null_out` which needs to handle `M8,m8` dtypes (rather than converting to float).\n- same cause is here: https://github.com/pydata/pandas/issues/12821 \n- xref #8617 (might be a duplicate)\n- xref #4147 \n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files": 6,
    "additions": 68,
    "deletions": 5,
    "changed_files_list": [
      ".gitignore",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/nanops.py",
      "pandas/tests/frame/test_timeseries.py",
      "pandas/tests/test_groupby.py",
      "pandas/types/cast.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12944,
    "reporter": "jreback",
    "created_at": "2016-04-21T15:42:49+00:00",
    "closed_at": "2016-04-26T21:51:34+00:00",
    "resolver": "jreback",
    "resolved_in": "084391126cc4edda35b41378d2905c788e5b573a",
    "resolver_commit_num": 3986,
    "title": "COMPAT: python-dateutil 2.5.3",
    "body": "so this parade of dateutil changes causes havoc in the testsuite as we adjust.\n\nprevious fixes: #12731, #12613 \n\nrevert this: \n\nLet's see what the correct fixes are and maybe need to ban certain buggy versions.\n\n\n",
    "labels": [
      "Timeseries",
      "Compat"
    ],
    "comments": [
      "xref #7599 \n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 7,
    "additions": 101,
    "deletions": 82,
    "changed_files_list": [
      "doc/source/timeseries.rst",
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/src/inference.pyx",
      "pandas/tseries/tests/test_tslib.py",
      "pandas/tseries/tools.py",
      "pandas/tslib.pxd",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12947,
    "reporter": "DavidEscott",
    "created_at": "2016-04-21T16:49:02+00:00",
    "closed_at": "2016-05-07T14:51:35+00:00",
    "resolver": "zhangxiangnick",
    "resolved_in": "40ba6eb3772f841625534a602447e3e2c9865511",
    "resolver_commit_num": 0,
    "title": "Confusing behavior with (multi-)assignment and _LocIndexer/_IXIndexer",
    "body": "\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Linux\nOS-release: 2.6.32-431.29.2.el6.x86_64\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.1.1\nsetuptools: 20.4\nCython: None\nnumpy: 1.11.0\nscipy: None\nstatsmodels: None\nxarray: None\nIPython: 4.1.2\nsphinx: None\npatsy: None\ndateutil: 2.5.2\npytz: 2016.3\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: None\nopenpyxl: 2.3.4\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: None\njinja2: None\nboto: None\n",
    "labels": [
      "Usage Question",
      "Indexing",
      "Docs",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "you are missing the point here, when you use multiple columns, pandas will align for you. so you need to give it a raw array/list if you are doing this.\n\n```\nIn [29]: df.loc[df.A== 1, [\"B\", \"C\"]] = df.loc[df.A == 1, [\"D\", \"E\"]].values\n\nIn [30]: df\nOut[30]: \n   A  B  C  D  E\n0  1  4  5  4  5\n```\n",
      "I suppose you could do a warning section in the docs. interested in that?\n",
      "I don't follow at all. Here is a little more strangeness:\n\n```\nIn [42]: df = pandas.DataFrame([[1,2,3,4,5]], columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\nIn [43]: type(df[[\"B\",\"C\"]])\nOut[43]: pandas.core.frame.DataFrame\n\nIn [44]: type(df.loc[df.A==1, [\"B\",\"C\"]])\nOut[44]: pandas.core.frame.DataFrame\n\nIn [45]: df[[\"B\", \"C\"]] = df[[\"D\", \"E\"]]\n\nIn [46]: df\nOut[46]:\n   A  B  C  D  E\n0  1  4  5  4  5\n```\n\nSo I can assign a DataFrame to another DataFrame (of compatible dimension just fine) \nUNLESS one is a .loc or .ix of the other (and then stuff gets nulled out).\n\nI don't understand the NaNs at all. LHS=RHS shouldn't result in LHS being None when RHS is not None. That doesn't sound like correct behavior at all.\n\nAnother weird thing that happens:\n\n```\nIn [93]: df = pandas.DataFrame([[1,2,3,4,5]], columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\nIn [94]: df2 = df.loc[:,[\"B\",\"C\"]]\n\nIn [95]: df3 = df.loc[:,[\"D\",\"E\"]]\n\nIn [96]: df2.loc[:,:] is df2\nOut[96]: True\n\nIn [97]: df2.loc[:,:] = df3\n\nIn [98]: df2\nOut[98]:\n    B   C\n0 NaN NaN\n\nIn [99]: df\nOut[99]:\n   A  B  C  D  E\n0  1  2  3  4  5\n```\n\nbut since `df2.loc[:,:] is df2` this should be equivalent to: `df.loc[:,[\"B\",\"C\"]] = df3` which of course we have seen is not the case.\n\nTherefore with Pandas `X.foo().bar()` is not the same thing as `_ = X.foo(); _.bar()`. That is something I find super scary.\n",
      "you are doing 2 different things, in `[45]` you are saying take these columns and assign to these, this ignores alignment because its a column asssignment.\n\nwhile above in my `[29]` you are assigning part of a frame, this is a conceptual difference and as expected.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 1,
    "additions": 20,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/indexing.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12963,
    "reporter": "tdhopper",
    "created_at": "2016-04-22T21:05:36+00:00",
    "closed_at": "2017-03-20T22:47:22+00:00",
    "resolver": "mroeschke",
    "resolved_in": "f2e942e185da9369f2c1f4d3b38f57af7b4243bd",
    "resolver_commit_num": 38,
    "title": "drop_duplicates slow for data frame of boolean columns",
    "body": "In trying to find out the fast way to count unique rows in a data frame, I [stumbled upon an issue]() where dropping duplicates from a data frame of booleans is quite slow. @jreback asked me to open a ticket.\n\n\n\n\n",
    "labels": [
      "Performance",
      "Dtypes"
    ],
    "comments": [
      "xref #10235\n"
    ],
    "events": [
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "milestoned",
      "demilestoned",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 12,
    "deletions": 1,
    "changed_files_list": [
      "asv_bench/benchmarks/reindex.py",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/algorithms.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12979,
    "reporter": "kdebrab",
    "created_at": "2016-04-25T11:39:10+00:00",
    "closed_at": "2016-04-29T16:09:14+00:00",
    "resolver": "sinhrks",
    "resolved_in": "a615dbe817fa5c4f9597f5d5748be46d14b3ee1d",
    "resolver_commit_num": 296,
    "title": "plot.bar misalignment when width=1",
    "body": "Two minor issues with the width optional argument of plot.bar() in Pandas 0.18.0:\n\n1) Using width=1 (integer) causes misalignment of the x-axis\n\n\n\n![figure_1](-0ae9-11e6-9b90-729d413861d1.png)\n\nCompare this with the correct behaviour when defining width as a float:\n\n\n\n![figure_2](-0ae9-11e6-9b72-031fccc5714f.png)\n\n2) Using width=None causes a TypeError, where I would expect the default behaviour.\n",
    "labels": [
      "Visualization",
      "Bug"
    ],
    "comments": [
      "For the first one, I don't see a problem:\n\n![gh](https://cloud.githubusercontent.com/assets/1312546/14783086/05828a3e-0ab3-11e6-89c1-08dd73e72b83.png)\n\nCould you post the output of `pd.show_versions()` along with your OS information?\n\nFor the default, it's 0.5 but we haven't finished documenting all the keyword arguments after the `.plot` namespace was added in 0.17.\n",
      "As requested, output of pd.show_versions():\n\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.10.final.0\npython-bits: 32\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 42 Stepping 7, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.1.1\nsetuptools: 20.3.1\nCython: 0.20\nnumpy: 1.10.4\nscipy: 0.16.0b2\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.1.2\nsphinx: 1.3.6\npatsy: 0.3.0\ndateutil: 2.5.1\npytz: 2016.2\nblosc: None\nbottleneck: 0.8.0\ntables: None\nnumexpr: None\nmatplotlib: 1.5.1\nopenpyxl: 1.6.2\nxlrd: 0.9.2\nxlwt: 0.7.5\nxlsxwriter: None\nlxml: None\nbs4: 4.3.2\nhtml5lib: 0.999999\nhttplib2: 0.9.1\napiclient: None\nsqlalchemy: 0.7.9\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\n\nSeeing the style of your figure, I'd guess you're using matplotlib 2.x (development)? Thus, it seems the first issue originates in matplotlib and is fixed in the development version of matplotlib, but not in the latest stable matplotlib version 1.5.1?\n\nWith regard to the second issue, I knew the default is 0.5. It is just a matter of usability/consistency. Most optional arguments use the default value (i.c. 0.5) when one explicitly sets them to None. Apparently this isn't the case for the 'width' argument. Not very important of course, but I just thought of mentioning this.\n\nThe fix/workaround is rather trivial. Adding the line `width = float(width or 0.5)` fixes both issues for me.\n",
      "As you use python2, looks to be caused by somewhere uses division by integer. PR is appreciated:)\n- https://github.com/pydata/pandas/blob/master/pandas/tools/plotting.py#L1874\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "cross-referenced",
      "labeled",
      "unlabeled",
      "milestoned"
    ],
    "changed_files": 3,
    "additions": 23,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/tests/test_graphics.py",
      "pandas/tools/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12981,
    "reporter": "ajenkins-cargometrics",
    "created_at": "2016-04-25T14:31:11+00:00",
    "closed_at": "2016-04-26T13:22:55+00:00",
    "resolver": "ajenkins-cargometrics",
    "resolved_in": "cc67b72cc7cd37302068135ececabb0efe16f8c5",
    "resolver_commit_num": 0,
    "title": "Timezone lost on DataFrame assignments with realignment",
    "body": "Starting from pandas 0.17, certain assignments to DataFrames cause offset-aware datetime columns to be converted to offset-naive columns.  Specifically, it seems that if any data realignment is required when assigning the RHS to a a slice of the DataFrame, then timezone info is lost.  Here's an example: \n\n\n\nThe output I'd expect, which is what I get from pandas 0.16.2, is:\n\n\n\nHowever when I run this with pandas 0.18.0, after the assignment the timezone info is lost:\n\n\n\nIt seems the custom timezone-aware dtype that pandas started using for timezone-aware time series in 0.17.x doesn't get correctly propagated in this operation.\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Bug",
      "Indexing",
      "Timezones",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "yep looks like a bug. pull-requests are welcome!\n",
      "After a little digging, I believe I've found the fix.  In `DataFrame._santize_column`, there is a statement which accesses the `values` property, which should access `_values`.  This statement:\n\n``` python\nvalue = value.reindex(self.index).values\n```\n\nshould be\n\n``` python\nvalue = value.reindex(self.index)._values\n```\n\nThe `values` property returns a numpy array, which loses the custom dtype, whereas `_values` returns a DateTimeIndex which preserves the dtype.  I'll submit a PR.\n",
      "@ajenkins-cargometrics great!\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 17,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/frame.py",
      "pandas/tests/frame/test_indexing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12983,
    "reporter": "amanhanda",
    "created_at": "2016-04-25T16:50:36+00:00",
    "closed_at": "2016-04-30T14:56:43+00:00",
    "resolver": "sinhrks",
    "resolved_in": "f6a1c41e3437aaac3ef2e51124ad16915c1b4c40",
    "resolver_commit_num": 301,
    "title": "DataFrame align() on a subclassed DataFrame does not invoke _constructor() for the subclassed DataFrame",
    "body": "\n\nWhen aligning with a series, the code seems to explicitly create the DataFrame using DataFrame() constructor.\n",
    "labels": [
      "Compat",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "is this not covered in: https://github.com/pydata/pandas/pull/12308 (or several recently merged issues)?\n",
      "@sinhrks \n"
    ],
    "events": [
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 5,
    "additions": 98,
    "deletions": 16,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.1.txt",
      "pandas/core/generic.py",
      "pandas/tests/frame/test_axis_select_reindex.py",
      "pandas/tests/frame/test_subclass.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 12995,
    "reporter": "jreback",
    "created_at": "2016-04-26T14:50:13+00:00",
    "closed_at": "2016-09-06T10:18:08+00:00",
    "resolver": "gfyoung",
    "resolved_in": "e54d4dbd96db243d041ce14f305c538fd0eb7104",
    "resolver_commit_num": 85,
    "title": "PEP: fix up *.pyx",
    "body": "we don't by default check the .pyx files, we should. So this issue will cover fixing & updating the `setup.cfg`, need to add: `filename: *.py, *.pyx`\n",
    "labels": [
      "Difficulty Novice",
      "Style",
      "Effort Medium"
    ],
    "comments": [
      "`flake8 windows.pyx --select=E501,E302,E203,E226,E111,E114,E221,E303,E128,E231,E126,E128`\n\nseems to catch whitespace things and not bork on some cython specific pep things.\n\nam going to include a filter for this in a future PR.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 17,
    "additions": 1114,
    "deletions": 774,
    "changed_files_list": [
      "ci/lint.sh",
      "pandas/algos.pyx",
      "pandas/hashtable.pyx",
      "pandas/index.pyx",
      "pandas/io/sas/saslib.pyx",
      "pandas/lib.pyx",
      "pandas/msgpack/_packer.pyx",
      "pandas/msgpack/_unpacker.pyx",
      "pandas/parser.pyx",
      "pandas/src/inference.pyx",
      "pandas/src/offsets.pyx",
      "pandas/src/period.pyx",
      "pandas/src/reduce.pyx",
      "pandas/src/skiplist.pyx",
      "pandas/src/sparse.pyx",
      "pandas/src/testing.pyx",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13006,
    "reporter": "rocurley",
    "created_at": "2016-04-27T00:15:47+00:00",
    "closed_at": "2016-06-03T15:04:18+00:00",
    "resolver": "gliptak",
    "resolved_in": "2061e9e5fbbd890c484b53232b0747e08d7d1739",
    "resolver_commit_num": 28,
    "title": "BUG: Type error when comparing numpy scalar to pandas series",
    "body": "Hi,\n\nI'm getting strange results when comparing a numpy float to a pandas series. It seems to think that the numpy float is some sort of array...\n\n\n#### Expected Output\n\n\n#### Actual Output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "Dtypes",
      "Effort Low"
    ],
    "comments": [
      "well a `np.float64(0)` IS an array, its a 0-dim one. You generally will never need to use numpy scalars like this. I guess this is a bug.\n\ntry\n\n```\nIn [35]: 0 < pd.Series([1,2,3],dtype=np.float64)\nOut[35]: \n0    True\n1    True\n2    True\ndtype: bool\n```\n",
      "A slightly more realistic example would be:\n\n``` python\n(np.array([0,1,2])[0]) < pd.Series([0,1,2])\n```\n",
      "Note that `pd.Series([1,2,3],dtype=np.float64) > np.float64(0)` works, so we can look at how the two code paths differ.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "commented",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 18,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/core/ops.py",
      "pandas/tests/series/test_operators.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13019,
    "reporter": "msure",
    "created_at": "2016-04-28T16:49:51+00:00",
    "closed_at": "2016-04-30T14:25:43+00:00",
    "resolver": "sinhrks",
    "resolved_in": "35e3933ab6449f1716b900e9cde75c330f2f9e70",
    "resolver_commit_num": 299,
    "title": "Categorical dtypes cause error when attempting stacked bar plot",
    "body": "#### Code Setup w/ Fake Dataset\n\n\n\nHere's data frame at this point:\n\n\n#### Expected Output\n\n\n\n![screen shot 2016-04-28 at 10 41 58 am](-0d2d-11e6-8d45-dcdb8376e6a5.png)\n#### Error\n\nHowever, my real dataset uses Categorical types:\n\n\n\nNow frame is:\n\n\n\nAnd I get the following error when trying to plot:\n\n\n\nInterestingly, taking out the stacked kwarg works:\n\n\n#### Output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.4.4.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.4.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.1.0\nsetuptools: 20.2.2\nCython: 0.22.1\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.1.2\nsphinx: 1.3.1\npatsy: 0.3.0\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.0\nnumexpr: 2.5\nmatplotlib: 1.5.1\nopenpyxl: 1.8.5\nxlrd: 0.9.3\nxlwt: 1.0.0\nxlsxwriter: 0.7.3\nlxml: 3.4.4\nbs4: 4.3.2\nhtml5lib: None\n 0.9.1\napiclient: None\nsqlalchemy: 1.0.5\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.38.0\n",
    "labels": [
      "Testing",
      "Visualization",
      "Categorical"
    ],
    "comments": [
      "this works on master.  not sure what specific issue fixed this.\n\n@TomAugspurger @sinhrks \n\ndo we need confirming tests for this?\n",
      "Thanks for reviewing. Just so I understand...master is probably somewhat ahead of the Anaconda distribution, correct?\n",
      "@msure yes, latest anaconda has 0.18.0 (as does conda). master is not released yet at 0.18.1 (though going to be shortly, next few days).\n",
      "Yeah it works. I'll prepare tests.\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 1,
    "additions": 26,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tests/test_graphics.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13020,
    "reporter": "patricktokeeffe",
    "created_at": "2016-04-28T18:58:36+00:00",
    "closed_at": "2016-05-18T22:10:30+00:00",
    "resolver": "evectant",
    "resolved_in": "4b501497a88c118c2bfdcdbc4a5b216b68b1b88c",
    "resolver_commit_num": 2,
    "title": "Unclear error produced by resampling with NaT in Index",
    "body": "Similar to #4746, trying to `resample` with an index containing `NaT` produces rather ambiguous error messages of the form: \n\n> ValueError: Shape of passed values is (2, 5), indices imply (2, 4)\n\nSomething like `cannot resample from index containing Not-a-Time (NaT)` would be more helpful to users. \n#### MWE\n\n\n\n_This example index contains duplicate values, but results are the same w/ or w/o them._\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Testing",
      "Missing-data",
      "Difficulty Novice",
      "Resample",
      "Effort Low"
    ],
    "comments": [
      "so this works in master. Note the syntax change in 0.18.0 (you have a quite old version).\n\n```\nIn [23]: df.resample('3s').mean()\nOut[23]: \n                            A         B\n2016-04-28 11:00:00  0.633764 -0.551855\n2016-04-28 11:00:03  0.024339  0.041184\n```\n\nI don't know if we are specifically testing for this. Welcome to have some confirming tests.\n",
      "```\nIn [24]: df = pd.DataFrame(np.repeat(np.arange(10),2).reshape(10,-1),columns=list('AB'),index=times)\n\nIn [25]: df\nOut[25]: \n                     A  B\n2016-04-28 11:00:00  0  0\n2016-04-28 11:00:01  1  1\n2016-04-28 11:00:02  2  2\n2016-04-28 11:00:03  3  3\n2016-04-28 11:00:04  4  4\nNaT                  5  5\n2016-04-28 11:00:01  6  6\n2016-04-28 11:00:02  7  7\n2016-04-28 11:00:03  8  8\n2016-04-28 11:00:04  9  9\n\nIn [26]: df.resample('3s').mean()\nOut[26]: \n                       A    B\n2016-04-28 11:00:00  3.2  3.2\n2016-04-28 11:00:03  6.0  6.0\n```\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 1,
    "additions": 26,
    "deletions": 0,
    "changed_files_list": [
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13029,
    "reporter": "DavidDobr",
    "created_at": "2016-04-29T11:50:55+00:00",
    "closed_at": "2016-05-13T23:14:43+00:00",
    "resolver": "Xndr7",
    "resolved_in": "01dd11109a0d1def8bc3b03d06c533817cc273f2",
    "resolver_commit_num": 0,
    "title": "DOC: additional join examples in \"10 Minutes to pandas\"",
    "body": "---\n\nTutorial at [pandas.pydata.org/pandas-docs/stable/10min.html](-docs/stable/10min.html) has the following code, where the obvious mistake is having only 1 key \"foo\" instead of 2 keys: \"foo\" and \"bar\" throughout the whole example \n\n---\n# Join\n## SQL style merges. See the Database style joining\n\n\n\nHere is how it **should** be (replaced second 'foo' with 'bar':\n\n![screenshot from 2016-04-29 14-49-45](-0e19-11e6-92f4-093f879824e4.png)\n",
    "labels": [
      "Docs",
      "Reshaping",
      "Difficulty Novice"
    ],
    "comments": [
      "@DavidDobr this is not a mistake, but probably a common case. You can certainly _add_ to this to show the example with foo & bar.\n"
    ],
    "events": [
      "renamed",
      "renamed",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "cross-referenced"
    ],
    "changed_files": 1,
    "additions": 11,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/10min.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13043,
    "reporter": "sinhrks",
    "created_at": "2016-04-30T14:02:26+00:00",
    "closed_at": "2016-05-11T22:51:03+00:00",
    "resolver": "sinhrks",
    "resolved_in": "4aa6323e7d72fe00417d8aab783a5f78cf497018",
    "resolver_commit_num": 303,
    "title": "BUG: Series with datetime-like object ops raises TypeError",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\nops must be applied to each elem\n#### output of `pd.show_versions()`\n\ncurrent master.\n\n**NOTE:** This works:\n\n\n",
    "labels": [
      "Bug",
      "Dtypes",
      "Numeric",
      "Difficulty Intermediate",
      "Effort Medium",
      "Timeseries",
      "Timedelta",
      "Timezones",
      "Period"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "cross-referenced"
    ],
    "changed_files": 4,
    "additions": 128,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/core/ops.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_timedeltas.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13048,
    "reporter": "jreback",
    "created_at": "2016-05-01T14:01:00+00:00",
    "closed_at": "2016-05-07T14:53:12+00:00",
    "resolver": "gfyoung",
    "resolved_in": "c089110dd5d7f001c3b06d72515d14ad339a1e7a",
    "resolver_commit_num": 17,
    "title": "API: sparse.cumsum should default to stats axis",
    "body": "xref #12810 \n\n\n",
    "labels": [
      "Sparse",
      "Compat"
    ],
    "comments": [
      "cc @gfyoung merging #12810\n\nbut I think this should default. Need to systematically check all of the cum functions.\n(axis = 0). dense already does this. (yes the `axis` is passed as `None` but its set if its `None`.\n\ncc @sinhrks \n",
      "e.g. [6] works\n\n```\nIn [4]: np.cumsum(sp, axis=0)\nOut[4]: \n     0\n0  1.0\n1  3.0\n2  6.0\n\nIn [5]: np.cumsum(sp.to_dense(), axis=0)\nOut[5]: \n     0\n0  1.0\n1  3.0\n2  6.0\n\nIn [6]: np.cumsum(sp.to_dense())\nOut[6]: \n     0\n0  1.0\n1  3.0\n2  6.0\n```\n",
      "@jreback : I'm not sure I fully understand.  How do you want to handle the case when `axis=None` (when you say \"stats\" axis)?  In `numpy` it's easy because you just flatten the array, but that doesn't make any sense with a `DataFrame`.\n",
      "see the dense impl\n\nit needs to be 0 (which is the stats axis)\n",
      "Ah, I see.  Just a simple:\n\n``` python\nif axis is None:\n   axis = self._stat_axis_number\n```\n\nbefore making the `self.apply` function call\n",
      "yep but merging soon\ndo after and need tests\n",
      "Yes, indeed, certainly!\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 49,
    "deletions": 9,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/sparse/frame.py",
      "pandas/sparse/tests/test_frame.py",
      "pandas/sparse/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13057,
    "reporter": "dancsi",
    "created_at": "2016-05-02T16:00:24+00:00",
    "closed_at": "2016-05-07T14:59:43+00:00",
    "resolver": "dancsi",
    "resolved_in": "5541fd7c8dc0ad017057cad00ce70e7f9d9ee1f8",
    "resolver_commit_num": 0,
    "title": "ENH: add option to tz_localize to return NaT instead of raising a NonExistentTimeError",
    "body": "It would be nice if the `tz_localize` function of a `DatetimeIndex` had an optional flag for silently returning `NaT` instead of throwing a `NonExistentTimeError`, if the timestamp is not valid in the given timezone (for example due to DST changes).\nI ran into this problem while trying to `tz_localize` a large index, and it seems to me that this would be a much better solution than manually handling the exception with a lambda expression in a (slow) python loop.\n",
    "labels": [
      "Timezones",
      "API Design",
      "Missing-data",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "pls show an example\n\ntz_localize already has the ambiguous argument for this purpose\n",
      "and pd.show_versions()\n",
      "Here is a minimal example\n\n``` Python\nimport pandas as pd\n\ndf = pd.DataFrame({'large_series': [pd.Timestamp('2015-03-08 02:30:00')]})\nind = pd.DatetimeIndex(df['large_series']) \nind = ind.tz_localize('America/Los_Angeles')\n```\n\n(imagine that `large_series` is indeed a long column, with some timestamps that are invalid)\nThe error that is thrown:\n\n```\nTraceback (most recent call last):\n  File \"C:/Dev/temp/pandas_demo.py\", line 5, in <module>\n    ind = ind.tz_localize('America/Los_Angeles')\n  File \"C:\\Dev\\Python35\\lib\\site-packages\\pandas\\util\\decorators.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Dev\\Python35\\lib\\site-packages\\pandas\\tseries\\index.py\", line 1843, in tz_localize\n    ambiguous=ambiguous)\n  File \"pandas\\tslib.pyx\", line 3914, in pandas.tslib.tz_localize_to_utc (pandas\\tslib.c:67511)\npytz.exceptions.NonExistentTimeError: 2015-03-08 02:30:00\n```\n\nNote that the exception is `pytz.exceptions.NonExistentTimeError`, and not `pytz.AmbiguousTimeError`, that is handled by the `ambiguous` flag. It seems that in the current master, [this line](https://github.com/pydata/pandas/blob/master/pandas/tslib.pyx#L4072) is responsible.\nFinally, the output of `pd.show_versions()`\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Windows\nOS-release: 10\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\nbyteorder: little\nLC_ALL: en_US.UTF_8\nLANG: en_US.UTF-8\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.1.1\nsetuptools: 20.10.1\nCython: 0.23.1\nnumpy: 1.11.0\nscipy: 0.17.0\nstatsmodels: None\nxarray: 0.7.2\nIPython: 4.2.0\nsphinx: None\npatsy: None\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: 1.0.0\ntables: None\nnumexpr: 2.5.2\nmatplotlib: 1.4.3\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: 4.4.0\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\n```\n",
      "so you want a `errors='coerce'` with the default being `'raise'`. which will `NaT` the datetime.\n\nok I suppose, though this indicates a fundamental issue that you have. I don't think hiding this is the right answer. How did you generate this in the first place?\n\ncc @rockg\ncc @ischwabacher\ncc @adamgreenhall\n",
      "Exactly. I got the data from an external source ([here](http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time), if you are interested). There are just a few timestamps out of 500k that are a few minutes after 2am on the day when DST becomes active, so I believe they are just an error in the dataset\n",
      "ok, unless other objections, I don't see adding a coercion option as a problem. pull-requests welcome!\n",
      "Here it is #13058. Hopefully, I didn't miss anything, as it is my first time contributing to a large OSS project.\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "milestoned",
      "labeled",
      "labeled",
      "referenced",
      "referenced",
      "cross-referenced",
      "commented"
    ],
    "changed_files": 5,
    "additions": 76,
    "deletions": 9,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timezones.py",
      "pandas/tseries/tests/test_tslib.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13061,
    "reporter": "michaelaye",
    "created_at": "2016-05-03T04:31:17+00:00",
    "closed_at": "2016-08-19T10:49:54+00:00",
    "resolver": "jzwinck",
    "resolved_in": "453bc26e07ebbfea33dda9ff2f4efab683718506",
    "resolver_commit_num": 1,
    "title": "DOC: DataFrame.to_hdf() docstring does not mention data_columns",
    "body": "#### Code Sample, a copy-pastable example if possible\n\nTo be able to do on-disk filtering per column, one needs to store not only in `table` format, but also create an index in the HDF file for the columns required to be indexed, or setting it to `True`, for all columns to be indexed.\nThis needs to be done by using the method parameter `data_columns`, which is completely missing from the docstring of `to_hdf`:\n\n\n\nDocstring is [here](#L1053-L1093)\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 14.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.0\nnose: None\npip: 8.1.1\nsetuptools: 20.7.0\nCython: 0.24\nnumpy: 1.11.0\nscipy: 0.17.0\nstatsmodels: None\nxarray: 0.7.1\nIPython: 4.2.0\nsphinx: 1.4.1\npatsy: None\ndateutil: 2.5.2\npytz: 2016.3\nblosc: None\nbottleneck: None\ntables: 3.2.2\nnumexpr: 2.5.2\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\n",
    "labels": [
      "Docs",
      "IO HDF5"
    ],
    "comments": [
      "There is a minimal entry in the docstring of HDFStore.append (https://github.com/pydata/pandas/blob/master/pandas/io/pytables.py#L895), but indeed missing in `to_hdf`. Interested in doing a PR?\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "commented",
      "cross-referenced",
      "milestoned",
      "milestoned",
      "demilestoned",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 2,
    "additions": 13,
    "deletions": 6,
    "changed_files_list": [
      "pandas/core/generic.py",
      "pandas/io/pytables.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13076,
    "reporter": "jreback",
    "created_at": "2016-05-03T23:27:27+00:00",
    "closed_at": "2016-05-21T23:58:25+00:00",
    "resolver": "sinhrks",
    "resolved_in": "82bdc1dc80330330ab34e8a3a8c1e37b7e3a9b43",
    "resolver_commit_num": 315,
    "title": "TST: assert_series/frame not comparing for categoricals",
    "body": "xref \n\n\n\nall of these should fail as the ordered categoricals don't compare equal in the categories, and ordered is not being compared (nor categories)\n\nFurthermore the `assert_series_equal` fail for the same.\n\n`assert_categorical_equal` DOES the right thing, but not being called here. (not that this _should_ say `categories` are not equal?)\n\n\n",
    "labels": [
      "Testing",
      "Categorical",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "cc @sinhrks (on the `assert_categorical_equal` question)\n",
      "Correct. In addition, `CategoricalIndex` should also check internal `Categorical`.\n\nI've once tried this, and found lots of errors including stata. Needs some time to check it one by one...\n- https://github.com/pydata/pandas/compare/master...sinhrks:test_strict?expand=1\n",
      "maybe lets do in 2 stages \nfirst fix assert_categorical_equal then series/frame\n",
      "Let me clarify... 2 stages are:\n1. Fix `assert_categorical_equal` to show correct assertion message (say categories are different or something)\n2. Fix `assert_frame/series/index_equal` to call `assert_categorical_equal` internally.\n",
      "yep might be less invasive that way\n\nand for series/frame could have an option that we gradually turn on so can do the change in pieces (not ideal but could be lots of test comparison issues)\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "renamed",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 13,
    "additions": 221,
    "deletions": 145,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/core/reshape.py",
      "pandas/io/tests/test_pickle.py",
      "pandas/io/tests/test_pytables.py",
      "pandas/io/tests/test_stata.py",
      "pandas/tests/frame/test_reshape.py",
      "pandas/tests/indexing/test_categorical.py",
      "pandas/tests/series/test_apply.py",
      "pandas/tests/test_categorical.py",
      "pandas/tests/test_generic.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_reshape.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13078,
    "reporter": "sinhrks",
    "created_at": "2016-05-04T01:53:08+00:00",
    "closed_at": "2016-07-10T21:02:56+00:00",
    "resolver": "sinhrks",
    "resolved_in": "713eaa6837127f619619bca8a5a32ed02b145754",
    "resolver_commit_num": 325,
    "title": "BUG/ERR: DatetimeIndex - Period shows ununderstandable error",
    "body": "#### Code Sample, a copy-pastable example if possible\n\nSplit from #13071. `DatetimeIndex` - `Period` is not supported ATM, but errors are not understandable.\n\n\n#### Expected Output\n\nshow understandable error message. \n\nOr it should be supported converting `Period` to `Timestamp`.\n\n\n#### output of `pd.show_versions()`\n\non current master\n",
    "labels": [
      "Bug",
      "Error Reporting",
      "Timedelta",
      "Period"
    ],
    "comments": [
      "> show understandable error message.\n\nI vote for this. \nI can't think of a reasonable way there could be arithmetic between points-in-time and spans. Tenuously, we _could_ convert a Period into a Timedelta and use that, but then the user could do that explicitly\n",
      "I agree an error is best here - maybe show how to convert in the error message though \n\ndfi and period arithmetic cannot be intermixed implicitly \n",
      "This has been fixed by #13071, adding tests and release note.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 2,
    "additions": 29,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/tseries/tests/test_base.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13092,
    "reporter": "jreback",
    "created_at": "2016-05-05T13:04:21+00:00",
    "closed_at": "2016-12-26T22:49:17+00:00",
    "resolver": "jreback",
    "resolved_in": "eecfa8843a0c90b5424e9e0e4d353bbb3b78a671",
    "resolver_commit_num": 4151,
    "title": "ENH: pd.read_feather/to_feather",
    "body": "conda packages are now available [here]()\n\nso this should be straightforward. a couple of things to note.\n- should support the `LocalPath/PathLib` objects (easy as we do this in other `read_*`)\n- non-supported on windows ATM. need to skip for testing. `feather-format` will just raise an `ImportError` for now.\n- catch and re-raise for non-supported things, mainly object dtypes that are not all strings. Alternatively / maybe better to introspect these columns (via `lib.infer_dtype`) to prevent in the first place\n- test unicode support / encoding (not sure supported as of yet)\n- biggest issue is how / if to support `Series` and a `DataFrame` with a non-default index. As we need a facility to write/read meta-data within feather [see here](). Both these can simply `.reset_index()`, but the problem is they can be stored but we don't know on read-back what to `.set_index()`. Can just raise for now.\n",
    "labels": [
      "Enhancement",
      "Data IO",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "@wesm \n",
      "I'd prefer to put most of the heavy lifting in the feather Python library itself, unless you are worried about versioning synchronization issues? Of course we can have convenience APIs in pandas to import and invoke the appropriate feather functions. \n",
      "@wesm no for sure. Just have an overlay of testing in pandas main I think.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 19,
    "additions": 348,
    "deletions": 10,
    "changed_files_list": [
      "appveyor.yml",
      "ci/install_travis.sh",
      "ci/requirements-2.7-64.run",
      "ci/requirements-2.7.sh",
      "ci/requirements-3.5-64.run",
      "ci/requirements-3.5.run",
      "ci/requirements-3.5.sh",
      "ci/requirements-3.5_OSX.run",
      "ci/requirements-3.5_OSX.sh",
      "doc/source/api.rst",
      "doc/source/install.rst",
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/api/tests/test_api.py",
      "pandas/core/frame.py",
      "pandas/io/api.py",
      "pandas/io/feather_format.py",
      "pandas/io/tests/test_feather.py",
      "pandas/util/print_versions.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13095,
    "reporter": "lherstix",
    "created_at": "2016-05-05T17:20:47+00:00",
    "closed_at": "2016-05-05T17:36:14+00:00",
    "resolver": "max-sixty",
    "resolved_in": "2e975b6f39e3ab3840097ec0aa07c44ac13c4467",
    "resolver_commit_num": 8,
    "title": "Problems installing pandas using pip",
    "body": "Hey everyone,\n\nI installed pandas via pip git\n\n`pip install git+git://github.com/pydata/pandas.git`\n\nand during the installation it says:\n\n`Running setup.py install for pandas ... done\nSuccessfully installed pandas-0.18.0+207.gc6110e2`\n\nBut when I want to import it, it gives an error\n\n`C extension: hashtable not built.\nIf you want to import pandas from the source directory, you may need to run \n'python setup.py build_ext --inplace' to build the C extensions first.`\n\nI already tried to run this but the response is always:\n\n`python: can't open file 'setup.py': [Errno 2] No such file or directory`\n\nMaybe I am in the wrong directory. So in which I am suppose to run it?\nAlso I looked in site-packages but can't find pandas or something similar.\n\nI am confused since it said it runs the setup.py during the installation...\n\nCan anyone help? What I am doing wrong?\n\nThanks in advance\n\nLeni\n",
    "labels": [
      "Build"
    ],
    "comments": [
      "are you trying to contribute? you need to `git clone`, see [here](http://pandas.pydata.org/pandas-docs/stable/contributing.html)\n\notherwise see installation instructions [here](http://pandas.pydata.org/pandas-docs/stable/install.html)\n\ngenerally a `pip install pandas` (or much better is simply to use `conda`).\n\nI _think_ that pip doesn't install the deps correctly if you are installing like this (and that its failing because you don't have the build deps installed, e.g. cython/numpy).\n",
      "http://stackoverflow.com/questions/31495657/development-build-of-pandas-giving-importerror-c-extension-hashtable-not-bui\n\nThink the error message from pandas could include `--force`\n",
      "@MaximilianR that's a good point. do you want to PR for an extended message?\n"
    ],
    "events": [
      "commented",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced"
    ],
    "changed_files": 1,
    "additions": 2,
    "deletions": 2,
    "changed_files_list": [
      "pandas/__init__.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13098,
    "reporter": "gieseke",
    "created_at": "2016-05-05T18:56:45+00:00",
    "closed_at": "2016-05-12T13:12:58+00:00",
    "resolver": "jreback",
    "resolved_in": "4de83d25d751d8ca102867b2d46a5547c01d7248",
    "resolver_commit_num": 4025,
    "title": "REGRP: Series.quantile returns NaN",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\nI would expect 2.5 as output (as with version 0.17.1).\n#### output of `pd.show_versions()`\n\ncommit: None\npython: 2.7.6.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.13.0-85-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: de_DE.UTF-8\n\npandas: 0.18.1\nnose: None\npip: 1.5.4\nsetuptools: 2.2\nCython: None\nnumpy: 1.11.0\nscipy: 0.16.1\nstatsmodels: None\nxarray: None\nIPython: None\nsphinx: None\npatsy: None\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 0.9.4\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: None\nboto: None\npandas_datareader: None\n",
    "labels": [
      "Bug",
      "Regression",
      "Numeric",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "Hmm, this did break somewhere.\n\n```\nIn [1]: s = pd.Series([1, 2, 3, 4, numpy.nan])\n\nIn [2]: s.quantile(0.5)\nOut[2]: 2.5\n\nIn [3]: pd.__version__\nOut[3]: u'0.18.0'\n```\n",
      "xref #12772 \n"
    ],
    "events": [
      "renamed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "renamed",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 12,
    "additions": 352,
    "deletions": 115,
    "changed_files_list": [
      "asv_bench/benchmarks/frame_methods.py",
      "codecov.yml",
      "doc/source/whatsnew/v0.18.1.txt",
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/core/frame.py",
      "pandas/core/internals.py",
      "pandas/core/series.py",
      "pandas/io/pytables.py",
      "pandas/src/inference.pyx",
      "pandas/tests/frame/test_quantile.py",
      "pandas/tests/series/test_quantile.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13104,
    "reporter": "blalterman",
    "created_at": "2016-05-06T18:19:22+00:00",
    "closed_at": "2016-05-31T14:12:51+00:00",
    "resolver": "pijucha",
    "resolved_in": "132c1c55c3d7884e149dd8f99655f1d2c720696c",
    "resolver_commit_num": 1,
    "title": "pd.DataFrame.describe percentile string precision",
    "body": "When using `pd.DataFrame.describe`, if your percentiles are different only at the 4th decimal place, a `ValueError` is thrown because the the percentiles that vary at the 4th decimal place become the same value.\n\n\n",
    "labels": [
      "Bug",
      "Indexing",
      "Output-Formatting",
      "Numeric",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "@bla1089 can you add the example which raises.\n",
      "It seems like by design someone coded in to only show 1 decimal place.\n`def pretty_name(x):\n            x *= 100\n            if x == int(x):\n                return '%.0f%%' % x\n            else:\n                return '%.1f%%' % x`\n\nWhat would be an appropriate fix? Change decimal place if input has more decimals in place?\n\nFirst timer here. Hope I'm not stepping bounds...just wanted to see how I can start contributing to the project and learn about python.\n",
      "You would have to determine the first significant decimal place that\nprovides unique string identifiers for each percentile. I'm not trained as\na programmer, but I would be surprised if there isn't a standard way to do\nthis.\n\nOn Tue, May 10, 2016, 09:23 teaspreader notifications@github.com wrote:\n\n> It seems like by design someone coded in to only show 1 decimal place.\n> def pretty_name(x):\n> x *= 100\n> if x == int(x):\n> return '%.0f%%' % x\n> else:\n> return '%.1f%%' % x\n> \n> What would be an appropriate fix? Change decimal place if input has more\n> decimals in place?\n> \n> First timer here. Hope I'm not stepping bounds...just wanted to see how I\n> can start contributing to the project and learn about python.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/pydata/pandas/issues/13104#issuecomment-218155326\n",
      "Makes sense. Thanks.\n",
      "In fact, it might be done by taking np.ediff1d(np.log10([100_percentiles]))\nor replacing 100_percentiles with the decimals that remain after taking\n100*percentiles. Round that result to the left on the number line, and take\nthe most negative of the numbers. That would be the denial place precision\nnecessary to get a unique index. Though this could be way too complicated a\nformulation.\n\nOn Tue, May 10, 2016, 09:45 Ben Alterman alterman.ben@gmail.com wrote:\n\n> You would have to determine the first significant decimal place that\n> provides unique string identifiers for each percentile. I'm not trained as\n> a programmer, but I would be surprised if there isn't a standard way to do\n> this.\n> \n> On Tue, May 10, 2016, 09:23 teaspreader notifications@github.com wrote:\n> \n> > It seems like by design someone coded in to only show 1 decimal place.\n> > def pretty_name(x):\n> > x *= 100\n> > if x == int(x):\n> > return '%.0f%%' % x\n> > else:\n> > return '%.1f%%' % x\n> > \n> > What would be an appropriate fix? Change decimal place if input has more\n> > decimals in place?\n> > \n> > First timer here. Hope I'm not stepping bounds...just wanted to see how I\n> > can start contributing to the project and learn about python.\n> > \n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly or view it on GitHub\n> > https://github.com/pydata/pandas/issues/13104#issuecomment-218155326\n",
      "How about just return string decimal format based on the input?\n\n`def pretty_name(x):\n            decimal_place = str(x)[::-1].find('.')-2\n            x *= 100\n            if x == int(x):\n                return '%.0f%%' % x\n            else:\n                return '%.*f%%' % (decimal_place, x)`\n",
      "Test it. Does it work for cases in which you have a different number of\ndecimal places in the percentiles input? How anesthetically clean are the\nresults?\n\nOn Tue, May 10, 2016, 09:58 teaspreader notifications@github.com wrote:\n\n> How about just...?\n> def pretty_name(x):\n> decimal_place = str(x)[::-1].find('.')-2\n> \n> x *= 100\n> if x == int(x):\n> return '%.0f%%' % x\n> else:\n> \n> return '%.*f%%' % (decimal_place, x)\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/pydata/pandas/issues/13104#issuecomment-218165380\n",
      "I cannot seem to reproduce this issue on my machine.\n\n```\n>>> s.describe( percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])\ncount     10.000000\nmean      -0.225519\nstd        0.841984\nmin       -1.910093\n0.0%      -1.909577\n0.1%      -1.907514\n0.1%      -1.904935\n50%        0.018802\n99.9%      0.752442\n100.0%     0.753580\n100.0%     0.754491\nmax        0.754719\ndtype: float64\n>>> pd.show_version()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: 'module' object has no attribute 'show_version'\n>>> pd.show_versions()\n\nINSTALLED VERSIONS\n------------------\ncommit: 2e975b6f39e3ab3840097ec0aa07c44ac13c4467\npython: 2.7.11.final.0\npython-bits: 32\nOS: Linux\nOS-release: 3.13.0-71-generic\nmachine: i686\nprocessor: i686\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.1+5.g2e975b6\nnose: 1.3.7\npip: 8.1.1\nsetuptools: 20.7.0\nCython: 0.24\nnumpy: 1.11.0\nscipy: None\nstatsmodels: None\nxarray: None\nIPython: 4.2.0\nsphinx: 1.4.1\npatsy: None\ndateutil: 2.5.2\npytz: 2016.3\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: None\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\npandas_datareader: None\n```\n",
      "Srib, you are reproducing it in your output too. \nLook at how 0.9995 and 0.9999 is formatted to be both 100% in the function's output.\n",
      "Same thing with 0.0005 and 0.001.\n\nOn Tue, May 10, 2016 at 8:53 PM teaspreader notifications@github.com\nwrote:\n\n> Srib, you are reproducing it in your output too.\n> Look at how 0.9995 and 0.9999 I formatted to be both 100% in the\n> function's output.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/pydata/pandas/issues/13104#issuecomment-218334327\n",
      "If I may add my 2 cents:\n1. I have a bit of doubts if a unique index is an issue here. Firstly, a user can very well call `s.describe(percentiles = [0.1, 0.1, 0.5])`. Secondly, `describe` is not a function people usually use to calculate percentiles. So a pretty output might be more important than an exact percentile identifier. (But it's only a humble opinion.)\n2. @ghost. Your function `pretty_name` has a minor issue:\n   \n   ``` python\n   pretty_name(0.00001)\n   Out[285]: '0%'\n   # becasue:\n   str(0.00001)\n   Out[286]: '1e-05'\n   ```\n3. I wrote another function which finds a cutoff precision for float percentiles (and rounds them):\n\n``` python\ndef prettify(percentiles):\n    pcts = 100 * percentiles\n    # Number of digits left to decimal point (needed for padding):\n    m = max(1, 1 + np.floor(np.log10(pcts.max())).astype(int))\n    is_int_arr = (pcts.astype(int) == pcts);\n    if np.all(is_int_arr):\n        out = pcts.astype(int).astype(str)\n        return [' ' * (m - len(i)) + i + '%' for i in out]\n    # precision:\n    prec = -np.floor(np.log10(                          # position of the first digit of\n                np.min(                                 # the minimum of\n                    np.ediff1d(                         # differences of adjacent\n                        np.sort(np.unique(              # sorted unique entries of\n                            np.append(pcts, [0, 100])   # pcts with extra boundary values \n                        ))        \n                    )\n                )\n            )).astype(int)\n    prec = max(1, prec)\n    out = np.empty_like(pcts, dtype = object)\n    out[is_int_arr] = pcts[is_int_arr].astype(int).astype(str)\n    out[~is_int_arr] = pcts[~is_int_arr].round(prec).astype(str)\n    return [' ' * (m - len(i)) + i + '%' if is_int\n            else ' ' * (m-i.find('.')) + i + '%'\n            for (is_int, i) in zip (is_int_arr, out)]\n```\n\nand also does some padding:\n\n``` python\nprettify(np.array([1e-5, 0.0001, 0.0005, 0.001, 0.1, 0.5, 0.999, 0.9995, 0.9999]))\nOut[448]: \n[' 0.001%',\n ' 0.01%',\n ' 0.05%',\n ' 0.1%',\n '10%',\n '50%',\n '99.9%',\n '99.95%',\n '99.99%']\n\nprettify(np.array([0.0199, 0.03, 0.2]))\nOut[449]: [' 2.0%', ' 3%', '20%']\n```\n\nI'm not sure if this isn't an overkill.\nBut if this is the expected behaviour and nobody works on this issue (ghost's pull request has been cancelled, I guess), I can submit it.\n",
      "@pijucha you can submit if you'd like, still some comments outstanding on https://github.com/pydata/pandas/pull/13132\n",
      "I'd better ask before I submit. Does anyone has an opinion on whether such a formatting (padding with blanks to align decimal points) is acceptable?\n\n```\n# Possible output of s.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])\ncount     10.000000\nmean       0.291571\nstd        1.057143\nmin       -1.453547\n 0.01%    -1.453107\n 0.05%    -1.451348\n 0.1%     -1.449149\n50%        0.637435\n99.9%      1.817201\n99.95%     1.820583\n99.99%     1.823288\nmax        1.823964\n```\n\n@jreback \n\n> still some comments outstanding on #13132\n> The author's  (ghost) github account is deleted. Does it affect the pull request?\n",
      "aligning on decimal point is fine\nI think you should have all the same number of digits \nto the right of decimal point as well\n\n0.01,0.05,0.10,50.00,99.90,99.95,99.99\n\nas they line up better\n",
      "OK. Thanks for the comment.\n",
      "Personally, I would prefer 0.9 over 0.90 because it implies less visual\nclutter and I don't think we're concerned about sig figs.\n\nOn Wed, May 18, 2016 at 10:15 PM pijucha notifications@github.com wrote:\n\n> OK. Thanks for the comment.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/pydata/pandas/issues/13104#issuecomment-220210511\n",
      "maybe just leave the sig figs like u have them but align on decimals and also align the % - might be a nicer look\n",
      "All right. I'm posting it once more to give some visual comparison.\n\n```\n# Possible output of s.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])\ncount     10.000000\nmean       0.291571\nstd        1.057143\nmin       -1.453547\n 0.01%    -1.453107\n 0.05%    -1.451348\n 0.1 %    -1.449149\n50   %     0.637435\n99.9 %     1.817201\n99.95%     1.820583\n99.99%     1.823288\nmax        1.823964\n```\n",
      "a small issue is that to actually get at these fields you have to use an exactly formatted label - if this is for display purposes only then that doesn't matter \n\ns[' 0.1  %'] might be a bit awkward \n",
      "Yes. This was one of the reasons I preferred to ask first. \nBut I don't think people really use it like this. (At least I can't think of a situation I'd need to use it.)\n",
      "I think that assuming this is for display purposes only might get us into\ntrouble later on. I'm sure that if it's there, some people use it for more\nthan display purposes.\n\nOn Wed, May 18, 2016 at 10:51 PM pijucha notifications@github.com wrote:\n\n> Yes. This was one of the reasons I preferred to ask first.\n> But I don't think people really use it like this. (At least I can't think\n> of a situation I'd need to use it.)\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/pydata/pandas/issues/13104#issuecomment-220215011\n",
      "True. I'm giving up on padding. Without blanks, it's both simpler to use and easier to code. \n\nNow, I'm a bit in favour of Version 2. (Version 1 automatically adds a decimal point to all entries if there is at least one non-integer.)\n\n```\nVersion 1                                  Version 2\n\ncount     10.000000                        count     10.000000\nmean       0.291571                        mean       0.291571\nstd        1.057143                        std        1.057143\nmin       -1.453547                        min       -1.453547\n0.01%     -1.453107                        0.01%     -1.453107\n0.05%     -1.451348                        0.05%     -1.451348\n0.1%      -1.449149                        0.1%      -1.449149\n50.0%      0.637435                        50%        0.637435\n66.66%     0.901020                        66.66%     0.901020\n75.0%      1.010203                        75%        1.010203\n99.0%      1.817201                        99%        1.817201\n99.9%      1.820583                        99.9%      1.820583\n99.99%     1.823288                        99.99%     1.823288\nmax        1.823964                        max        1.823964\n\ncount     10.000000                        count     10.000000\nmean       0.291571                        mean       0.291571\nstd        1.057143                        std        1.057143\nmin       -1.453547                        min       -1.453547\n0.01%     -1.453107                        0.01%     -1.453107\n25.0%      0.637435                        25%        0.637435\n50.0%      0.637435                        50%        0.637435\n75.0%      1.010203                        75%        1.010203\nmax        1.823964                        max        1.823964\n\ncount     10.000000                        count     10.000000\nmean       0.291571                        mean       0.291571\nstd        1.057143                        std        1.057143\nmin       -1.453547                        min       -1.453547\n25%        0.637435                        25%        0.637435\n50%        0.637435                        50%        0.637435\n75%        1.010203                        75%        1.010203\nmax        1.823964                        max        1.823964\n```\n",
      "I think I like version 2\n",
      "@jorisvandenbossche @TomAugspurger @sinhrks \n",
      "+1 for version2, as readabily looks important rather than precision consistency.\n",
      "I was about to submit a pull request but discovered that the rounding is only a part of the problem here. The other part got slightly overlooked. Namely, users themselves can supply non-unique percentiles. It works fine with Series\n\n```\ns = pd.Series(np.arange(11))\ns.describe(percentiles = [0.1, 0.2, 0.2])\nOut[52]: \ncount    11.000000\nmean      5.000000\nstd       3.316625\nmin       0.000000\n10%       1.000000\n20%       2.000000\n20%       2.000000\n50%       5.000000\nmax      10.000000\n```\n\nbut not with DataFrame (or other multidimensional objects):\n\n```\ndf = pd.DataFrame(np.arange(11))\ndf.describe(percentiles = [0.1, 0.2, 0.2])\n# ... longer traceback ...\nValueError: cannot reindex from a duplicate axis\n```\n\n`.describe()` internally uses `pd.concat()` for DataFrames and it breaks with non-unique entries in indexes (here, percentiles identifiers).\n\nSo, what would be the most sensible action when a user supplies non-unique percentiles:\n1. raising a ValueError with an informative message (and if so, for all objects or excluding Series?)\n2. modifying code to allow non-unique percentiles?\n",
      "There's a third option. Use the unique percentiles and issue a warning to\nthe user that they provided non-unique percentiles.\n\nPersonally, I'm in favor of raising a ValueError for all objects so that\nthings are standardized and to ensure people are getting the output they\nexpect.\n\nOn Tue, May 24, 2016 at 1:22 AM pijucha notifications@github.com wrote:\n\n> I was about to submit a pull request but discovered that the rounding is\n> only a part of the problem here. The other part got slightly overlooked.\n> Namely, users themselves can supply non-unique percentiles. It works fine\n> with Series\n> \n> s = pd.Series(np.arange(11))\n> s.describe(percentiles = [0.1, 0.2, 0.2])\n> Out[52]:\n> count    11.000000\n> mean      5.000000\n> std       3.316625\n> min       0.000000\n> 10%       1.000000\n> 20%       2.000000\n> 20%       2.000000\n> 50%       5.000000\n> max      10.000000\n> \n> but not with DataFrame (or other multidimensional objects):\n> \n> df = pd.DataFrame(np.arange(11))\n> df.describe(percentiles = [0.1, 0.2, 0.2])\n> \n> # ... longer traceback ...\n> \n> ValueError: cannot reindex from a duplicate axis\n> \n> .describe() internally uses pd.concat() for DataFrames and it breaks with\n> non-unique entries in indexes (here, percentiles identifiers).\n> \n> So, what would be the most sensible action when a user supplies non-unique\n> percentiles:\n> 1. raising a ValueError with an informative message (and if so, for all\n> objects or excluding Series?)\n> 2. modifying code to allow non-unique percentiles?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/pydata/pandas/issues/13104#issuecomment-221169359\n",
      "I think raising on duplicated percentiles if fine.\n",
      "I like @bla1089 's idea (unique percentiles plus a warning). So if it's also acceptable, I'd go for it.\n\nAnd another minor issue: I'd always sort percentiles on output. Now, it depends on whether you enter 0.5, so some weird reordering may happen:\n\n```\ns.describe(percentiles = [0.3, 0.6, 0.2])\nOut[72]: \ncount    11.000000\nmean      5.000000\nstd       3.316625\nmin       0.000000\n30%       3.000000\n20%       2.000000\n50%       5.000000\n60%       6.000000\nmax      10.000000\ndtype: float64\n```\n",
      "sorting is fine. I am not really in favor of duplicates. yes they are handled generally, but not particularly useful IMHO.\n",
      "I'm actually not in favor of the warning because (1) it is only going to be\nraised once; (2) we don't know how the user will use the percentiles (e.g.do\nthey iterate through them in the describe output?); and (3) it permits\nsloppy coding that could cause someone problems later.\n\nIf you want the ability for duplicate percentiles, then how about a kwarg\nlike `warn_on_dupe` so that the default is an error and someone has to\nexplicitly request duplicates?\n\nI was only proposing the warning for completeness. That being said, the\npercentiles should be sorted so that the index is monotonic.\n\nOn Tue, May 24, 2016, 11:11 Jeff Reback notifications@github.com wrote:\n\n> sorting is fine. I am not really in favor of duplicates. yes they are\n> handled generally, but not particularly useful IMHO.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/pydata/pandas/issues/13104#issuecomment-221302784\n",
      "Yes, this sounds reasonable. Exceptions then.\n\nSlightly off-topic: if I keep finding other bugs and can fix them, should I open a new issue to discuss the code changes I propose? Or is it enough to discuss it in a pull request comments?\n\nAs an illustration, another bug with `describe`, this time unrelated to percentiles:\n\n``` python\ndf = pd.DataFrame({'A': list(\"BCDE\"), 0: [1,2,3,4]})\ndf.describe()\n# long traceback listing several internal functions\nValueError: Buffer dtype mismatch, expected 'Python object' but got 'long'\n```\n\nI've found the source of the error and have a seemingly reasonable solution (it still needs testing, though).\n",
      "This seems reasonably distinct and should probably be handled on its own.\n\nOn Wed, May 25, 2016, 10:06 pijucha notifications@github.com wrote:\n\n> Yes, this sounds reasonable. Exceptions then.\n> \n> Slightly off-topic: if I keep finding other bugs and can fix them, should\n> I open a new issue to discuss the code changes I propose? Or is it enough\n> to discuss it in a pull request comments?\n> \n> As an illustration, another bug with describe, this time unrelated to\n> percentiles:\n> \n> df = pd.DataFrame({'A': list(\"BCDE\"), 0: [1,2,3,4]})\n> df.describe()# long traceback listing several internal functionsValueError: Buffer dtype mismatch, expected 'Python object' but got 'long'\n> \n> I've found the source of the error and have a seemingly reasonable\n> solution (it still needs testing, though).\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/pydata/pandas/issues/13104#issuecomment-221586696\n",
      "Yes it's distinct and I don't want to discuss it here. But I was going to fix it in the same commit because the code changes will probably be contained within the function `describe`. \n\nMy question was essentially: does any code change require opening a new issue? Wiki says [not necessarily](https://github.com/pydata/pandas/wiki/Pandas-Development-FAQ#criteria-for-pr) but mentions only insignificant changes.\n",
      "generally we like to have issues to go with PR's. \n",
      "Ok, thanks. I'll open a new one then.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 212,
    "deletions": 22,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/core/generic.py",
      "pandas/formats/format.py",
      "pandas/tests/formats/test_format.py",
      "pandas/tests/test_generic.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13107,
    "reporter": "sinhrks",
    "created_at": "2016-05-06T23:26:33+00:00",
    "closed_at": "2016-09-03T15:31:03+00:00",
    "resolver": "sinhrks",
    "resolved_in": "4488f18076226894c0f07be67ee700663e32623e",
    "resolver_commit_num": 398,
    "title": "BUG/API: Index.equals should check input class?",
    "body": "#### Code Sample, a copy-pastable example if possible\n\nDecide the behavior if non `Index` with same values is passed. Currently, there is incompatibility between dtypes. \n\n\n#### output of `pd.show_versions()`\n\non current master\n",
    "labels": [
      "Bug"
    ],
    "comments": [
      "these have to be dtype compat as well as values\n\nright so the top should work \n",
      "@jreback OK, always return `False` if input is not `Index`.  Regarding `dtype`, there are some tests which allows the difference.\n- https://github.com/pydata/pandas/blob/master/pandas/tests/indexes/test_numeric.py#L523\n\nYou mean following must be `False`?\n\n```\npd.Index([1, 2]).equals(pd.Index([1., 2.]))\n# True\n```\n",
      "no what I mean is another `Index` is ok, but NOT a `Series`.\n"
    ],
    "events": [
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 18,
    "additions": 177,
    "deletions": 84,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/indexes/base.py",
      "pandas/indexes/category.py",
      "pandas/indexes/multi.py",
      "pandas/indexes/numeric.py",
      "pandas/tests/indexes/common.py",
      "pandas/tests/indexes/test_base.py",
      "pandas/tests/indexes/test_category.py",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tests/indexes/test_numeric.py",
      "pandas/tests/indexes/test_range.py",
      "pandas/tseries/base.py",
      "pandas/tseries/index.py",
      "pandas/tseries/period.py",
      "pandas/tseries/tdi.py",
      "pandas/tseries/tests/test_base.py",
      "pandas/tseries/tests/test_timedeltas.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13108,
    "reporter": "max-sixty",
    "created_at": "2016-05-07T01:13:25+00:00",
    "closed_at": "2016-05-08T15:17:16+00:00",
    "resolver": "max-sixty",
    "resolved_in": "af7bdd3883c8d61e9d9388d3aa699930eee7fff8",
    "resolver_commit_num": 9,
    "title": "DOC: MultiIndex sort docs",
    "body": "I found this confusing, despite being a moderately competent pandas user:\n\nfrom -docs/stable/advanced.html#the-need-for-sortedness-with-multiindex\n\n> Caveat emptor: the present implementation of MultiIndex requires that the labels be sorted for some of the slicing / indexing routines to work correctly. You can think about breaking the axis into unique groups, where at the hierarchical level of interest, each distinct group shares a label, but no two have the same label. However, the MultiIndex does not enforce this: you are responsible for ensuring that things are properly sorted. There is an important new method sort_index to sort an axis within a MultiIndex so that its labels are grouped and sorted by the original ordering of the associated factor at that level. Note that this does not necessarily mean the labels will be sorted lexicographically!\n\nIs this right, that calling `sort_index` doesn't guarantee lex sortedness? How to guarantee it then?\n\nAnd this:\n\n> Some indexing will work even if the data are not sorted, but will be rather inefficient and will also return a copy of the data rather than a view:\n\n...seems to contradict this:\n\n> Thus, if you try to index at a depth at which the index is not sorted, it will raise an exception.\n\n...neither of which seems tightly consistent with the passage above.\n\nAm I misunderstanding something?\n",
    "labels": [
      "Docs",
      "MultiIndex",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "> Thus, if you try to index at a depth at which the index is not sorted, it will raise an exception.\n> This has been somewhat mitigated recently (we will now show a `PerformanceWarning` when you do this. So doc's could be amended.\n\n`.sort_index()` does guaranteed that we can index propely. So you could certainly simpify this. The key point is that its the USERS's reponsiblity for this\n",
      "Can this whole section be simplified to:\n- For MultiIndex-ed objects to be indexed & sliced effectively, they need to be sorted. \n- `.sort_index()` sorts the index\n- Where they're not sorted, pandas will show a `PerformanceWarning`*\n\n...and potentially keep a couple of the examples as examples of sorting MultiIndexed dfs\n\n * is this correct - there's no KeyError?\n",
      "yes that would be a nice improvement\n\niirc we have an example somewhere of a Perdormancewarning in the docs but not sure where \n",
      "OK PR-ing.\n\nWhat's the policy on when we remove version warnings? This is probably not needed / over emphasized:\n\n> Warning In 0.15.0 Index has internally been refactored to no longer sub-class ndarray but instead subclass PandasObject, similarly to the rest of the pandas objects. This should be a transparent change with only very limited API implications (See the Internal Refactoring)\n",
      "yeah u can leave I guess once in s while I take those types of things out \n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 1,
    "additions": 32,
    "deletions": 41,
    "changed_files_list": [
      "doc/source/advanced.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13109,
    "reporter": "mdickinson",
    "created_at": "2016-05-07T07:07:04+00:00",
    "closed_at": "2016-08-21T13:37:54+00:00",
    "resolver": "rkern",
    "resolved_in": "ce61b3f1c85c1541cfbe1b3bb594431b38689946",
    "resolver_commit_num": 0,
    "title": "\"import pandas\" changes NumPy error-handling settings",
    "body": "A simple `import pandas` apparently does a `np.seterr(all='ignore')`: #L8-L9\n\nThis is a problem when using Pandas as a library, particularly during testing: a test (completely unrelated to Pandas) that should have produced warnings can fail to do so just because some earlier test happened to import a library that imported a library that imported something from Pandas (for example). Or a test runner that's done a `np.seterr(all='raise')` specifically to catch potential issues can end up catching nothing, because some Pandas import part-way through the test run turned the error handling off again.\n\nI'm working around this right now by wrapping every pandas import in `with np.errstate():`. For example:\n\n\n\nBut that's (a) ugly, and (b) awkward if the pandas import is in a third-party library out of your control.\n\nPlease consider removing this feature!\n#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\nI expected to see a `RuntimeWarning` from the division by zero above, as in:\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Compat",
      "API Design"
    ],
    "comments": [
      "This came up somewhat recently in https://github.com/pydata/pandas/issues/12464, but I don't think anything came out of it.\n",
      "@mdickinson this is very very common to do. The entire point of `pandas` is to turn things to `NaN` AND propogate them. \n\nIt IS possible to turn this off by having practially every deferral option to numpy wrap this in a context manager (rather than setting the global). \n\nIt would'nt be that invasive (could do it with a decorator), but adds complexity, may be somewhat non-performant, and its a fair amount of work.\n\nIf you want to give this a stab, then i'll reopen.\n",
      "> The entire point of pandas is to turn things to NaN AND propogate them.\n\nSure, and that's fine when you're using Pandas to do data analysis at a command line. It's a long way from fine when you're using Pandas as a library for a couple of small tasks (in this particular case, flexible reading from .csv files) within a larger application. Then you're in the situation where one of your application dependencies has unilaterally and silently turned off NumPy warnings for the _entire_ application, oblivious to the needs of that application or any of its many other libraries. That's just rude. :-) Deciding to turn off NumPy warnings (or any warnings, for that matter) globally is a decision that should be made at application level, not at the level of one particular library.\n\nIt's interesting to compare with the `seaborn` library, where `import seaborn` changes several matplotlib settings. Again, that's a useful thing to do for the purposes of visual exploration from the command line, but you want to avoid that import-time side-effect when using `seaborn` as a library in a larger application. The seaborn folks have provided a `seaborn.apionly` module for that exact purpose, so that you do `import seaborn.apionly as sns` and any changes that have been made to the matplotlib settings elsewhere in your app are unaffected. Would something like this be possible for Pandas?\n",
      "well this setting has existed since pandas inception. as I layed out above I don't see much gain in changing this. If you would like to make an attempt by all means.\n\n> Sure, and that's fine when you're using Pandas to do data analysis at a command line. It's a long way from fine when you're using Pandas as a library for a couple of small tasks (in this particular case, flexible\n\nyour statement is very odd. Virtually the entire user base uses pandas for data analysis. Want to drop back to some exception handling in numpy w/o using pandas seems dubious at best.\n",
      "I agree that this behavior is very annoying.  Most of the time, my only use of pandas is to have a (great) implementation of groupby, and do not want other errors to pass silently.  Likewise, because seaborn depends on pandas, it is easy to end up with this change even without explicitly importing pandas.\n\nTwo possible solutions may be\n- Provide `pandas.apionly`, similar to `seaborn.apionly`.  The problem is that this is going to take a while to propagate down to other packages that depend on pandas.\n- Only change the errstate if at an interactive prompt (as defined by the presence of `sys.ps1`, not `sys.flags.interactive`, as the latter is unset when in IPython).\n",
      "I do not believe that unilaterally modifying numpy's error-handling is a common practice. If you proposed that over at `numpy-discussion`, I think you would get some pushback. Importing `numpy.ma` used to do this for basically the same reasons, and we eventually received enough complaints about it to fix it. If you would like to propose that pandas' default settings are better than numpy's (and I am rather more sympathetic to that idea now than I was in the past), then let's change numpy's defaults so that we have consistent, documented behavior across the board.\n\nPart of the reason that you have seen only a few complaints about this behavior since pandas' inception is that it changes behavior silently. It took a couple of years after the same behavior's inception in `numpy.ma` for us to notice it and fix it too.\n\nHaving an application that has both data analysis (where NaN may mean missing data) and moderately-complicated numerical algorithms (where NaNs can appear from perfectly-clean inputs due to domain violations and other numerical errors) is not at all dubious. Most nontrivial data analysis tasks have both. [Cleaning missing data](http://pandas.pydata.org/pandas-docs/stable/missing_data.html#cleaning-filling-missing-data) first before doing the heavy numerical work of data analysis/statistics/machine learning is quite common. That's why numpy has the ability to control the FPE-handling at a pretty fine grain.\n",
      "well @rkern this has been in place from < 2012 AFAICT. This IS possible, but performance of the context managers would need to be considered, and much more importantly someone to do it; are you volunteering?\n",
      "In progress.\n\nIn so doing, I found another reason to avoid the fire-and-forget `np.seterr()` in favor of more fine-grained `np.errstate()`. pandas assumes that no one else has changed the settings. If someone uses `with np.errstate(invalid='raise'):` then binary comparisons will be silently wrecked.\n\n```\n[~]\n|1> import pandas\n\n[~]\n|2> df = pandas.DataFrame(dict(x=[np.nan, 1, 2]))\n\n[~]\n|3> df\n     x\n0  NaN\n1  1.0\n2  2.0\n\n[~]\n|4> df < 0\n       x\n0  False\n1  False\n2  False\n\n[~]\n|5> df > 0\n       x\n0  False\n1   True\n2   True\n\n[~]\n|6> with np.errstate(invalid='raise'): print df < 0\n      x\n0  True\n1  True\n2  True\n\n[~]\n|7> with np.errstate(invalid='raise'): print df > 0\n      x\n0  True\n1  True\n2  True\n```\n",
      "ok thanks @rkern that would be awesome!\n\nyeah I have seen that inverse case as well.\n",
      "@jreback how do we envision this interacting with the internals refactor? This is kind of a gray-zone on the _defined_ behavior right now, I don't think we would want to commit to anything that could change with the refactor. \nAs an example:\n\n``` python\ns = pd.Series([1, 2, 3])\nwith np.errstate(invalid='raise'): s.reindex(range(4)) > 1\n```\n\nThat casts to float and uses the `np.nan` marker for NaN, so it raises. Post internals-refactor that should still be an int dtype, so it won't use the `np.nan` missing values, so presumably it wouldn't raise, unless pandas explicitly does so. Should we give Wes a heads up?\n",
      "no @TomAugspurger I don't think it will have much of an effect. IF its done, then computation that is deferred to numpy will use a context manager to wrap the err state. That's it, as it is one now. Ideally one would have a function like:\n\n```\ndef compute_numpy(name, arr, *args, **kwargs):\n    with np.errstate(invalid='raise'):\n          return getattr(arr, name)(*args, **kwargs)\n```\n\nsomething like that. There would have to be some logic in this function to handle some of this but its not a big deal. It will mostly be isolated.\n",
      "Just hit this problem myself, I'm definitely in favor of a fix. In this case, Pandas was a dependency of another package I was using---and indeed I wasn't even using the Pandas-dependent functionality---but I was bitten by this anyway. Thanks to @rkern for working on this.\n"
    ],
    "events": [
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "closed",
      "milestoned",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "demilestoned",
      "reopened",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 35,
    "additions": 449,
    "deletions": 314,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/compat/numpy/__init__.py",
      "pandas/computation/align.py",
      "pandas/computation/expressions.py",
      "pandas/computation/ops.py",
      "pandas/computation/tests/test_eval.py",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/internals.py",
      "pandas/core/nanops.py",
      "pandas/core/ops.py",
      "pandas/core/panel.py",
      "pandas/core/series.py",
      "pandas/core/window.py",
      "pandas/formats/format.py",
      "pandas/indexes/base.py",
      "pandas/indexes/range.py",
      "pandas/sparse/array.py",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_arithmetics.py",
      "pandas/sparse/tests/test_array.py",
      "pandas/tests/formats/test_format.py",
      "pandas/tests/frame/test_apply.py",
      "pandas/tests/frame/test_misc_api.py",
      "pandas/tests/frame/test_operators.py",
      "pandas/tests/indexes/common.py",
      "pandas/tests/series/test_analytics.py",
      "pandas/tests/series/test_apply.py",
      "pandas/tests/series/test_operators.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_nanops.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_panel4d.py",
      "pandas/tests/test_util.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13135,
    "reporter": "aliabbasjp",
    "created_at": "2016-05-11T07:09:03+00:00",
    "closed_at": "2016-08-21T13:37:54+00:00",
    "resolver": "rkern",
    "resolved_in": "ce61b3f1c85c1541cfbe1b3bb594431b38689946",
    "resolver_commit_num": 0,
    "title": "df.query bug giving RuntimeWarning: divide by zero encountered in log10 in align.py\", line 98, in _align_core     ordm = np.log10(abs(reindexer_size - term_axis_size)) ",
    "body": "Getting the following bug:\n\nRuntimeWarning: divide by zero encountered in log10\nC:\\Users\\Nick\\Anaconda3\\lib\\site-packages\\pandas\\computation\\align.py:98: \n  ordm = np.log10(abs(reindexer_size - term_axis_size))\nwhile running query().\n\ndtypes:\n\nCGI            int64\nSITEID         int32\nLONGITUDE    float64\nLATITUDE     float32\ndtype: object\nQuery:\n\ntowerData = towerData.query('CGI != 0')\n\nonce you get this runtime warning it just halts everything  and you have to restart the program.\n\nThis bug happens only in multithreaded calls to query function. I am using the concurrent.futures module for the same.\n\nruns fine for non threaded application\n",
    "labels": [
      "Numeric",
      "Bug"
    ],
    "comments": [
      "Thanks for the report. Do you have minimal copy-pastable example?\n",
      "Note: I'll be avoiding that warning explicitly in my fix for #13109. The numpy error state is thread-local IIRC, so the current fire-and-forget `np.seterr(all='ignore')` probably only takes effect in the main thread. Why the OP's program halts after the warning, I don't know.\n",
      "I'm having a similar issue even without threading.  See example code below.\n\n`\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame.from_items([('A', [1, 2, 3, 4, 5]), \n                              ('B', [14, 15, 16, 17, 18]), \n                              ('C', list(np.random.randn(5)))])\ndf.set_index(['A', 'B'], inplace = True)\ndf.query('15 <= B <= 17')\n\nC:\\Anaconda3\\lib\\site-packages\\pandas\\computation\\align.py:98: \nRuntimeWarning: divide by zero encountered in log10\nordm = np.log10(abs(reindexer_size - term_axis_size))\nOut[1]: \n             C\nA B \n2 15 -0.852411\n3 16 -0.665470\n4 17  0.132162\n`\n\nWindows 7 64 bit\nSpyder 3.0.0.dev0\nPython 3.5.2\nPandas 0.18.1\nNumpy 1.11.1\n"
    ],
    "events": [
      "commented",
      "labeled",
      "commented",
      "cross-referenced",
      "commented",
      "milestoned",
      "labeled"
    ],
    "changed_files": 35,
    "additions": 449,
    "deletions": 314,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/compat/numpy/__init__.py",
      "pandas/computation/align.py",
      "pandas/computation/expressions.py",
      "pandas/computation/ops.py",
      "pandas/computation/tests/test_eval.py",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/internals.py",
      "pandas/core/nanops.py",
      "pandas/core/ops.py",
      "pandas/core/panel.py",
      "pandas/core/series.py",
      "pandas/core/window.py",
      "pandas/formats/format.py",
      "pandas/indexes/base.py",
      "pandas/indexes/range.py",
      "pandas/sparse/array.py",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_arithmetics.py",
      "pandas/sparse/tests/test_array.py",
      "pandas/tests/formats/test_format.py",
      "pandas/tests/frame/test_apply.py",
      "pandas/tests/frame/test_misc_api.py",
      "pandas/tests/frame/test_operators.py",
      "pandas/tests/indexes/common.py",
      "pandas/tests/series/test_analytics.py",
      "pandas/tests/series/test_apply.py",
      "pandas/tests/series/test_operators.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_nanops.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_panel4d.py",
      "pandas/tests/test_util.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13139,
    "reporter": "gaow",
    "created_at": "2016-05-11T14:35:13+00:00",
    "closed_at": "2016-10-24T22:13:24+00:00",
    "resolver": "tserafim",
    "resolved_in": "13088842a7218e8e4626ab68f0c4f204f25f0ba4",
    "resolver_commit_num": 0,
    "title": "ERR: better error message on invalid .query input",
    "body": "#### Code Sample\n\n\n#### Expected output\n\nIf I run query with an empty string I should expect no output, however:\n\n\n\nThe problem lies on the last line of \n\nI'm not sure of a proper fix so I'm opening this issue instead of pull request. \n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Difficulty Novice",
      "Error Reporting",
      "Effort Low"
    ],
    "comments": [
      "I supppose this could be a better error message. That is invalid input.\n",
      "There is already a helper function for that: [_check_expression(expr)](https://github.com/pandas-dev/pandas/blob/65362aa4f06f01efdc20ca487c1c3c1f090613ee/pandas/computation/eval.py#L85-L99)\n\nProblem is, this function is only executed in the for loop: [eval.py#L246-L247](https://github.com/pandas-dev/pandas/blob/65362aa4f06f01efdc20ca487c1c3c1f090613ee/pandas/computation/eval.py#L246-L247)\n\nA simple `_check_expression(expr)`  before the for loop can fix it. Or maybe on the beginning of the eval function. Is there a better way, according to pandas code style?\n\nI could send a PR(first OS contribution, ever)\n",
      "yes u can move it\npls add a test as well\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 22,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.1.txt",
      "pandas/computation/eval.py",
      "pandas/computation/tests/test_eval.py",
      "pandas/tests/frame/test_query_eval.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13144,
    "reporter": "bryanwweber",
    "created_at": "2016-05-11T19:53:31+00:00",
    "closed_at": "2016-05-13T13:21:47+00:00",
    "resolver": "sinhrks",
    "resolved_in": "00d4ec3e7b7fa68d5cf226f7b63a5eea23167b45",
    "resolver_commit_num": 311,
    "title": "SparseSeries throws a ValueError exception when printing large arrays",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\nThe expected output is that both cases print the SparseSeries. However, only the second case is printed properly. The first case throws the following exception:\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Bug",
      "Output-Formatting",
      "Sparse",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "In Pandas 0.18.0, a different exception is thrown:\n\n```\nimport scipy.sparse as sps\nimport pandas as pd\nA = sps.rand(350, 18)\nss = pd.SparseSeries.from_coo(A)\nprint(ss)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/bryan/anaconda3/envs/pandas-test/lib/python3.5/site-packages/pandas/core/base.py\", line 42, in __str__\n    return self.__unicode__()\n  File \"/Users/bryan/anaconda3/envs/pandas-test/lib/python3.5/site-packages/pandas/sparse/series.py\", line 287, in __unicode__\n    series_rep = Series.__unicode__(self)\n  File \"/Users/bryan/anaconda3/envs/pandas-test/lib/python3.5/site-packages/pandas/core/series.py\", line 959, in __unicode__\n    max_rows=max_rows)\n  File \"/Users/bryan/anaconda3/envs/pandas-test/lib/python3.5/site-packages/pandas/core/series.py\", line 1000, in to_string\n    dtype=dtype, name=name, max_rows=max_rows)\n  File \"/Users/bryan/anaconda3/envs/pandas-test/lib/python3.5/site-packages/pandas/core/series.py\", line 1027, in _get_repr\n    max_rows=max_rows)\n  File \"/Users/bryan/anaconda3/envs/pandas-test/lib/python3.5/site-packages/pandas/core/format.py\", line 144, in __init__\n    self._chk_truncate()\n  File \"/Users/bryan/anaconda3/envs/pandas-test/lib/python3.5/site-packages/pandas/core/format.py\", line 158, in _chk_truncate\n    series.iloc[-row_num:]))\n  File \"/Users/bryan/anaconda3/envs/pandas-test/lib/python3.5/site-packages/pandas/tools/merge.py\", line 834, in concat\n    copy=copy)\n  File \"/Users/bryan/anaconda3/envs/pandas-test/lib/python3.5/site-packages/pandas/tools/merge.py\", line 890, in __init__\n    raise TypeError(\"cannot concatenate a non-NDFrame object\")\nTypeError: cannot concatenate a non-NDFrame object\n\npd.show_versions()\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.4.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.0\nnose: None\npip: 8.1.1\nsetuptools: 20.7.0\nCython: None\nnumpy: 1.11.0\nscipy: 0.17.0\nstatsmodels: None\nxarray: None\nIPython: None\nsphinx: None\npatsy: None\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: None\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: None\nboto: None\n```\n",
      "hmm, that seems odd. welcome for you to take a look!\n",
      "cc @sinhrks \n",
      "Yeah currently indexing doesn't work well with `MultiIndex`...\n"
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 7,
    "additions": 214,
    "deletions": 43,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/indexes/multi.py",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_format.py",
      "pandas/sparse/tests/test_indexing.py",
      "pandas/sparse/tests/test_series.py",
      "pandas/tests/formats/test_format.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13149,
    "reporter": "mao-liu",
    "created_at": "2016-05-12T02:42:59+00:00",
    "closed_at": "2016-05-23T20:45:45+00:00",
    "resolver": "pijucha",
    "resolved_in": "afde7187e22b2013147d0a15911f6ec72e056a43",
    "resolver_commit_num": 0,
    "title": "NaNs in Float64Index are converted to silly integers using index.astype('int')",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Bug",
      "Dtypes",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "This is numpy behaviour:\n\n```\nIn [22]: np.array([np.nan, 1.]).astype(int)\nOut[22]: array([-2147483648,           1])\n```\n\nBut, we should probably check for the occurence of NaNs, just as we do for Series:\n\n```\nIn [29]: df.iloc[0,0] = np.nan\n\nIn [30]: df.a\nOut[30]:\nNaN   NaN\n 1      2\nName: a, dtype: float64\n\nIn [31]: df.a.astype(int)\n...\n\nC:\\Anaconda\\lib\\site-packages\\pandas\\core\\common.pyc in _astype_nansafe(arr, dty\npe, copy)\n   2726\n   2727         if np.isnan(arr).any():\n-> 2728             raise ValueError('Cannot convert NA to integer')\n   2729     elif arr.dtype == np.object_ and np.issubdtype(dtype.type, np.intege\nr):\n   2730         # work around NumPy brokenness, #1987\n\nValueError: Cannot convert NA to integer\n```\n",
      "I wanted to fix this bug but noticed a similar behaviour of other objects: DatetimeIndex, TimedeltaIndex, Categorical, CategoricalIndex. Namely (all four of them behave identically):\n\n```\nA = pd.DatetimeIndex([1e10,2e10,None])\nA\nOut[76]: DatetimeIndex(['1970-01-01 00:00:10', '1970-01-01 00:00:20', 'NaT'], dtype='datetime64[ns]', freq=None)\nA.astype(int)\nOut[77]: array([         10000000000,          20000000000, -9223372036854775808])\n```\n\nHowever, unlike with Float64Index, this is invertible:\n\n```\npd.DatetimeIndex(A.astype(int))\nOut[78]: DatetimeIndex(['1970-01-01 00:00:10', '1970-01-01 00:00:20', 'NaT'], dtype='datetime64[ns]', freq=None)\n```\n\nMy question: is this behaviour also a bug and should be fixed the same way (raising a ValueError)? And if so, should all the fixes be placed into one commit/pull request?\n\nBy the way, there might be other objects with the same issue, which call numpy.ndarray.astype(). And numpy is also a bit inconsistent here:\n\n```\nnp.array([1,np.nan]).astype(int)\nOut[84]: array([                   1, -9223372036854775808])\nnp.array([1,np.nan], dtype = int)\nTraceback...\nValueError: cannot convert float NaN to integer\n```\n",
      "@ch41rmn these are all as expected. converting to `int` converts to the underlying integer based representation. \n\nThe _only_ issue is that `Float64Index.astype(int)` should raise (as its effectively non-convertible).\n",
      "@jreback I actually think we should raise in the datetimeindex case as well (ideally). A `NaT` cannot be converted to int (just as float nan cannot be converted). There is the `asi8` attribute if you want this. \nBut, of course, that is not really back compat. Internally I think we consequently use `asi8`? But not sure about external use of course\n",
      "Raising for CategoricalIndex seems less of a problem (not a common thing to do)\n",
      "This is excactly what should be returned (and is useful). yes its equivalen to internal `.asi8`, but I dont' see a good reason to NOT do this.\n\n```\nIn [20]: pd.DatetimeIndex([1e10,2e10,None]).astype(int)\nOut[20]: array([         10000000000,          20000000000, -9223372036854775808])\n```\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 17,
    "additions": 330,
    "deletions": 132,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/core/categorical.py",
      "pandas/core/common.py",
      "pandas/core/ops.py",
      "pandas/indexes/base.py",
      "pandas/indexes/multi.py",
      "pandas/indexes/numeric.py",
      "pandas/tests/indexes/test_datetimelike.py",
      "pandas/tests/indexes/test_numeric.py",
      "pandas/tests/test_common.py",
      "pandas/tseries/index.py",
      "pandas/tseries/period.py",
      "pandas/tseries/tdi.py",
      "pandas/tseries/tests/test_base.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_timedeltas.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13160,
    "reporter": "mineo",
    "created_at": "2016-05-12T19:05:12+00:00",
    "closed_at": "2016-07-21T14:31:18+00:00",
    "resolver": "sahildua2305",
    "resolved_in": "4caacdff9c7c6499651269e5829a5a3412a46033",
    "resolver_commit_num": 1,
    "title": "The documentation around frequency strings is unclear",
    "body": "-docs/stable/generated/pandas.DataFrame.rolling.html?highlight=rolling#pandas.DataFrame.rolling and various other places in the documentation talk about something called \"frequency strings\", however, there doesn't seem to be any explanation of what this term means anywhere. -docs/stable/timeseries.html#dateoffset-objects mentions the term again without explaining it. From the few examples I found, I now think -docs/stable/timeseries.html#offset-aliases (note that this is after the dateoffset objects in the documentation) is the complete list of frequency strings, but for some reason they're called \"offset aliases\" here. Are those the frequency strings? If not, where are they defined?\n",
    "labels": [
      "Docs",
      "Frequency",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "I think another confusion is caused by legacy aliases.  We've formally deprecated legacy offsets in 0.17 (#10951), thus 0.19 may be a good timing to re-organize the doc (including your point).\n",
      "I'd like to take this for my first PR. Could it be assigned to me?\n",
      "@ckrapu go ahead an submit a PR when you are ready.\n",
      "@ckrapu are you still working on this? If not, I'd like to work on this. Please let me know.\n",
      "Nope, all yours. I got stuck trying to build the docs and sort out what\nactually needed to be edited. If you do work on it, I'd really like to see\nwhat you end up doing, as a sort of tutorial.\nOn Jul 12, 2016 5:20 PM, \"Sahil Dua\" notifications@github.com wrote:\n\n> @ckrapu https://github.com/ckrapu are you still working on this? If\n> not, I'd like to work on this. Please let me know.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/pydata/pandas/issues/13160#issuecomment-232185166,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AKBR4ia-wV3-LxXts89gPTK3Pd5w9kwrks5qVAU4gaJpZM4IdYXW\n> .\n",
      "Awesome!\n\n@jreback can you please suggest some ideas about including the point mentioned by @mineo? One thing can be including the link to [this](http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases) explanation wherever `frequency string` is mentioned/used. Other approach I can think of is to bring the same explanation part in the beginning.\n\nAm I thinking in the right direction?\n",
      "sure u can include the link\n",
      "@jreback I am not sure from where I can edit [this](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html?highlight=rolling#pandas.DataFrame.rolling) page. I think that it's coming up from the Class documentation in [`window.py`](https://github.com/pydata/pandas/blob/master/pandas/core/window.py#L263). Should I add additional note for the `frequency strings` here?\n",
      "that page is the actual doc-string defined in window.py\n\ndocs themselves r in pandas/doc/source\n",
      "Thanks! I'll submit the PR in a while. \n",
      "@ckrapu If you got stuck with building the docs, always welcome to specify what the problem was, so we can try to make contributing to the documentation as easy as possible\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "commented",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 6,
    "additions": 52,
    "deletions": 7,
    "changed_files_list": [
      "doc/source/timeseries.rst",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/generic.py",
      "pandas/stats/moments.py",
      "pandas/tseries/index.py",
      "pandas/tseries/tdi.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13166,
    "reporter": "ruoyu0088",
    "created_at": "2016-05-13T02:34:39+00:00",
    "closed_at": "2016-06-15T01:48:21+00:00",
    "resolver": "chris-b1",
    "resolved_in": "f98b4b541e7a9d1b0d8f6674a84dc10080c2568b",
    "resolver_commit_num": 35,
    "title": "Float64Index is very slow in some condition.",
    "body": "The following code is very slow:\n\n\n\nafter debug it, I found `Float64Engine.get_loc()` is slow. Here is a demo:\n\n\n\noutputs:\n\n\n",
    "labels": [
      "Indexing",
      "Performance",
      "Dtypes"
    ],
    "comments": [
      "This might have been true on older versions of pandas (maybe < 0.16.0, I don't recall the exact version) as these were object based. but these moved to true float based hashtables (typed). \n\n```\nIn [1]: pd.__version__\nOut[1]: u'0.18.1'\n\nIn [2]: a = np.arange(1000000)\n\nIn [3]: ind1 = pd.Float64Index(a * 4.8e-08)\n\nIn [4]: ind2 = pd.Float64Index(a * 4.8000000418824129e-08)\n\nIn [5]: %timeit -n 1 -r 1 ind1._engine.get_loc(0)\n1 loop, best of 1: 116 ms per loop\n\nIn [6]: %timeit -n 1 -r 1 ind2._engine.get_loc(0)\n1 loop, best of 1: 107 ms per loop\n```\n\nclosing, but pls show your versions\n\nnote you have to time these with a single iteration as these build the hash tables which are then cached (so of course that tells you the lookup time, but you want the build time as well)\n",
      "@jreback \n\nI am using pandas 0.18.1, python 3.5 64bit, I confirmed this problem both on Linux and Windows 7. \nThe `pd.hashtable.Float64HashTable` is slow for `ind2` on my system. I think this is due to the khash library.\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.10.18\nmachine: x86_64\nprocessor: Intel(R) Celeron(R) CPU N2840 @ 2.16GHz\nbyteorder: little\nLC_ALL: en_US.utf8\nLANG: None\n```\n",
      "@jreback It seems that Python 3.5 has the problem, but Python 2.7 has no problem. Can you reopen this issue? You can confirm this on https://try.jupyter.org/.\n",
      "hmm interesting\n\nso you want to try profiling the cython ?\n",
      "I think here is the problem, but I don't know why:\n\n`khash_python.h`\n\n```\n#define kh_float64_hash_func _Py_HashDouble\n```\n\nI changed the line to following code, it view the double value as int64 and use the same formula as `kh_int64_hash_func`:\n\n```\ninline khint64_t asint64(double key)\n{\n  return *(khint64_t *)(&key);\n}\n\n#define kh_float64_hash_func(key) (khint32_t)((asint64(key))>>33^(asint64(key))^(asint64(key))<<11)\n```\n\nThe %time result is almost the same, and it's even 2x faster  for `ind0`.\n",
      "does that break any tests? can you run the asv suite as well (and add benchmark for this).\n",
      "https://github.com/python/cpython/blob/master/Python/pyhash.c#L85 is the existing PyHash_double.\n\nIts probably generating 'better' hashes that your change, but in the end of the day I don't see why that's preferable.\n",
      "@ruoyu0088 see also #13335 \n"
    ],
    "events": [
      "commented",
      "closed",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "reopened",
      "commented",
      "commented",
      "commented",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced"
    ],
    "changed_files": 4,
    "additions": 66,
    "deletions": 106,
    "changed_files_list": [
      "asv_bench/benchmarks/groupby.py",
      "asv_bench/benchmarks/indexing.py",
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/src/klib/khash_python.h"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13172,
    "reporter": "starplanet",
    "created_at": "2016-05-13T16:24:49+00:00",
    "closed_at": "2016-05-17T13:40:50+00:00",
    "resolver": "starplanet",
    "resolved_in": "20ea4064b0c94f99c275bfc4217664cc8aea75c5",
    "resolver_commit_num": 0,
    "title": "COMPAT: unicode_literals conflict with to_records",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n\nthe code above will report following error:\n\n\n\nIf I comment out `from __future__ import unicode_literals`, the code above will work fine.\n\n\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.4.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: zh_CN.UTF-8\n\npandas: 0.18.1\nnose: None\npip: 8.1.1\nsetuptools: 19.4\nCython: None\nnumpy: 1.11.0\nscipy: None\nstatsmodels: None\nxarray: None\nIPython: 4.2.0\nsphinx: 1.3.5\npatsy: None\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: None\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: 4.4.1\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\npandas_datareader: None\n",
    "labels": [
      "Unicode",
      "2/3 Compat",
      "Compat"
    ],
    "comments": [
      "https://github.com/pydata/pandas/blob/master/pandas/core/frame.py#L1068 should prob be\n\n`lmap(compat.u, self.columns)`\n\nwant to do a PR?\n",
      "OK, I will have a try.\n\n\u539f\u59cb\u90ae\u4ef6\n\u53d1\u4ef6\u4eba:Jeff Rebacknotifications@github.com\n\u6536\u4ef6\u4eba:pydata/pandaspandas@noreply.github.com\n\u6284\u9001:starplanetzhangjinjie@yimian.com.cn; Authorauthor@noreply.github.com\n\u53d1\u9001\u65f6\u95f4:2016\u5e745\u670814\u65e5(\u5468\u516d)\u200700:56\n\u4e3b\u9898:Re: [pydata/pandas] unicode_literals conflict with to_records(#13172)\n\nhttps://github.com/pydata/pandas/blob/master/pandas/core/frame.py#L1068 should prob be\nlmap(compat.u, self.columns)\nwant to do a PR?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 10,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/core/frame.py",
      "pandas/tests/frame/test_convert_to.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13174,
    "reporter": "starplanet",
    "created_at": "2016-05-14T01:07:46+00:00",
    "closed_at": "2016-05-14T12:01:13+00:00",
    "resolver": "jreback",
    "resolved_in": "feee089e41cc2dd5ff88e1068a5ca5595b6ff2f6",
    "resolver_commit_num": 4026,
    "title": "resample strange behavior on get_item",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n\nIf I then run `resampler['buyer'].count()` first time, it will output normally:\n\n\n\nIf I then run `resampler['buyer'].count()` again, it will report error:\n\n\n\nIf I run `df.groupby('id').resample('1D')['buyer'].count()` instead of `resampler['buyer'].count()`, the problem will not appear.\n\nWhat's the problem?\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.4.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: zh_CN.UTF-8\n\npandas: 0.18.1\nnose: None\npip: 8.1.1\nsetuptools: 19.4\nCython: None\nnumpy: 1.11.0\nscipy: None\nstatsmodels: None\nxarray: None\nIPython: 4.2.0\nsphinx: 1.3.5\npatsy: None\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: None\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: 4.4.1\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\npandas_datareader: None\n",
    "labels": [
      "Bug",
      "Groupby",
      "Resample"
    ],
    "comments": [
      "weird aliasing issue, fixed by #13175 \n"
    ],
    "events": [
      "renamed",
      "referenced",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented"
    ],
    "changed_files": 4,
    "additions": 36,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/tests/test_window.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13179,
    "reporter": "mpschr",
    "created_at": "2016-05-14T18:30:12+00:00",
    "closed_at": "2017-02-22T18:50:40+00:00",
    "resolver": "kernc",
    "resolved_in": "f6385506dd668ae461581c9af564be5b98e6ff16",
    "resolver_commit_num": 2,
    "title": "BUG: groupby upon categorical and sort=False triggers ValueError",
    "body": "#### Code that triggers ValueError\n\n**The combination of `sort=False` and a missing category in the data causes the bug - see below**\n\nFirst off, see this notebook which showcases the bug nicely: [github.com/mpschr/pandas_missing_cat_bug](%20missing_category_in_data.ipynb)\n\n\n##### Summaries of the scenarios where this bug appears:\n\n**Bug scenarios with ordered categories:**\n- Default (`sort = True`): No error\n- `chromosome 1` filtered out and `sort=True`: No error \n- `chromosome 1` filtered out and `sort=False`: **Error**\n- `sort = False`: **Error**\n\n**Bug scenarios without ordered categories:**\n\nthe 4 scenarios:\n- Default (`sort = True`): No error\n- `chromosome 1` filtered out and `sort=True`: No error \n- `sort = False`: No error\n- `chromosome 1` filtered out and `sort=False`: **Error**\n#### Expected Output\n\nNot an error, but this:\n\n\n#### output of `pd.show_versions()`\n\npd.show_versions()\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.4.4.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.13.0-34-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: 1.3.4\npip: 8.1.1\nsetuptools: 20.7.0\nCython: 0.22\nnumpy: 1.10.4\nscipy: 0.16.0\nstatsmodels: 0.6.0.dev-9ce1605\nxarray: None\nIPython: 4.1.2\nsphinx: 1.2.3\npatsy: 0.3.0\ndateutil: 2.5.2\npytz: 2016.3\nblosc: None\nbottleneck: None\ntables: 3.1.1\nnumexpr: 2.4.4\nmatplotlib: 1.4.3\nopenpyxl: 1.8.5\nxlrd: 0.9.3\nxlwt: None\nxlsxwriter: 0.6.7\nlxml: 3.4.2\nbs4: 4.3.2\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 0.9.9\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.36.0\npandas_datareader: None\n",
    "labels": [
      "Bug",
      "Groupby",
      "Categorical",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "I cannot be a 100% sure, but it seems that this bug is closely related with: pydata/pandas#10505 and  pydata/pandas#10508\n",
      "So the purpose of the `.unique()` here is to put the categoricals in the order of appearance, BUT, crucially unused categories are removed (and that's the error that's popping up). \n\nSo in this case you are removing the values for category '1', BUT that should still show up in the results as its a categorical.\n\nfor `sort=True`, actually is already sorted in order of the categoricals.\n\n```\nIn [7]: df.query(\"chromosomes != '1'\").groupby('chromosomes').A.sum()\nOut[7]: \nchromosomes\n1       NaN\n2     157.0\n3     115.0\n4     477.0\n5     274.0\n6     172.0\n7     221.0\n8     290.0\n9     231.0\n10    434.0\n11    196.0\n12    243.0\n13    109.0\n14    217.0\n15     89.0\n16    193.0\n17    417.0\n18     58.0\n19    149.0\n20    144.0\n21    166.0\n22    334.0\nX     147.0\nY     316.0\nName: A, dtype: float64\n```\n\nI suppose for `sort=False` you then can put the NA groups at the front or back (e.g the '1' group), the remainder will then be in the order of appearance (e.g. the `uniquie`). \n\nI think would just do this in groupby (or _maybe_ add a kw arg to `.unique` to return all of the categories, even unsued ones; maybe we should do that by default)? not really sure why we are excluding unused ones.\n\ncc @jorisvandenbossche \ncc @janschulz \n",
      "cc @sinhrks \n",
      "Hi @jreback - Thanks for receiving the bug report. I have just a little doubt as a layman here: is it convention to return a 'group' (in the groupby) for all the categories even tough there is no data for them available in the supplied data?\n\nImagine I make a query for just chromosomes `4` and `5` for whatever (biological investigative) reason - I would not expect results back for the other chromosomes I think (as follows):\n\n``` python\nquery_chroms = ['4', '5']\ndf[df.chromosomes.isin(query_chroms)].groupby('chromosomes').A.sum()\n\nchromosomes\n4     195.0\n5     394.0\nName: A, dtype: float64\n\n\n# as opposed to :\n\nchromosomes\n1       NaN\n2       NaN\n3       NaN\n4     195.0\n5     394.0\n6       NaN\n7       NaN\n8       NaN\n9       NaN\n10      NaN\n11      NaN\n12      NaN\n13      NaN\n14      NaN\n15      NaN\n16      NaN\n17      NaN\n18      NaN\n19      NaN\n20      NaN\n21      NaN\n22      NaN\nX       NaN\nY       NaN\nName: A, dtype: float64\n```\n",
      "@mpschr yes, this is the purpose of `Categoricals`. to return full categories. You made an explict choice to use them and so it must be explict to drop them; that is the point here.\n\nIf you think about it would be buggy to remove them! IOW, how would the code know its 'ok' to drop them?\n",
      "Hi @jreback I am not sure if we are talking about the same thing. I elaborte: I was referring to the data available in the `DataFrame`. Of course the categories which have been established as categories in `df.chromosomes.cat.categories` should never be dropped - even tough they are not represented in the DataFrame. Exactly as shown here:\n\n``` python\n\nquery_chroms = ['4', '5']\ndf[df.chromosomes.isin(query_chroms)].chromosomes\n71    4\n72    4\n73    4\n74    5\n75    5\n76    5\n77    5\n78    5\n79    5\n80    5\n81    5\nName: chromosomes, dtype: category\nCategories (24, object): [1 < 2 < 3 < 4 ... 21 < 22 < X < Y]\n```\n\n**But**, analogously to this I would expect the following output after doing groupby:\n\n``` python\n\ndf[df.chromosomes.isin(query_chroms)].groupby('chromosomes').A.sum().reset_index().chromosomes\n\n#expected output:\nchromosomes\n1      4\n2      5\nName: chromosomes, dtype: category\nCategories (24, object): [1 < 2 < 3 < 4 ... 21 < 22 < X < Y]\n\n#but actual output is.\n\n0      1\n1      2\n2      3\n3      4\n4      5\n5      6\n6      7\n7      8\n8      9\n9     10\n10    11\n11    12\n12    13\n13    14\n14    15\n15    16\n16    17\n17    18\n18    19\n19    20\n20    21\n21    22\n22     X\n23     Y\nName: chromosomes, dtype: category\nCategories (24, object): [1 < 2 < 3 < 4 ... 21 < 22 < X < Y]\n```\n\nThe actual output (2nd option) we get here is misleading since all chromosomes except 4 and 5 are not in the supplied data, they are just 'acceptable' options. Is it possible that this two different viewpoints may contribute to the bug reported here?\n",
      "@mpschr:\nThere is a different \"view\" for categoricals and groupby: if I have a lickert scale and want to get number of times each value was ticked, I want \"unused\" groups to show up as `0`. That was at least the idea behind having all groups show up in groupby and such things.\n\n> I think would just do this in groupby (or maybe add a kw arg to .unique to return all of the categories, even unsued ones; maybe we should do that by default)? Not really sure why we are excluding unused ones.\n\nI think there was a specific reason why unique is now not returning the whole categories (AFAIK remember the first implementation simply returned the categories). I think because someone argued that the implicit API contract for `unique` ist that it returns only used values (and ordered in in appearance as that was what seaborn/plots expected).\n",
      "Ok, so this is the current behaviour:\n\n``` python\n# 1.\n\nquery_chroms = ['4', '5']\ndf[df.chromosomes.isin(query_chroms)].chromosomes.unique()\n#output\n[4, 5]\nCategories (2, object): [4 < 5]\n\n```\n\nagain - here what a layman like me would expect is the following.\n\n``` python\n# 2.\n\nquery_chroms = ['4', '5']\ndf[df.chromosomes.isin(query_chroms)].chromosomes.unique()\n#output\n[4, 5]\nCategories (24, object): [1 < 2 < 3 < 4 ... 21 < 22 < X < Y]\n\n```\n\nNow I understood the bug :) The `seaborn` library should be able to work with the `unique` _used_ values as in example 2, right?\n",
      "@mpschr not sure what you mean. This is as expected. The point is that the category dtype IS propogated to ALL operations. There is extensive documentation on this. What exactly is not clear? (the bug in this issue is independent / not related to this).\n\n```\nIn [8]: df[df.chromosomes.isin(query_chroms)].chromosomes\nOut[8]: \n61    4\n62    4\n63    4\n64    4\n65    4\n66    4\n67    4\n68    4\n69    5\n70    5\n71    5\n72    5\nName: chromosomes, dtype: category\nCategories (24, object): [1 < 2 < 3 < 4 ... 21 < 22 < X < Y]\n```\n",
      "Yep @jreback - I think I went a bit off-topic with the groupby behaviour (including unused categories in the output of group aggregations).\n\nIn any case I totally agree with you on the matter with the `unique` behavior, as posted in my last comment. The unused categories should not be discarded from the `cat.categories` when gettting `df.chromosomes.unique()`\n",
      "@janschulz \n\n> I think would just do this in groupby (or maybe add a kw arg to .unique to return all of the categories, even unsued ones; maybe we should do that by default)? Not really sure why we are excluding unused ones.\n> \n> > I think there was a specific reason why unique is now not returning the whole categories (AFAIK remember the first implementation simply returned the categories). I think because someone argued that the implicit API contract for unique ist that it returns only used values (and ordered in in appearance as that was what seaborn/plots expected).\n\nyeah I don't really recall all of the discussion about `.unique` (though there were many!).\n\nYeah I can see how we just return the observed values\n",
      "Has this not been fixed yet (just curiosity)",
      "@mpschr issues get closed when they are fixed. you are welcome to submit a PR to fix this. Community PR's push things along."
    ],
    "events": [
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced"
    ],
    "changed_files": 6,
    "additions": 141,
    "deletions": 18,
    "changed_files_list": [
      "asv_bench/benchmarks/groupby.py",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/categorical.py",
      "pandas/core/groupby.py",
      "pandas/indexes/category.py",
      "pandas/tests/groupby/test_categorical.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13180,
    "reporter": "lvphj",
    "created_at": "2016-05-15T01:39:28+00:00",
    "closed_at": "2016-05-20T14:06:49+00:00",
    "resolver": "jreback",
    "resolved_in": "123f2ee16d713c94f41d2a85945c8df0a2244061",
    "resolver_commit_num": 4030,
    "title": "Converting float64 values to datetime64[ns] format using pd.to_datetime results in NaT if errors='coerce'",
    "body": "#### Code Sample, a copy-pastable example if possible\n\nA Pandas dataframe contains a date variable that is formatted as float64 as follows:\n\n\n\nWhich appears as:\n\n\n\nThe date variables can be formatted to datetime64[ns] as follows:\n\n\n\nWhich works as expected:\n\n\n\nHowever, if errors='coerce' is included in the do_datetime() function, the dates are returned as NaT.\n\n\n\nWhich produces the following:\n\n\n\nThis issue was identified as a result of a reported bug (#12821 ) where datetime64[ns] columns are returned as float64 following agg() function where all dates in a group are NaT. Converting columns back to datetime64[ns] format is necessary. However, identifying valid numeric values as errors when errors='coerce' and returning NaT values results in unnecessary data-loss.\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Bug",
      "Timeseries"
    ],
    "comments": [
      "fixed in #13183 \n\nyeah I had previously removed the path for unit processing from `array_to_datetime` as was subsumed by `array_with_unit_to_datetime`. But I also removed the default unit to `None` from `ns` (IOW it would only trigger if passed). So this was an oversite.\n\nthanks for the report.\n",
      "Thanks for the rapid response!\n"
    ],
    "events": [
      "renamed",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "commented",
      "commented",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 176,
    "deletions": 81,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/tests/series/test_internals.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tools.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13191,
    "reporter": "chris-b1",
    "created_at": "2016-05-15T21:43:14+00:00",
    "closed_at": "2016-05-18T13:19:39+00:00",
    "resolver": "chris-b1",
    "resolved_in": "009d1df85ec6e6f80cace1d949bb7cdc8d35df7c",
    "resolver_commit_num": 32,
    "title": "BUG: Series _transform_fast fails for datetime with null groups",
    "body": "xref #10972 - but not the same issue.  I've got a fix for this + #12737 coming.\n#### Code Sample, a copy-pastable example if possible\n\n\n",
    "labels": [
      "Bug",
      "Groupby"
    ],
    "comments": [],
    "events": [
      "cross-referenced",
      "labeled",
      "labeled",
      "milestoned"
    ],
    "changed_files": 5,
    "additions": 82,
    "deletions": 31,
    "changed_files_list": [
      "asv_bench/benchmarks/groupby.py",
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/core/groupby.py",
      "pandas/tests/test_categorical.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13202,
    "reporter": "jreback",
    "created_at": "2016-05-17T13:44:35+00:00",
    "closed_at": "2016-12-21T17:23:05+00:00",
    "resolver": "Dr-Irv",
    "resolved_in": "f79bc7a9d128187f3a93a3dae84ff03f4f4a62f4",
    "resolver_commit_num": 4,
    "title": "DOC: pandas cheat sheet",
    "body": "-content/uploads/2015/02/data-wrangling-cheatsheet.pdf\n\nI think we could pretty much rip this off exactly as is and just substitute pandas functions directly.\n\nFurther could update `comparison with R` a bit.\n\nanyone up for this?\n",
    "labels": [
      "Docs",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "+1 I have wanted to do this already for a long time! (but not sure I will be able to :-))\n",
      "+1. shall we close #1618?\n",
      "+1\n",
      "I don't know R, but know that having a cheatsheet like this would be helpful.  I'm going to give it a shot in Powerpoint, because I know it well and can make it pretty.  I'm not sure what other tool to use that would let other people edit it and provide the good formatting.",
      "@Dr-Irv we will just check in the pdf and whatever format it's in",
      "Here is what I did today.  Plagiarizing is blatant!  Will be flying a lot next week, so this will be good to do on the plane.\r\n[Pandas Cheat Sheet.pdf](https://github.com/pandas-dev/pandas/files/628437/Pandas.Cheat.Sheet.pdf)\r\n",
      "Nice start! That original from RStudio is [CC 4.0](https://creativecommons.org/licenses/by/4.0/), so AFAIK it's fine to copy / transform it as long as you acknowledge RStudio for the original.",
      "Here is the first page of the cheat sheet.  I'll be working on page 2 next.   In most places, I found the pandas way of doing things. I used @jreback initial comment as a guideline, i.e., \"I think we could pretty much rip this off exactly as is and just substitute pandas functions directly.\"  There were places where the R example didn't make sense, so I made my own arbitrary choices. \r\n\r\nComments and criticism are welcome. I think it is better to get feedback here rather than anywhere else.\r\n[Pandas Cheat Sheet.pdf](https://github.com/pandas-dev/pandas/files/643329/Pandas.Cheat.Sheet.pdf)\r\n",
      "for the subset section where you have:\r\n\r\n```\r\ndf[['width','length','species']]\r\n  Select multiple columns with specific names.\r\ndf['width'] or df.width\r\n  Select single column with specific name.\r\ndf[[i for i in df.columns if '.' in i]]\r\n  Select columns whose name contains a character string.\r\ndf[[i for i in df.columns if i.endswith(\"Length\")]]\r\n  Select columns whose name ends with a character string.\r\ndf[[i for i in df.columns if re.match('.t.',i)]]\r\n  Select columns whose name matches a regular expression.\r\ndf[[\"x\"+str(i) for i in range(1,6)]\r\n  Select columns named x1, x2, x3, x4, x5.\r\ndf[[i for i in df.columns if i.startswith(\"Length\")]]\r\n  Select columns whose name starts with a character string.\r\ndf.loc[:,\"x2\":\"x4\"]\r\n  Select all columns between x2 and x4 (inclusive).\r\ndf[[i for i in df.columns if i != \"Species\"]]\r\n  Select all columns except Species.\r\ndf.iloc[:,[1,2,5]]\r\n   Select columns in positions 1, 2 and 5\r\n```\r\n\r\nmore idiomatic to do:\r\n```\r\ndf[['width','length','species']]\r\n  Select multiple columns with specific names.\r\ndf['width'] or df.width\r\n  Select single column with specific name.\r\ndf.filter(regex='\\.')\r\n  Select columns whose name contains a character string.\r\ndf.filter(regex='Length$')\r\n  Select columns whose name ends with a character string.\r\ndf.fitler(regex='.t.')\r\n  Select columns whose name matches a regular expression.\r\ndf.filter(regex='x\\d')\r\n  Select columns named x1, x2, x3, x4, x5.\r\ndf.filter(regex='^Length')\r\n  Select columns whose name starts with a character string.\r\ndf.loc[:,\"x2\":\"x4\"]\r\n  Select all columns between x2 and x4 (inclusive).\r\ndf.filter(regex=\"^(?!Species|\\\\.).*\")\r\n  Select all columns except Species.\r\ndf.iloc[:,[1,2,5]]\r\n   Select columns in positions 1, 2 and 5\r\n```\r\n\r\nthough maybe elminate some of these column selections and show usage of .loc instead",
      "@jreback Thanks for the suggestions.  I've made changes in my current working copy.\r\n\r\nSeparate question - R has a cume_dist() method that computes the cumulative distribution of a vector. Similar to rank(pct=True), but different.  Is there a pandas equivalent?\r\n",
      "Here's my latest version.  Only thing left to do is to do groupby() examples in the open space on the second page. Some notes:  (1) Don't have correspondence to R's cume_dist(), so left it out, and added clip() instead.  (2) Setdiff between DataFrames is not there (discussion in #4480) so the example at bottom right of page 2 isn't so pretty.  Maybe someone knows a better way.  \r\n[Pandas Cheat Sheet.pdf](https://github.com/pandas-dev/pandas/files/650423/Pandas.Cheat.Sheet.pdf)\r\n",
      "@Dr-Irv looking good!.\r\n\r\nI would remove use of ``.between`` (this is going to be removed, and is trivially replaced by ``.loc``)\r\n\r\nI would add use of ``.rolling()`` & ``.resample()`` (you said adding groupby). the ``.expanding().apply(any)`` examples is not very typical, would show something like ``.expanding().sum()`` (or .agg(['sum', 'count'])`` or whatever.\r\n\r\nmaybe use .plot somewhere?\r\n\r\nof course its ONLY 2 pages! hahah",
      "Looks really good. Few points:\r\n\r\n- Reshaping Data\r\n  - ``.assign`` example looks incorrect.  It duplicates with ``Make New Variable`` section.\r\n  - Personally feels ``.rename`` is less used than setting ``columns`` directly.\r\n  - Add ``.reindex``?\r\n- Subset Rows\r\n  - Add ``.dropna``?\r\n- Make New Variable\r\n  - ``.drop`` should be moved to ``Subset Variables``?",
      "@jreback regarding your last comments.  I've removed `between()`.  I had it in there because it was in the R cheat sheet.  I've taken out the two `expanding().apply()` examples that computed cumulative all and cumulative any. Those examples were in there because they were in the R version. In terms of adding `expanding()` examples, `expanding().sum()` and `cumsum()` are the same thing, so I added `expanding().median()` instead. \r\n\r\nMy plan was to use remaining space on second page for `groupby()`, and after that is done, I'll look for space for `.rolling()` and `.resample()`.  As for `.plot()`, maybe that's a third page........",
      "@sinhrks I deleted the `.assign` example. Was trying to copy something from R, but the example in \"Make New Variables\" is better.  I added `.reset_index()` in its place, thinking that is used more often than `.reindex`. As for `.rename`, it's the method to use when doing method chaining.  I wish there was a way to change all column names when using method chaining. \r\n\r\nI'm space constrained to add `.dropna` to \"Subset Variables\" and to move `.drop` to \"Subset Variables\". One reason to keep `.drop` in \"Make New Variables\" is because after `.assign`, you often want to drop the original variables.\r\n\r\nLet me get the full first draft done, and we discuss further enhancements. \r\n\r\nThank you all for the comments and feedback!\r\n",
      "Here is a proposed first draft with the 2 pages that correspond pretty well to what was in the R cheat sheet. Based on suggestions above, I added some content that doesn't appear in the R version. Comments and criticism are welcome.\r\n\r\nIf you have ideas of things to add, due to space constraints, please suggest something to be deleted.\r\n\r\n@jreback I have limited availability between 12/16 and 12/20, so I can take comments into account on 12/21, and then hopefully be done with it.  I would like suggestions of where to put this in the source (i.e., which directory). Should the Powerpoint source and PDF both be in there (so others could modify the Powerpoint and create the PDF)? Also, would I then just submit a pull request with those 2 new documents (which would cause the tests to be run, which seems a bit silly since I'm just adding 2 new files that are not touched by the test scripts)?\r\n[Pandas Cheat Sheet.pdf](https://github.com/pandas-dev/pandas/files/655828/Pandas.Cheat.Sheet.pdf)\r\n\r\n",
      "@Dr-Irv looks really good! is ``.resample`` anywhere?\r\n\r\nyes do a PR with the powerpoint & pdf. as well as a small readme (or script) on how to build the pdf.\r\n\r\nI would put in pandas\\docs\\cheatsheet\\",
      "@jreback Here's are my challenges with including `.resample`.  One is space.  The second is that it is specific to datetime-like indexes. Nowhere else in the cheat sheet do I have info about how pandas manages dates and times, so without that context, putting in `.resample` becomes difficult to explain. I think it would also need downsampling and upsampling examples.  So space becomes a bigger issue.\r\n\r\nWith respect to conversion, I have Powerpoint on Windows, and it has an export PDF feature. Scripting would be operating system dependent (and probably not worth the time to figure out). So I will just document how to do it in a text file.\r\n\r\nOne other question - once published, where would links to the cheat sheet exist? On the pandas.pydata.org web site?  Or from within the documentation itself?  If the latter, how do we refer to something outside the documentation tree?\r\n",
      "a README is fine.\r\n\r\nyou can refer to links like: https://github.com/pandas-dev/pandas/blob/master/doc/README.rst (for example), which is a static link to whatever is there.\r\n\r\nwe can add a link on the website as well.\r\n\r\non ``.resample()`` it would be nice to mention as this is one of pandas big strengths (esp compared to R), but I get the space issue.\r\n\r\nis adding a 3rd page *too* much?\r\n\r\n",
      "@jreback Adding a 3rd page is a possibility.  In addition to discussing datetime-like issues, as well as `.resample`, I could add more on `MultiIndex`, and I was also thinking of adding information about input and output for common formats.\r\n\r\nIf I were to do that, should I work on that before submitting the PR?  Or do the PR now for the current version, so it's out there, and then do a new PR once I finish a third page?\r\n\r\n",
      "@Dr-Irv Really cool work! Thanks a lot for this!\r\n\r\nI have some comments, but will save them for later. Just a quick one: I think the `zdf`dataframe on the second page (bottom right corner, \"Set like operations\") is not correct. Shouldn't also have the x2 column instead of x3? (otherwise the example results are not correct).\r\n\r\nRegarding how to include this into pandas: another possibility is that you keep it in a pandas-cheatsheet repo from your own instead of putting it into pandas code base itself. But then of course include the same clear links to it in the pandas documentation as in the other case. \r\nTwo reasons I propose this: first, you keep more the credit and ownership of it. Second, what exactly is included will always be a bit subjective / based on personal taste. And I would like to avoid such discussion (although useful to see feelings about certain functionality, eg I rather use rename instead of assigning columns directly) on the main pandas issue tracker (it's already crowded enough :-)). In your repo you just make the final decision. \r\nBut if you would like to see it included in the pandas codebase (and not only link to it), that's fine for me as well.\r\n\r\n",
      "@Dr-Irv BTW, would it be possible to already post a pptx version as well? I am giving a 3-day pandas course beginning next week, and was thinking to use this to give to the students. But with the pptx version I can make small adaptions (like the incorrect frame) myself, as your new version that you can make on 12/21 will be to late for the course.",
      "@Dr-Irv btw, no problem with including a mention of the author as well!",
      "@jorisvandenbossche I made an error in the definition of the frame 'zdf' in the Cheat Sheet.  That is now fixed, so the examples are correct.  I will include the PPTX in the Pull Request. I'd prefer it be part of the pandas main project, and then others can contribute.  I have revealed my secret identity at the bottom.  :-> \r\n\r\nSorry that I couldn't get things to you prior to today as I was on vacation.\r\n\r\n[Pandas Cheat Sheet.pdf](https://github.com/pandas-dev/pandas/files/666850/Pandas.Cheat.Sheet.pdf)\r\n",
      "looks good @Dr-Irv \r\n\r\nif you want to put this in a PR, add to the 0.19.2 whatsnew (with a link to the location would be great)."
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 6,
    "deletions": 0,
    "changed_files_list": [
      "doc/cheatsheet/Pandas_Cheat_Sheet.pdf",
      "doc/cheatsheet/Pandas_Cheat_Sheet.pptx",
      "doc/cheatsheet/README.txt",
      "doc/source/whatsnew/v0.19.2.txt"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13204,
    "reporter": "toasteez",
    "created_at": "2016-05-17T15:07:21+00:00",
    "closed_at": "2016-07-03T23:28:31+00:00",
    "resolver": "pijucha",
    "resolved_in": "8f8d75d315f5a61bbf2060ff821bff9b282a1582",
    "resolver_commit_num": 2,
    "title": "groupby with index = False returns NANs when column is categorical. ",
    "body": "Please see stackoverflow for example of issue\n\n-doesnt-pandas-allow-a-categorical-column-to-be-used-in-groupby?noredirect=1#comment62084780_37279260\n\n\n",
    "labels": [
      "Bug",
      "Groupby",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "pls post an example & show_versions. SO links are nice, but an in-line example much better.\n",
      "FWIW my example would be something like\n\n```\n>>> pd.__version__\n'0.18.1'\n>>> \n>>> df = pd.DataFrame({\"A\": [1,1,1], \"B\": [2,2,2], \"C\": pd.Categorical([1,2,3])})\n>>> df.groupby([\"A\",\"C\"]).sum().reset_index()\n   A  C  B\n0  1  1  2\n1  1  2  2\n2  1  3  2\n>>> df.groupby([\"A\",\"C\"],as_index=False).sum()\n      A   C   B\nA C            \n1 1 NaN NaN NaN\n  2 NaN NaN NaN\n  3 NaN NaN NaN\n```\n",
      "yeah, this is reindexing I think somewhere inside and is prob not setting it up right. pull-requests welcome.\n",
      "Looks quite easy to fix. Function [`_reindex_output()`](https://github.com/pydata/pandas/blob/master/pandas/core/groupby.py#L3724) doesn't take account of the variable `self.as_index`. \n\nAnother issue in the same function. The [multiindex](https://github.com/pydata/pandas/blob/master/pandas/core/groupby.py#L3724) loses information about dtypes. For example:\n\n``` python\ndf = pd.DataFrame({'cat': pd.Categorical([5,6,6,7,7], [5,6,7,8]),\n                  'i1' : [10, 11, 11, 10, 11],\n                  'i2' : [101,102,102,102,103]})\n\ndf.groupby(['cat', 'i1']).sum().reset_index().dtypes\nOut[12]: \ncat      int64\ni1       int64\ni2     float64\ndtype: object\n```\n\nWhile for a usual one level index:\n\n``` python\ndf.groupby(['cat']).sum().reset_index().dtypes\nOut[13]: \ncat    category\ni1      float64\ni2      float64\ndtype: object\n```\n\nAnd I guess `df.groupby(..., as_index=False).agg(...)` should be consistent with `df.groupby(..., as_index=True).agg(...).reset_index()`.\n\n**Edit:** On second thought, I'd rather leave the index as it is. If a change is needed, it'd better be done in MultiIndex constructor, I suppose.\n\nI'll prepare a PR for it later.\n\n---\n\nBTW, I couldn't find any info whether the following behaviour of categoricals in DataFrame is by design or just a side effect:\n\n``` python\n# df - same as above\ndf.sum()\nOut[14]: \ncat     31.0\ni1      53.0\ni2     510.0\ndtype: float64\n\ndf[['cat']].sum()\nOut[15]: \ncat    31\ndtype: int64\n\n# while for Series:\ndf['cat'].sum()\n...\nTypeError: Categorical cannot perform the operation sum\n```\n\nShouldn't categricals be rather excluded when aggregating as it is with datetime columns?\n"
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "milestoned",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 86,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13212,
    "reporter": "max-sixty",
    "created_at": "2016-05-18T05:13:41+00:00",
    "closed_at": "2016-05-21T14:12:53+00:00",
    "resolver": "max-sixty",
    "resolved_in": "d2b581960168502b1f7dfd73cedfe03ffbf91aee",
    "resolver_commit_num": 11,
    "title": "Empty DataFrame with PeriodIndex doesn't maintain index type on resample",
    "body": "ref: #discussion_r63645752\n\n\n",
    "labels": [
      "Period",
      "Resample",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [],
    "events": [
      "cross-referenced",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "cross-referenced"
    ],
    "changed_files": 8,
    "additions": 168,
    "deletions": 89,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/core/groupby.py",
      "pandas/tests/indexing/test_indexing.py",
      "pandas/tests/test_groupby.py",
      "pandas/tseries/period.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13213,
    "reporter": "fmarczin",
    "created_at": "2016-05-18T12:43:27+00:00",
    "closed_at": "2016-05-19T13:14:46+00:00",
    "resolver": "fmarczin",
    "resolved_in": "eeccd058a5199c3e4fd9900b95e00672f701b3e9",
    "resolver_commit_num": 0,
    "title": "json_normalize() can't deal with non-ascii characters in unicode keys",
    "body": "Example code:\n\n\n\nOutput:\n\n\n\nExpected output\n\n\n\nThe cause are probably\n#L618\nand #L620\n\nThose lines seemingly were introduced to deal with numeric types, but fail when `k` is a Unicode object containing non-ascii characters.\n\nIt seems to be the same bug in principle as \n",
    "labels": [
      "Bug",
      "Unicode"
    ],
    "comments": [],
    "events": [
      "referenced",
      "referenced",
      "cross-referenced",
      "labeled",
      "labeled"
    ],
    "changed_files": 3,
    "additions": 27,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/io/json.py",
      "pandas/io/tests/json/test_json_norm.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13218,
    "reporter": "marcomayer",
    "created_at": "2016-05-18T16:10:15+00:00",
    "closed_at": "2016-12-31T16:56:53+00:00",
    "resolver": "wcwagner",
    "resolved_in": "b2cdc02a9fbbcf0efbfbb6195cc4d2fb69348f5a",
    "resolver_commit_num": 2,
    "title": "loffset doesn't work in resample if used with agg()",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\nI'd expect the same as with mean(), this is how resample worked in the past with resample(how=...).\n\nBut maybe I also misunderstood something about the change. If so please enlighten me.\n\nSince I need this to keep things going, I use the following workaround for now, please let me know if this is the way to go or if there's a more efficient way:\n\n\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: de_DE.UTF-8\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 20.3\nCython: 0.23.4\nnumpy: 1.10.4\nscipy: 0.17.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.2.0\nsphinx: 1.3.5\npatsy: 0.4.0\ndateutil: 2.5.1\npytz: 2016.2\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.5.2\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.8.4\nlxml: 3.6.0\nbs4: 4.4.1\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.39.0\npandas_datareader: 0.2.1\nIn [92]:\n",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "Resample",
      "Effort Low"
    ],
    "comments": [
      "hmm this was fixed for defined functions (e.g. `.count/mean`) in https://github.com/pydata/pandas/pull/12757\n\nprob not tested for aggregations; should be straightforward though.\n\nwant to do a pull-request?\n",
      "I'd love to Jeff but I'm already back on schedule on my current project and will be busy for at least another half day to get all my stuff updated to work with 0.18.x - I'm a heavy resample() and rolling() user :/\n",
      "@marcomayer no hurry :)\n",
      "Okay I've put it on my todo-list ;)\n",
      "gr8!\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 63,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13219,
    "reporter": "PeterKucirek",
    "created_at": "2016-05-18T16:35:22+00:00",
    "closed_at": "2016-06-01T11:13:41+00:00",
    "resolver": "hassanshamim",
    "resolved_in": "fcd73ad2e7482414b61d47056c6c9c220b11702c",
    "resolver_commit_num": 0,
    "title": "Unicode not acceptable input for `usecols` kwarg in `read_csv()`",
    "body": "I caught this bug while updating from version 0.18.0 to 0.18.1. The kwarg `usecols` no longer accepts unicode column labels.\n\nExample below:\n\n\n\nI note that 0.18.1 introduced the requirement that `usecols` be all string or all ints. This makes sense but it looks like the implementation also throws away unicode strings.\n",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "Unicode",
      "Effort Low",
      "CSV"
    ],
    "comments": [
      "makes sense, pull-requests welcome to fix\n\ncc @gfyoung \n",
      "@PeterKucirek: good catch!  PR is definitely in order here.  Shouldn't be too bad to implement.\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 111,
    "deletions": 9,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/io/parsers.py",
      "pandas/io/tests/parser/usecols.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13231,
    "reporter": "chrish42",
    "created_at": "2016-05-19T18:31:16+00:00",
    "closed_at": "2016-06-05T14:08:12+00:00",
    "resolver": "chrish42",
    "resolved_in": "5a9b498e43a41744470732438e9422a407b0b380",
    "resolver_commit_num": 2,
    "title": "Automatic detection of HDF5 dataset identifier fails when data contains categoricals",
    "body": "We use HDF5 to store our pandas dataframes on disk. We only store one dataframe per HDF5, so the feature of pandas.read_hdf() that allows omitting the key when a HDF file contains a single Pandas object is very nice for our workflow.\n\nHowever, said feature doesn't work when the dataframe saved contains one or more categorical columns:\n\n\n\nIt looks like this is because pandas.read_hdf() doesn't ignore the metadata used to store the categorical codes:\n\n\n\nit'd be nice if this feature worked even when some of the columns are categoricals. It should be possible to ignore that metadata that pandas creates when looking if there is only one dataset stored, no?\n",
    "labels": [
      "Bug",
      "IO HDF5",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "Cc @laufere.\n",
      "yeah this detection needs to be a bit smarter to consider the uniques of the top-level groups (rather than just multiple keys). should be a straightforward fix.\n",
      "pull-requests welcome! \n",
      "@jreback While looking in the code, it seems that in such a case the list returned by store.keys() is empty which causes it to produce error.\nHowever as you mention just checking the keys will not be a good approach. \n\n`store.groups()`\n\n`[/data (Group) ''`\n  `children := ['table' (Table), 'meta' (Group)], /data/meta/values_block_1/meta (Group) ''`\n`children := ['table' (Table)]]\n`\n\nRunning the above code yields the results as shown above. Now if we compare this with a HDF5 file which actually has two datasets we get something like this:\n\n> > > import pandas as pd\n> > > df = pd.DataFrame({'col1': [11, 21, 31], 'col2': ['a', 'b', 'a']})\n> > > df2 = pd.DataFrame({'col1': [11, 21, 31], 'col2': ['a', 'b', 'acc']})\n> > > df.to_hdf('no_cat.hdf5', 'data', format='table')\n> > > df2.to_hdf('no_cat.hdf5', 'data2', format='table')\n> > > df3 = pd.read_hdf('no_cat.hdf5')\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File \"pandas/io/pytables.py\", line 336, in read_hdf\n> > >     raise ValueError('key must be provided when HDF file contains '\n> > > ValueError: key must be provided when HDF file contains multiple datasets.\n> > > pd.HDFStore('no_cat.hdf5').groups()\n> > > [/data (Group) ''\n> > >   children := ['table' (Table)], /data2 (Group) ''\n> > >   children := ['table' (Table)]]\n\nAs we can see, if there are actually two datasets, instead of meta(Group) , we get /data2(Group) where data2 is the key provided when writing to the file. \nMaybe we can leverage this?\n",
      "@jreback One more thing comes to mind along the lines of what you suggested.\n\n> > > pd.HDFStore('cat.hdf5').keys()\n> > > ['/data', '/data/meta/values_block_1/meta']\n\nNow we can use these keys to get the unique values\n\n> > > df = pd.HDFStore('cat.hdf5').select('/data')\n> > > np.unique(df)\n> > > array([11, 21, 31, 'a', 'b'], dtype=object)\n> > > \n> > > df = pd.HDFStore('cat.hdf5').select('/data/meta/values_block_1/meta')\n> > > np.unique(df)\n> > > array(['a', 'b'], dtype=object)\n\nWe can see, that the uniques produced by giving /data/meta/values_block_1/meta as a key is a subset of when we provide /data. But if we go down this road we will also have to consider the key name while making a decision because it might happen that there are two dataframes in the hdf5 file where the uniques of one is a subset of other.\n\nAm I missing something here?\n",
      "@chrish42 you can just iterate over the groups with tables. look at how `.keys()` is implemented.\n",
      "@jreback did you want to refer to me or to chrish42 only?\n",
      "oh sorry meant that as a general comment \n",
      "@jreback Using the approach you suggested, I can get key names and then use them to get the individual tables as well, but my question, like I asked in a comment before, is that using those tables, even if we get unique values from both the tables, how can we be certain that one of them has meta information just because the unique values of one of the tables is a subset of another?\n",
      "@jreback any comments?Can you guide me in the right direction?\n",
      "@pfrcks just look at the top-level groups.\n",
      "@jreback I'm working on this during (alone so far) the PyCon sprints. So far, I've set up a development environment and added a test that fails. I have a couple questions. \n\nFirst, should the metadata (like the categories, etc.) be hidden from the user by HDFStore or not? (i.e. the keys(), groups(), etc. method don't show the metadata table.) \n\nAnd second, is there way to know from the attributes or other that a table is a metadata table? What would be the best way to do this? I see that the HDFStore.groups() method already does a bunch of filtering out. Not sure what is the best way to do this for categorical metadata...\n",
      "we don't currently hide the metadata from the main display. Its prob ok to hide it (though do that after).\n\nwhen you are iterating over groups, you can tell if there is meta data by seeing if you are on a `table` node that also has `meta`\n",
      "Let me know what you think of that pull request. Should I open a separate bug to hide the meta data?\n\nAlso, while readings the tests for pytables IO, I noticed an (old?) `var = something_truthy and value1 or value2`, that I replaced by an if-expression. But if the required version of pytables is now >= 2.2, that line could simply go away.\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "cross-referenced",
      "commented"
    ],
    "changed_files": 3,
    "additions": 52,
    "deletions": 7,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/io/pytables.py",
      "pandas/io/tests/test_pytables.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13232,
    "reporter": "sinhrks",
    "created_at": "2016-05-19T20:54:42+00:00",
    "closed_at": "2017-03-04T21:15:37+00:00",
    "resolver": "max-sixty",
    "resolved_in": "ca6d88b7367de415770bf2c171887c5bece38d9f",
    "resolver_commit_num": 12,
    "title": "PeriodIndex with float input inconsistency",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\nConsistently raise or coerce to `Period`\n#### output of `pd.show_versions()`\n\n0.18.1\n\nCC: @MaximilianR\n",
    "labels": [
      "Bug",
      "API Design",
      "Period"
    ],
    "comments": [
      "ref https://github.com/pydata/pandas/pull/13079, which should fix\n",
      "@sinhrks @jreback closed by #13079 \n",
      "@MaximilianR if you think we need to add a note (and/or include these issues) in the whatsnew, pls do a PR. I don't care about the issues per-se being referenced by GH does that. But if you are a user and want to know (and the existing notes don't cover), then pls add.\n",
      "@jreback this seems like pretty deep in the internals - shall I still add one?\n",
      "nah I looked again, its fine. thxs!\n\nPeriods really coming along!\n",
      "@MaximilianR #13079 only checks `_shallow_copy` with normal float input right?\n\non current master (d2b581960168502b1f7dfd73cedfe03ffbf91aee):\n\n```\npd.PeriodIndex(np.array([1.1, np.nan, np.inf]), freq='M')\n# PeriodIndex(['1970-02', 'NaT', 'NaT'], dtype='int64', freq='M')\n\npd.PeriodIndex([1.1, np.nan, np.inf], freq='M')\n# ValueError: Value must be Period, string, integer, or datetime\n```\n\nreopens.\n",
      "```\nIn [2]: pd.PeriodIndex(np.array([1.1]), freq='M')\nOut[2]: PeriodIndex(['1970-02'], dtype='int64', freq='M')\n```\n\nactually the original [2] should be an error as well (and `ValueError` is correct here)\n",
      "Apologies. Will PR\n",
      "Why a `ValueError` rather than `TypeError`?\n",
      "oh these could be `TypeError`, but we mostly raise `ValueError` now\n",
      "Looking through this, I think this is a pretty weird code path - only gets hit with ints that can represent Periods, which I think are just years (e.g. '2000'). Not ints representing the underlying values... https://github.com/pydata/pandas/blob/master/pandas/tseries/period.py#L229\n",
      "hmm, yeah that does seem weird; I prob would just try to `astype('i8')` if it fails take that path.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "milestoned",
      "closed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "reopened",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 8,
    "additions": 98,
    "deletions": 86,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/algorithms.py",
      "pandas/indexes/base.py",
      "pandas/io/packers.py",
      "pandas/tests/indexes/period/test_construction.py",
      "pandas/tests/indexes/period/test_period.py",
      "pandas/tseries/period.py",
      "setup.cfg"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13235,
    "reporter": "roycoding",
    "created_at": "2016-05-20T07:41:17+00:00",
    "closed_at": "2016-05-25T12:14:52+00:00",
    "resolver": "roycoding",
    "resolved_in": "87492737f2f81183700849d453c38d507f128811",
    "resolver_commit_num": 0,
    "title": "label keyword argument for resample causes an error when used with a groupby object",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### The dataframe\n\n\n#### Expected Output\n\n\n#### Actual ouput\n\n\n\nWithout the `label` keyword, I get this (with expected right labeled dates):\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Bug",
      "Resample",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "So the `**kwargs` should not be passed [here](https://github.com/pydata/pandas/blob/master/pandas/tseries/resample.py#L913)\n\nsimple fix. These args are all eaten by the `TimeGrouper` except those explicity named.\n\nwant to do a PR?\n",
      "> want to do a PR?\n\nI'll take a shot at it. Thanks.\n"
    ],
    "events": [
      "renamed",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 27,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13247,
    "reporter": "jennolsen84",
    "created_at": "2016-05-21T05:34:17+00:00",
    "closed_at": "2017-03-14T13:33:58+00:00",
    "resolver": "jaehoonhwang",
    "resolved_in": "7d34d4d5c2d2c6c68b4124076571cfab9c3b4aee",
    "resolver_commit_num": 0,
    "title": "BUG: upcasting on reshaping ops",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Current Output\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Bug",
      "Reshaping",
      "Dtypes",
      "Missing-data",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "Thanks for the report. Though `Panel` is being replaced with `xarray` near future, sending a PR is appreciated.\n",
      "@sinhrks same behavior with `DataFrame`, is that going away too?\n",
      "```\nIn [7]: pn1 = pd.DataFrame(np.array([1.0], dtype=np.float32, ndmin=2)) # df of 1.0\n\nIn [8]: pn2 = pd.DataFrame(np.array([np.nan], dtype=np.float32, ndmin=2)) # df of nan\n\nIn [9]: print(pd.concat([pn2, pn2]).values.dtype)\nfloat64\n\nIn [10]: print(pd.concat([pn1, pn1]).values.dtype)\nfloat32\n```\n",
      "yeah, I suspect the null forces immediate upcast to `float64`. This should be cognizant that floats are available in their native itemsize even w/NaN's (and so should take a size that can deal).\n\nI don't think this is very hard to fix in a general way.\n\ncare to take a stab?\n",
      "@jreback how's this for starters?  https://github.com/jennolsen84/pandas/commit/3b3797af12dcfa8608a904df8d65454e6904d0ce .  I can add the tests, whatsnew, etc. if it looks good to you.\n\nI did some tests and it seems to work for me for float types.  I did a quick check with ints, and that seemed to work just fine as well (I am guessing it just uses np rules there).\n\nI also checked concating int32 + float16, and in that case we use float64.  In that case, one might want a float64, so I left that alone.\n",
      "@jennolsen84 some comments. Ideally make this as general as possible, but the type stuff is a bit all over the place now #13147 should fix this a bit more (though that independent of this)\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "referenced",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files": 6,
    "additions": 35,
    "deletions": 6,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/internals.py",
      "pandas/tests/indexing/test_partial.py",
      "pandas/tests/test_internals.py",
      "pandas/tests/test_reshape.py",
      "pandas/tests/tools/test_concat.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13251,
    "reporter": "jreback",
    "created_at": "2016-05-21T14:16:25+00:00",
    "closed_at": "2016-05-22T00:05:22+00:00",
    "resolver": "sinhrks",
    "resolved_in": "b88eb35ad98ac7a99451b505acc74e5d0e3a81b1",
    "resolver_commit_num": 316,
    "title": "TST: assert that IncompatibleFrequency is raise (rather than ValueError)",
    "body": "#issuecomment-220780044\n\njust need to make sure that we are raising IncompatibleFrequency as appropriate (and testing explicity for this rather than ValueError) when constructing Periods.\n",
    "labels": [
      "Testing",
      "Period"
    ],
    "comments": [
      "#13250  prob convers most/all of this.\n",
      "Yes, covered in #13250.\n",
      "thanks\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "commented",
      "commented"
    ],
    "changed_files": 4,
    "additions": 276,
    "deletions": 184,
    "changed_files_list": [
      "pandas/src/period.pyx",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_resample.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13256,
    "reporter": "wildwild",
    "created_at": "2016-05-23T07:52:07+00:00",
    "closed_at": "2016-05-23T12:17:12+00:00",
    "resolver": "jreback",
    "resolved_in": "83436af8ae1ccad49b7ceac7471c060d823d10ab",
    "resolver_commit_num": 4496,
    "title": "64bit read_csv with dtype problem",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n\n\n  File \"pandas\\parser.pyx\", line 805, in pandas.parser.TextReader.read (pandas\\parser.c:8620)\n  File \"pandas\\parser.pyx\", line 827, in pandas.parser.TextReader._read_low_memory (pandas\\parser.c:8876)\n  File \"pandas\\parser.pyx\", line 904, in pandas.parser.TextReader._read_rows (pandas\\parser.c:9893)\n  File \"pandas\\parser.pyx\", line 1011, in pandas.parser.TextReader._convert_column_data (pandas\\parser.c:11286)\n  File \"pandas\\parser.pyx\", line 1092, in pandas.parser.TextReader._convert_tokens (pandas\\parser.c:12659)\nValueError: cannot safely convert passed user dtype of <f8 for object dtyped data in column 0\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 18.5\nCython: 0.23.4\nnumpy: 1.11.0\nscipy: 0.16.0\nstatsmodels: None\nxarray: None\nIPython: 4.0.1\nsphinx: 1.3.1\npatsy: 0.4.0\ndateutil: 2.5.3\npytz: 2015.7\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.5.2\nmatplotlib: 1.5.0\nopenpyxl: 2.2.6\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.7.7\nlxml: 3.4.4\nbs4: 4.4.1\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.9\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.38.0\npandas_datareader: None\n",
    "labels": [],
    "comments": [
      "this was already closed in #13237 is there a reason you opened again?\n"
    ],
    "events": [],
    "changed_files": 11,
    "additions": 187,
    "deletions": 68,
    "changed_files_list": [
      "doc/source/whatsnew/v0.21.0.txt",
      "pandas/core/base.py",
      "pandas/core/categorical.py",
      "pandas/core/indexes/base.py",
      "pandas/core/indexes/category.py",
      "pandas/core/series.py",
      "pandas/core/sparse/array.py",
      "pandas/tests/frame/test_api.py",
      "pandas/tests/frame/test_convert_to.py",
      "pandas/tests/series/test_io.py",
      "pandas/tests/test_base.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13260,
    "reporter": "rladeira",
    "created_at": "2016-05-23T18:37:22+00:00",
    "closed_at": "2016-05-26T18:15:44+00:00",
    "resolver": "pfrcks",
    "resolved_in": "5d6772074a89c1ed7a5c24b078215cb7f9cc6eb3",
    "resolver_commit_num": 0,
    "title": "DOC: Strange behavior when combining astype and loc",
    "body": "I am trying to use `astype` together with `loc` to convert just a subset of columns to a specified dtype. However, I can't understand what is actually happening. Is this the expected behavior when combining \n`astype` and `loc`?\n#### Code Sample, a copy-pastable example if possible\n\n\n\nOutput:\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.6.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.2.0-36-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.0\nnose: None\npip: 8.1.1\nsetuptools: 20.6.7\nCython: None\nnumpy: 1.11.0\nscipy: 0.17.0\nstatsmodels: None\nxarray: None\nIPython: 4.1.2\nsphinx: None\npatsy: None\ndateutil: 2.5.2\npytz: 2016.3\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\n",
    "labels": [],
    "comments": [
      "This is as expected; use getitem if you want to re-assign w/o masking. In this case your mask is ':' (meaning the entire columns), but upcasting is done when used in this way (and that's why its upcast to `int64`).\n\nIOW, `.loc` will try to fit in what you are assigning to the current dtypes, while `[]` will overwrite them taking the dtype from the rhs. subtle point.\n\n```\nIn [1]: df = pd.DataFrame({'a': [1,2,3], 'b': [4,5,6], 'c': [7, 8, 9]})\n\nIn [2]: df\nOut[2]: \n   a  b  c\n0  1  4  7\n1  2  5  8\n2  3  6  9\n\nIn [3]: df[['a','b']] = df[['a','b']].astype(np.uint8)\n\nIn [4]: df\nOut[4]: \n   a  b  c\n0  1  4  7\n1  2  5  8\n2  3  6  9\n\nIn [5]: df.dtypes\nOut[5]: \na    uint8\nb    uint8\nc    int64\ndtype: object\n```\n",
      "This is a bit related to the docs we added here: https://github.com/pydata/pandas/pull/13070\n\nIf you'd like to propose an example for this section would take it: http://pandas-docs.github.io/pandas-docs-travis/basics.html#astype (e.g. showing pitfalls, e.g. this example)\n",
      "Subtle point, indeed. Thanks for the clarification.\n",
      "let me reopen as a doc-issue.\n"
    ],
    "events": [
      "commented",
      "closed",
      "commented",
      "commented",
      "commented",
      "reopened",
      "milestoned",
      "renamed",
      "cross-referenced"
    ],
    "changed_files": 1,
    "additions": 22,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/basics.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13282,
    "reporter": "nparley",
    "created_at": "2016-05-25T10:03:45+00:00",
    "closed_at": "2016-05-25T17:35:10+00:00",
    "resolver": "nparley",
    "resolved_in": "b4e2d34edcbc404f6c90f76b67bcc5fe26f0945f",
    "resolver_commit_num": 0,
    "title": "pandas.show_versions causing malloc_error_break",
    "body": "If blosc is installed show_versions() will caused python to produce a malloc_error_break (double free) error on exiting. This can be seen in the travis runs under `source activate pandas && ci/print_versions.py` section but can also be easily replicated on Linux and Mac. The fix for this is to replace imp (which has been deprecated) with importlib. I will create and link a PR.\n\n\n\n>>> \npython(1726,0x7fff77074000) malloc: *** error for object 0x102573a00: pointer being freed was not allocated\n*** set a breakpoint in malloc_error_break to debug\nAbort trap: 6\n\n\n\n``````\n",
    "labels": [],
    "comments": [
      "excellent @nparley \n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files": 1,
    "additions": 2,
    "deletions": 6,
    "changed_files_list": [
      "pandas/util/print_versions.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13288,
    "reporter": "pijucha",
    "created_at": "2016-05-25T22:17:16+00:00",
    "closed_at": "2016-05-31T14:12:51+00:00",
    "resolver": "pijucha",
    "resolved_in": "132c1c55c3d7884e149dd8f99655f1d2c720696c",
    "resolver_commit_num": 1,
    "title": "BUG: DataFrame.describe() breaks with a column index of object type and numeric entries",
    "body": "Preparing a commit for another issue in `.describe()`, I encountered this puzzling bug, surprisingly easy to trigger.\n#### Symptoms\n\n\n\nHowever:\n\n\n\nSame issue happens with a simpler data frame:\n\n\n\nCurrent version (but the bug is also present in pandas release 0.18.1):\n\n\n#### Reason\n\nSome internal function gets confused by dtypes of a column index, I guess. But the faulty index is created in `.describe()`.\n\n\n\n`_shallow_copy()` in the marked line changes `d.columns`:\n\n\n#### Possible solutions\n\nLines 4957-4958 are actually used to fix issues that `pd.concat` brings about. They try to pass the column structure from `self` to `d`.\nI think a simpler solution is replacing these lines with:\n\n\n\nor\n\n\n\n`data` is a subframe of `self` and retains the same column structure.\n\n`pd.concat` has some parameters that help pass a hierarchical index but can't do anything on its own with a categorical one.\n\nI'm going to submit a pull request with this fix together with some others related with `describe()`. I hope I haven't overlooked anything obvious. But if so, any comments are very welcome.\n",
    "labels": [],
    "comments": [
      "simple enough to stringify column names.\n",
      "https://github.com/pydata/pandas/blob/master/pandas/core/generic.py#L4957\n\nshould be\n\n```\nd.columns = Index(d.columns, dtype='object')\n```\n\nSeparately, this violates our guarantees on Index creation. I think we should assert that the dtype of a create `Index` is `object` if its not a sub-class.\n\n@sinhrks \n\n```\nIn [2]: i = Index([0,'A'])\n\nIn [3]: i._shallow_copy([0])\nOut[3]: Index([0], dtype='int64')\n```\n",
      "Not sure if I understand. Don't we want `d.columns` to be of the same type as `self.columns`?\n",
      "The trouble is the columns are split up by dtype, so the sub-indexes need to be constructed similarly. `._shallow_copy` is an internal (to Index) method and should not be used here.\n\nactually you don't even need to specify the dtype, I think\n\n``d.columns = d.columns.copy()` will prob work here. The problem is `.values` converts to a base form which may change things (e.g. try this with a datetime for the columns and see what happens).\n",
      "Oh, you mean \n\n``` python\nd.columns = data.columns.copy()    #(1)\n```\n\nand earlier \n\n``` python\nd.columns = Index(data.columns, dtype='object').  #(2) \n```\n\n(1) does work. Or at least it passes tests from the repository plus some others I tried. (Actually, I ran nosetests with `d.columns = data.columns` but it shouldn't make a difference I guess.)\n\nOn the other hand, (2) fails with `datetime64[ns]` in columns. When I specify `dtype=data.columns.dtype`, it breaks with localized datetime.\n\nMy understanding is that since `data = self.loc[bool_arr]`, then `data.columns` is just a subset of the original column index and preserves its structure (including dtype and dtypes of its elements). So, why not just simply pass/copy it to d. \n\nAnother advantage of (1) is that we can skip the next line\n\n```\nd.columns.names = data.columns.names\n```\n",
      "yeah (2) is not what we want, we don't need to coerce. so use (1). This was using some internal code which it shouldn't have. (Index has a public API and we try to use whenever possible, except when deeply needed). The reason is that there are certain guarantees, which in this case were violated (the separate issue I opened).\n",
      "@jreback Thanks for clarifying.\n"
    ],
    "events": [
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "renamed",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 212,
    "deletions": 22,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/core/generic.py",
      "pandas/formats/format.py",
      "pandas/tests/formats/test_format.py",
      "pandas/tests/test_generic.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13303,
    "reporter": "tumalow-will",
    "created_at": "2016-05-27T03:40:45+00:00",
    "closed_at": "2017-03-20T17:51:28+00:00",
    "resolver": "mroeschke",
    "resolved_in": "771e36c32f922c6a0c4a147f08fef32a011d534f",
    "resolver_commit_num": 37,
    "title": "Timezone aware Timestamp.dayofyear does not report correct day ",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n\r\n#### Expected Output\r\n\r\nI expect the dayofyear to agree with tm_yday, but they do not.\r\n\r\n#### output of `pd.show_versions()`\r\n\r\n<details>\r\n## INSTALLED VERSIONS\r\n\r\ncommit: None\r\npython: 2.7.10.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 8\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 61 Stepping 4, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\n\r\npandas: 0.16.2\r\nnose: 1.3.7\r\nCython: 0.22.1\r\nnumpy: 1.9.2\r\nscipy: 0.15.1\r\nstatsmodels: 0.6.1\r\nIPython: 3.2.0\r\nsphinx: 1.3.1\r\npatsy: 0.3.0\r\ndateutil: 2.4.2\r\npytz: 2015.4\r\nbottleneck: 1.0.0\r\ntables: 3.2.0\r\nnumexpr: 2.4.3\r\nmatplotlib: 1.4.3\r\nopenpyxl: 1.8.5\r\nxlrd: 0.9.3\r\nxlwt: 1.0.0\r\nxlsxwriter: 0.7.3\r\nlxml: 3.4.4\r\nbs4: 4.3.2\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: 1.0.5\r\npymysql: None\r\npsycopg2: None\r\n</details>",
    "labels": [],
    "comments": [
      "So this is working properly for `DatetimeIndexes`; which converts to 'local timestamps' before computing the field accessors. However for `Timestamps` it is NOT converting and doing this in UTC (which is what things are stored). \n\nPretty straightforward to fix if you'd like to do a PR. Relevant code is `panda/tslib.pyx/Timestamp._get_field`\n\n```\nIn [37]: pd.to_datetime(['2015-1-01 23:59:00']).tz_localize('US/Eastern')\nOut[37]: DatetimeIndex(['2015-01-01 23:59:00-05:00'], dtype='datetime64[ns, US/Eastern]', freq=None)\n\nIn [38]: pd.to_datetime(['2015-1-01 23:59:00']).tz_localize('US/Eastern').dayofyear\nOut[38]: array([1], dtype=int32)\n\nIn [40]: pd.to_datetime(['2015-1-01 23:59:00']).tz_localize('US/Eastern').tz_convert('UTC')\nOut[40]: DatetimeIndex(['2015-01-02 04:59:00+00:00'], dtype='datetime64[ns, UTC]', freq=None)\n\nIn [41]: pd.to_datetime(['2015-1-01 23:59:00']).tz_localize('US/Eastern').tz_convert('UTC').dayofyear\nOut[41]: array([2], dtype=int32)\n```\n\nBut not working for Timestamps\n\n```\nIn [43]: ts\nOut[43]: Timestamp('2015-01-01 23:59:00-0500', tz='US/Eastern')\n\nIn [44]: ts.dayofyear\nOut[44]: 2\n```\n"
    ],
    "events": [
      "commented",
      "milestoned",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 117,
    "deletions": 78,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/_libs/tslib.pyx",
      "pandas/tests/indexes/datetimes/test_misc.py",
      "pandas/tests/scalar/test_timestamp.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13304,
    "reporter": "ilrico",
    "created_at": "2016-05-27T13:32:58+00:00",
    "closed_at": "2016-05-31T14:31:03+00:00",
    "resolver": "gfyoung",
    "resolved_in": "d1916404bc604ebb319710740b5dcc7eabd4fc89",
    "resolver_commit_num": 27,
    "title": "order not preserved when constructing DataFrame from a list of OrderedDicts",
    "body": "issue is explained here:\n-dataframe-construction-from-a-list-of-ordereddict-preserving-columns-ord?noredirect=1#comment62465925_37484738\n\nworkaround with constructor option \"columns\" works but not elegant.\n",
    "labels": [],
    "comments": [
      "pls post a copy-pastable reproducible example at the top of the PR.\n\nyes this could be an enhancement. pull-requests are welcome.\n",
      "Here's one:\n\n``` python\n>>> from pandas.compat import OrderedDict\n>>> from pandas import DataFrame\n>>>\n>>> data = OrderedDict()\n>>> data['c'] = 2\n>>> data['b'] = 1\n>>>\n>>> data\nOrderedDict([('c', 2), ('b', 1)])\n>>>\n>>> DataFrame([data])\n   b  c\n0  1  2\n```\n",
      "I should point out though that if you pass in the `OrderedDict` with _arrays_ as the values, the order is in fact preserved!\n\n``` python\n>>> from pandas.compat import OrderedDict\n>>> from pandas import DataFrame\n>>>\n>>> data = OrderedDict()\n>>> data['c'] = [2]\n>>> data['b'] = [1]\n>>>\n>>> data\nOrderedDict([('c', [2]), ('b', [1])])\n>>>\n>>> DataFrame(data)\n   c  b\n0  2  1\n```\n",
      "The \"bug\" traces back to the initialization of arrays from the list of dictionaries.  The initialization sorts the keys at the end, so we just need a boolean to tell it not to sort when we have `OrderedDict`.  Can take this one home.  A welcomed change of scenery from the land of `read_csv`! :smile:\n"
    ],
    "events": [
      "commented",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 81,
    "deletions": 7,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/core/frame.py",
      "pandas/lib.pyx",
      "pandas/tests/frame/test_constructors.py",
      "pandas/tests/test_lib.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13306,
    "reporter": "dlpd",
    "created_at": "2016-05-27T22:38:17+00:00",
    "closed_at": "2016-06-02T18:00:21+00:00",
    "resolver": "uwedeportivo",
    "resolved_in": "ce56542d1226adf8b3439c51f0c34b49dd53bb28",
    "resolver_commit_num": 0,
    "title": "Hour overflow in tz-aware datetime conversions",
    "body": "Comparison of tz-aware timestamps fails across DST boundaries. The comment in tslib.pyx:3845\n\n\n\nperhaps implies this is a known problem that was never resolved, so apologies if a new issue is not appropriate.\n#### Self contained example\n\n\n#### Output\n\nExpected output:\n\n\n\nActual output of timedelta computation:\n\n\n\nComputed timedelta for rows after the 2008-12-12 date are off by an hour.\n\nOutput of tz_convert:\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [],
    "comments": [
      "I get exactly the output you have above. What exactly is the issue?\n",
      "Sorry for being unclear, the output is wrong - these timestamps are not an hour apart. The output above is the actual (buggy) output, not what it should be.\n",
      "maybe the example is not clear. Can you show something and have an expected which is obvious. \n",
      "The expected output is the correct timedelta - updated to reflect that.\n",
      "```\nIn [36]: df = DataFrame(\n{'A' : pd.to_datetime(['2008-05-12 13:50:33',\n                       '2008-12-12 14:50:35',\n                       '2008-05-12 13:50:32']).tz_localize('US/Eastern'), \n'B' : Timestamp('2008-05-12 13:50:33',tz='US/Eastern')}\n)\n\nIn [37]: df\nOut[37]: \n                          A                         B\n0 2008-05-12 13:50:33-04:00 2008-05-12 13:50:33-04:00\n1 2008-12-12 14:50:35-05:00 2008-05-12 13:50:33-04:00\n2 2008-05-12 13:50:32-04:00 2008-05-12 13:50:33-04:00\n\nIn [39]: df.dtypes\nOut[39]: \nA    datetime64[ns, US/Eastern]\nB    datetime64[ns, US/Eastern]\ndtype: object\n\nIn [40]: df.B-df.A\nOut[40]: \n0       0 days 00:00:00\n1   -215 days +22:59:58\n2       0 days 01:00:01\nName: B, dtype: timedelta64[ns]\n```\n",
      "So we are not converting to UTC somewhere. Should be a straightforward fix. want to do a pull-request?\n\n```\nIn [41]: df2 = df.astype('M8[ns]')\n\nIn [42]: df2\nOut[42]: \n                    A                   B\n0 2008-05-12 17:50:33 2008-05-12 17:50:33\n1 2008-12-12 19:50:35 2008-05-12 17:50:33\n2 2008-05-12 17:50:32 2008-05-12 17:50:33\n\n# this also seems to have lost the name of the Series (B), oddly.\nIn [43]: df2.B-df2.A\nOut[43]: \n0       0 days 00:00:00\n1   -215 days +21:59:58\n2       0 days 00:00:01\ndtype: timedelta64[ns]\n```\n",
      "Please look at the second half of my comment. The (perhaps known?) bug is in tz_convert() itself, whose implementation assumes the array is sorted.\n",
      "@dlpd you are reaching into the implementation. So not really sure what you are doing. You would have to demonstrate a bug using the public API which you did for the case above but not for anything else.\n",
      "@jreback Thanks for the comments. You wrote:\n\n> So we are not converting to UTC somewhere\n\nMy point is that the bug is in the tz-conversion itself, not lack thereof.\n\nIf pandas.tslib.tz_convert() is not considered public, I'm happy to reproduce with DatetimeIndex.tz_convert() which is. If you trace the execution of the timedelta computation, you will see that these two examples are actually the same.\n",
      "well if you'd like to submit a pull request would be great\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 99,
    "deletions": 20,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/tseries/tests/test_timezones.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13322,
    "reporter": "michaelaye",
    "created_at": "2016-05-30T00:03:29+00:00",
    "closed_at": "2016-07-29T00:20:26+00:00",
    "resolver": "shawnheide",
    "resolved_in": "e9087339cd6ee67418a54e11f2a88df4e8c0f690",
    "resolver_commit_num": 4,
    "title": "categories in HDFStore don't filter correctly",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n### Actual Output:\n\n\n#### Expected Output\n\nEmpty DataFrame for both cases.\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 14.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.13.1+5155.g9baf452\nnose: None\npip: 8.1.1\nsetuptools: 21.2.1\nCython: 0.24\nnumpy: 1.11.0\nscipy: 0.17.1\nstatsmodels: None\nxarray: 0.7.2\nIPython: 4.2.0\nsphinx: 1.4.1\npatsy: None\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: None\ntables: 3.2.2\nnumexpr: 2.5.2\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: 4.4.1\nhtml5lib: 0.9999999\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\npandas_datareader: None\n",
    "labels": [],
    "comments": [
      "ok the issue is here:\n\n```\n> /Users/jreback/pandas/pandas/computation/pytables.py(198)convert_value()\n-> result = metadata.searchsorted(v, side='left')\n(Pdb) p metadata\narray(['ESP_012345_6789', 'ESP_987654_3210'], dtype=object)\n(Pdb) l\n193             elif kind == u('timedelta64') or kind == u('timedelta'):\n194                 v = _coerce_scalar_to_timedelta_type(v, unit='s').value\n195                 return TermValue(int(v), v, kind)\n196             elif meta == u('category'):\n197                 metadata = com._values_from_object(self.metadata)\n198  ->             result = metadata.searchsorted(v, side='left')\n199                 return TermValue(result, result, u('integer'))\n200             elif kind == u('integer'):\n201                 v = int(float(v))\n202                 return TermValue(v, v, kind)\n203             elif kind == u('float'):\n(Pdb) n\n> /Users/jreback/pandas/pandas/computation/pytables.py(199)convert_value()\n-> return TermValue(result, result, u('integer'))\n(Pdb) p result\n0\n(Pdb) p metadata\narray(['ESP_012345_6789', 'ESP_987654_3210'], dtype=object)\n(Pdb) p metadata.searchsorted(v, side='left')\n0\n```\n\nWe are searching for 'B' in the categories. And we get 0 when searched from the left, which actually means its not there (or IS the first element).\n\nSo you need to check if it IS the first element\n\n```\n(Pdb) p metadata.searchsorted('ESP_012345_6789', side='left')\n0\n```\n\nIf not, then prob easiest to just return -1 (which is not a valid code, rather than 0 which IS)\n\nwant to do a PR?\n",
      "Sorry, not this time, I'm teaching for the first time this month.\n",
      "np thanks for the report\n"
    ],
    "events": [
      "commented",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 36,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/computation/pytables.py",
      "pandas/io/tests/test_pytables.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13335,
    "reporter": "RogerThomas",
    "created_at": "2016-05-31T16:41:01+00:00",
    "closed_at": "2016-06-15T01:48:21+00:00",
    "resolver": "chris-b1",
    "resolved_in": "f98b4b541e7a9d1b0d8f6674a84dc10080c2568b",
    "resolver_commit_num": 35,
    "title": "Pandas groupby extremely slow in python3 for certain sets of single precision floating point data",
    "body": "In python3 with certain sets of single precision floating point data pandas groupby is up to ~150 slower than the same data in python2\n#### Code Sample, a copy-pastable example if possible\n\n\n\nOn my machine\npython2 this_file.py\nThe groupby takes around 0.1s\n\nWhere as python3 this_file.py\nThe groupby takes around 11s\n\nWith some investigation the discrepancy in run time between python versions varies hugely between the actual data but seems to have the biggest difference when half the data is roughly 100 times smaller than the other half.\n\nHaving profiled this, it seems this function is taking almost all the 11s in the python3 version in this method\n#L538\n\nHowever I have no idea what is causing the run time discrepancy.\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.4.0-22-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_IE.UTF-8\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 21.2.2\nCython: 0.23.4\nnumpy: 1.11.0\nscipy: 0.16.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.2.0\nsphinx: 1.3.1\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: None\ntables: 3.2.0\nnumexpr: 2.5.2\nmatplotlib: 1.5.0\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n 0.9.2\napiclient: None\nsqlalchemy: 1.0.9\npymysql: 0.7.4.None\npsycopg2: None\njinja2: 2.8\nboto: 2.38.0\npandas_datareader: None\n",
    "labels": [
      "Dtypes",
      "Numeric",
      "Performance"
    ],
    "comments": [
      "is the same issue I think as here: https://github.com/pydata/pandas/issues/13166\n\nfrom @pitrou \n\n```\nPyHash_double is doing this so that hash(float(x)) == hash(x) for every integer x that's exactly representable as a double\nActually, as the comment suggests, it also ensures that the invariant holds if x is a Fraction\n >>> x = Fraction(5, 4)\n>>> hash(x)\n576460752303423489\n>>> hash(float(x))\n576460752303423489\n```\n\nSo I think we could simply change as suggested in #13166 and see. Probably seeing LOTS of hash collisions.\n",
      "I suspect the upcasting from float32 -> float64 might contribute to this as well.\n",
      "@jreback i've been looking into this, but I'm not getting very far, any advice on why we're seeing this issue in python3 and not python2, since both are being compiled into c code I don't see how the python version can have an effect\n",
      "see the issue I referenced. I think you can fix it by changing the hashing code as indicated.\n\nI think that python 3 is taking great care with the hashing, somewhat to the detriment of perf. (which is what the `Float64Hashtable` ultimately does via `kh_*` functions). The adjustment makes it much faster (and for all intents and purposes works the same).\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 4,
    "additions": 66,
    "deletions": 106,
    "changed_files_list": [
      "asv_bench/benchmarks/groupby.py",
      "asv_bench/benchmarks/indexing.py",
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/src/klib/khash_python.h"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13338,
    "reporter": "jreback",
    "created_at": "2016-05-31T21:46:16+00:00",
    "closed_at": "2016-05-31T23:18:16+00:00",
    "resolver": "jreback",
    "resolved_in": "2e3c82e81bf00af157268a842a270a6181fcb168",
    "resolver_commit_num": 4043,
    "title": "TST: slow computation/test_eval.py tests need adjustment",
    "body": "-ci.org/pydata/pandas/jobs/134204278\n\nI think #13311 broke some of these\n\nhere's some partial fixes\n\n\n",
    "labels": [
      "Testing"
    ],
    "comments": [
      "cc @sinhrks \n"
    ],
    "events": [
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 2,
    "additions": 18,
    "deletions": 10,
    "changed_files_list": [
      "pandas/computation/tests/test_eval.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13352,
    "reporter": "gfyoung",
    "created_at": "2016-06-02T23:57:36+00:00",
    "closed_at": "2016-07-10T21:12:56+00:00",
    "resolver": "gfyoung",
    "resolved_in": "675a6e35cc78063f68a14338ae69c099588e23d1",
    "resolver_commit_num": 42,
    "title": "Add compact_ints and use_unsigned to pd.to_numeric",
    "body": "Follow-up from #13323.  This functionality can be made applicable outside of the parser and therefore should not exist at the parser level.\n",
    "labels": [
      "Enhancement",
      "Dtypes",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 273,
    "deletions": 37,
    "changed_files_list": [
      "asv_bench/benchmarks/inference.py",
      "doc/source/basics.rst",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/tools/tests/test_util.py",
      "pandas/tools/util.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13354,
    "reporter": "ekeydar",
    "created_at": "2016-06-03T07:56:45+00:00",
    "closed_at": "2016-06-18T15:25:15+00:00",
    "resolver": "cmazzullo",
    "resolved_in": "35bb1a1c2d915d862ca0daadbe1d32180a998ccf",
    "resolver_commit_num": 0,
    "title": "BUG: df.pivot_table: margins_name is ignored when there aggfunc is list",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n\nWill ignore the margins_name\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Bug",
      "Reshaping",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "yep, looks like this is not being passed thru [here](https://github.com/pydata/pandas/blob/master/pandas/tools/pivot.py#L86)\n\npull-request welcomed!\n"
    ],
    "events": [
      "renamed",
      "renamed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 24,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/tools/pivot.py",
      "pandas/tools/tests/test_pivot.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13356,
    "reporter": "jreback",
    "created_at": "2016-06-03T16:39:28+00:00",
    "closed_at": "2016-07-24T14:14:06+00:00",
    "resolver": "aterrel",
    "resolved_in": "6efd743ccbf2ef6c13ea0c71b7b2e2a022a99455",
    "resolver_commit_num": 0,
    "title": "ENH: support encoding in read_json",
    "body": "the hooks already exist, just need to define the argument and pass thru to `_get_filepath_or_buffer`\n",
    "labels": [
      "Unicode",
      "IO JSON",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "xref #13351 \n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "assigned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 5,
    "additions": 142,
    "deletions": 11,
    "changed_files_list": [
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/generic.py",
      "pandas/io/json.py",
      "pandas/io/tests/json/test_pandas.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13383,
    "reporter": "mikegraham",
    "created_at": "2016-06-07T00:19:52+00:00",
    "closed_at": "2016-06-21T10:09:29+00:00",
    "resolver": "priyankjain",
    "resolved_in": "0f351dc475eaa5fdddf1ecd24e70ac0114e58e9b",
    "resolver_commit_num": 1,
    "title": "Series.rolling/DataFrame.rolling don't fully check arguments, have odd behavior when used with invalid inputs",
    "body": "\n",
    "labels": [
      "Bug",
      "Reshaping",
      "Difficulty Novice",
      "Error Reporting",
      "Effort Low"
    ],
    "comments": [
      "this particular one is pretty trivial to check when `Rolling` is created; OTOH, _some_ errors will still be raised when a function is called e.g. `kurt/skew`. But this would be an improvement.\n",
      "As part of validation that happens towards the end of initialization, we can check whether the window is a natural number. @jreback Don't get you when you say \"some errors will still be raised for kurt/skew\". The Rolling object should never be created when passed window is not >0.\n",
      "kurt/skew require even more min_periods, but you don't know the calling methods until run-time.\n"
    ],
    "events": [
      "renamed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 19,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/core/window.py",
      "pandas/tests/test_window.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13389,
    "reporter": "b11z",
    "created_at": "2016-06-07T17:48:20+00:00",
    "closed_at": "2016-07-26T22:49:35+00:00",
    "resolver": "sinhrks",
    "resolved_in": "a3cddfa9c4302c13747509caef7a056159010adf",
    "resolver_commit_num": 357,
    "title": "BUG: TypeError in merge with timedelta64 column",
    "body": "\n### Expected output:\n\n\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 22.0.5\nCython: None\nnumpy: 1.11.0\nscipy: 0.17.0\nstatsmodels: None\nxarray: None\nIPython: 4.0.1\nsphinx: None\npatsy: None\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: 2.5.1\nmatplotlib: None\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.10\npymysql: None\npsycopg2: 2.6.1 (dt dec pq3 ext)\njinja2: 2.7.3\nboto: 2.38.0\npandas_datareader: None\n\n(It also appears on master)\n",
    "labels": [
      "Bug",
      "Reshaping",
      "Timedelta",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "yeah this is prob not tested very well. pull-requests are welcome.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 6,
    "additions": 227,
    "deletions": 96,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/tests/types/test_missing.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/types/common.py",
      "pandas/types/missing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13398,
    "reporter": "mbrucher",
    "created_at": "2016-06-08T13:18:20+00:00",
    "closed_at": "2016-07-02T19:31:58+00:00",
    "resolver": "mbrucher",
    "resolved_in": "30d710f4c8a07cb7ea3bc91f6eb05c4bbdfa2f24",
    "resolver_commit_num": 0,
    "title": "TemporaryFile as input to read_table raises TypeError: '_TemporaryFileWrapper' object is not an iterator",
    "body": "Although the requirement in the doc says that the input can be a file like object, it doesn't work with objects from tempfile. On Windows, they can't be reopened, so I need to pass the object itself.\n#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\nNot an exception!\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\n\npandas: 0.18.0\n",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "CSV",
      "Effort Low"
    ],
    "comments": [
      "this is only with `engine='python'` as the sep you gave is a regex (if you use `sep='\\s+'` which is a more typical whitespace) it works as expected.\n",
      "in the future, pls show the entire `show_versions()`. you are missing crucial information there (the platform); though you did put it in the comments. we ask for these things to make it easier for people to look.\n",
      "pull-requests are welcome\n",
      "Do you mean that if I used sep='\\s+', there is no exception?\nYes, I removed some info because it's not relevant here (except the platform, I didn't see I removed the OS) + there are some things that I can't send as well.\n",
      "yes if u were splitting on white space it would use the c engine which would give u an error that the data file is empty\n\nsince u used a regex it went to the python engine and gives that weird error (only on Windows)\n",
      "Oh, OK. The thing is that I may have several spaces between columns, so I have to use the regex :(\n",
      "\\s+ is white space with at least a single space having 0 spaces is very weird\n",
      "Yes, agreed that 0 spaces is weird :)\nBTW, the data file is not empty, I'm passing the file like object, it shouldn't fail in any case!\n",
      "oh the example above it IS empty\n\nin any case I'd u would like to debug - I think it's a simple fix \n",
      "Oh yes, sorry. I forgot I had to remove the data as it is confidential!\n",
      "The issue is that you can't call next() on a file apparently.\n",
      "@mbrucher : \n\n1) If you can't provide the original data, create dummy data that can trigger the exception, particularly example data that could be reproduced by just calling `read_table(new_file)`.\n\n2) If you have confidentiality issues, can you try reproducing the issue on another machine?  Full version output is extremely useful when trying to debug.\n\n3) How does your tempfile have data?  Are you calling `new_file.write` before you call `read_table`?  If so, make sure to call `new_file.seek(0)` first so as to reset the stream position.  Otherwise, none of your written data will be read (you can see this for yourself if you call `new_file.read()` before and after calling `new_file.seek(0)`).\n\nI should add that this advise also applies to normal file objects (i.e. those created by calling `open(...)`), so this issue with tempfiles is not unique IIUC.\n",
      "@gfyoung this repros exactly as above with an empty file \n",
      "I know but I thought @mbrucher said the file contained data, and I was addressing that.  In any case, unless a more convincing example can provided, I think this is safe to close, as the function does work with tempfiles in the manner I described , data or no data.\n",
      "no it doesn't on Windows \n",
      "```\nIn [2]: import pandas as pd\n\nIn [3]: pd.__version__\nOut[3]: '0.18.1+139.ge24ab24'\n\nIn [4]: import pandas as pd\n\nIn [5]: from tempfile import TemporaryFile\n\nIn [6]: new_file = TemporaryFile(\"w+\")\n\nIn [7]: dataframe = pd.read_table(new_file, skiprows=3, header=None, sep=r\"\\s*\")\nC:\\Miniconda2\\envs\\pandas3.5\\Scripts\\ipython-script.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not\n support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying\nengine='python'.\n  if __name__ == '__main__':\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-7-43d01852f446> in <module>()\n----> 1 dataframe = pd.read_table(new_file, skiprows=3, header=None, sep=r\"\\s*\")\n\nC:\\Users\\conda\\Documents\\pandas3.5\\pandas\\io\\parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, s\nqueeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_va\nlues, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, itera\ntor, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols,\nerror_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lin\nes, memory_map, float_precision)\n    627                     skip_blank_lines=skip_blank_lines)\n    628\n--> 629         return _read(filepath_or_buffer, kwds)\n    630\n    631     parser_f.__name__ = name\n\nC:\\Users\\conda\\Documents\\pandas3.5\\pandas\\io\\parsers.py in _read(filepath_or_buffer, kwds)\n    380\n    381     # Create the parser.\n--> 382     parser = TextFileReader(filepath_or_buffer, **kwds)\n    383\n    384     if (nrows is not None) and (chunksize is not None):\n\nC:\\Users\\conda\\Documents\\pandas3.5\\pandas\\io\\parsers.py in __init__(self, f, engine, **kwds)\n    710             self.options['has_index_names'] = kwds['has_index_names']\n    711\n--> 712         self._make_engine(self.engine)\n    713\n    714     def close(self):\n\nC:\\Users\\conda\\Documents\\pandas3.5\\pandas\\io\\parsers.py in _make_engine(self, engine)\n    894             elif engine == 'python-fwf':\n    895                 klass = FixedWidthFieldParser\n--> 896             self._engine = klass(self.f, **self.options)\n    897\n    898     def _failover_to_python(self):\n\nC:\\Users\\conda\\Documents\\pandas3.5\\pandas\\io\\parsers.py in __init__(self, f, **kwds)\n   1742         # infer column indices from self.usecols if is is specified.\n   1743         self._col_indices = None\n-> 1744         self.columns, self.num_original_columns = self._infer_columns()\n   1745\n   1746         # Now self.columns has the set of columns that we will process.\n\nC:\\Users\\conda\\Documents\\pandas3.5\\pandas\\io\\parsers.py in _infer_columns(self)\n   2068         else:\n   2069             try:\n-> 2070                 line = self._buffered_line()\n   2071\n   2072             except StopIteration:\n\nC:\\Users\\conda\\Documents\\pandas3.5\\pandas\\io\\parsers.py in _buffered_line(self)\n   2136             return self.buf[0]\n   2137         else:\n-> 2138             return self._next_line()\n   2139\n   2140     def _empty(self, line):\n\nC:\\Users\\conda\\Documents\\pandas3.5\\pandas\\io\\parsers.py in _next_line(self)\n   2164             while self.pos in self.skiprows:\n   2165                 self.pos += 1\n-> 2166                 next(self.data)\n   2167\n   2168             while True:\n\nC:\\Users\\conda\\Documents\\pandas3.5\\pandas\\io\\parsers.py in _read()\n   1869         else:\n   1870             def _read():\n-> 1871                 line = next(f)\n   1872                 pat = re.compile(sep)\n   1873                 yield pat.split(line.strip())\n\nTypeError: '_TemporaryFileWrapper' object is not an iterator\n```\n",
      "So if the file is populated, of course same issue:\n\n```\nimport pandas as pd\nfrom tempfile import TemporaryFile\nnew_file = TemporaryFile(\"w+\")\nnew_file.write(\"0 0\")\nnew_file.flush()\nnew_file.seek(0)\ndataframe = pd.read_table(new_file, header=None, sep=r\"\\s+\", engine=\"python\")\nprint(dataframe)\n```\n\nTested on OS X with Python 2.7 (brew version), works like a charm, so there must be a difference in the implementation. I don't have a 3.5 on my Mac, so can't try it to see if it's the OS or the Python version :/\n\n@gfyoung I know perfectly well how files work, thank you very much. I've been writing Python for more than a decade now, I hit all these issues in the past and obviously I know how to avoid them. But I guess you haven't tried my code before posting your message.\n\nAs @jreback said, it should be \"easy\" to fix, so I'll have a try when I have time.\nA completely different question, be can't use a list of strings to generate a DataFrame? (for instance a filtered file would end up being a list of strings that could be read in pandas, that's actually my use use case. Using a TemporaryFile because I couldn't figure another way).\n",
      "@mbrucher what do you mean a 'list of strings', do you mean?\n\nyou can! The difference is that this is not very efficient as have to be introspected (to figure out what exactly you are passing, as there are many possibilities), and then converted to a storage format (e.g. numpy). These may not necessarily be cheap; hence from the parser has more info available (e.g. it already knows the layout and can infer dtypes directly).\n\n```\nIn [12]: DataFrame(['foo', 'bar', 'baz'])\nOut[12]: \n     0\n0  foo\n1  bar\n2  baz\n\nIn [13]: DataFrame([['foo', 'bar', 'baz']])\nOut[13]: \n     0    1    2\n0  foo  bar  baz\n```\n",
      "Actually I was thinking of something like pd.read_table([\"0 0\", \"1 1\"], header=None, sep=r\"\\s+\", engine=\"python\") as the data is not yet parsed in my case (reading a report file that mixes lots of things together, only looking for specific tables that I then append to a list).\n",
      "Much more efficient to do this with the c-engine, you have whitespace separating. Introduce line separation and you are set. \n\n```\nIn [5]: pd.read_csv(StringIO('\\n'.join([\"0 0\", \"1 1\"])), header=None, sep=\"\\s+\")\nOut[5]: \n   0  1\n0  0  0\n1  1  1\n```\n",
      "OK, thanks.\n\nIt seems that file like object don't implement **next**(). The issue comes from the fact that to select the type of reader, we check the attribute readline which is used for separators of length 1, but pandas uses next() for the other separators.\n",
      "@mbrucher : Whoa, slow down there, aren't we letting our ego get bit in the way of rationale conversation?  First of all, your code gave no indication that you were aware of this, so if you would like to update your code example in the initial post, go right ahead and do so.\n\nSecond, I did in fact try it out on a newly-acquired Windows 7 machine using Python 2.7.11 using `v0.18.1` and could not reproduce the Exception.  In addition, I tested the new examples that were later posted and also got not Exception.\n",
      "@gfyoung Which is why I specified the Python version, as there is a change in the API AFAIK on the behavior of next. Anyway, the pull request fixes it and I'm adding a test as we speek.\n",
      "@mbrucher : fair enough - but it's worthwhile to note since this issue you raise isn't then a general Windows bug but rather a change in the way `TemporaryFile` is written between Python versions.\n",
      "They must have forgotten when they changed the **next** API :(\n",
      "@jreback : this issue should have been closed with @mbrucher 's commit (I think it didn't because the commit says \"dcloses\" instead of \"closes\")\n"
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "milestoned",
      "demilestoned",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 16,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/io/parsers.py",
      "pandas/io/tests/parser/python_parser_only.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13399,
    "reporter": "jreback",
    "created_at": "2016-06-08T13:51:13+00:00",
    "closed_at": "2016-07-26T10:51:50+00:00",
    "resolver": "sinhrks",
    "resolved_in": "98c5b88d6f9f7bb0afa4fbae49a045d93a1cb33f",
    "resolver_commit_num": 356,
    "title": "BLD: should we use tempita for cython templating",
    "body": "see [here]()\n\nmight replace some of the manual code paths in `generate_code.py`\n",
    "labels": [
      "Build"
    ],
    "comments": [
      "@gfyoung can you comment on your experience\n",
      "+1 from me - it's currently used in `scipy` and `numpy` and I in fact put it to good use in one of my PR's against `numpy`.  Also, since `tempita` comes with `Cython` (`Cython.Tempita` --> no extra build dependency), if it's possible to use in order to cut down on manual code (which it seems based on a quick skim of the code in `pandas/src/generate_code.py`), certainly!\n",
      "Quite interesting. I'm just adding new cython templates to support more sparse dtypes, and it can be much simpler using it.\n",
      "@gfyoung can u post a link to the PR where u r using this? \n",
      "Link to PR <a href=\"https://github.com/numpy/numpy/pull/6938\">here</a>.  Key file to examine is `randint_helpers.pxi.in` and the templating code <a href=\"https://github.com/numpy/numpy/pull/6938/files#diff-6e2c9b85250e2af0fe46f180ace4283cR104\">here</a>.  The key part is getting the template right, as once you do, filling it in is really easy.\n\nUseful discussion points regarding Tempita in my PR start <a href=\"https://github.com/numpy/numpy/pull/6938#issuecomment-224208311\">here</a>, as there are links to other libraries that use Tempita as well.\n",
      "thanks @gfyoung \n\nnote that I have found fused types pretty useful as well. [here](https://github.com/pydata/pandas/blob/master/pandas/src/util.pxd#L27) is an example of a `numeric` type which is used in `window.pyx` & some of the `algos.pyx`. These are pretty easy to define and work with; and somewhat remove the need for templating.\n",
      "@jreback : yes, I remember you mentioning that in that same PR <a href=https://github.com/numpy/numpy/pull/6938#discussion_r64143615\">here</a>.  I guess not all maintainers are a fan of it though, and it did take some getting used to as well when I first attempted it.\n",
      "right they are in newer cython (though our min 0.19.1 certainly has it)\n",
      "Well different repository, different rules :smile: - `fused_types` may not be as applicable in this case though given that there are some things in the template to `generate_code.py` that aren't fuse-type-able FLOABW.\n",
      "@gfyoung yeah, though if I _could_ use a fused type I think we should. Just easier to read/maintain. They seem to act very much like 'real' types, with a tiny exception. You need to this to avoid warnings when compiling.\n\n```\ncdef numeric val\n\nif numeric in cython.floating and val == val:\n     # do something if we are not-NaN\n```\n",
      "I agree that fused types are nice, but in my experience they end up falling short. Tempita looks pretty nice, better than our ad-hoc current system.\n"
    ],
    "events": [
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 13,
    "additions": 13188,
    "deletions": 12707,
    "changed_files_list": [
      "ci/lint.sh",
      "pandas/algos.pyx",
      "pandas/src/algos_common_helper.pxi",
      "pandas/src/algos_common_helper.pxi.in",
      "pandas/src/algos_groupby_helper.pxi",
      "pandas/src/algos_groupby_helper.pxi.in",
      "pandas/src/algos_join_helper.pxi",
      "pandas/src/algos_join_helper.pxi.in",
      "pandas/src/algos_take_helper.pxi",
      "pandas/src/algos_take_helper.pxi.in",
      "pandas/src/generate_code.py",
      "pandas/src/generated.pyx",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13412,
    "reporter": "Tux1",
    "created_at": "2016-06-09T14:18:36+00:00",
    "closed_at": "2016-12-06T11:35:03+00:00",
    "resolver": "mroeschke",
    "resolved_in": "6e514dacc131f044deb74f0a562a51ef3b1201eb",
    "resolver_commit_num": 3,
    "title": "BUG: in _nsorted for frame with duplicated values index",
    "body": "The function below has been incorrectly implemented. If the frame has an index with duplicated values, you will get a result with more than `n` rows and not properly sorted. So `nsmallest` and `nlargest` for DataFrame doesn't return a correct frame in this particular case.\n\n\n",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "Indeed:\n\n```\nIn [71]: df = pd.DataFrame({'a':[1,2,3,4], 'b':[4,3,2,1]}, index=[0,0,1,1])\n\nIn [72]: df.nlargest(1, 'a')\nOut[72]:\n   a  b\n1  4  1\n1  3  2\n\nIn [73]: df.nlargest(2, 'a')\nOut[73]:\n   a  b\n1  4  1\n1  4  1\n1  3  2\n1  3  2\n```\n\n(@Tux1 side note for future reference, it is always nice to provide a small reproducible example when opening an issue) \nInterested in doing a PR to fix this?\n",
      "Yes I will fix that soon\nSorry about example \n\n> Le 9 juin 2016 \u00e0 23:30, Joris Van den Bossche notifications@github.com a \u00e9crit :\n> \n> Indeed:\n> \n> In [71]: df = pd.DataFrame({'a':[1,2,3,4], 'b':[4,3,2,1]}, index=[0,0,1,1])\n> \n> In [72]: df.nlargest(1, 'a')\n> Out[72]:\n>    a  b\n> 1  4  1\n> 1  3  2\n> \n> In [73]: df.nlargest(2, 'a')\n> Out[73]:\n>    a  b\n> 1  4  1\n> 1  4  1\n> 1  3  2\n> 1  3  2\n> (@Tux1 side note for future reference, it is always nice to provide a small reproducible example when opening an issue) \n> Interested in doing a PR to fix this?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n",
      "my fix is not very elegant but I don't see any other solution to deal with MultiIndex and duplicated value index\n"
    ],
    "events": [
      "renamed",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced"
    ],
    "changed_files": 7,
    "additions": 82,
    "deletions": 15,
    "changed_files_list": [
      "asv_bench/benchmarks/frame_methods.py",
      "doc/source/whatsnew/v0.19.2.txt",
      "pandas/core/algorithms.py",
      "pandas/core/frame.py",
      "pandas/core/series.py",
      "pandas/tests/frame/test_analytics.py",
      "pandas/tests/series/test_analytics.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13431,
    "reporter": "zym1010",
    "created_at": "2016-06-12T14:13:48+00:00",
    "closed_at": "2017-04-07T15:18:25+00:00",
    "resolver": "jreback",
    "resolved_in": "f478e4f4b0a353fa48ddb19e70cb9abe5b36e1b5",
    "resolver_commit_num": 4359,
    "title": "BUG: different behaviors of sort_index() and sort_index(level=0)",
    "body": "Inspired by some bug reports around multiindex sortedness (-lexicographical-sort-in-pandas-multiindex, #10651, #9212), I found that `sort_index()` sometimes can't make a multiindex ready for slicing, but `sort_index(level=0)` (so does `sortlevel()`) can.\n\n\n\nWhile `df2.sort_index()` does give a visually lexicographically sorted output, it DOES NOT support slicing.\n\n\n\nSo I have two questions.\n1. Is this the intended behavior? I thought `level=0` and `level=None` are synonyms to me, but they are not. Looking at the code, #L3245-L3247 indeed there's a special processing when `level` is not `None`.\n2. What does \"lexicographically sorted\" mean? I think it should mean sorted in terms of levels, not labels. Is this what \"lexicographically sorted\" means in the doc for Advanced Indexing? If this is true, then I think `make_index(level=0)` is correct, yet `make_index()` is not.\n\nThanks.\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "MultiIndex",
      "Bug",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "Might be a bug, see the docs [here](http://pandas-docs.github.io/pandas-docs-travis/advanced.html#sorting-a-multiindex), these have been updated since 0.18.1; these should be the same. But you will have to step thru and have a look.\n\n```\nIn [10]: df2.sort_index().index.lexsort_depth\nOut[10]: 0\n\nIn [12]: df2.sort_index(level=0).index.lexsort_depth\nOut[12]: 2\n```\n",
      "@jreback  right I have seen that. Thus I think the updated doc is good, without all that complicated descriptions about lexicographically sortedness. But maybe the current implementation is inconsistent with the updated doc...\n\nAnyway, I think some workarounds for all multiindex related issues are\n1. when constructing the dataframe, always use the vanilla, integer index\n2. after constructing the data in the dataframe, use `set_index` to create multiindex\n3. don't ever play around the levels of the index. Basically, don't use factors (categorical variables with orders)\n\nCurrently my workflow is compatible with the above three, so this bug is not a problem for me now, but I think this is a big issue needing some clarification.\n",
      "@jreback \n\nI think the problem is due to a potential re initialization of factor levels in https://github.com/pydata/pandas/blob/4de83d25d751d8ca102867b2d46a5547c01d7248/pandas/core/frame.py#L3245-L3259.\n\nAccording to the code, if `level` is not `None` (which I believe implies index being a `MultiIndex`), then the `MultiIndex.sortlevel` is used, which respects factor level.\n\nHowever, if `level` is `None` and index is `MultiIndex`, the code will first check if it's sorted (in terms of factor levels, not naive lexicographical order). If not, then the levels will be re initialized in naive lexicographical order, and `_lexsort_indexer` will make the labels follow naive lexicographical order, yet levels are not updated when returning the new dataframe, because the reinitialized `labels` in https://github.com/pydata/pandas/blob/4de83d25d751d8ca102867b2d46a5547c01d7248/pandas/core/frame.py#L3255-L3256 is NOT used to update the original index, making the levels and labels inconsistent.\n\nThis may explain #9212, I believe. Basically, whether `sort_index()` will do naive lexicographical sorting, or factor-respecting sorting depends on whether the original index is sorted in a factor-respecting way, which looks like a bug to me.\n",
      "could very well be\n\ndo you want to put in place tests for this issue (and other); and make a change -- see if you can fix without breaking em anything else?\n",
      "@jreback I'd like to, though may be no time this month, and I'm a beginner into pandas, starting to use it just yesterday... But maybe this looks easy enough to me.\n\nMy main question is\n1. what's the expected behavior of `DataFrame.sort_index()`? I believe in any case, it should return a sorted index ready for slicing, but should the factor levels in naive order, or should the original factor levels be preserved?\n\nActually I found comments on https://github.com/pydata/pandas/blob/4de83d25d751d8ca102867b2d46a5547c01d7248/pandas/core/frame.py#L3253-L3254 to be funny. If `labels.is_lexsorted()`, then why any further more sorting is needed anymore?\n",
      "`.sort_index()` and `.sort_index(level=0)` should yield the same index. The factors are the same (and don't change) in all cases.\n\nSee [here for the contributing docs](http://pandas.pydata.org/pandas-docs/stable/contributing.html)\n",
      "looks to me simply removing https://github.com/pydata/pandas/blob/4de83d25d751d8ca102867b2d46a5547c01d7248/pandas/core/frame.py#L3255-L3256 should do. But I have one question: does `na_position` have any effect for `sort_index`? looks there can't be any `NaN` in index labels (which are integers).\n",
      "there _can_ be `NaN` (though generally you shouldn't have them), behavior may be slightly odd / not completely tested though.\n",
      "Looks that removing those lines will undo #8017. Actually, the current version of pandas (0.18.1) won't have same behavior on `sort_index(level=0)` and `sort_index()` on the example dataframe in that one either. Looks that adding columns with a multiindex is really troublesome.\n\nI think the fundamental problem is that when using a multindex, each subindex in it becomes a categorical variable implicitly (labels and levels), including things that are not categorical in nature, such as float.\n",
      "Basically I don't think given the current implementation, one can tell the difference between \"true\" `is_lexsorted()` and accidental `is_lexsorted()` due to the fact that when there's a new value added to a multiindex, it's always in the back (so if an multiindex is sorted in the beginning, it will be always sorted when adding more values to it). Maybe add some flag, say `flush` to `sort_index`, and let users choose whether to reconstruct the index or not.\n",
      "its possible that issue was confused because it exposed a printing issue.\n\nI don't think there is any difference between true lexsorted and accidental, except that accidental might just be not recorded as such (so its a bug in keeping state).\n",
      "as you can see from the example in #8017, although `print` gives correct result, the actual index level for the float index is wrong, and it's not lexsorted, not ready for slicing.\n\n``` python\nIn [1]: import pandas as pd\n\nIn [2]: import numpy as np\n\nIn [3]:\n\nIn [3]: np.random.seed(0)\n\nIn [4]: data = np.random.randn(3,4)\n\nIn [5]:\n\nIn [5]: df_multi_float = pd.DataFrame(data, index=list('def'), columns=pd.MultiIndex.from_tuples([('red', i) for i in [1., 3., 2., 5.]]))\n\nIn [6]: df_multi_float[('red', 4.0)] = 'world'\n\nIn [7]: a=df_multi_float.sort_index(axis=1)\n\nIn [8]: a\nOut[8]:\n        red\n        1.0       2.0       3.0    4.0       5.0\nd  1.764052  0.978738  0.400157  world  2.240893\ne  1.867558  0.950088 -0.977278  world -0.151357\nf -0.103219  0.144044  0.410599  world  1.454274\n\nIn [9]: a.columns\nOut[9]:\nMultiIndex(levels=[[u'red'], [1.0, 2.0, 3.0, 5.0, 4.0]],\n           labels=[[0, 0, 0, 0, 0], [0, 1, 2, 4, 3]])\n\nIn [10]: pd.__version__\nOut[10]: u'0.18.1'\n\nIn [11]: a.columns.is_lexsorted()\nOut[11]: False\n```\n"
    ],
    "events": [
      "commented",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 15,
    "additions": 593,
    "deletions": 57,
    "changed_files_list": [
      "asv_bench/benchmarks/timeseries.py",
      "doc/source/advanced.rst",
      "doc/source/api.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/core/sorting.py",
      "pandas/indexes/multi.py",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tests/series/test_analytics.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tests/tools/test_hashing.py",
      "pandas/tests/tools/test_pivot.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13434,
    "reporter": "tdszyman",
    "created_at": "2016-06-13T13:24:04+00:00",
    "closed_at": "2017-03-29T23:28:23+00:00",
    "resolver": "brianhuey",
    "resolved_in": "0ab081345eb191937fd4152eba48b8c9692b02bf",
    "resolver_commit_num": 0,
    "title": "read_html() doesn't handle tables with multiple header rows ",
    "body": "The `read_html()` function seems to treat every `<th>` in a table as a column, even if they occur in separate `<tr>`s. This means that it breaks even on simple tables generated by pandas' `to_html()` function.\n#### Code Sample, a copy-pastable example if possible\n\n\n\nThis is the value of `html`, generated by the `to_html()` function on the original data frame:\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Party</th>\n    </tr>\n    <tr>\n      <th>Name</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Hillary</th>\n      <td>68</td>\n      <td>D</td>\n    </tr>\n    <tr>\n      <th>Bernie</th>\n      <td>74</td>\n      <td>D</td>\n    </tr>\n    <tr>\n      <th>Donald</th>\n      <td>69</td>\n      <td>R</td>\n    </tr>\n  </tbody>\n</table>\n\n\n\n\nAnd this is the printed output of the newly-parsed dataframe `df2`:\n\n\n\nWhat happens is that the `to_html()` function produces an html table with two header rows, one for the column names and one with the index name. However the `read_html()` parser interprets each individual `th` cell as an expected column, resulting in twice the number of columns. Even worse, this produces a column with the same name as the original index but without any data. \n#### Expected Output\n\nThe `read_html` parser could either treat the multi-row header fully correctly:\n\n\n\nOr it could just ignore any rows after the first one:\n\n\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 14.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_IE.UTF-8\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.1\nsetuptools: 21.0.0\nCython: 0.23.4\nnumpy: 1.11.0\nscipy: 0.17.0\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.1.1\nsphinx: 1.3.5\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.5\nmatplotlib: 1.5.1\nopenpyxl: 2.1.2\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: 3.3.5\nbs4: 4.4.1\nhtml5lib: 1.0b3\n 0.9.2\napiclient: None\nsqlalchemy: 1.0.11\npymysql: None\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\njinja2: 2.8\nboto: 2.40.0\npandas_datareader: None\n",
    "labels": [
      "IO HTML",
      "Difficulty Intermediate",
      "Effort Medium",
      "Enhancement"
    ],
    "comments": [
      "Correct me if I'm wrong here... would you be able to differentiate between HTML where the first row really is two blank strings, and a table with a header spanning multiple rows? My thoughts are `read_html` are that the user should expect to have a bit of cleanup work to do. But if the change to handle this case doesn't break anything and isn't too complicated, I'd say it'd be a good addition.\n",
      "@TomAugspurger the case I'm thinking of is where the first two rows are in the `<thead>` part of the `<table>`, and the other rows are in the `<tbody>` part. So yes they can clearly be distinguished from a row that is simply empty. Also, in the example I gave, every single `<tr>` element contains the same number of cells/columns (whether they are `<th>` or `<td>`), so there is no reason to generate a data frame with a different number of columns.\n"
    ],
    "events": [
      "renamed",
      "commented",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 43,
    "deletions": 20,
    "changed_files_list": [
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/io/html.py",
      "pandas/tests/io/test_html.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13438,
    "reporter": "jreback",
    "created_at": "2016-06-14T12:14:00+00:00",
    "closed_at": "2016-06-16T21:02:21+00:00",
    "resolver": "priyankjain",
    "resolved_in": "fca35fbe40e7749e8202c64292f10a24c5effa19",
    "resolver_commit_num": 0,
    "title": "ERR: invalid input to .str.replace does not raise",
    "body": "This should raise; `None` is not an allowed option for the replacement.\n\n\n\ncc @kwsmith\n",
    "labels": [
      "Difficulty Novice",
      "Error Reporting",
      "Strings",
      "Effort Low"
    ],
    "comments": [
      "It's not only None that gives non-sensical output, also eg numerical values:\n\n```\nIn [115]: s.str.replace('a', 3)\nOut[115]:\n0   NaN\n1   NaN\n2   NaN\ndtype: float64\n```\n\nSo rather than checking for None, maybe just check that the replacement is string-like? \nOr are there arguments that do work that are not string like?\n",
      "Gotcha, is the following output valid:\n\n```\ns.str.replace('','a')\n\n0     aaa\n1     aba\n2    None\ndtype: object\n```\n",
      "Hmm, not sure about that one. At first, it seems invalid as well, but this is the standard behaviour of `str.replace`:\n\n```\nIn [125]: 'a'.replace('', 'b')\nOut[125]: 'bab'\n```\n",
      "I think it makes more sense to consider this as valid, since that's how str.replace works. Also it would make sense to throw TypeError instead of ValueError:\n\n```\n'abc'.replace('a', None)\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-19-51f0617eaa9d> in <module>()\n----> 1 'abc'.replace('a', None)\n\nTypeError: Can't convert 'NoneType' object to str implicitly\n\n```\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "mentioned",
      "subscribed",
      "milestoned",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 13,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/core/strings.py",
      "pandas/tests/test_strings.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13444,
    "reporter": "jreback",
    "created_at": "2016-06-14T21:40:00+00:00",
    "closed_at": "2016-06-17T00:37:39+00:00",
    "resolver": "jreback",
    "resolved_in": "b06bc7ab10552079b5300f53ae9458b5b1583402",
    "resolver_commit_num": 4049,
    "title": "Revert this when conda 4.1 is stabilized",
    "body": "\n (partial revert)\n\nfixing to 4.0.8\n\nseems 4.1 breaks things.\n",
    "labels": [
      "Build"
    ],
    "comments": [],
    "events": [
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 2,
    "additions": 19,
    "deletions": 8,
    "changed_files_list": [
      "appveyor.yml",
      "ci/install_travis.sh"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13448,
    "reporter": "jreback",
    "created_at": "2016-06-15T11:30:26+00:00",
    "closed_at": "2016-06-16T12:13:47+00:00",
    "resolver": "chris-b1",
    "resolved_in": "f67dd4bc2acf5ab7b871ced4d249503faf789afa",
    "resolver_commit_num": 36,
    "title": "BLD: py2.7 failing on perf hash changes (for windows)",
    "body": "xref #13436 \n\nso this is failing on windows py2.7 (3.5 works fine).\n\n\n\nwould be ok with just having this change affect py3 instead \n",
    "labels": [
      "Build",
      "Windows"
    ],
    "comments": [
      "cc @chris-b1 \n",
      "I at least got a more helpful error message locally, I can take a look later.  It might be preferable to keep this in py2 too, since I think it's a little faster than the py2 hash, and for consistency.\n\n```\npandas/src/klib\\khash_python.h(16) : error C2054: expected '(' to follow 'inline'\npandas/src/klib\\khash_python.h(16) : error C2085: 'asint64' : not in formal parameter list\npandas/src/klib\\khash_python.h(16) : error C2143: syntax error : missing ';' before '{'\n```\n",
      "yes I think that Py_ssize_t is 32-bits on windows is the issue. But I think for simplicity just make this for py3, then should be fine.\n",
      "Older MSVC is apparently weird about `inline`, it looks like all that needs to be done is replace it with the `PANDAS_INLINE` macro, which expands to `__inline`\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "renamed",
      "commented",
      "commented",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 2,
    "additions": 9,
    "deletions": 2,
    "changed_files_list": [
      "pandas/src/klib/khash_python.h",
      "pandas/tests/indexing/test_coercion.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13451,
    "reporter": "tdhopper",
    "created_at": "2016-06-15T14:36:41+00:00",
    "closed_at": "2016-06-17T22:04:34+00:00",
    "resolver": "ravinimmi",
    "resolved_in": "9d33c7be38fd09fa493c68ab81c50ee7a681de34",
    "resolver_commit_num": 0,
    "title": "to_datetime can't handle int16 or int8",
    "body": "#### Code sample\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n\n``````\n",
    "labels": [
      "Bug",
      "Timeseries",
      "Dtypes",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "yeah I think these overflow (as we are multiplying by a factor relative to position). should convert first (use `.astype('int64', copy=False)`)\n\npull-requests welcome!\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 34,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tools.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13453,
    "reporter": "jcrist",
    "created_at": "2016-06-15T19:05:16+00:00",
    "closed_at": "2017-02-16T14:28:47+00:00",
    "resolver": "aiguofer",
    "resolved_in": "5a8883b965610234366150897fe8963abffd6a7c",
    "resolver_commit_num": 0,
    "title": "Resampler.nunique counting data more than once",
    "body": "xref addtl example in #13795 \n\nPandas `Resampler.nunique` appears to be putting the same data in multiple bins:\n\n\n\nIn pandas 0.18.1 and 0.18.0 these don't give the same results, when they should\n\n\n\nIn pandas 0.17.0 and 0.17.1 (adjusting to old style resample syntax), the `nunique` one fails due to a \"`ValueError: Wrong number of items passed 4, placement implies 5`\" somewhere in the depths of `internals.py`. If I go back to 0.16.2, I do get the same result for each.\n\nI'm not sure what's going on here. Since the `nunique` results sum to larger than the length, it appears data is being counted more than once.\n\n---\n\n\n",
    "labels": [
      "Resample",
      "Bug",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "May be related to https://github.com/pydata/pandas/issues/10914.\n",
      "Interestingly everything seems to work fine if `agg` is used with `pd.Series.nunique` instead:\n\n```\nIn [11]: r = s.resample('M')\n\nIn [12]: r.agg(pd.Series.nunique)\nOut[12]:\n2000-01-31    744\n2000-02-29    337\n2000-03-31      0\n2000-04-30    384\n2000-05-31    337\nFreq: M, dtype: int64\n\nIn [13]: r.nunique()    # same result as r.agg('nunique')\nOut[13]:\n2000-01-31    337\n2000-02-29    744\n2000-03-31      0\n2000-04-30    744\n2000-05-31    337\nFreq: M, dtype: int64\n```\n",
      "CC:  @behzadnouri \n",
      "I think the root cause of the problem is in groupby.nunique(), which I believe is eventually is called by resample.nunique().  Note that groupby.nunique() has the same bug:\n\n``` python\nimport pandas as pd\nfrom pandas import Timestamp\n\ndata = ['1', '2', '3']\ntime = time = [Timestamp('2016-06-28 09:35:35'), Timestamp('2016-06-28 16:09:30'), Timestamp('2016-06-28 16:46:28')]\ntest = pd.DataFrame({'time':time, 'data':data})\n\n#wrong counts\nprint test.set_index('time').groupby(pd.TimeGrouper(freq='h'))['data'].nunique(), \"\\n\"\n#correct counts\nprint test.set_index('time').groupby(pd.TimeGrouper(freq='h'))['data'].apply(pd.Series.nunique)\n```\n\nThis gives\n\n```\ntime  \n2016-06-28 09:00:00    1  \n2016-06-28 10:00:00    0  \n2016-06-28 11:00:00    0  \n2016-06-28 12:00:00    0  \n2016-06-28 13:00:00    0  \n2016-06-28 14:00:00    0  \n2016-06-28 15:00:00    0  \n2016-06-28 16:00:00    1  \nFreq: H, Name: data, dtype: int64   \n\ntime  \n2016-06-28 09:00:00    1  \n2016-06-28 10:00:00    0  \n2016-06-28 11:00:00    0  \n2016-06-28 12:00:00    0  \n2016-06-28 13:00:00    0  \n2016-06-28 14:00:00    0  \n2016-06-28 15:00:00    0  \n2016-06-28 16:00:00    2  \nFreq: H, Name: data, dtype: int64  \n```\n\nI believe the problem is in the second to last line of groupby.nunique(), \ni.e. line 2955 in [groupby.py](https://github.com/pydata/pandas/blob/master/pandas/core/groupby.py)\n\n``` python\nres[ids] = out\n```\n\nI suspect `ids` should not be used for the indexing--it has different dimensions than `out`.\n\n``` python\npd.show_versions()\n```\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.12.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 14.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\nLOCALE: None.None\n\npandas: 0.19.0\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 27.2.0\nCython: 0.24.1\nnumpy: 1.11.2\nscipy: 0.18.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 5.1.0\nsphinx: 1.4.6\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.6.1\nblosc: None\nbottleneck: 1.1.0\ntables: 3.2.3.1\nnumexpr: 2.6.1\nmatplotlib: 1.5.3\nopenpyxl: 2.3.2\nxlrd: 1.0.0\nxlwt: 1.1.2\nxlsxwriter: 0.9.3\nlxml: 3.6.4\nbs4: 4.5.1\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.13\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.42.0\npandas_datareader: 0.2.1\n```\n",
      "@mgalbright why don't you submit a pull-request with your test examples (and those from the issue), and the proposed fix. See if that breaks anything else. Would be greatly appreciated!\n",
      "Hey, is there any advancement on this? I just realized that a report that I've been building is giving the wrong results and I believe it's due to this. I can't share all the code but here's a comparison of `groupby.unique` and `groupby.nunique`:\n\n``` python\nIn [216]: ents.groupby(pd.Grouper(freq='1W-SAT', key='startdate'))['ent_id'].unique().tail(1)\nOut[216]: \n\nstartdate\n2016-11-12    [550A00000033DHUIA2]\nFreq: W-SAT, Name: ent_id, dtype: object\n\nIn [217]: ents.groupby(pd.Grouper(freq='1W-SAT', key='startdate'))['ent_id'].nunique().tail(1)\nOut[217]: \n\nstartdate\n2016-11-12    7\nFreq: W-SAT, Name: ent_id, dtype: int64\n\nIn [218]: ents.groupby(pd.Grouper(freq='1W-SAT', key='startdate'))['ent_id'].count().tail(1)\nOut[221]: \n\nstartdate\n2016-11-12    1\nFreq: W-SAT, Name: ent_id, dtype: int64\n```\n",
      "@aiguofer pull-requests are welcome to fix.\n",
      "Not really adding anything, but I just ran into this issue for a work report as well (pandas version 0.19.2). Passing to .agg(pd.Series.nunique) works great - thanks for the tip",
      "Took a look at @mgalbright coment and suggestion and if I'm understanding the code correctly, the above PR should fix it. I ran `nosetests pandas/tests/groupby` and only had one unrelated test fail (`test_series_groupby_value_counts() takes exactly 2 arguments (0 given)`)."
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 38,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/groupby.py",
      "pandas/tests/groupby/test_groupby.py",
      "pandas/tests/tseries/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13459,
    "reporter": "ravinimmi",
    "created_at": "2016-06-16T08:49:44+00:00",
    "closed_at": "2016-06-21T12:43:02+00:00",
    "resolver": "ravinimmi",
    "resolved_in": "1a12eadaf92eb1ee05446d24e5d7a12cde971ce3",
    "resolver_commit_num": 1,
    "title": "Test case failing - test_timezones.py:TestTimeZones.test_normalize_tz",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Testing",
      "Timezones",
      "Bug"
    ],
    "comments": [
      "this passes on travis & on 15.4. maybe there is something peculiar on your setup. If you can debug great.\n",
      "@jreback \nThis code in ipython shell\n\n``` python\nfrom pandas.tseries.index import date_range\nfrom dateutil.tz import tzlocal\nrng = date_range('1/1/2000 9:30', periods=1, freq='D', tz=tzlocal())\nresult = rng.normalize()\nprint(result)\nprint(result.is_normalized)\n```\n\ngives the following output\n\n```\nDatetimeIndex(['2000-01-01 00:00:00+05:30'], dtype='datetime64[ns, tzlocal()]', freq=None)\nFalse\n```\n\nI printed value of dts [after this line](https://github.com/pydata/pandas/blob/e24ab24df2e092c90ef2f0b49bca9016139ec50c/pandas/tslib.pyx#L4808) which gives \n\n```\n{'year': 1999, 'month': 12, 'day': 31, 'sec': 0, 'as': 0, 'ps': 0, 'hour': 18, 'us': 0, 'min': 30}\n```\n\nMy local timezone is 'Asia/Kolkata'\n",
      "you are referring to a line which is not triggered. This is a local timezone (from whatever tz you are in).\n\n```\nIn [1]: from dateutil.tz import tzlocal\n\nIn [2]: tzlocal\nOut[2]: dateutil.tz.tz.tzlocal\n\nIn [3]: tzlocal()\nOut[3]: tzlocal()\n\nIn [4]: rng = date_range('1/1/2000 9:30', periods=1, freq='D', tz=tzlocal())\n\nIn [5]: rng\nOut[5]: DatetimeIndex(['2000-01-01 09:30:00-05:00'], dtype='datetime64[ns, tzlocal()]', freq='D')\n\nIn [6]: rng.normalize()\nOut[6]: DatetimeIndex(['2000-01-01 00:00:00-05:00'], dtype='datetime64[ns, tzlocal()]', freq=None)\n\nIn [7]: rng.normalize().is_normalized\nOut[7]: True\n\nIn [8]: rng = date_range('1/1/2000 9:30', periods=1, freq='D', tz='Asia/Kolkata')\n\nIn [9]: rng\nOut[9]: DatetimeIndex(['2000-01-01 09:30:00+05:30'], dtype='datetime64[ns, Asia/Kolkata]', freq='D')\n\nIn [10]: rng.normalize().is_normalized\nOut[10]: True\n\nIn [11]: rng.normalize()\nOut[11]: DatetimeIndex(['2000-01-01 00:00:00+05:30'], dtype='datetime64[ns, Asia/Kolkata]', freq=None)\n```\n",
      "@jreback Sorry I meant after [this line](https://github.com/pydata/pandas/blob/e24ab24df2e092c90ef2f0b49bca9016139ec50c/pandas/tslib.pyx#L4813)\nCould you please check once by changing your timezone?\n\n``` python\nimport os\nimport time\n\nfrom pandas.tseries.index import date_range\nfrom dateutil.tz import tzlocal\nos.environ['TZ'] = 'Asia/Kolkata'\ntime.tzset()\ndate_range('1/1/2000 9:30', periods=1, freq='D', tz=tzlocal()).normalize().is_normalized\n```\n",
      "that doesn't make sense\nyou would have to do that before dateutil imports \n",
      "yes\n\n``` python\nimport os\nimport time\nos.environ['TZ'] = 'Asia/Kolkata'\ntime.tzset()\n\nfrom pandas.tseries.index import date_range\nfrom dateutil.tz import tzlocal\ndate_range('1/1/2000 9:30', periods=1, freq='D', tz=tzlocal()).normalize().is_normalized\n```\n\nBut what does it print?\n",
      "So it _looks_ normalized, but possibly `is_normalized` has a bug in it. the `tzlocal` has a different path (and might not be right).\n\nSo use that as a test and see what you can do about fixing.\n\nYou will have to capture the current tz and reset it after. We do something like this in `io/tests/test_pytables.py` (near the end)\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "labeled",
      "unlabeled",
      "milestoned"
    ],
    "changed_files": 5,
    "additions": 76,
    "deletions": 34,
    "changed_files_list": [
      "doc/source/whatsnew/v0.18.2.txt",
      "pandas/io/tests/test_pytables.py",
      "pandas/tseries/tests/test_timezones.py",
      "pandas/tslib.pyx",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13472,
    "reporter": "jreback",
    "created_at": "2016-06-17T01:11:56+00:00",
    "closed_at": "2016-06-17T22:04:34+00:00",
    "resolver": "jreback",
    "resolved_in": "e24ab24df2e092c90ef2f0b49bca9016139ec50c",
    "resolver_commit_num": 4051,
    "title": "BLD: fix appveyor to work with latest conda/conda-build",
    "body": "-465/build/job/ic8ytmkwcc8nfucc\n\nCurrent are\n- `conda==4.1.1` is good\n- `conda-build==1.21.0` seems buggy\n\na working version is:\n- `conda=4.0.8`\n- `conda-build=1.20.3`\n\nmay need to fix the version if conda-build not fixed soon\n",
    "labels": [
      "Build",
      "CI"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 2,
    "additions": 3,
    "deletions": 3,
    "changed_files_list": [
      "appveyor.yml",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13500,
    "reporter": "chris-b1",
    "created_at": "2016-06-23T13:42:17+00:00",
    "closed_at": "2016-08-31T13:13:55+00:00",
    "resolver": "chris-b1",
    "resolved_in": "8654a9ed3cc2246ef9eaf2fe8725369a2e885d35",
    "resolver_commit_num": 45,
    "title": "API: Expanded resample",
    "body": "Idea after seeing the gitter between @ssanderson  and @jorisvandenbossche \n\nConsider this data:\n\n\n\nRight now to resample by `MultiIndex` level `'d'` or column `'date'` it would be:\n\n\n\nWhat if we instead allowed this as a convenience?\n\n\n",
    "labels": [
      "Difficulty Novice",
      "API Design",
      "Resample",
      "Effort Medium"
    ],
    "comments": [
      "yes this should be a very easy add\n"
    ],
    "events": [
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 6,
    "additions": 221,
    "deletions": 54,
    "changed_files_list": [
      "doc/source/timeseries.rst",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13520,
    "reporter": "chris-b1",
    "created_at": "2016-06-27T14:19:57+00:00",
    "closed_at": "2016-07-19T13:15:22+00:00",
    "resolver": "chris-b1",
    "resolved_in": "4c9ae94f1ee4d867e2d92735d5755d43daef618d",
    "resolver_commit_num": 37,
    "title": "DOC: better warning on chained resample(...)",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\nno warning\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Resample",
      "Deprecate"
    ],
    "comments": [
      "Actually, this isn't spurious - what I meant to write is this.  I'll leave this open - I think the warning could be better, to make it more clear that the `mean` of the data is being taken.\n\n``` python\ndf.resample('M').last().pct_change()\n```\n",
      "Do you have a suggestion for a better warning? (PR's welcome :-))\n\n> `df.resample('M').last().pct_change()`\n\nIt's not fully clear what you mean with this\n",
      "I'll do a PR, what I want it to say is (in essence, hopefully in less words) - \n\n`.resample()` is a deferred operation.  You used that deferred object as if it were a {klass} - this materializes the deferred object by taking the mean() of each date group, which is deprecated behavior.  Update your code <...>\n\nWhat I was actually trying to do is take the month over month percent change - which is what `.last().pct_change()` does.  But I forgot the `last`, so it took the mean of month, then the `pct_change` of those means.\n",
      "yes this is mainly for back-compat.\n\nbasically in < 0.18.0 you should have been doing\n\n```\ndf.resample('M', how='mean').pct_change()\n```\n\nand so in >= 0.18.0 you implicity get this. It IS correct actually. I think the warning _could_ also include the deferred op reference (e.g. `pct_change()`) I suppose, BUT the warning actually comes out before.\n\nYou might have a look at `Resampler.plot()`.\n",
      "thanks!\n"
    ],
    "events": [
      "commented",
      "renamed",
      "commented",
      "labeled",
      "labeled",
      "commented",
      "closed",
      "reopened",
      "commented",
      "cross-referenced",
      "milestoned",
      "commented"
    ],
    "changed_files": 3,
    "additions": 39,
    "deletions": 22,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13535,
    "reporter": "frehoy",
    "created_at": "2016-06-30T12:06:29+00:00",
    "closed_at": "2016-07-19T01:59:54+00:00",
    "resolver": "bashtage",
    "resolved_in": "fafef5d91126d6a145f86f2ab4c4725039f3d739",
    "resolver_commit_num": 25,
    "title": "Document pandas.DataFrame.to_stata data_label",
    "body": "I work with Pandas and Stata and found the DataFrame.to_stata() method very valuable. I would like to be able to assign variable labels in my .dta files but the data_label parameter of the DataFrame.to_stata() method is not documented so I do not know in which format to supply my variable labels. \n\nI tried a dictionary of the form {'df column name' : 'wanted label'} but that returns \n\nTypeError: unhashable type: 'slice'\n\nHere is the page in the documentation I am referring to: -docs/stable/generated/pandas.DataFrame.to_stata.html#pandas.DataFrame.to_stata\n\nIf it could be updated to provide a working example of data_label I would be eternally grateful. This is my first issue on github, hope I specified it correctly, feel free to point out if I messed up the format somehow.  \n\nThanks!\n#### Code Sample, a copy-pastable example if possible\n\nimport pandas as pd\nd = {'one' : [1., 2., 3., 4.]}\ndf = pd.DataFrame(d)\nlabdict = {'one': 'foo'}\ndf.to_stata('test.dta', write_index=False, data_label=labdict)\n#### Expected Output\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.17.1\nnose: 1.3.7\npip: 8.1.1\nsetuptools: 19.6.2\nCython: 0.23.4\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: 0.6.1\nIPython: 4.0.3\nsphinx: 1.3.5\npatsy: 0.4.0\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.4.6\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.8.4\nlxml: 3.5.0\nbs4: 4.4.1\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.11\npymysql: None\npsycopg2: None\nJinja2: 2.8\n",
    "labels": [
      "Docs",
      "IO Stata",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "The release note adding it is [here](https://github.com/pydata/pandas/blob/1c49cafe3cce2e76bc484ca93fd9b774cbd4feb3/doc/source/whatsnew/v0.14.0.txt#L786)\n\n> `DataFrame.to_stata` and `StataWriter` will accept keyword arguments time_stamp\n>   and data_label which allow the time stamp and dataset label to be set when creating a\n>   file. (:issue:`6545`)\n\nSo it looks like a single label for the entire dataset, not one per column. Though I could be wrong as I have only used stata once.\n\ncc @bashtage who added this.\n",
      "Thanks Tom, yes that seems to be the case unfortunately. I would really like to be able to add labels for individual variables though. Should I open a feature request for that? \n",
      "Only dataset labels are implemented, not per variable labels. The dataset label in the dta file format is:\n\n```\n5.1.5  Dataset label\n\n\n    The dataset label is recorded as\n\n\n              <label>llccccc........c</label>\n                       |------------|\n                          ll bytes\n\n\n    Requirements:\n\n\n                ccc..c        Up to 80 UTF-8 characters.\n                              UTF-8 characters each require 1 to 4 bytes.\n                              No trailing \\0 is written.\n\n\n                ll            The byte length of the UTF-8 characters, \n                              whose length is recorded in a 2-byte unsigned \n                              integer encoded according to byteorder.\n\n\n                              Because ccc..c is allowed to contain up \n                              to 80 characters, 0 <= ll <= 4*80  \n                              (4*80 = 320 = 0x140).\n\n\n    If no characters are recorded (there is no data label), the .dta file\n    contains\n\n\n                <label>0000</label>\n\n\n    where 0000 represents 2 bytes of 0.\n```\n",
      "@frehoy This would need a feature request.  Should also document these two inputs.\n",
      "It seems that is is _almost_ implemented here.\n\nSee\nhttps://github.com/pydata/pandas/blob/master/pandas/io/stata.py#L2057\n\nand\n\nhttps://github.com/pydata/pandas/blob/master/pandas/io/stata.py#L2134\n\nAs you can see, this always passes the default `None` and so no labels are written.\n\nMostly needs an external interface and a tiny amount of wiring up.  And testing, esp that the labels can be read into Stata.\n",
      "Thanks @bashtage.\n\n@frehoy, are you interested in submitting a pull request for documenting `data_label`, or working on the per-variable labels? I'll make a second issue for the variable labels.\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 114,
    "deletions": 22,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/frame.py",
      "pandas/io/stata.py",
      "pandas/io/tests/test_stata.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13536,
    "reporter": "TomAugspurger",
    "created_at": "2016-06-30T13:14:56+00:00",
    "closed_at": "2016-07-19T01:59:54+00:00",
    "resolver": "bashtage",
    "resolved_in": "fafef5d91126d6a145f86f2ab4c4725039f3d739",
    "resolver_commit_num": 25,
    "title": "ENH: Support Stata variable labels",
    "body": "xref #issuecomment-229652201\n\ncc @frehoy\n\n> It seems that is is almost implemented here.\n> \n> See\n> #L2057\n> \n> and\n> \n> #L2134\n> \n> As you can see, this always passes the default None and so no labels are written.\n> \n> Mostly needs an external interface and a tiny amount of wiring up. And testing, esp that the labels can be read into Stata.\n\nA possible API\n\n\n",
    "labels": [
      "Enhancement",
      "IO Stata",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "mentioned",
      "subscribed",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 114,
    "deletions": 22,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/frame.py",
      "pandas/io/stata.py",
      "pandas/io/tests/test_stata.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13541,
    "reporter": "jreback",
    "created_at": "2016-07-01T11:47:48+00:00",
    "closed_at": "2016-07-03T23:23:08+00:00",
    "resolver": "gfyoung",
    "resolved_in": "ffb582c433dcb48cd06d219c21c68847cf19de32",
    "resolver_commit_num": 41,
    "title": "CLN: unused parameters in cum_* functions",
    "body": "xref #13167 \n\ncc @gfyoung \n",
    "labels": [
      "Clean"
    ],
    "comments": [
      "Can this be squeezed in for `0.18.2`?  I understand the release is coming up this month.\n",
      "yes\n"
    ],
    "events": [
      "labeled",
      "mentioned",
      "subscribed",
      "milestoned",
      "renamed",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 2,
    "additions": 20,
    "deletions": 4,
    "changed_files_list": [
      "pandas/compat/numpy/function.py",
      "pandas/core/generic.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13543,
    "reporter": "bashtage",
    "created_at": "2016-07-01T13:01:41+00:00",
    "closed_at": "2017-03-25T18:40:39+00:00",
    "resolver": "bashtage",
    "resolved_in": "80f30b44e3c79f26b20fada91995c1874c2e5cdf",
    "resolver_commit_num": 31,
    "title": "BUG: groupby.transform passing `Series` to transformation",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Comment\n\nThe `slow_path` operated series by series rather than on a group DataFrame.  Once the slow path is accepted, it operated on the group DataFrames.  I have 59 groups in my example with 8 columns, and so it runs 8 times with Series from the first group DataFrame and then, once happy, runs 58 more times on the DataFrames.\n\nThe description says that it onlly operated on the group DataFrames (which is the correct behavior IMO)\n#### Expected Output\n\nMany lines of\n\n\n#### Actual Output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Usage Question",
      "Groupby",
      "API Design",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "this is correct, it tries a series reduction first. not sure why this implementation detail actually matters.\n",
      "Because when the transformation makes use of DataFrame only attributes\ngroupby.transform produces an error.\n\nOn Fri, Jul 1, 2016, 2:04 PM Jeff Reback notifications@github.com wrote:\n\n> Closed #13543 https://github.com/pydata/pandas/issues/13543.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/pydata/pandas/issues/13543#event-710464452, or mute\n> the thread\n> https://github.com/notifications/unsubscribe/AFU5RcV9LjNXtj31MzXCRyxpDf-scxJdks5qRRA2gaJpZM4JDH31\n> .\n",
      "this doesn't matter as the exception is caught and next things are tried. you would have to show a compelling example.\n",
      "I will when I get back to the office. IMO there is a really simple fix -\nonly run slow path if fast path fails, rather than requiring slow path to\nwork.\n\nOn Fri, Jul 1, 2016, 2:22 PM Jeff Reback notifications@github.com wrote:\n\n> this doesn't matter as the exception is caught and next things are tried.\n> you would have to show a compelling example.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/pydata/pandas/issues/13543#issuecomment-229945104,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AFU5RWmVFVjGR1OlNLjgI3G20y08S2Kyks5qRRSWgaJpZM4JDH31\n> .\n",
      "could be\n",
      "Here is my real example -- I am trying to do a selective groupby-demeaning where onlly a subset of columns are demeaned, but the retruend DataFrame has all columns.  \n\n``` python\n# Setup\nnp.random.seed(12345)\npanel = pd.Panel(np.random.randn(125,200,10))\npanel.iloc[:,:,0] = np.round(panel.iloc[:,:,0])\npanel.iloc[:,:,1] = np.round(panel.iloc[:,:,1])\nx = panel\ncols = [0,1]\n\ndemean_cols = []\nfor df_col in _x:\n    if df_col not in cols and pd.core.common.is_numeric_dtype(_x[df_col].dtype):\n        demean_cols.append(df_col)\n\n# Function start\n_x = x.swapaxes(0, 2).to_frame()\ndef _safe_demean(df):\n    print(type(df))\n    df[demean_cols] -= df[demean_cols].mean(0)\n    return df\n\nindex = _x.index\n_x.index = pd.RangeIndex(0, _x.shape[0])\ngroups = _x.groupby(cols)\nout = groups.transform(_safe_demean)\nout.index = index\n```\n\nThe function fulfils the requrements in the docstring.  It fails when it gets a `Series` since `demean_cols` is not in the series index.  I can, of course, workaroudn with using an `isinstance` and handling paths, but this requires knowledge of the implementation.  So at least this is a doc-bug, but I think it is an actual bug in the sense that the described (and reasonably correct) behavior of applying a group-wise transform to a DataFrame does not work when the callable returns a DataFrame.  Another \"real world\" bug can be produced by using a group-wise grand demeaning, which would be a function like\n\n``` python\ndef grand_demean(df):\n    return df - df.mean().mean()\n```\n\nin this case, I don't think it is possible to ever get the correct answer in the current implementation since it isn't possible to compute the grand mean without the entire group DF.  Even a simpler method would produce incorrect numbers:\n\n``` python\ndef grand_demean_numpy(df):\n    return df - np.mean(df) #  np-mean is all elements\n```\n\nThe errors are\n\n``` python\n\nC:\\anaconda\\lib\\site-packages\\pandas\\core\\series.py in _set_labels(self, key, value)\n    806         if mask.any():\n--> 807             raise ValueError('%s not contained in the index' % str(key[mask]))\n    808         self._set_values(indexer, value)\n\nValueError: ('[2 3 4 5 6 7 8 9] not contained in the index', 'occurred at index 2')\n\nDuring handling of the above exception, another exception occurred:\n\nValueErrorTraceback (most recent call last)\n<ipython-input-143-07044111bf11> in <module>()\n     22 _x.index = pd.RangeIndex(0, _x.shape[0])\n     23 groups = _x.groupby(cols)\n---> 24 out = groups.transform(_safe_demean)\n     25 out.index = index\n     26 \n\nC:\\anaconda\\lib\\site-packages\\pandas\\core\\groupby.py in transform(self, func, *args, **kwargs)\n   3455                 result = getattr(self, func)(*args, **kwargs)\n   3456         else:\n-> 3457             return self._transform_general(func, *args, **kwargs)\n   3458 \n   3459         # a reduction transform\n\nC:\\anaconda\\lib\\site-packages\\pandas\\core\\groupby.py in _transform_general(self, func, *args, **kwargs)\n   3403                 except ValueError:\n   3404                     msg = 'transform must return a scalar value for each group'\n-> 3405                     raise ValueError(msg)\n   3406             else:\n   3407                 res = path(group)\n\nValueError: transform must return a scalar value for each group\n```\n",
      "FWIW the worksournd function to implement the selective group-wise demeaning looks like\n\n``` python\ndef _safe_demean(df):\n    if isinstance(df, pd.Series):\n        if df.name in demean_cols:\n            return df - df.mean()\n        else:\n            return df\n    df = df.copy()\n    df[demean_cols] -= df[demean_cols].mean(0)\n    return df\n```\n",
      "this is VERY inefficient and not idiomatic. It might technically fulfull the doc-string, but that should simply be fixed.\n\nyou are MUCH better off doing something like this:\n\n```\ndf - df.groupby(....).transform('mean')\n```\n",
      "its realated to this: https://github.com/pydata/pandas/issues/13281\n\ngroupby/transform is an immutable operation though its not technically marked as such. modification in the function should be banned (at least in the doc-string). If not actually banned (which is quite tricky to detect).\n",
      "I agree that the df should be considered immutable - my first example is poor (and in fact has terrible performance, a `.copy()` fixes that though).  \n\nI suppose a better docstring would highlight that\n- `transform` is only safe for within-variable transformations.  In particular the transform function must produce the same results if applied to the target df using `.apply` or directly.  This will make it clear that operations like a groupby-grand-demeaning aren't possible.\n- the function will be executed column-by-column.\n",
      "ok @bashtage if you want to do a better do-string (and maybe just turn off allowing mutation for transform), changing for apply would be too much ATM. Then I think that would be great.\n"
    ],
    "events": [
      "renamed",
      "commented",
      "closed",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "reopened",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 2,
    "additions": 48,
    "deletions": 5,
    "changed_files_list": [
      "doc/source/groupby.rst",
      "pandas/core/groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13563,
    "reporter": "jorisvandenbossche",
    "created_at": "2016-07-05T00:18:14+00:00",
    "closed_at": "2017-04-07T19:10:25+00:00",
    "resolver": "jreback",
    "resolved_in": "c25fbde09272f369f280212e5216441d5975687c",
    "resolver_commit_num": 4361,
    "title": "Deprecation of Panel ?",
    "body": "This is a topic that has come up recently (, , pandas-dev mailing list discussion), let's make this an issue to track the discussion about it.\n- #issuecomment-152613473 experience of the 'experimental' status of Panels by @MaximilianR and following discussion on pointing users to xarray\n- Issue for docs on using xarray instead of Panels: \n- \"WIP for transitioning from Panel docs\" PR \n\nDeprecating Panels would be a rather large change, so:\n- Do we need to further discuss if we actually want to do this?\n- Are there people who make intensive use of Panels to ask feedback?\n- How do we go about such a deprecation? First making a note in the whatsnew / pinging mailing list or other fora before actually deprecating?\n\ncc @pydata/pandas @MaximilianR \n",
    "labels": [
      "API Design",
      "Deprecate",
      "Needs Discussion",
      "Multi Dimensional",
      "Panel"
    ],
    "comments": [
      "I'm +1 on moving to xarray, but GitHub search shows the deprecation is not easy... As long as I know about popular packages, pydata/data-reader and quantopian/zipline uses `Panel`.\n\nCC @davidastephens @ehebert \n",
      "No change this end - we are still using xarray heavily, and it's working beautifully. We've also improved the integration of xarray & pandas, so that should ease the path to deprecation.\n",
      "I'm +1 on deprecating Panels; @jreback moved mountains to create a consistent internal object model from 1 to N dimensions, but there is still a feeling of second-class citizenry when it comes to working with data over 2 dimensions. I think we would be better served in the long run by really optimizing for the 1 and 2-dimensional use cases (similar to what the R community has done, though the API surface area of dplyr, data.table, and built-in data frames is quite a bit smaller than pandas -- primarily lacking in the level of indexing complexity).\n\nI maintain that we should plan for a pandas 0.X.Y long-term support LTS release branch that becomes bugfix only so that we can start investing in renovations. I'm interested in feedback from the other core devs how realistic you feel this is. \n\nI've long worried about the amount of baggage we are carrying forward -- there are many organizations with large codebases that have made their peace with pandas's rough edges (data type issues, view / copying semantics, etc.), and it doesn't make sense to abandon them. On the flip side, it would be a shame to be held back from undertaking a more aggressive cleanup and retool of the internals to introduce better performance, extensibility, missing data / data type issues, etc. I regret that 6 months have passed since I brought up this grand scheme and I haven't been able to carve out the time to make a dent, beyond demo'ing a proof-of-concept of integer NAs. Also, I would feel much better about working on this on a long-lived branch (similar to what happened with IPython) under some kind of feature freeze.\n\nAnyway, some of these comments are beyond the scope of this issue. I don't think we should deprecate Panel unless we're collectively on board to the idea of cleaning up pandas internals over the next 12-24 months (which is as much of a code organization problem as anything -- particularly quarantining unit tests that we are contemplating \"breaking\"). \n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "mentioned",
      "subscribed",
      "labeled",
      "cross-referenced",
      "mentioned",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "cross-referenced",
      "labeled",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 15,
    "additions": 3193,
    "deletions": 2724,
    "changed_files_list": [
      "doc/source/computation.rst",
      "doc/source/dsintro.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/panel.py",
      "pandas/core/window.py",
      "pandas/indexes/frozen.py",
      "pandas/indexes/multi.py",
      "pandas/tests/io/test_pytables.py",
      "pandas/tests/test_expressions.py",
      "pandas/tests/test_generic.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_window.py",
      "pandas/tests/tools/test_concat.py",
      "pandas/tests/types/test_missing.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13564,
    "reporter": "jorisvandenbossche",
    "created_at": "2016-07-05T00:23:38+00:00",
    "closed_at": "2016-07-25T23:34:18+00:00",
    "resolver": "jreback",
    "resolved_in": "309e1fef435dc8bc04ec84a0f3dff6024bb88879",
    "resolver_commit_num": 4061,
    "title": "Deprecation of Panel4D/PanelND",
    "body": "Related to  (deprecation of Panels), we could/should also deprecate Panel4D / PanelND. \n(xref )\n\nDeprecating Panel4D/ND could probably be done sooner than deprecating Panels (e.g. actually deprecating in 0.19.0 ?)\n",
    "labels": [
      "Multi Dimensional",
      "Deprecate",
      "Needs Discussion"
    ],
    "comments": [
      "I'm very in favor of this. I will leave some comments on #13563 \n",
      "Let's do this in 0.19.0 then\n"
    ],
    "events": [
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "milestoned",
      "demilestoned",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 17,
    "additions": 992,
    "deletions": 1043,
    "changed_files_list": [
      "doc/source/dsintro.rst",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/api/tests/test_api.py",
      "pandas/core/common.py",
      "pandas/core/panel4d.py",
      "pandas/core/panelnd.py",
      "pandas/io/tests/test_packers.py",
      "pandas/io/tests/test_pytables.py",
      "pandas/sparse/panel.py",
      "pandas/sparse/tests/test_panel.py",
      "pandas/tests/test_generic.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_panel4d.py",
      "pandas/tests/test_panelnd.py",
      "pandas/tests/types/test_missing.py",
      "pandas/tools/tests/test_concat.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13566,
    "reporter": "matthew-brett",
    "created_at": "2016-07-05T11:29:17+00:00",
    "closed_at": "2016-07-09T18:16:40+00:00",
    "resolver": "jreback",
    "resolved_in": "5701c69369264f3aa6f571384602ceec1133dabc",
    "resolver_commit_num": 4053,
    "title": "Test failures on 32-bit Linux",
    "body": "Testing 32-bit Manylinux1 wheels for Pandas 0.18.1 gives the following errors:\n\n\n\n-ci.org/MacPython/pandas-wheels/jobs/142424407\n",
    "labels": [
      "Numeric",
      "CI"
    ],
    "comments": [
      "Failures only on Python 2.7 32-bit builds : https://travis-ci.org/MacPython/pandas-wheels/builds/142424402\n",
      "yeah these are cases where we need to use platform_int to do things. I wonder how much 32-bit is used anymore. maybe we should drop support for 32-bit linux. (as we don't normally test this).\n\ncc @jorisvandenbossche @TomAugspurger \n",
      "I still have a linux 32 bit (although more by mistake :-) and it's only because I will have a new laptop shortly that I didn't change it anymore). But in any case, I can take a look at those failures\n\nRegarding supporting 32bit, are there some data points? \nAny ideas about usage? Untill shortly, eg Ubuntu did still provide 32bit downloads by default.\nAre there other packages not supporting this anymore?\nFor example, `conda-forge` does not build packages for linux 32bit (but does for windows)\n",
      "http://www.lfd.uci.edu/~gohlke/pythonlibs/#pandas\n\ndoes build 32 bit for Windows\n\nbut no reason at all to support this going forward \nit's just extra testing\n"
    ],
    "events": [
      "cross-referenced",
      "commented",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 13,
    "additions": 104,
    "deletions": 82,
    "changed_files_list": [
      "pandas/core/internals.py",
      "pandas/tests/indexes/test_datetimelike.py",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tests/series/test_analytics.py",
      "pandas/tests/test_algos.py",
      "pandas/tests/test_categorical.py",
      "pandas/tests/test_groupby.py",
      "pandas/tools/merge.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tools/tests/test_tile.py",
      "pandas/tseries/tests/test_base.py",
      "pandas/tseries/tests/test_timedeltas.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13577,
    "reporter": "mhaseebtariq",
    "created_at": "2016-07-07T09:04:06+00:00",
    "closed_at": "2016-08-18T10:49:30+00:00",
    "resolver": "mhaseebtariq",
    "resolved_in": "1919e26ead5d156c2b505a0ad8d233b02eb1b573",
    "resolver_commit_num": 0,
    "title": "GbqConnector should be able to fetch default credentials on Google Compute Engine",
    "body": "\n\n\n\nGoogle Compute Engine and Google Dataproc etc. already have default application credentials on them. Therefore, there is no need of running `OAuth2WebServerFlow` while the method `get_user_account_credentials` is being called - instead `return GoogleCredentials.get_application_default()` is all you need. This change will allow you to run `GbqConnector` on commandline-only systems on Google Compute Engine without a need of a service account json key.\n",
    "labels": [
      "Google I/O"
    ],
    "comments": [
      "cc @parthea @aaront @jacobschaer\n",
      "Fix for this: https://github.com/pydata/pandas/pull/13608\n"
    ],
    "events": [
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "milestoned"
    ],
    "changed_files": 4,
    "additions": 124,
    "deletions": 9,
    "changed_files_list": [
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/io/gbq.py",
      "pandas/io/tests/test_gbq.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13582,
    "reporter": "tjader",
    "created_at": "2016-07-07T22:02:16+00:00",
    "closed_at": "2016-07-15T00:30:34+00:00",
    "resolver": "sinhrks",
    "resolved_in": "0a70b5fef3ae2363fea040ea47dd52247811c8c8",
    "resolver_commit_num": 335,
    "title": "Checking for NaT in PeriodIndex doesn't work with None, pd.NaT",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n\nIncorrect behavior for `pd.NaT, None, float('nan'), np.nan`\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Missing-data",
      "Period"
    ],
    "comments": [
      "xref #12759\n"
    ],
    "events": [
      "cross-referenced",
      "labeled",
      "labeled",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "milestoned"
    ],
    "changed_files": 8,
    "additions": 407,
    "deletions": 302,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/src/period.pyx",
      "pandas/tests/indexes/test_datetimelike.py",
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_base.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_tslib.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13598,
    "reporter": "toobaz",
    "created_at": "2016-07-09T15:22:08+00:00",
    "closed_at": "2016-07-19T01:11:18+00:00",
    "resolver": "wcwagner",
    "resolved_in": "5a521713f3892539b648bc2735d3cc502feb2b48",
    "resolver_commit_num": 0,
    "title": "Series.str.zfill() doesn't check type",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\nA `ValueError`.\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Bug",
      "Error Reporting",
      "Strings",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "yep that's an error\n",
      "Rather than adding type check logics on everywhere, adding a dummy call (like `\" \".zfill(width)` may be easier.\n",
      "not sure about that. Most strings can only accept string-likes, OR integers. Having a couple of common routines might help readablity.\n\n`_assure_string`, `_assure_integer`\n",
      "@jreback I'd like to fix this one as well. Can I go ahead and put a check in `zfill` or should I create separate routine as you mentioned?\n",
      "I think make a separate check\n",
      "Is there any place where such common routines are kept? If not, where should I keep this one?\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 16,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/strings.py",
      "pandas/tests/test_strings.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13599,
    "reporter": "toobaz",
    "created_at": "2016-07-09T15:24:31+00:00",
    "closed_at": "2016-07-20T22:25:22+00:00",
    "resolver": "sahildua2305",
    "resolved_in": "1ce8f8e0b8540252dac25497f29d4de66a8bea3f",
    "resolver_commit_num": 0,
    "title": "MultiIndex.from_arrays() does not check lenghts",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\nThe first previous command should already raise an error.\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Error Reporting",
      "MultiIndex",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "@jreback Hi, I'd like to contribute to this library. Can you please help me in starting with it as well as tagging me in some task perfect for a beginner?\n\nThanks! :smile: \n",
      "@sahildua2305 see contributing docs [here](http://pandas.pydata.org/pandas-docs/stable/contributing.html)\n\nthings tagged a 'Difficulty Novice' are generally pretty straightforward (like this issue)\n",
      "@jreback Thanks for pointers!\n\nHowever [this](http://pandas.pydata.org/pandas-docs/stable/contributing.html) link doesn't work for me. Can you please check once?\n",
      "seems ok to me.\n",
      "Accessible now. Thanks once again! \n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 24,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/indexes/multi.py",
      "pandas/tests/indexes/test_multi.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13603,
    "reporter": "tjader",
    "created_at": "2016-07-09T21:57:45+00:00",
    "closed_at": "2016-07-19T01:14:14+00:00",
    "resolver": "yui-knk",
    "resolved_in": "9f635cd74316d26110809bf1bb2a5525ac4d23fe",
    "resolver_commit_num": 6,
    "title": "Checking for any NaT-like objects in a TimedeltaIndex always returns True",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n\nReturns `True` for any one of `[pd.NaT, None, float('nan'), np.nan]`\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Bug",
      "Indexing",
      "Timedelta"
    ],
    "comments": [],
    "events": [
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 25,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/tseries/tdi.py",
      "pandas/tseries/tests/test_timedeltas.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13613,
    "reporter": "jorisvandenbossche",
    "created_at": "2016-07-11T08:16:12+00:00",
    "closed_at": "2016-08-15T22:27:20+00:00",
    "resolver": "gfyoung",
    "resolved_in": "8b50d8c854196e3025c9a881cafeedc5f509aaef",
    "resolver_commit_num": 68,
    "title": "BUG: to_timedelta still raises with errors='ignore'",
    "body": "From the added docs in \n\n\n\nIt's the same error as you get with `errors='raise'` (so not captured somewhere), `errors='coerce` is working correctly.\n",
    "labels": [
      "Bug",
      "Timedelta"
    ],
    "comments": [
      "The reason is because `errors='ignore'`, is well, ignored.  I think I would just wrap the current `try-except` logic as follows:\n\n``` python\ntry:\n    try:\n        for i in range(n):\n            result[i] = parse_timedelta_string(values[i])  # always raise\n    except Exception:\n        for i in range(n):\n            result[i] = convert_to_timedelta64(values[i], unit, is_coerce)\nexcept Exception:  # when is_coerce=False\n    if errors == 'raise':\n        raise\n   elif errors == 'ignore':\n        return values\nreturn iresult\n```\n\nThe reason is that one could consider the original `try-except` block as the process through which we convert to `timedelta`, then all three options for `errors` are satisfied:\n\n1) `errors='raise'`\n\nTry to convert via method 1, which fails, so we try with method 2.  If that fails, we raise the exception.\n\n2) `errors='coerce'` --> `is_coerce=True`\n\nTry to convert via method 1, which fails, so we try with method 2.  In method 2, if converting one of the elements fails, we return `NPY_NAT`.\n\n3) `errors='ignore'`\n\nTry to convert via method 1, which fails, so we try with method 2.  If that fails, we just return the original `values` array as promised in documentation.\n",
      "yeah this prob needs some logic like that\n\nthis is similar to how datetime parsing works\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "commented",
      "milestoned",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 7,
    "additions": 146,
    "deletions": 108,
    "changed_files_list": [
      "asv_bench/benchmarks/timedelta.py",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/src/inference.pyx",
      "pandas/tseries/tests/test_timedeltas.py",
      "pandas/tseries/timedeltas.py",
      "pandas/tslib.pxd",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13615,
    "reporter": "RoyalTS",
    "created_at": "2016-07-11T10:53:24+00:00",
    "closed_at": "2016-08-03T10:30:30+00:00",
    "resolver": "parthea",
    "resolved_in": "97de42abbea588fc5a46be5c58788958fd817b7f",
    "resolver_commit_num": 8,
    "title": "Add option to use Standard SQL dialect in BigQuery",
    "body": "BigQuery now sports a SQL dialect that's compatible with the 2011 SQL standard. Queries are still run using BigQuery's Legacy SQL dialect at the moment (see -reference/enabling-standard-sql) but they may soon switch to Standard SQL by default. `pandas.io.gbq.read_gbq()` should therefore have an argument that determines the SQL dialect that's used to run the query.\n",
    "labels": [
      "IO Google"
    ],
    "comments": [
      "cc @parthea @aaront @jacobschaer\n"
    ],
    "events": [
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned"
    ],
    "changed_files": 4,
    "additions": 78,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/io/gbq.py",
      "pandas/io/tests/test_gbq.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13618,
    "reporter": "jorisvandenbossche",
    "created_at": "2016-07-11T14:04:18+00:00",
    "closed_at": "2016-07-19T13:15:22+00:00",
    "resolver": "chris-b1",
    "resolved_in": "4c9ae94f1ee4d867e2d92735d5755d43daef618d",
    "resolver_commit_num": 37,
    "title": "Resampler: IPython 5.0 triggers wrong FutureWarning",
    "body": "With the latest IPython:\n\n\n\nwhile using IPython 4.2:\n\n\n",
    "labels": [
      "Output-Formatting",
      "Resample"
    ],
    "comments": [
      "@takluyver This seems to be related to https://github.com/ipython/ipython/pull/9289 (previously, it was first checked if `method` is a callable, and only then it was checked if it could retrieve a non-existing attribute, while now the callable check is moved to the end of `get_real_method`). \nThe problem is that the deprecation warning is only triggered once you \"do\" something with it, and now it is triggered by displaying its repr \n",
      "That's unfortunate. But if our check for the canary attribute triggers this, won't our checks for `_repr_pretty_`, `_repr_json_`, etc. also trigger it?\n",
      "Apparently not. Where are those checks in the IPython code?\n\nIt does trigger the warning if I manually do `getattr(r, '_repr_json_')`, but this does check does not then not seem to occur when displaying an object.\n",
      "`IPython.core.formatters` - they're somewhat hidden in infrastructure, but if you search for `print_method` in that module, it should be possible to work out what's going on.\n",
      "Ah, I was in a terminal, so I suppose there only the plain text displayer will be used? \nIf I try it in a notebook, but using IPython 4.2, the same issue already shows up\n",
      "I thought the same machinery would check for `_repr_pretty_` even in the terminal, but maybe it's done a bit differently.\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned"
    ],
    "changed_files": 3,
    "additions": 39,
    "deletions": 22,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13634,
    "reporter": "jreback",
    "created_at": "2016-07-13T02:01:01+00:00",
    "closed_at": "2017-04-27T21:32:11+00:00",
    "resolver": "jreback",
    "resolved_in": "c8dafb5a7ae9fe42b9d15c47082a6fb139e78b5d",
    "resolver_commit_num": 4392,
    "title": "API: pandas.api",
    "body": "In xref #13147\n\nestablished a bit of a public api in `pandas.api`; ATM this only contains the type introspection routines.\n## 1st question\n\nSome disagreement on whether we should not do this, and rather just expose `pandas.types` directly.\n\nI think `pandas.api` is actually very important because:\n\n1) limits scope of what we choose to expose in the future; its not simply 'everything' that isnt nailed down (the case now). This does change pandas to make everything by default private, EXCEPT what is explicitly public.\n\n2) a single entry point for other packages to use the pandas public API that is known / stable / maintained (will with c-API as well)\n\n3) provide consistent naming externally (and allow us to fix / hide internally as needed)\n\n4) namespaced. I only import _what_ I need as a user / other package, rather than _everything_\n## ~~2nd question~~\n\n as discussed [here](#discussion_r70594911), should these deprecated API functions should be `DeprecationWrarning` rather than `FutureWarning`?\n-> done in \n\nIdeally we should resolve this before the end of 0.19.0 (or remove it).\n",
    "labels": [
      "API Design",
      "Needs Discussion"
    ],
    "comments": [
      "@jorisvandenbossche @shoyer @wesm @TomAugspurger @sinhrks \n",
      "@jorisvandenbossche updated the top section.\n",
      "We have discussed this on some several places, recently also in the PR about the `errors` module (https://github.com/pandas-dev/pandas/pull/15541) and previously when `pandas.types` was created (https://github.com/pandas-dev/pandas/pull/13147#issuecomment-220199044 and comments below). \r\nFor the exceptions, we now have a top-level `pandas.errors` module, for type-related we have a private `pandas.types` and public `pandas.api.types`.\r\n\r\nWith the risk of keeping a settled discussion alive, I would still like to see this discussed some more.\r\n\r\nTo repeat the comment of @shoyer:\r\n\r\n> Ultimately, I don't think this matters too much, but given a choice between:\r\n> 1. `pandas.*` indicates private namespace, `pandas.api.*` includes a public namespace, and\r\n> 2. `pandas.*` includes public namespace, `pandas._internals` (and so forth) indicates private namespace\r\n>\r\n> I think the later (option 2) is more user friendly and consistent with the top level namespace `pandas` already being public facing. \r\n>\r\n> On the other hand, it's true that option 2 is a little harder to transition to (for us), so maybe it's not worth the trouble.\r\n\r\nWe have recently added new submodules to the top-level namespace (`pandas.types`, `pandas.indexes`, `pandas.formats`, code that was before mainly in `pandas.core. ..`), but towards the user these should be regarded as private. IMO this is not the good direction. For new code / refactors, I think we should stick to: only what is public can go in the top-level namespace. \r\n\r\nLet's take the example of `types`. We now have a private `pandas.types` and a public `pandas.api.types`, so the less nested location is private, which I don't find ideal. \r\nIf we use the approach of `pandas.api.types`, I think we should at least make the actual implementation in `pandas._types`, so you don't have the two publicly visible locations. \r\nThe other option is to make put now what is in `pandas.api.types`, in `pandas.types` (while the exact implementation and organization in the different submodules of `pandas.types` remains a private implementation detail).\r\n",
      "Here's my rationale:\r\n\r\nWe have 3 stakeholders.\r\n\r\n- users: regular folks who do ``import pandas as pd``\r\n- library builders / advanced users: folks who need more 'library' code\r\n- implementation\r\n\r\n``implementation``: this may seem strange to mention, but *pandas* is a large library. We need the flexibility to move code around, expand modules, rename and generally get stuff done internally, with out having to constantly deprecate things. This would be extremely disruptive to the public (not to mention burdensome).\r\n\r\nFor example. ``pandas.types`` exists with several sub-modules that are logically defined, simply named and concise. There is not reason a for 'users' or 'library' writers to access this at all. It exists simply to organize the code. Sure they can reach in if they really want, but we want to discourage this. Making it ``pandas._types`` would surely do this, but that is an ugly name and since this is pure python code a bit misleading.\r\n\r\n'users': they are served by the top-level ``pd`` namespace, supplemented by ``pandas.errors``. Everything that one could need is here (or as a method).\r\n\r\n'library': these folks need code generally for instrospection that is just too cumbersome to be in a 'user' namespace. Things like ``is_integer_dtype`` is a canonicial way to introspect things in pandas. We *use* these internally and provide them as an external API to library writers.\r\n\r\nI recently added ``pandas.api.lib`` with ``infer_dtype``, which is a useful routine, again to external library writers. This is (or another sub-module of ``pandas.api``) can also be the home of a c-api / development api.\r\n\r\nSo, everything as public is just fine. If someone wants to reach in and use a routine from ``pandas.format.*`` or whatever they should have no expectations that this is stable across releases.\r\n\r\nFor simplicity from a user AND a library pov, its much better to have a single namespace ``pandas.api`` that say: hey they routines are what I should use, they are documented and won't change\r\n",
      "I don't see a strong division between \"users\" and \"advanced users\". There is a continuum of use cases. Certainly we should group more things into submodules: this makes it easier to find related functionality.\r\n\r\nOne choice that would make this super clear is move all internal stuff into a top level submodule called `internals`, e.g., `pandas.internals.core`, `pandas.internals.indexes`, etc. Then if you're importing from `pandas.internals.something`, you know it's an internal routine. Otherwise, as a user it's not obvious whether `pandas.something` exists to as a namespace to logically group together the `something` routines or if it's an internal implementation detail.",
      "> For example. pandas.types exists with several sub-modules that are logically defined, simply named and concise. There is not reason a for 'users' or 'library' writers to access this at all. It exists simply to organize the code. \r\n\r\nThat is not fully what I meant. The `pandas.types` module could still have different sub-modules, but what I meant was that only the 'top-level' of `pandas.types` would be public, so we are still free to implement it as we like in the submodules. \r\nAnd sure, we will change things in the future, but whether we expose the public names in `pandas.api.types` or in `pandas.types` (top-level, not submodules) does not really matter for that? In both cases the functions will not actually be defined in that location.\r\n\r\n> Making it pandas._types would surely do this, but that is an ugly name and since this is pure python code a bit misleading.\r\n\r\nThis is IMO not misleading at all, it is actually very explicit that it is internal. \r\nAnd I agree it is a bit uglier, but is only about a few import lines in each pandas file. I don't think it is that problematic there is a bit ugliness there, and IMO worth the clarity (if we use the `pandas.api` approach). \r\nThe `internals` namespace is also a possiblity, but this is actually more or less what `core` is currently. So we could use it for that I think (although less explicit in name). But we moved `types`, `formats`, `indexes` out of core .., so this would be putting it back (but keeping the reorganization in submodules). \r\n\r\n> 'users': they are served by the top-level pd namespace, supplemented by pandas.errors. Everything that one could need is here (or as a method).\r\n\r\nThis is already not fully the case. There are public methods in `pandas.io.json`, `pandas.tseries.offsets`, `pandas.plotting` (when that PR is merged), ..\r\n\r\n> I recently added pandas.api.lib with infer_dtype, \r\n\r\nSlightly of topic, but I personally would rather put this in `pandas.types`, as it is type related. And the fact that it is in our `lib` is more an implemenation detail for the user.",
      "ok so plan is to:\r\n\r\n- document that ``pandas.core`` is private\r\n- potentially search on github to see if we should be deprecating things more explicity\r\n- move out of ``pandas`` to ``pandas.core`` namspace\r\n  - ``computation``\r\n  - ``formats``\r\n  - ``indexes``\r\n  - ``sparse`` \r\n  - ``tools``\r\n  - ``tseries``\r\n  - ``types``\r\n  - ``util`` (maybe)?\r\n\r\n- going to leave ``stats`` (as when we remove the deprecations for things like ``pd.rolling_mean`` this will go away anyhow\r\n\r\ncc @wesm \r\n@jorisvandenbossche \r\n@TomAugspurger ",
      "I started some search on github, but most of the things on the first pages were just internal imports in people who embedded the full pandas codebase in their repo ... Does anybody know of more advanced github search methods for such things?",
      ">people who embedded the full pandas codebase in their repo \r\n\r\nwho the *heck* does that????",
      "Regarding the above list, I am not sure we can easily move `tseries`, there are too many public functions in that module (offsets and frequencies). So we should think how to organize that one.\r\n\r\nFor `tools`, I would take the opportunity to reorganize that a bit. For example add a `reshaping` submodule for stack/unstack/pivot etc functionality? (just an idea, the current 'tools' name is not very descriptive)",
      "> I am shortly going to move a bunch of things around. But pandas.io is going to stay (pandas.formats will move).\r\n\r\nCan you maybe first give some more details here, so we can discuss some things first before you do the work? (I also have some time this weekend to look at it)\r\n\r\nSee my questions above (tseries). Further, `formats` could maybe be moved to `io` instead of `core` ? \r\nRegarding `util` (your questionmark), I  would maybe leave `util` where it is (that feels more logically), but then state that this is also private (together with core) ?",
      "quickly did #15997 (stil WIP), moving ``pandas.formats`` -> ``pandas.core.formats``. (will rebase after @TomAugspurger #15954 \r\n\r\nI think easy to move ``.computation``, ``.types``. Let's do that (I can provide a proxy deprecation module if needed). Then go from there.\r\n\r\nFurther we will then have a big warning somewhere that ``pandas.core`` is now considered private.",
      "``.util`` yes could leave there where it is.\r\n\r\nI think we should also state that ``.tseries`` is private as well.\r\n\r\n``.tools`` I will also move to ``.core``\r\n\r\n``.formats`` belongs in ``.core`` its about printing core stuff (not *really* about io per se).",
      "> formats belongs in .core its about printing core stuff (not really about io per se).\r\n\r\nBut if we expose certain aspects of it in `io` or `io.formats`, it seems logical to just move it there. It is about printing core objects, yes, but the line between output and repr is not that clear (to_html, to_string are basically used for both)",
      "could move to io just as well. ok, i'll do that after @TomAugspurger merges then.\r\n\r\nany problem with the others that I mentioned?",
      "> I think we should also state that .tseries is private as well.\r\n\r\nThen we first need a assessment on what exactly is public in tseries (as there certainly are public objects there) and how we expose this.",
      "> Then we first need a assessment on what exactly is public in tseries (as there certainly are public objects there) and how we expose this.\r\n\r\nthere is *nothing* that is explicity public. people reach in.",
      "> there is nothing that is explicity public. people reach in.\r\n\r\nFrequencies and offsets are *explicitly* imported from there and used in our docs (although that is probably everything from tseries in our docs)",
      "``pandas.offsets`` is a thing, maybe we should make ``pandas.frequencies`` as well.",
      "> pandas.offsets is a thing, maybe we should make pandas.frequencies as well.\r\n\r\nWe can probably combine both? (the distinction is not always clear) \r\n(I thought we deprecated pandas.offsets, but apparently I am wrong, and of course could have un-deprecated it :-))",
      "Here are the interesting uses of `tseries.*` in the docs:\r\n\r\n\r\n```\r\nsource/timedeltas.rst:18:   from pandas.tseries.offsets import *\r\nsource/timeseries.rst:735:   from pandas.tseries.offsets import *\r\nsource/timeseries.rst:878:    from pandas.tseries.offsets import CustomBusinessDay\r\nsource/timeseries.rst:902:    from pandas.tseries.holiday import USFederalHolidayCalendar\r\nsource/timeseries.rst:917:    from pandas.tseries.offsets import CustomBusinessMonthBegin\r\nsource/timeseries.rst:1047:    from pandas.tseries.holiday import USFederalHolidayCalendar\r\nsource/timeseries.rst:1252:    from pandas.tseries.holiday import Holiday, USMemorialDay,\\\r\nsource/timeseries.rst:1272:    from pandas.tseries.offsets import CDay\r\nsource/timeseries.rst:1306:    from pandas.tseries.holiday import get_calendar, HolidayCalendarFactory,\\\r\nsource/timeseries.rst:1513:    from pandas.tseries.frequencies import to_offset\r\n```\r\n\r\nSo I think `holiday` in addition to offsets and frequencies.",
      "I actually don't have a problem with ``pandas.tseries`` per-se, its everything underneath it!\r\n\r\n(which I *will* move (mostly to ``core``). So we *could* keep the namespace (and just move everything out and import what is needed in ``__init__``).",
      "maybe we leave those 3 modules, e.g. ``offsets, holiday, frequncies``? (they do make logical sense there).",
      "We need to move ``pandas.indexes`` as well.",
      "So to summarize the *public* parts other than stuff imported into the top-level namespace (feel free to edit / move into the top post)\r\n\r\n- pandas.api\r\n- pandas.errors\r\n- pandas.io.api\r\n- pandas.stats (but deprecated)\r\n- pandas.tools.plotting\r\n- pandas.tseries.offsets (or are these 3 .tseries modules going to move?)\r\n- pandas.tseries.holiday\r\n- pandas.tseries.frequencies\r\n- pandas.util (but documented as private?)\r\n\r\nIs there anything else public in pandas.tools? (hashing?)\r\n\r\nI think `pandas.util.testing` has to be public right?\r\nAnything in that list incorrect?",
      "``pandas.util.testing``, maybe move to ``pandas.testing`` ? (though I suspect this is imported in the outside world a lot!)",
      "> is there anything else public in pandas.tools? (hashing?)\r\n\r\nyeah maybe I should move ``hashing`` -> ``pandas.util``\r\n\r\n",
      "> pandas.util.testing, maybe move to pandas.testing ?\r\n\r\nI would go for `pandas.testing`, see https://github.com/pandas-dev/pandas/issues/9895. Will do a PR for that.",
      "@jorisvandenbossche cool. will need a depr_module for ``pandas.util.testing`` as well I think.",
      "Unless we leave it like that? If we leave util in place, we will still use `pandas.util.testing` internally, and we could opt for not deprecating it but just documenting the public ones?\r\n\r\nThe PR I am doing now is just exposing a selection of `pandas.utils.testing` in `pandas.testing`.\r\n\r\nCould also move everything to `pandas.testing`, but then it is more difficult to only have a subset of the testing functions public.",
      "that sound pretty reasonable ",
      "\r\nleaving\r\n- ``util``: leave for now I think\r\n- ``stats``: deprecated and will be removed in next version\r\n\r\nmoving\r\n- ``tools``:\r\n- ``indexes``\r\n- parts of ``tseries``: this is a bit tricky, but will move some of this (leaving those we discussed above).",
      "I just saw that `pandas.tools` is not yet fully removed (apart from the compat shims) due to `hashing`. It's a bit pity to have to keep that submodule just for hashing, so maybe we can move that to `pandas.core.tools` or to `pandas.utils` ?",
      "Ah, I just see @jreback mentioned this above:\r\n\r\n> yeah maybe I should move hashing -> pandas.util\r\n\r\n+1 then\r\n\r\nAre the hashing functions considered as public? Dask uses them, so maybe we should then? \r\nI also think they are only added in 0.20dev, so we can change location without deprecation?",
      "these are used by dask. They are semi-public. yes can move I think. \r\n\r\n``pandas.util.hashing`` ?",
      "Ah, it seems I am mistaken. Those were already added in 0.19.2? (but not mentioned in whatsnew)",
      "yeah they are using in a private API (they do fallback though). This was never 'public'",
      "So options: a) keep them as they are, b) move to util (see PR) but keep as semi-public, c) move to util and document as public function?",
      "I think b) is fine for now. These should really be internal with a couple of things exposed via ``pandas.api.lib`` (e.g. just ``hash_pandas_object`` and ``hash_array``), so could do that as well, but b) allows us to do that *later*",
      "(using this issue as the general public/private api discussion issue).\r\n\r\nIn the PR in dask to be compatible with 0.20.0rc1 (https://github.com/dask/dask/pull/2249), they had to change imports for:\r\n\r\n- `pandas.computation.expressions.set_use_numexpr` (now in `core`)\r\n- `pandas.formats.printing.pprint_thing` (now in `io.formats`)\r\n- `pandas.algos.groupsort_indexer` (now in `_libs.algos`)\r\n- `pandas.tseries.resample.Resampler` (now in `core.resample`)\r\n- (`pandas.formats.format._put_lines` -> only in testing)\r\n\r\n(and further some of the testing functionality which they changed to using directly pytest (what we are doing as well))\r\n\r\nWould we want to expose some of those in a public location? So they can use it more safely ? Or do we keep those private (and usage \"on own risk\")",
      "I am all for making deprecations, but we should be very narrow in the scope. non-common things *should* break. We cannot willy nilly just deprecate everything that was moved otherwise noone will ever pay attention until we actually remove it and that's essentially the same problem. better to provide targeted fine grained deprecation warnings.",
      "*dask* IS using private API's (for perf), so I think its ok to specfically change things for them. IOW no guarantees if you update pandas but NOT dask.",
      "This is something to be said for supporting a low-level API for performance sensitive tasks (NumPy does this pretty nicely, for example).\r\n\r\nThat said, most of these specifics that dask was using feel like implementation details.\r\n\r\nProbably `set_use_numexpr` at least should be public API, via the options interface.",
      "closing, but if anyone finds another place that thinks needs deprecation pls comment / open a new issue."
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "assigned",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "commented",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "closed",
      "referenced",
      "reopened",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "closed",
      "reopened",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 78,
    "additions": 727,
    "deletions": 697,
    "changed_files_list": [
      "doc/source/api.rst",
      "pandas/_libs/period.pyx",
      "pandas/_libs/tslib.pyx",
      "pandas/compat/pickle_compat.py",
      "pandas/core/api.py",
      "pandas/core/computation/pytables.py",
      "pandas/core/datetools.py",
      "pandas/core/dtypes/cast.py",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/indexes/accessors.py",
      "pandas/core/indexes/api.py",
      "pandas/core/indexes/base.py",
      "pandas/core/indexes/datetimelike.py",
      "pandas/core/indexes/datetimes.py",
      "pandas/core/indexes/period.py",
      "pandas/core/indexes/timedeltas.py",
      "pandas/core/internals.py",
      "pandas/core/ops.py",
      "pandas/core/resample.py",
      "pandas/core/series.py",
      "pandas/core/tools/__init__.py",
      "pandas/core/tools/datetimes.py",
      "pandas/core/tools/numeric.py",
      "pandas/core/tools/timedeltas.py",
      "pandas/io/excel.py",
      "pandas/io/formats/format.py",
      "pandas/io/parsers.py",
      "pandas/io/sql.py",
      "pandas/plotting/_converter.py",
      "pandas/plotting/_core.py",
      "pandas/plotting/_timeseries.py",
      "pandas/tests/dtypes/test_cast.py",
      "pandas/tests/frame/test_alter_axes.py",
      "pandas/tests/frame/test_analytics.py",
      "pandas/tests/frame/test_timeseries.py",
      "pandas/tests/groupby/test_groupby.py",
      "pandas/tests/groupby/test_timegrouper.py",
      "pandas/tests/indexes/common.py",
      "pandas/tests/indexes/datetimes/test_date_range.py",
      "pandas/tests/indexes/datetimes/test_ops.py",
      "pandas/tests/indexes/datetimes/test_setops.py",
      "pandas/tests/indexes/datetimes/test_tools.py",
      "pandas/tests/indexes/period/test_construction.py",
      "pandas/tests/indexes/period/test_ops.py",
      "pandas/tests/indexes/period/test_setops.py",
      "pandas/tests/indexes/period/test_tools.py",
      "pandas/tests/indexes/test_base.py",
      "pandas/tests/io/json/test_ujson.py",
      "pandas/tests/io/parser/parse_dates.py",
      "pandas/tests/io/test_sql.py",
      "pandas/tests/plotting/test_datetimelike.py",
      "pandas/tests/reshape/test_concat.py",
      "pandas/tests/scalar/test_period.py",
      "pandas/tests/scalar/test_timedelta.py",
      "pandas/tests/series/test_analytics.py",
      "pandas/tests/series/test_api.py",
      "pandas/tests/series/test_combine_concat.py",
      "pandas/tests/series/test_constructors.py",
      "pandas/tests/series/test_datetime_values.py",
      "pandas/tests/series/test_internals.py",
      "pandas/tests/series/test_operators.py",
      "pandas/tests/series/test_period.py",
      "pandas/tests/series/test_quantile.py",
      "pandas/tests/series/test_timeseries.py",
      "pandas/tests/sparse/test_frame.py",
      "pandas/tests/test_base.py",
      "pandas/tests/test_categorical.py",
      "pandas/tests/test_resample.py",
      "pandas/tests/tools/__init__.py",
      "pandas/tests/tools/test_numeric.py",
      "pandas/tests/tseries/test_frequencies.py",
      "pandas/tests/tseries/test_offsets.py",
      "pandas/tests/tseries/test_timezones.py",
      "pandas/tseries/api.py",
      "pandas/tseries/offsets.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13646,
    "reporter": "jreback",
    "created_at": "2016-07-13T21:13:35+00:00",
    "closed_at": "2016-07-15T10:21:32+00:00",
    "resolver": "jreback",
    "resolved_in": "1bee56ed9aa96ffe99aa62d5e8c0212d6dc947ee",
    "resolver_commit_num": 4055,
    "title": "BUG: Series construction w/integer tuples failing on windows",
    "body": "This just started failing on windows on master, in the last few PR's. Prob related to #13147.\n\ntuples are not being upcast like lists.\n\n\n\n\n",
    "labels": [
      "Bug",
      "Dtypes",
      "Windows"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 15,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/series.py",
      "pandas/tests/frame/test_operators.py",
      "pandas/tests/series/test_constructors.py",
      "pandas/types/cast.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13652,
    "reporter": "bdrosen96",
    "created_at": "2016-07-14T10:59:43+00:00",
    "closed_at": "2016-07-20T21:54:09+00:00",
    "resolver": "bdrosen96",
    "resolved_in": "210fea9d4dc4314f9bc4ddb5f7dab6fa87912ca9",
    "resolver_commit_num": 0,
    "title": "Read CSV using c engine silently swallows useful exceptions",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Actual Output\n\n\n#### Expected Behavior\n\nThe C engine should behave like python engine. This should be possible by using PyErr_Occurred .\n",
    "labels": [
      "Unicode",
      "Error Reporting",
      "IO CSV"
    ],
    "comments": [
      "you have a pretty old version of pandas, current is 0.18.1 and 0.19.0 releasing soon.\n\nand you can simply pass in the the `encoding` argument if you need to.\n\nI am closing, but if you can provide a copy-pastable example that reproduces a non-obvious error on latest, then pls reopen.\n",
      "This should not have been closed.\n\n1 Even though the version I have is old, I think this issue still exists in latest version.\n\n2 The encoding issue was just an example that was easy to produce and should not be dismised because of the existence of the encoding option. If the file handle was a socket and the connection was reset, it would also raise an exception and there would not be a workaround\n",
      "I just verified this with same code using newer pandas\n\n```\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.4.3.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.13.0-55-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: 1.3.7\npip: 7.1.0\nsetuptools: 20.2.2\nCython: 0.24.0a0\nnumpy: 1.9.2\nscipy: 0.16.0\nstatsmodels: None\nxarray: None\nIPython: 4.1.2\nsphinx: None\npatsy: None\ndateutil: 2.4.2\npytz: 2015.4\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: 2.4.3\nmatplotlib: None\nopenpyxl: None\nxlrd: 0.9.4\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: 0.9999999\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: None\nboto: 2.38.0\npandas_datareader: None\nPandas version: 0.18.1\n\nPython version: sys.version_info(major=3, minor=4, micro=3, releaselevel='final', serial=0)\n\nShowing stream error on read\n\nTraceback (most recent call last):\n  File \"pandas_bug.py\", line 34, in <module>\n    data = stream.read()\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/codecs.py\", line 798, in read\n    data = self.reader.read(size)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/codecs.py\", line 497, in read\n    newchars, decodedbytes = self.decode(data, self.errors)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x83 in position 12: invalid start byte\nShowing stream error on read_csv (python engine)\n\nTraceback (most recent call last):\n  File \"pandas_bug.py\", line 41, in <module>\n    stream = test_pandas(True)\n  File \"pandas_bug.py\", line 28, in test_pandas\n    df = pandas.read_csv(stream, engine=engine)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas/io/parsers.py\", line 562, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas/io/parsers.py\", line 315, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas/io/parsers.py\", line 645, in __init__\n    self._make_engine(self.engine)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas/io/parsers.py\", line 805, in _make_engine\n    self._engine = klass(self.f, **self.options)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas/io/parsers.py\", line 1608, in __init__\n    self.columns, self.num_original_columns = self._infer_columns()\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas/io/parsers.py\", line 1823, in _infer_columns\n    line = self._buffered_line()\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas/io/parsers.py\", line 1975, in _buffered_line\n    return self._next_line()\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas/io/parsers.py\", line 2006, in _next_line\n    orig_line = next(self.data)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/codecs.py\", line 820, in __next__\n    data = next(self.reader)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/codecs.py\", line 638, in __next__\n    line = self.readline()\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/codecs.py\", line 551, in readline\n    data = self.read(readsize, firstline=True)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/codecs.py\", line 497, in read\n    newchars, decodedbytes = self.decode(data, self.errors)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x83 in position 0: invalid start byte\nShowing missing stream error on read_csv (python c)\n\nTraceback (most recent call last):\n  File \"pandas_bug.py\", line 48, in <module>\n    stream = test_pandas(False)\n  File \"pandas_bug.py\", line 28, in test_pandas\n    df = pandas.read_csv(stream, engine=engine)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas/io/parsers.py\", line 562, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas/io/parsers.py\", line 315, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas/io/parsers.py\", line 645, in __init__\n    self._make_engine(self.engine)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas/io/parsers.py\", line 799, in _make_engine\n    self._engine = CParserWrapper(self.f, **self.options)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas/io/parsers.py\", line 1213, in __init__\n    self._reader = _parser.TextReader(src, **kwds)\n  File \"pandas/parser.pyx\", line 520, in pandas.parser.TextReader.__cinit__ (pandas/parser.c:5129)\n  File \"pandas/parser.pyx\", line 671, in pandas.parser.TextReader._get_header (pandas/parser.c:7259)\n  File \"pandas/parser.pyx\", line 868, in pandas.parser.TextReader._tokenize_rows (pandas/parser.c:9602)\n  File \"pandas/parser.pyx\", line 1865, in pandas.parser.raise_parser_error (pandas/parser.c:23325)\npandas.io.common.CParserError: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n```\n",
      "I do not have permissions to reopen this issue. Can you do so?\n",
      "@bdrosen96 then pls show an example which can be copy-pasted. e.g. your file is not there. you need to create it to repro (e.g. write out a test csv file or something), better yet is to use StringIO\n",
      "and you are testing on 0.18.1, I am pretty sure these are already fixed on master.\n",
      "``` python\nimport pandas\nimport codecs\nimport traceback\nimport sys\nimport io\n\n# Data in UTF-8\n\nDATA = '''num, text\n1,\u30b5\u30a6\u30ed\u30f3\uff08Sauron\u3001\u30a2\u30a4\u30cc\u30a2\u306e\u5275\u9020\u306e\u6642 - \u7b2c\u4e09\u7d003019\u5e743\u670825\u65e5\uff09\u306f\u3001J\u30fbR\u30fbR\u30fb\u30c8\u30fc\u30eb\u30ad\u30f3\u306e\u4e2d\u3064\u56fd\u3092\u821e\u53f0\u3068\u3057\u305f\u5c0f\u8aac\n\u300e\u30db\u30d3\u30c3\u30c8\u306e\u5192\u967a\u300f\u300e\u6307\u8f2a\u7269\u8a9e\u300f\u300e\u30b7\u30eb\u30de\u30ea\u30eb\u306e\u7269\u8a9e\u300f\u306e\u767b\u5834\u4eba\u7269\u3002\n2,\u300e\u30db\u30d3\u30c3\u30c8\u306e\u5192\u967a\u300f\u306b\u8a00\u53ca\u306e\u3042\u308b\u300c\u6b7b\u4eba\u3046\u3089\u306a\u3044\u5e2b\u300d\uff08\u6620\u753b\u300e\u30db\u30d3\u30c3\u30c8\u30b7\u30ea\u30fc\u30ba\u300f\u306e\u5b57\u5e55\u3067\u306f\u300c\u6b7b\u4eba\u9063\u3044\uff08\u30cd\u30af\u30ed\u30de\u30f3\u30b5\u30fc\uff09\u300d\uff09\u3068\u306f\u5f7c\u306e\u3053\u3068\u3067\u3042\u308b\u3002\n3,\u305d\u306e\u7d9a\u7de8\u3067\u3042\u308b\u300e\u6307\u8f2a\u7269\u8a9e\u300f\u306b\u304a\u3044\u3066\u306f\u300c\u4e00\u3064\u306e\u6307\u8f2a\uff08the One Ring\uff09\u300d\u306e\u4f5c\u308a\u4e3b\u3001\u300c\u51a5\u738b\uff08Dark Lord\uff09\u300d\u3001\u300c\u304b\u306e\u8005\n\uff08the One\uff09[1]\u300d\u3068\u3057\u3066\u767b\u5834\u3059\u308b\u3002\u524d\u53f2\u306b\u3042\u305f\u308b\u300e\u30b7\u30eb\u30de\u30ea\u30eb\u306e\u7269\u8a9e\u300f\u3067\u306f\u3001\u521d\u4ee3\u306e\u51a5\u738b\u30e2\u30eb\u30b4\u30b9\u306e\u6700\u3082\u529b\u3042\u308b\u5074\u8fd1\u3067\u3042\u3063\u305f\u3002\n4,\u30b5\u30a6\u30ed\u30f3\u306f\u5143\u6765\u3001\u30a2\u30eb\u30c0\uff08\u5730\u7403\uff09\u306e\u5275\u9020\u3092\u62c5\u3063\u305f\u5929\u4f7f\u7684\u7a2e\u65cf\u30a2\u30a4\u30cc\u30a2\u306e\u4e00\u54e1\u3067\u3042\u3063\u305f\u304c\u3001\u4e3b\u30e1\u30eb\u30b3\u30fc\u30eb\u306e\u53cd\u9006\u306b\u52a0\u62c5\u3057\u3066\u5815\u843d\u3057\u3001\u30a2\u30eb\u30c0\u306b\u5bb3\u3092\u306a\u3059\u5b58\u5728\u3068\u306a\u3063\u305f\u3002\n5,\u300c\u30b5\u30a6\u30ed\u30f3\u300d\u3068\u306f\u30af\u30a6\u30a7\u30f3\u30e4\u3067\u300c\u8eab\u306e\u6bdb\u306e\u3088\u3060\u3064\u3082\u306e\u300d\u3068\u3044\u3046\u610f\u5473\u3067\u3042\u308a\u3001\u30b7\u30f3\u30c0\u30ea\u30f3\u3067\u540c\u69d8\u306e\u610f\u5473\u3067\u3042\u308b\u540d\u524d\u300c\u30b4\u30eb\u30b5\u30a6\u30a2\u300d\u3068\u547c\u3070\u308c\u308b\u3053\u3068\u3082\u3042\u308b\u3002\n6,\u3053\u308c\u3089\u306f\u3001\u30b5\u30a6\u30ed\u30f3\u3092\u6050\u308c\u3001\u5fcc\u307f\u5acc\u3063\u305f\u30a8\u30eb\u30d5\u306b\u3088\u308b\u540d\u3067\u3042\u308a\u3001\u300e\u6307\u8f2a\u7269\u8a9e\u300f\u4f5c\u4e2d\u306b\u304a\u3044\u3066\u30a2\u30e9\u30b4\u30eb\u30f3\u306f\u300c\u304b\u308c\uff08\u30b5\u30a6\u30ed\u30f3\uff09\u306f\u81ea\u5206\u306e\u672c\u5f53\u306e\u540d\u306f\u4f7f\u308f\u306a\u3044\u3057\u3001\u305d\u308c\u3092\u5b57\u306b\u66f8\u3044\u305f\u308a\u53e3\u306b\u51fa\u3057\u305f\u308a\u3059\u308b\u3053\u3068\u3082\u8a31\u3055\u306a\u3044\u300d\u3068\u767a\u8a00\u3057\u3066\u3044\u308b\u3002\n7,\u305d\u306e\u307b\u304b\u3001\u7b2c\u4e8c\u7d00\u306b\u30a8\u30eb\u30d5\u306b\u5bfe\u3057\u3066\u81ea\u79f0\u3057\u305f\u3068\u3055\u308c\u308b\u540d\u306b\u3001\u300c\u30a2\u30f3\u30ca\u30bf\u30fc\u30eb\uff08\u7269\u8d08\u308b\u541b\uff09\u300d\u3001\u300c\u30a2\u30eb\u30bf\u30ce\uff08\u9ad8\u8cb4\u306a\u7d30\u5de5\u5e2b\uff09\u300d\u3001\u300c\u30a2\u30a6\u30ec\u30f3\u30c7\u30a3\u30eb\uff08\u30a2\u30a6\u30ec\u306e\u4e0b\u50d5\uff09\u300d\u304c\u3042\u308b\u3002\n8,\u7b2c\u4e00\u7d00\u306e\u9803\u306e\u30b5\u30a6\u30ed\u30f3\u306f\u3001\u81ea\u5728\u306b\u5909\u8eab\u3059\u308b\u80fd\u529b\u3092\u6301\u3063\u3066\u3044\u305f\u3002\n9,\u305d\u306e\u80fd\u529b\u3092\u4f7f\u3048\u3070\u898b\u76ee\u9e97\u3057\u3044\u7acb\u6d3e\u306a\u5916\u898b\u3092\u88c5\u3046\u3053\u3068\u3084\u3001\u307e\u305f\u5de8\u5927\u306a\u72fc\u3084\u5438\u8840\u3053\u3046\u3082\u308a\u3068\u3044\u3063\u305f\u602a\u7269\u306b\u5909\u3058\u308b\u3053\u3068\u3082\u3067\u304d\u3001\u30a8\u30eb\u30d5\u304b\u3089\u6050\u308c\u3089\u308c\u305f\u3002\n10,\u7b2c\u4e8c\u7d00\u306b\u4e00\u3064\u306e\u6307\u8f2a\u3092\u4f5c\u308a\u4e0a\u3052\u305f\u30b5\u30a6\u30ed\u30f3\u306f\u3001\u4ed6\u306e\u529b\u306e\u6307\u8f2a\u3067\u6210\u3055\u308c\u308b\u4e8b\u67c4\u3084\u305d\u306e\u6240\u6709\u8005\u3092\u652f\u914d\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u3063\u305f\u3002\n11,\u307e\u305f\u3001\u8089\u4f53\u304c\u6ec5\u3073\u3066\u3082\u6307\u8f2a\u304c\u3042\u308b\u9650\u308a\u4f55\u5ea6\u3067\u3082\u8607\u308b\u3053\u3068\u304c\u3067\u304d\u305f\u3002\n12,\u305f\u3060\u3057\u30cc\u30fc\u30e1\u30ce\u30fc\u30eb\u6ca1\u843d\u306e\u969b\u306b\u7f8e\u3057\u3044\u8089\u4f53\u3092\u7834\u58ca\u3055\u308c\u305f\u5f8c\u306f\u3001\u4e8c\u5ea6\u3068\u7f8e\u3057\u304f\u5909\u8eab\u3059\u308b\u3053\u3068\u306f\u3067\u304d\u306a\u304f\u306a\u308a\u3001\u305d\u306e\u60aa\u610f\u306e\n\u5177\u73fe\u306e\u3088\u3046\u306a\u898b\u308b\u3082\u6050\u308d\u3057\u3044\u59ff\u3057\u304b\u3068\u308c\u306a\u304f\u306a\u3063\u305f\u3068\u3044\u3046\u3002\n13,\u307e\u305f\u3057\u3070\u3057\u3070\u300c\u307e\u3076\u305f\u306e\u306a\u3044\u706b\u306b\u7e01\u53d6\u3089\u308c\u305f\u76ee\u300d\u3068\u3044\u3063\u305f\u5fc3\u8c61\u8868\u73fe\u3067\u6349\u3048\u3089\u308c\u305f\u3002\n'''\n\npandas.show_versions()\n\nprint(\"Pandas version: {}\\n\".format(pandas.__version__))\nprint(\"Python version: {}\\n\".format(sys.version_info))\n\ndef build_stream():\n\n    bytes_data = DATA.encode(\"shift-jis\")\n    handle = io.BytesIO(bytes_data)\n    codec = codecs.lookup(\"utf-8\")\n    utf8 = codecs.lookup('utf-8')\n    # stream must be binary UTF8\n    stream = codecs.StreamRecoder(\n        handle, utf8.encode, utf8.decode, codec.streamreader, codec.streamwriter)\n    return stream\n\ndef test_pandas(use_python):\n    stream = build_stream()\n\n    if use_python:\n        engine = 'python'\n    else:\n        engine = 'c'\n    df = pandas.read_csv(stream, engine=engine)\n\n\nprint(\"Showing stream error on read\\n\")\ntry:\n    stream = build_stream()\n    data = stream.read()\nexcept Exception as exc:\n    traceback.print_exc(file=sys.stdout)\n\n\nprint(\"Showing stream error on read_csv (python engine)\\n\")\ntry:\n    stream = test_pandas(True)\nexcept Exception as exc:\n    traceback.print_exc(file=sys.stdout)\n\n\nprint(\"Showing missing stream error on read_csv (python c)\\n\")\ntry:\n    stream = test_pandas(False)\nexcept Exception as exc:\n    traceback.print_exc(file=sys.stdout)\n```\n\n```\n```\n",
      "I just ran this again on master and got same behavior.\n\nPandas version: 0.18.1+198.g3f6d4bd\n",
      "pls show the output from master\n",
      "again, using a non-decoded stream is really really odd; this is not supported\n",
      "I'll reopen. If you can provide a PR which 'fixes' this I think it will be easier to look/test.\n\ncc @gfyoung \n",
      "```\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.4.3.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.13.0-55-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\nLOCALE: en_US.UTF-8\n\npandas: 0.18.1+198.g3f6d4bd\nnose: 1.3.7\npip: 7.1.0\nsetuptools: 20.2.2\nCython: 0.24.0a0\nnumpy: 1.9.2\nscipy: 0.16.0\nstatsmodels: None\nxarray: None\nIPython: 4.1.2\nsphinx: None\npatsy: None\ndateutil: 2.4.2\npytz: 2015.4\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: 2.4.3\nmatplotlib: None\nopenpyxl: None\nxlrd: 0.9.4\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: 0.9999999\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: None\nboto: 2.38.0\npandas_datareader: None\nPandas version: 0.18.1+198.g3f6d4bd\n\nPython version: sys.version_info(major=3, minor=4, micro=3, releaselevel='final', serial=0)\n\nShowing stream error on read\n\nTraceback (most recent call last):\n  File \"pandas_bug.py\", line 57, in <module>\n    data = stream.read()\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/codecs.py\", line 798, in read\n    data = self.reader.read(size)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/codecs.py\", line 497, in read\n    newchars, decodedbytes = self.decode(data, self.errors)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x83 in position 12: invalid start byte\nShowing stream error on read_csv (python engine)\n\nTraceback (most recent call last):\n  File \"pandas_bug.py\", line 64, in <module>\n    stream = test_pandas(True)\n  File \"pandas_bug.py\", line 51, in test_pandas\n    df = pandas.read_csv(stream, engine=engine)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg/pandas/io/parsers.py\", line 631, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg/pandas/io/parsers.py\", line 384, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg/pandas/io/parsers.py\", line 714, in __init__\n    self._make_engine(self.engine)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg/pandas/io/parsers.py\", line 898, in _make_engine\n    self._engine = klass(self.f, **self.options)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg/pandas/io/parsers.py\", line 1746, in __init__\n    self.columns, self.num_original_columns = self._infer_columns()\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg/pandas/io/parsers.py\", line 1988, in _infer_columns\n    line = self._buffered_line()\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg/pandas/io/parsers.py\", line 2140, in _buffered_line\n    return self._next_line()\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg/pandas/io/parsers.py\", line 2171, in _next_line\n    orig_line = next(self.data)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/codecs.py\", line 820, in __next__\n    data = next(self.reader)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/codecs.py\", line 638, in __next__\n    line = self.readline()\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/codecs.py\", line 551, in readline\n    data = self.read(readsize, firstline=True)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/codecs.py\", line 497, in read\n    newchars, decodedbytes = self.decode(data, self.errors)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x83 in position 0: invalid start byte\nShowing missing stream error on read_csv (python c)\n\nTraceback (most recent call last):\n  File \"pandas_bug.py\", line 71, in <module>\n    stream = test_pandas(False)\n  File \"pandas_bug.py\", line 51, in test_pandas\n    df = pandas.read_csv(stream, engine=engine)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg/pandas/io/parsers.py\", line 631, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg/pandas/io/parsers.py\", line 384, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg/pandas/io/parsers.py\", line 714, in __init__\n    self._make_engine(self.engine)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg/pandas/io/parsers.py\", line 892, in _make_engine\n    self._engine = CParserWrapper(self.f, **self.options)\n  File \"/home/brett/.virtualenvs/datasets-service/lib/python3.4/site-packages/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg/pandas/io/parsers.py\", line 1340, in __init__\n    self._reader = _parser.TextReader(src, **kwds)\n  File \"pandas/parser.pyx\", line 527, in pandas.parser.TextReader.__cinit__ (pandas/parser.c:5137)\n  File \"pandas/parser.pyx\", line 701, in pandas.parser.TextReader._get_header (pandas/parser.c:7700)\n  File \"pandas/parser.pyx\", line 898, in pandas.parser.TextReader._tokenize_rows (pandas/parser.c:10058)\n  File \"pandas/parser.pyx\", line 1890, in pandas.parser.raise_parser_error (pandas/parser.c:24033)\npandas.io.common.CParserError: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n```\n",
      "```\ndiff --git a/pandas/parser.pyx b/pandas/parser.pyx\nindex 3928bc8..61a1e03 100644\n--- a/pandas/parser.pyx\n+++ b/pandas/parser.pyx\n@@ -10,7 +10,9 @@ import warnings\n from csv import QUOTE_MINIMAL, QUOTE_NONNUMERIC, QUOTE_NONE\n from cpython cimport (PyObject, PyBytes_FromString,\n                       PyBytes_AsString, PyBytes_Check,\n-                      PyUnicode_Check, PyUnicode_AsUTF8String)\n+                      PyUnicode_Check, PyUnicode_AsUTF8String,\n+                      PyErr_Occurred, PyErr_Fetch)\n+from cpython.ref cimport PyObject, Py_XDECREF\n from io.common import CParserError, DtypeWarning, EmptyDataError\n\n\n@@ -1878,6 +1880,17 @@ cdef kh_float64_t* kset_float64_from_list(values) except NULL:\n\n\n cdef raise_parser_error(object base, parser_t *parser):\n+    cdef:\n+        object old_exc\n+        PyObject *type, *value, *traceback\n+    if PyErr_Occurred():\n+        PyErr_Fetch(&type, &value, &traceback);\n+        Py_XDECREF(type)\n+        Py_XDECREF(traceback)\n+        if value != NULL:\n+            old_exc = <object> value\n+            Py_XDECREF(value)\n+            raise old_exc\n     message = '%s. C error: ' % base\n     if parser.error_msg != NULL:\n         if PY3:\n```\n",
      "@bdrosen96 : thanks for pointing this out!  You can submit a PR for this and make sure to include a test as well (I would think in `common.py` if possible but otherwise `c_parser_only.py`)!\n",
      "I cannot submit a PR without creating a fork first (permissions issue)\n",
      "Of course.  It clearly says that in the documentation for contributing.\n",
      "https://github.com/pydata/pandas/pull/13693\n"
    ],
    "events": [
      "commented",
      "closed",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "reopened",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files": 5,
    "additions": 59,
    "deletions": 7,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/io/tests/parser/common.py",
      "pandas/io/tests/parser/data/sauron.SHIFT_JIS.csv",
      "pandas/io/tests/parser/test_parsers.py",
      "pandas/parser.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13672,
    "reporter": "ChadFulton",
    "created_at": "2016-07-16T07:43:04+00:00",
    "closed_at": "2016-07-23T13:09:18+00:00",
    "resolver": "sinhrks",
    "resolved_in": "e533947cf828fbc75d6b754dbe0e3fa862b1647a",
    "resolver_commit_num": 346,
    "title": "BUG: DatetimeIndex with nanosecond frequency does not include `end`",
    "body": "I'm not sure if this is a bug or intended behavior, but documentation says \"If periods is none, generated index will extend to first conforming time on or just past end argument\", and it appears here that in the nanosecond frequency case, the generated index only extends to just _before_ the end argument.\n#### Code Sample, a copy-pastable example if possible\n\n\n\nwhereas a similar call with annual frequency gives:\n\n\n#### output of `pd.show_versions()`\n\npandas: 0.18.0\n",
    "labels": [
      "Timeseries",
      "Frequency",
      "Bug"
    ],
    "comments": [
      "Thanks for the report. Because the range is created by `np.arange`, we should have +1 margin if offset is `Nano`. \n- https://github.com/pydata/pandas/blob/master/pandas/tseries/index.py#L2015\n\nPR is appreciated:)\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "cross-referenced",
      "milestoned"
    ],
    "changed_files": 4,
    "additions": 77,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13695,
    "reporter": "chrisaycock",
    "created_at": "2016-07-18T18:38:35+00:00",
    "closed_at": "2016-07-19T01:16:41+00:00",
    "resolver": "jreback",
    "resolved_in": "b05453631270d4b78f79dc272222d5f3fe499ad7",
    "resolver_commit_num": 4056,
    "title": "pd.merge_asof() matches out of tolerance when allow_exact_matches=False",
    "body": "Using these DataFrames for this example:\n\n\n\nI can perform the `pd.merge_asof()` as expected:\n\n\n\nIf I disable the exact matches, then I get the prior entry as expected:\n\n\n\nBut the tolerance isn't respected when I disable the exact matches!\n\n\n\nI expect to get a null here.\n\nThis is pandas version `0.18.0+408.gd8f3d73`.\n",
    "labels": [
      "Bug",
      "Reshaping",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "hmm, that does look buggy\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 44,
    "deletions": 9,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/src/join.pyx",
      "pandas/tools/tests/test_merge_asof.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13703,
    "reporter": "ivannz",
    "created_at": "2016-07-19T09:54:15+00:00",
    "closed_at": "2016-07-27T10:49:48+00:00",
    "resolver": "ivannz",
    "resolved_in": "31f8e4dc8af8f0d109f366d0b726aef210bf7904",
    "resolver_commit_num": 0,
    "title": "Unexpected segmentation fault in pd.read_csv C-engine",
    "body": "Dear developers,\n\nI am using pandas in an application where I need to process large **csv** files (around 1Gb each) which have approximately 800k records and 400+ columns of mixed type. That is why I decided to use data iterator functionality of **pd.read_csv()**. When experimenting with `chunksize` my application seems to crash somewhere inside **TextReader__string_convert** call.\n\nHere is an archive with a sample CSV data file that seems to cause the crash (it also includes crash dump reports, a copy of the example, and a snapshot of versions of installed python packages).\n[read_csv_crash.tar.gz]()\n#### Code Sample\n\nTo run this example you would have to extract `dataset.csv` from the supplied archive.\n\n\n\nPlease, note that this crash does not seem to occur when the file is less than 260Kib. Also note that playing with `low_memory` setting did not alleviate the problem.\n#### Expected Output\n\nThis code sample outputs this:\n\n\n#### output of `pd.show_versions()`\n\nThe output of this call is attached to this issue.\n[pd_show_versions.txt]()\n#### Python greetings string\n\n\n#### OSX version\n\n\n",
    "labels": [
      "Bug",
      "IO CSV"
    ],
    "comments": [
      "1) For the expected output section, put what you're actually expecting to see, not what you actually saw.  Underneath that, you should then put what you saw.\n\n2) Can you try your code sample again with the `master` branch?\n\n3) While I cannot reproduce this (either with `0.18.1` or `master`) on Linux (sorry, no access to Mac ATM), the fact that it's crashing with `string` and `object` dtype bares resemblance to an earlier segfault we were seeing in a different part of the code.\n\nIf the issue persists on `master`, in <a href=\"https://github.com/pydata/pandas/blob/master/pandas/parser.pyx\">parser.pyx</a>, you can find the `string_convert` function <a href=\"https://github.com/pydata/pandas/blob/master/pandas/parser.pyx#L1197\">here</a>.  Judging from your versions output, I suspect the segfault is occurring in this function <a href=\"https://github.com/pydata/pandas/blob/master/pandas/parser.pyx#L1221\">here</a> in fact.  If my suspicion is correct, can you further specify which method call is causing the crash?\n",
      "Hello,  @gfyoung !\n\nI cloned the master branch, but still the problem persisted.\n\nI've managed to trace the source to a read access beyond the allocated large memory block in either **kh_get_str** or **kh_get_strbox** (defined in **src/klib/khash.h**) in **_string_box_factorize** (from **parser.pyx**) called from the [very last branch](https://github.com/pydata/pandas/blob/master/pandas/parser.pyx#L1221) in **_string_convert** for the remaining 5 lines of the text input. The particular function depends on the **na_filter** setting.\n\nI `debug-patched` **COLITER_NEXT** in **tokenizer.h** with:\n\n``` C\n#define COLITER_NEXT(iter, word) do { \\\n    const int i = *iter.line_start++ + iter.col; \\\n    word = i < *iter.line_start ? iter.words[i]: \"\"; \\\n    printf(\"%d, %p\\n\", i, (const void*) iter.words[i]); \\\n    } while(0)\n```\n\n-- to print out the last address just before the crash. It printed `0` and the address at which the `EXC_BAD_ACCESS` happens. Any attempt to print the **word** returned by **COLITER_NEXT** results in **segfault**. Note that the **word** is later fed into both  **kh_&ast;** functions.\n\nI also added this to **tokenizer.c**:\n\n``` C\nvoid _dump(const void *addr, size_t len) \n{\n    size_t i;\n    unsigned char buff[17];\n    unsigned char *pc = (unsigned char*)addr;\n\n    printf(\"%p:\\n\", addr);\n    for (i = 0; i < len; i++) {\n        if ((i % 16) == 0) {\n            if (i != 0)\n                printf(\"  %s\\n\", buff);\n            printf(\"  %04x \", i);\n        }\n        printf(\" %02x\", pc[i]);\n        if ((pc[i] < 0x20) || (pc[i] > 0x7e)) {\n            buff[i % 16] = '.';\n        } else {\n            buff[i % 16] = pc[i];\n        }\n        buff[(i % 16) + 1] = '\\0';\n    }\n    while ((i % 16) != 0) {\n        printf(\"   \");\n        i++;\n    }\n    printf(\"  %s\\n\", buff);\n}\n```\n\nI borrowed it with simplifications from [this gist](https://gist.github.com/domnikl/af00cc154e3da1c5d965) to dump memory contents of **parser->words** and **parser->stream** in a call to **_string_box_factorize**. It turns out that just before the crash the pointers in **parser->words** point to a memory region starting with the address that causes the crash.\n\nI strongly suspect that this problem is specific to OS X memory allocation.\n\nPS: It seems that [**end_field**](https://github.com/pydata/pandas/blob/master/pandas/src/parser/tokenizer.c#L411) records the pointers to words in a memory region into **parser->word_starts**, which later becomes unaccessible.\n\nPPS: I suspect [**parser_trim_buffers**](https://github.com/pydata/pandas/blob/master/pandas/src/parser/tokenizer.c#L1217) changes allocated memory but does not re-initialize **parser->word_starts**.\n\nPPPS: Here is a snippet which does not use the `dataset.csv` file, and instead uses StringIO, but still crashes with **segfault**.\n\n``` python\nimport pandas as pd\nfrom cStringIO import StringIO\nrecord_ = \"\"\"9999-9,99:99,,,,ZZ,ZZ,,,ZZZ-ZZZZ,.Z-ZZZZ,-9.99,,,9.99,ZZZZZ,,-99,9,ZZZ-ZZZZ,ZZ-ZZZZ,,9.99,ZZZ-ZZZZZ,ZZZ-ZZZZZ,ZZZ-ZZZZ,ZZZ-ZZZZ,ZZZ-ZZZZ,ZZZ-ZZZZ,ZZZ-ZZZZ,ZZZ-ZZZZ,999,ZZZ-ZZZZ,,ZZ-ZZZZ,,,,,ZZZZ,ZZZ-ZZZZZ,ZZZ-ZZZZ,,,9,9,9,9,99,99,999,999,ZZZZZ,ZZZ-ZZZZZ,ZZZ-ZZZZ,9,ZZ-ZZZZ,9.99,ZZ-ZZZZ,ZZ-ZZZZ,,,,ZZZZ,,,ZZ,ZZ,,,,,,,,,,,,,9,,,999.99,999.99,,,ZZZZZ,,,Z9,,,,,,,ZZZ,ZZZ,,,,,,,,,,,ZZZZZ,ZZZZZ,ZZZ-ZZZZZZ,ZZZ-ZZZZZZ,ZZ-ZZZZ,ZZ-ZZZZ,ZZ-ZZZZ,ZZ-ZZZZ,,,999999,999999,ZZZ,ZZZ,,,ZZZ,ZZZ,999.99,999.99,,,,ZZZ-ZZZ,ZZZ-ZZZ,-9.99,-9.99,9,9,,99,,9.99,9.99,9,9,9.99,9.99,,,,9.99,9.99,,99,,99,9.99,9.99,,,ZZZ,ZZZ,,999.99,,999.99,ZZZ,ZZZ-ZZZZ,ZZZ-ZZZZ,,,ZZZZZ,ZZZZZ,ZZZ,ZZZ,9,9,,,,,,ZZZ-ZZZZ,ZZZ999Z,,,999.99,,999.99,ZZZ-ZZZZ,,,9.999,9.999,9.999,9.999,-9.999,-9.999,-9.999,-9.999,9.999,9.999,9.999,9.999,9.999,9.999,9.999,9.999,99999,ZZZ-ZZZZ,,9.99,ZZZ,,,,,,,,ZZZ,,,,,9,,,,9,,,,,,,,,,ZZZ-ZZZZ,ZZZ-ZZZZ,,ZZZZZ,ZZZZZ,ZZZZZ,ZZZZZ,,,9.99,,ZZ-ZZZZ,ZZ-ZZZZ,ZZ,999,,,,ZZ-ZZZZ,ZZZ,ZZZ,ZZZ-ZZZZ,ZZZ-ZZZZ,,,99.99,99.99,,,9.99,9.99,9.99,9.99,ZZZ-ZZZZ,,,ZZZ-ZZZZZ,,,,,-9.99,-9.99,-9.99,-9.99,,,,,,,,,ZZZ-ZZZZ,,9,9.99,9.99,99ZZ,,-9.99,-9.99,ZZZ-ZZZZ,,,,,,,ZZZ-ZZZZ,9.99,9.99,9999,,,,,,,,,,-9.9,Z/Z-ZZZZ,999.99,9.99,,999.99,ZZ-ZZZZ,ZZ-ZZZZ,9.99,9.99,9.99,9.99,9.99,9.99,,ZZZ-ZZZZZ,ZZZ-ZZZZZ,ZZZ-ZZZZZ,ZZZ-ZZZZZ,ZZZ-ZZZZZ,ZZZ,ZZZ,ZZZ,ZZZ,9.99,,,-9.99,ZZ-ZZZZ,-999.99,,-9999,,999.99,,,,999.99,99.99,,,ZZ-ZZZZZZZZ,ZZ-ZZZZ-ZZZZZZZ,,,,ZZ-ZZ-ZZZZZZZZ,ZZZZZZZZ,ZZZ-ZZZZ,9999,999.99,ZZZ-ZZZZ,-9.99,-9.99,ZZZ-ZZZZ,99:99:99,,99,99,,9.99,,-99.99,,,,,,9.99,ZZZ-ZZZZ,-9.99,-9.99,9.99,9.99,,ZZZ,,,,,,,ZZZ,ZZZ,,,,,\"\"\"\ncsv_data = \"\\n\".join([record_]*173) + \"\\n\"\n\nfor _ in range(2):\n    iterator_ = pd.read_csv(StringIO(csv_data), header=None, engine=\"c\",\n                            dtype=object, chunksize=84, na_filter=True)\n    for chunk_ in iterator_:\n        print chunk_.iloc[0, 0], chunk_.iloc[-1, 0]\n    print \">>>NEXT\"\n```\n",
      "The problems seems to be in the [**parser_trim_buffers**](https://github.com/pydata/pandas/blob/master/pandas/src/parser/tokenizer.c#L1217) as it appears not to move word pointers.\n\nIf I swap blocks **L1224 -- L1237** (**/&ast; trim stream &ast;/**) and **L1239 -- L1256**(**/&ast; trim words, word_starts &ast;/**) and then copy the initialization of **parser->words** from [**make_stream_space**](https://github.com/pydata/pandas/blob/master/pandas/src/parser/tokenizer.c#L289) to the **trim stream** block (as shown in the snippet below), the problem goes away.\n\nHere is a new version of **parser_trim_buffers**\n\n``` C\n\nint parser_trim_buffers(parser_t *self) {\n    /*\n      Free memory\n     */\n    size_t new_cap;\n    void *newptr;\n\n    int i;\n\n    /* trim words, word_starts */\n    new_cap = _next_pow2(self->words_len) + 1;\n    if (new_cap < self->words_cap) {\n        TRACE((\"parser_trim_buffers: new_cap < self->words_cap\\n\"));\n        newptr = safe_realloc((void*) self->words, new_cap * sizeof(char*));\n        if (newptr == NULL) {\n            return PARSER_OUT_OF_MEMORY;\n        } else {\n            self->words = (char**) newptr;\n        }\n        newptr = safe_realloc((void*) self->word_starts, new_cap * sizeof(int));\n        if (newptr == NULL) {\n            return PARSER_OUT_OF_MEMORY;\n        } else {\n            self->word_starts = (int*) newptr;\n            self->words_cap = new_cap;\n        }\n    }\n\n    /* trim stream */\n    new_cap = _next_pow2(self->stream_len) + 1;\n    TRACE((\"parser_trim_buffers: new_cap = %zu, stream_cap = %zu, lines_cap = %zu\\n\",\n           new_cap, self->stream_cap, self->lines_cap));\n    if (new_cap < self->stream_cap) {\n        TRACE((\"parser_trim_buffers: new_cap < self->stream_cap, calling safe_realloc\\n\"));\n        newptr = safe_realloc((void*) self->stream, new_cap);\n        if (newptr == NULL) {\n            return PARSER_OUT_OF_MEMORY;\n        } else {\n            // realloc sets errno when moving buffer?\n            if (self->stream != newptr) {\n                // uff\n                /* TRACE((\"Moving word pointers\\n\")) */\n\n                self->pword_start = newptr + self->word_start;\n\n                for (i = 0; i < self->words_len; ++i)\n                {\n                    self->words[i] = newptr + self->word_starts[i];\n                }\n            }\n\n            self->stream = newptr;\n            self->stream_cap = new_cap;\n\n        }\n    }\n\n...\n\n    return 0;\n}\n\n```\n",
      "Awesome that you were able to fix your segfault!  Here's what I would do now:\n\n1) run all of the unit tests to see if your changes break any existing functionality\n\n2) if they don't, then submit this as a PR so that all of us can have a look!\n\n3) if they do cause failures, then I'll leave it up to you whether you want to investigate the failure causes. Feel free to provide a patch that we can clone and figure out.\n",
      "I issued a pull request (#13788), however there was one failed test and several _deprecation warnings_:\n\n```\nFAIL: test_round_trip_frame_sep (pandas.io.tests.test_clipboard.TestClipboard)\n```\n",
      "that test is ok, its not currently engaged on travis and fails on linux/macosx (and already has an outstanding issue).\n\ndeprecation warnings are ok (though I do try to eliminate them periodically).\n"
    ],
    "events": [
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "labeled",
      "labeled"
    ],
    "changed_files": 3,
    "additions": 101,
    "deletions": 14,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/io/tests/parser/c_parser_only.py",
      "pandas/src/parser/tokenizer.c"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13717,
    "reporter": "flatberg",
    "created_at": "2016-07-20T12:50:45+00:00",
    "closed_at": "2016-07-21T12:08:26+00:00",
    "resolver": "gfyoung",
    "resolved_in": "ee6c0cdbca17fb4a852fc8099e79c49faa662687",
    "resolver_commit_num": 53,
    "title": "Seqfault on creation of dataframe with np.empty_like",
    "body": "This code segfaults:\n\n\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.2.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.13.0-91-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 23.0.0\nCython: 0.24\nnumpy: 1.11.1\nscipy: 0.17.1\nstatsmodels: 0.6.1\nxarray: 0.7.2\nIPython: 5.0.0\nsphinx: None\npatsy: 0.4.1\ndateutil: 2.3\npytz: 2016.4\nblosc: None\nbottleneck: 1.1.0\ntables: 3.2.2\nnumexpr: 2.6.0\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: 4.4.1\nhtml5lib: 0.999\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\npandas_datareader: None\n",
    "labels": [
      "Bug",
      "Reshaping"
    ],
    "comments": [
      "```\nIn [1]: pd.DataFrame(np.empty_like(np.array([1,2])))\nOut[1]:\n     0\n0  109\n1    0\n\nIn [2]: pd.DataFrame(np.empty_like(np.array(['a', 'b'])))\nOut[2]:\n  0\n0\n1\n\n# setfault\nIn [3]: pd.DataFrame(np.empty_like(Index(['a', 'b'])))\nOut[3]:\n```\n\nnot really sure what `np.empty_like` actually does under the hood. \n\ncc @charris\ncc @gfyoung \n",
      "I suspect it is a case of memory corruption on the `numpy` end when creating a `None` for the `object` dtype, as evidenced below:\n\n``` python\n>>> import numpy as np\n>>> import pandas as pd\n>>> arr = np.empty_like(pd.Index(['a', 'b']))\n>>> arr\narray([None, None], dtype=object)\n>>> arr2 = np.array([None, None], dtype=object)\n>>> arr == arr2\nTrue\n>>> arr.data.tobytes()    # segfault\nb'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n>>> arr2.data.tobytes()   # no segfault\nb'0\\x91\\xad\\x8b\\xd8\\x7f\\x00\\x000\\x91\\xad\\x8b\\xd8\\x7f\\x00\\x00'\n```\n\nYou can see here that the data is not corrupted when you have integers:\n\n``` python\n>>> import numpy as np\n>>> import pandas as pd\n>>> arr = np.empty_like(pd.Index([1, 2]))\n>>> arr\narray([140568033367712, 140568033367744], dtype=int64)\n>>> arr2 = np.array([140568033367712, 140568033367744], dtype=np.int64)\n>>> arr == arr2\nTrue\n>>> arr.data.tobytes()    # no segfault\nb'\\xa0\\x16\\xb2\\x8b\\xd8\\x7f\\x00\\x00\\xc0\\x16\\xb2\\x8b\\xd8\\x7f\\x00\\x00'\n>>> arr2.data.tobytes()   # no segfault\nb'\\xa0\\x16\\xb2\\x8b\\xd8\\x7f\\x00\\x00\\xc0\\x16\\xb2\\x8b\\xd8\\x7f\\x00\\x00'\n```\n\nAlso, you don't need an `Index` to trigger the segfault:\n\n``` python\n>>> import numpy as np\n>>> import pandas as pd\n>>> pd.DataFrame(np.empty_like([None]))\n```\n",
      "You can do more wierd tricks with this:\n\n``` python\nnp.empty_like(np.array([None, None], dtype=object)).data.tobytes()\nOut[41]: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n\nnp.empty_like(np.array([None, None], dtype=object)).astype(np.int64).data.tobytes()\nOut[42]: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n\nnp.empty_like(np.array([None, None], dtype=object)).astype(np.int64).astype(np.object).data.tobytes()\nOut[43]: b'\\xd0\\x01\\x9ac\\x00\\x00\\x00\\x00\\xd0\\x01\\x9ac\\x00\\x00\\x00\\x00'\n```\n\nThis looks like a NumPy bug.\n\nI suspect that an object array initialized using `None` should contain the value of `id(None)` if correct.  0-filled \"Nones\" don't make any sense, and seem to an incorrect treatment of NumPy.\n\n``` python\n>>> np.array([None]).data.tobytes()[::-1].hex()\n'0000000063979420'\n\n>>> hex(id(None))\n'0x63979420'\n```\n",
      "agree with @bashtage here - some memory aliasing going on\n\nwant to open a numpy side issue?\n",
      "@jreback : I opened a PR now in `numpy` that should fix the issue.  However, even if it doesn't get merged, technically, this issue could be closed since the bug is on the `numpy` side.\n",
      "running this in the debugger actually works. I think there is a reference issue to the Nones; an element-wise copy would fix it I suppose (from pandas perspective). but yes this is a numpy-bug.\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "labeled",
      "milestoned",
      "labeled",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 4,
    "additions": 79,
    "deletions": 4,
    "changed_files_list": [
      "asv_bench/benchmarks/frame_methods.py",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/lib.pyx",
      "pandas/tests/test_lib.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13727,
    "reporter": "jreback",
    "created_at": "2016-07-20T21:40:47+00:00",
    "closed_at": "2016-07-25T11:40:10+00:00",
    "resolver": "sinhrks",
    "resolved_in": "5f524d61fd336b850d34f13d5ffb2b6136073f21",
    "resolver_commit_num": 353,
    "title": "DEPR/ENH: isleapyear",
    "body": "xref #13706 \n\nshould deprecate `pandas.util.isleapyear`\nand impement as an accessort to `DTI`, `Timestamp` and `.dt`\n",
    "labels": [
      "Enhancement",
      "Deprecate",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "@jreback from https://github.com/pydata/pandas/pull/13706#issuecomment-234097643\n\n> I think is_leapyear is fine\n> \n> leapyear is really the common english word (I see though already getting inconsistent as we should really do is_year_leap)?\n\nNo native speaker, but the correct word is \"leap year\" I think? (and seems also the one most used based on google search). \nIf that is the case, I think `is_leap_year` is maybe more consistent with the other options?\n",
      "ok that's fine (slightly inconsistent with is_year_\\* but oh well)\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 12,
    "additions": 149,
    "deletions": 47,
    "changed_files_list": [
      "doc/source/api.rst",
      "doc/source/timeseries.rst",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/src/period.pyx",
      "pandas/tests/series/test_datetime_values.py",
      "pandas/tseries/index.py",
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tseries/tests/test_util.py",
      "pandas/tseries/util.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13730,
    "reporter": "achabotl",
    "created_at": "2016-07-20T23:20:27+00:00",
    "closed_at": "2016-08-08T14:11:31+00:00",
    "resolver": "agraboso",
    "resolved_in": "81819b7aa2537469448fbaeb4cd9e3d500f4e2a1",
    "resolver_commit_num": 2,
    "title": "period_range creates wrong dates when freq has multiple offsets",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n\n`date_range` works fine:\n\n\n#### Expected Output\n\nThe `freq` is right, but I'd expect the PeriodIndex to start at 2016-07-20, not in 1970, and to display 2h30 increments, something like:\n\n\n\nOr, if `freq` that combine multiple offsets are note supported, to at least raise and error.\n#### output of `pd.show_versions()`\n\nINSTALLED VERSIONS\n\n---\n\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: en_US.UTF-8\nLANG: en_US.UTF-8\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 23.1.0\nCython: 0.24\nnumpy: 1.10.4\nscipy: 0.17.1\nstatsmodels: 0.6.1\nxarray: 0.7.2\nIPython: 4.1.2\nsphinx: 1.4.1\npatsy: 0.4.1\ndateutil: 2.5.2\npytz: 2016.3\nblosc: 1.2.8\nbottleneck: 1.1.0\ntables: 3.2.2\nnumexpr: 2.6.0\nmatplotlib: 1.5.1\nopenpyxl: 2.3.4\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.9.2\nlxml: 3.6.0\nbs4: 4.4.1\nhtml5lib: 0.999\n 0.9.2\napiclient: None\nsqlalchemy: 1.0.12\npymysql: 0.6.7.None\npsycopg2: None\njinja2: 2.8\nboto: 2.40.0\n",
    "labels": [
      "Bug",
      "Period"
    ],
    "comments": [
      "Confirmed on master, thanks for reporting!\n",
      "For now, of course, you can use `pd.period_range('2016-07-20', periods=4, freq='150T')`. \n\nBefore 0.17 this raised as multiples of a freq were not supported before (#7832). When adding that feature probably not checked for combinations.\n\ncc @sinhrks \n",
      "Yeah, `Period` can't support combinasions as it is under current impl. Should raise or coerce to single freq.\n",
      "@sinhrks In some way it already _does_ coerce the combination correctly to a single freq, as in the output of the examples you see `150T`\n",
      "@jorisvandenbossche yes \"150T\" should work after #7832. What I tried to mean is coercing \"2H30T\" to \"150T\" internally.\n",
      "Yep, I understood that :-) What I meant is that somewhere this coercing already happens (only not in the right place for letting this work), since:\n\n```\nIn [44]:  pd.period_range('2016-07-20', periods=4, freq='2H30min').freq\nOut[44]: <150 * Minutes>\n```\n\ngives the correct freq, only not the correct values\n",
      "Ah i see... i haven't understood the phenomenon.\n",
      "A [question](http://stackoverflow.com/questions/38681319/pandas-period-range-gives-strange-results/38685169#38685169) about this has been posted on StackOverflow today.\n\nAn interesting observation made there is that\n\n```\npd.period_range(start='2016-01-01 10:00', freq = '1H1D', periods = 10)\n```\n\ngives the ~~correct~~ (see @sinhrks's comment below) output\n\n```\nPeriodIndex(['2016-01-01 10:00', '2016-01-01 11:00', '2016-01-01 12:00',\n             '2016-01-01 13:00', '2016-01-01 14:00', '2016-01-01 15:00',\n             '2016-01-01 16:00', '2016-01-01 17:00', '2016-01-01 18:00',\n             '2016-01-01 19:00'],\n            dtype='int64', freq='25H')\n```\n\nwhile\n\n```\npd.period_range(start='2016-01-01 10:00', freq = '1D1H', periods = 10)\n```\n\n(notice the reversal of the `freq` string) does not.\n\n```\nPeriodIndex(['1971-12-02 01:00', '1971-12-02 02:00', '1971-12-02 03:00',\n             '1971-12-02 04:00', '1971-12-02 05:00', '1971-12-02 06:00',\n             '1971-12-02 07:00', '1971-12-02 08:00', '1971-12-02 09:00',\n             '1971-12-02 10:00'],\n            dtype='int64', freq='25H')\n```\n",
      "@agraboso Your example both look incorrect. 1st one should have 25H freq rather than 1H. 1st and 2nd should output the same result.\n",
      "@sinhrks You're right, of course. I looked at the first element and the hour on the rest and thought it was fine, but it is not.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 7,
    "additions": 268,
    "deletions": 40,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/src/period.pyx",
      "pandas/tseries/frequencies.py",
      "pandas/tseries/period.py",
      "pandas/tseries/tests/test_offsets.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13743,
    "reporter": "martijnvermaat",
    "created_at": "2016-07-21T17:51:57+00:00",
    "closed_at": "2016-09-02T22:08:10+00:00",
    "resolver": "pijucha",
    "resolved_in": "d26363b96481ba2df3978925e18ca567c79901dd",
    "resolver_commit_num": 5,
    "title": "groupby on multiple columns does not preserve (categorical) dtype",
    "body": "When doing a `groupby` on more than one column, the resulting `MultiIndex` does not seem to preserve the original column dtypes. I noticed it when working with `Categorical` columns, expecting `CategoricalIndex` when grouping on them, but this is only the case when grouping on just one column.\n\nI did see that the behaviour was [discussed in a PR](#issuecomment-225072647), but it ultimately was not addressed.\n#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Groupby",
      "MultiIndex",
      "Categorical",
      "Bug"
    ],
    "comments": [
      "I thought I'd quickly workaround it by converting the resulting `MultiIndex` to one with two `CategoricalIndex`s via `reset_index()` and `set_index()`, but it seems that `set_index` similarly forgets the column dtypes:\n\n``` python\nIn [6]: df.groupby(['a', 'b']).sum().reset_index().assign(\n   ...:     a=lambda df: df.a.astype('category', categories=list('xyz')),\n   ...:     b=lambda df: df.b.astype('category', categories=list('xyz'))\n   ...: ).set_index(['a', 'b']).reset_index().dtypes\nOut[6]: \na     object\nb     object\nc    float64\ndtype: object\n```\n\nSo I guess my bug report is now for `groupby` as well as for `set_index`.\n",
      "> I did see that the behaviour was discussed in a PR, but it ultimately was not addressed.\n\nI still have it in mind and will submit a fix soon.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "referenced",
      "cross-referenced",
      "milestoned",
      "labeled",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 14,
    "additions": 370,
    "deletions": 91,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/categorical.py",
      "pandas/core/panel.py",
      "pandas/core/reshape.py",
      "pandas/indexes/multi.py",
      "pandas/io/pytables.py",
      "pandas/tests/frame/test_alter_axes.py",
      "pandas/tests/frame/test_reshape.py",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tests/test_categorical.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/test_reshape.py",
      "pandas/tests/types/test_dtypes.py",
      "pandas/tools/merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13746,
    "reporter": "pkch",
    "created_at": "2016-07-22T01:00:31+00:00",
    "closed_at": "2016-07-27T10:39:32+00:00",
    "resolver": "wcwagner",
    "resolved_in": "63285a4dbb50f139f6996c94ca6d473e7b42ae0f",
    "resolver_commit_num": 1,
    "title": "DOC: document the perils of reading mixed dtypes / how to handle",
    "body": "#### Code Sample, a copy-pastable example if possible\n\nSee for example -read-csv-dtype-inference-issue\n(the behavior didn't since 3 years ago, except a warning is now issued)\n\n\n\nSimilar examples: the column that contains '1' and 1 is inferred as string; one that contains 1 and '1' is inferred as numeric.\n\n\n\nFurthermore, `pd.to_numeric(df[1])` won't actually work in the above case.\n\nWhile it's an understandable behavior, it is unexpected to many users and isn't even documented (beyond the general warnings that type inference isn't perfect). Given the number of people who use pandas for reading csv files, without knowing the history of the library or the mechanics of type inference, this results in a lot of wasted time identifying and fixing a problem that could have been prevented.\n\nI suggest at least adding clear documentation on this, but preferably changing the _default_ behavior to either fail with an error stating that type inference failed and that dtypes should be explicitly provided (I don't think it's possible to use two passes since the input data may be a generator that is exhausted after the first read). \n#### Expected Output\n\nthe column \n#### output of `pd.show_versions()`\n",
    "labels": [
      "IO CSV",
      "Dtypes",
      "Docs",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "I could see an argument for the first case raising - although it's been this way forever and the warning is decent and gives explicit corrective action?\n\n```\ndf2 = pd.read_csv('test', sep='\\t')\nC:\\Users\\Chris\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2705: DtypeWarning: \nColumns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n```\n\nCase 2 is a different issue altogether - you have a space in your data and need to pass `skipinitialspace=True`\n\n``` python\nIn [26]: from pandas.compat import StringIO\n    ...: df = pd.read_csv(StringIO('1, \"1\"\\n\"1\",1'), header=None, skipinitialspace=True)\n    ...: df\nOut[26]: \n   0  1\n0  1  1\n1  1  1\n\nIn [27]: df.dtypes\nOut[27]: \n0    int64\n1    int64\ndtype: object\n```\n",
      "@chris-b1  Understood. Perhaps then clarify the docs? \nThe most common case I see where this was a problem is this:\n\n```\ndf = pd.read_csv(io.StringIO('1,\\n\"\",\\n3a,\\n4,'), header=None)\n```\n\nNow `df[0]` is inferred as object due to some values that couldn't be converted. But there's no easy to identify which rows caused this problem. Applying `pd.numeric(df[0], errors='ignore')` somehow leaves the entire column as a string as long as at least one value couldn't be parsed (is it a bug btw?). Applying `pd.numeric(df[0], errors='coerce')` would not separate the (usually numerous) empty strings that are interpreted as NaN from the actual data issues that cause conversion to fail.\n\nThe ideal solution would have been to produce an optional log of parse errors. Barring that, perhaps modifying `pd.to_numeric` to actually ignore only the unconvertible value rather than the entire column.\n",
      "@pkch For converting the object column to numeric, you need to set `errors='coerce'`:\n\n```\nIn [61]: pd.to_numeric(df[0], errors='coerce')\nOut[61]:\n0    1.0\n1    NaN\n2    NaN\n3    4.0\nName: 0, dtype: float64\n```\n\nAs the docstring says, `errors='ignore'` will \"invalid parsing will return the input\". But maybe it can be made clearer that the entire input is returned, and not for the single element that failed parsing. (PRs always welcome!)\n\nIf you want to find the values that did cause the conversion to fail, you can first drop the NaN values before applying `to_numeric`, or you can compare where there were NaN values before and after. Eg this identifies the problematic values: `df0_converted = pd.to_numeric(df[0], errors='coerce'); df[0].notnull() &  df0_converted.isnull()`\n\nIt would maybe also be nice if `to_numeric(df[0], errors='raise')` actually showed the failing string.\n",
      "yeah at best this is a doc issue, a nice note section under dtypes in io.rst / cookbook of the perils of having mixed dtypes.\n"
    ],
    "events": [
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 64,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/basics.rst",
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.19.0.txt"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13793,
    "reporter": "jreback",
    "created_at": "2016-07-25T23:44:00+00:00",
    "closed_at": "2016-07-27T10:22:38+00:00",
    "resolver": "sinhrks",
    "resolved_in": "10da3ae14d25f28d1c6bcfe368a03f0b0b754cc5",
    "resolver_commit_num": 358,
    "title": "ERR: disallow RangeIndex()",
    "body": "xref #13749 \n\nthis is not consistent with other Indexes where we don't allow them to be constructed with no data passed (it can be `None`), but that's the idea. `start` must be not-None.\n\nIOW this should raise `ValueError`\n\n\n",
    "labels": [
      "Indexing",
      "Difficulty Novice",
      "API Design",
      "Error Reporting",
      "Effort Low"
    ],
    "comments": [
      "will send a PR soon. Shouldn't the error be `TypeError` compat with `Index`?\n\n```\npd.Index(None)\n# TypeError: Index(...) must be called with a collection of some kind, None was pa\nssed\n```\n",
      "yep - was looking at something else\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 38,
    "deletions": 18,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/indexes/range.py",
      "pandas/tests/indexes/test_range.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13797,
    "reporter": "ygriku",
    "created_at": "2016-07-26T01:55:40+00:00",
    "closed_at": "2016-09-13T22:25:32+00:00",
    "resolver": "ygriku",
    "resolved_in": "fb25cca654db9b626a3993bb62a01857807794b4",
    "resolver_commit_num": 0,
    "title": "BUG: iloc fails with non lex-sorted MultiIndex",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n\ndf1 (whose index is not lex sorted) fails with iloc access:\n\n\n#### Expected Output\n\ndf2 (whose index is lex sorted) works as expected:\n\n\n\nThis attributeError does not occur when the DataFrame.values consist of numpy objects (e.g. numpy.int32) because they have the ndim attribute. (Although the performance warning remains, it may be another issue).\n\nI found that the addition of an **if** statement can remedy this in pandas/core/indexing.py. This just makes the **_getitem_tuple(self, tup)** be aware of objects without the ndim attribute, as **_getitem_nested_tuple(self, tup)** is (I will prepare a pull request if it is helpful.)\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Bug",
      "Indexing",
      "MultiIndex"
    ],
    "comments": [
      "@YG-Riku actually the fix should start [here](https://github.com/pydata/pandas/blob/master/pandas/core/indexing.py#L905)\n\n`.iloc` should not hit the `MultiIndex` branch testing at all. If you want to give this a try would be great.\n",
      "@jreback \nThank you very much for your response.\n\nThat's the point where I stumbled on too. Just commenting out the lines:\n\n``` python\n#        ax0 = self.obj._get_axis(0)\n#        if isinstance(ax0, MultiIndex):\n#            result = self._handle_lowerdim_multi_index_axis0(tup)\n#            if result is not None:\n#                return result\n```\n\ndiminishes the error (and the warning too). But I am not sure about the consequence of this removal of the codes (any side-effect?). \n",
      "almost certainly needs to be more selective\neg dispatch on self.name == 'iloc'\n\nlots and lots of indexing tests must still pass\n",
      "I think I can do some trials-and-errors with the existing test cases of pandas/tests. But if there is a need to prepare a decent set of new test codes, that will be too much burden for me. (Anyway, I will start looking into the test cases ... )\n",
      "no as I said there are hundreds of tests already\nyou can just add this case\n",
      "Thank you for the comment. I will try. It is exciting to have a chance to contribute!. (But, please don't wait for me. Probably the time I can spare for this would be mostly on weekends.)\n",
      "I found that the following simple amendment works as jreback suggested.\nThis passed `nosetests pandas/tests/test_multilevel.py` and no significant performance change was detected (at least in some trial). \n\n``` python\n@@ -903,7 +903,7 @@ class _NDFrameIndexer(object):\n\n         # we maybe be using a tuple to represent multiple dimensions here\n         ax0 = self.obj._get_axis(0)\n-        if isinstance(ax0, MultiIndex):\n+        if isinstance(ax0, MultiIndex) and self.name != 'iloc':\n             result = self._handle_lowerdim_multi_index_axis0(tup)\n             if result is not None:\n                 return result\n```\n\nWhat is the next step?\n",
      "you submit a PR with tests and the fix\n\nsee here: http://pandas.pydata.org/pandas-docs/stable/contributing.html\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 32,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/indexing.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13813,
    "reporter": "mynci",
    "created_at": "2016-07-27T08:23:37+00:00",
    "closed_at": "2016-07-29T10:22:23+00:00",
    "resolver": "ivannz",
    "resolved_in": "54b2777089df8d723fabfb28d8a1759a388b95a3",
    "resolver_commit_num": 1,
    "title": "Groupby and shift causing coredump",
    "body": "Hi, \n\nI have attempted to reduce this to the smallest example that exhibits this issue - rather than a useful example. The problem is that the operation causes python to core dump.\n\nIn the original case in which I discovered this the core dump would only occur sometimes (and when I put it in a loop it would occur on different iterations). This code seems to core dump on the third iteration every time I have run it.\n\nCode example:\n\n\n\nWith the attached data file (tab separated) - code assumes in the same directory:\n[error_report.txt]()\n\nThis the output I get:\n\n\n\nIf I modify the code to use apply and then add the shifted column inside the apply function then there is no error. Similarly if I use .shift(0) I do not get the error.\n\nVersion info:\n\n\n\nRegards\nStephen\n",
    "labels": [
      "Bug",
      "Groupby"
    ],
    "comments": [
      "ok, will take a volunteer to debug this.\n",
      "Using exception stack traces I managed to pinpoint the problem. I believe it is within the [group_shift_indexer](https://github.com/pydata/pandas/blob/master/pandas/src/algos_groupby_helper.pxi#L1328) procedure.\n\nWhen I reintroduced the cython array boundary check option (**@cython.boundscheck(True)**) your use case did not crash, but instead raised a boundary violation error. The core of the problem is that the **labels** array, obtained from the **groupby's grouper** property, besides proper group integer-coded labels might contain the so called **null keys** (with value -1).\n\nLines L1358-L1359 do not properly check for this corner case. When I inject this patch:\n\n``` python\n...\n                lab = labels[ii]\n\n                # Skip null keys\n                if lab == -1:\n                    continue\n\n                label_seen[lab] += 1\n...\n```\n\nthe problem goes away.\n",
      "**-1** values occur in the **labels** array when a groupby-key contains a missing value.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "milestoned"
    ],
    "changed_files": 4,
    "additions": 54,
    "deletions": 17,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/src/algos_groupby_helper.pxi",
      "pandas/src/algos_groupby_helper.pxi.in",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13822,
    "reporter": "Wilfred",
    "created_at": "2016-07-27T19:02:37+00:00",
    "closed_at": "2016-08-02T10:50:50+00:00",
    "resolver": "shawnheide",
    "resolved_in": "768bf495b9b1f2e6a51708ca6ba83da239cbe504",
    "resolver_commit_num": 5,
    "title": "KeyError shows incorrect column name when DataFrame has duplicate columns",
    "body": "\n\nI expected to see `KeyError: \"['y'] not in index\"`.\n\nI've tested this on the latest code in master (and on 0.16):\n\n\n",
    "labels": [
      "Indexing",
      "Error Reporting",
      "Effort Low",
      "Difficulty Novice"
    ],
    "comments": [
      "Thanks for the report.\n\nThe bug is [here](https://github.com/pydata/pandas/blob/31f8e4dc8af8f0d109f366d0b726aef210bf7904/pandas/core/indexing.py#L1224). The code assumes that they're the same length, which is true if there aren't any duplicates.\n\nShouldn't be too hard to fix.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 13,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/indexing.py",
      "pandas/tests/indexing/test_indexing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13834,
    "reporter": "lopezco",
    "created_at": "2016-07-29T12:48:05+00:00",
    "closed_at": "2016-08-09T21:48:25+00:00",
    "resolver": "wcwagner",
    "resolved_in": "49f99ac59f0bf185df48dd23735a919a330a6b0d",
    "resolver_commit_num": 3,
    "title": "[BUG]: Wrong unix timestamp parsing with floating point using pd.to_datetime",
    "body": "Hello!\n\nI'm trying to parse floating point timestamps with `pd.to_datetime` but the only way that it works is using the **deprecated parameter** `coerce=True`\n\nFor example, without `coerce=True` the result is wrong.\n\n\n\nHowever, with `coerce=True` the result is correct.\n\n\n\nI'm happy to help if you need more details.\n#### Code Sample (copy-pastable example)\n\n\n#### pd.show_versions() output\n\n\n",
    "labels": [
      "Bug",
      "Timeseries",
      "Dtypes",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "Problem is around here - there's some kind of fastpath that tries directly converting to integers, but doesn't seem to handle truncation correctly.\nhttps://github.com/pydata/pandas/blob/master/pandas/tslib.pyx#L2098\n\nNote that you can also workaround this using the non-deprecated `errors` kwarg.\n\n```\nIn [41]: pd.to_datetime(1.1, unit='s', errors='coerce')\nOut[41]: Timestamp('1970-01-01 00:00:01.100000')\n```\n",
      "Thank you the workaround!\n",
      "@chris-b1 \n\nThis should cast like this\n\n```\n In [14]: np.array([1.1]).astype('i8', casting='no')\nTypeError: Cannot cast array from dtype('float64') to dtype('int64') according to the rule 'no'\n```\n\ninstead I think (so this will then hit the other path)\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 10,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13844,
    "reporter": "VelizarVESSELINOV",
    "created_at": "2016-07-29T21:03:30+00:00",
    "closed_at": "2016-08-26T20:43:20+00:00",
    "resolver": "tom-bird",
    "resolved_in": "0db43045508c474f1fcaf8c3f10a306c0e571c91",
    "resolver_commit_num": 0,
    "title": "Empty dataset: typerror ufunc add cannot use operands with types dtype('<M8[ns]') and dtype('O')",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Current Output\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Bug",
      "Timeseries",
      "Timedelta",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "Here is a simpler replication.\n\n```\nIn [63]: df = pd.DataFrame({'A' : Series(dtype='M8[ns]'), 'B' : Series(dtype='m8[ns]')})\n\nIn [64]: df\nOut[64]: \nEmpty DataFrame\nColumns: [A, B]\nIndex: []\n\nIn [65]: df.dtypes\nOut[65]: \nA     datetime64[ns]\nB    timedelta64[ns]\ndtype: object\n\nIn [66]: df.A+df.B\nTypeError: ufunc add cannot use operands with types dtype('<M8[ns]') and dtype('O')\n```\n\nYeah this empty case is prob address by coercion, maybe @sinhrks has a better idea.\n",
      "The op is performed in `_TimeOp` class, and it regards empty input as `offset` even if it has a `dtype`. \n- https://github.com/pydata/pandas/blob/master/pandas/core/ops.py#L596\n\nThen, offset is coerced to `object` here. \n- https://github.com/pydata/pandas/blob/master/pandas/core/ops.py#L532\n\nFixing `_TimeOp._is_offset` should solve the problem. PR is appreciated!\n",
      "Happy to take a look at this and submit a PR\n",
      "So, we shouldn't regard something as `offset` if its empty? This is a three line fix, and certainly resolves the error in the above two examples.\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 12,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/ops.py",
      "pandas/tests/series/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13863,
    "reporter": "jreback",
    "created_at": "2016-08-01T10:52:02+00:00",
    "closed_at": "2016-08-01T20:00:45+00:00",
    "resolver": "ivannz",
    "resolved_in": "49243d6efcaf4c267b0b1bbab73d9f202f00557c",
    "resolver_commit_num": 2,
    "title": "COMPAT: windows builds breaking after #13788",
    "body": "see [here](#issuecomment-236409748)\n\nlooks like a pointer specification, see #13788 \n",
    "labels": [
      "Windows",
      "Compat"
    ],
    "comments": [
      "cc @pijucha \ncc @ivannz \n",
      "Sorry, for that. I can make PR fixing this right away.\n",
      "Unfortunately I am able to test the build only on win7 64bit virtual machine with anaconda 4.1.1 and python 2.7.12 using the [VC for python2.7](https://www.microsoft.com/en-us/download/details.aspx?id=44266). I can't get VS2015 just yet.\n",
      "I tested the build on python 3.5 with VS2015 and on python 2.7 with VS2008 as outlined [here](http://pandas.pydata.org/pandas-docs/stable/contributing.html#contributing-windows) and it was successful.\n",
      "we have appveyor CI which does not agree: https://ci.appveyor.com/project/jreback/pandas-465/build/1.0.825\n\nthis also fails locally for me as well.\n",
      "I am terribly sorry, I meant the build of the branch for the PR I am currently making.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 1,
    "additions": 2,
    "deletions": 2,
    "changed_files_list": [
      "pandas/src/parser/tokenizer.c"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13884,
    "reporter": "jzwinck",
    "created_at": "2016-08-02T19:08:37+00:00",
    "closed_at": "2016-08-04T21:04:30+00:00",
    "resolver": "jzwinck",
    "resolved_in": "9c1e738df7effbf89c98dea59b0482b057c8c8b8",
    "resolver_commit_num": 0,
    "title": "Round trip through HDF5 with format=table and localized DatetimeIndex discards index name",
    "body": "This should work, but the assert fails:\n\n\n\nIt works fine if you don't localize the DatetimeIndex, or if you don't use format='table'.\n\nThe index name \"expected\" is actually stored in the \"info\" attribute inside the HDF5 file whether it's localized or not.  But the format is slightly different.  If not localized:\n\n\n\nIf localized:\n\n\n\nI don't know enough to say whether the bug is in read_hdf(), to_hdf(), or PyTables.\n\nI'm using Pandas 0.18.1.\n",
    "labels": [
      "Bug",
      "Indexing",
      "Difficulty Novice",
      "IO HDF5",
      "Effort Low"
    ],
    "comments": [
      "thought there was an issue about this, but can't seem to find it.\n\nSo the attribute is saved. Must not be set on the read-back somehow.\n\n```\nIn [38]: store.root.world._v_attrs\nOut[38]:\n/world._v_attrs (AttributeSet), 15 attributes:\n   [CLASS := 'GROUP',\n    TITLE := '',\n    VERSION := '1.0',\n    data_columns := [],\n    encoding := 'UTF-8',\n    index_cols := [(0, 'index')],\n    info := {1: {'type': 'Index', 'names': [None]}, 'index': {'tz': <UTC>, 'index_name': 'expected'}},\n    levels := 1,\n    metadata := [],\n    nan_rep := 'nan',\n    non_index_axes := [(1, ['a'])],\n    pandas_type := 'frame_table',\n    pandas_version := '0.15.2',\n    table_type := 'appendable_frame',\n    values_cols := ['values_block_0']]\n```\n",
      "@jzwinck HDFStore is a meta-data layer on top of PyTables. pull-requests are welcome.\n"
    ],
    "events": [
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 18,
    "deletions": 5,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/io/pytables.py",
      "pandas/io/tests/test_pytables.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13891,
    "reporter": "lia-simeone",
    "created_at": "2016-08-03T20:00:38+00:00",
    "closed_at": "2016-08-08T14:06:28+00:00",
    "resolver": "agraboso",
    "resolved_in": "72be37bcc855ef1fb01ebda78f6aa4ef3bcc6315",
    "resolver_commit_num": 1,
    "title": "BUG: allow describe() for on boolean-only columns",
    "body": "I know I can obtain the expected output by using `include=['bool']`, but that feels bad to me as a user. I want `describe()` to know that I'm only asking for boolean columns and not freak out.\n\nThank you!\n#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n\n#### Actual output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Bug",
      "Numeric",
      "Dtypes",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "must be something in the exlusion logic, as an all object dtype works. PR's are welcome.\n",
      "The problem is in [`generic.py#L5141-L5143`](https://github.com/pydata/pandas/blob/master/pandas/core/generic.py#L5141):\n\n``` python\nif len(self._get_numeric_data()._info_axis) > 0:\n    # when some numerics are found, keep only numerics\n    data = self.select_dtypes(include=[np.number])\n```\n\n`_get_numeric_data()` keeps boolean columns ([`BoolBlock`](https://github.com/pydata/pandas/blob/master/pandas/core/internals.py#L1749) inherits from [`NumericBlock`](https://github.com/pydata/pandas/blob/master/pandas/core/internals.py#L1506)), but `select_dtypes(include=[np.number])` does not.\n\n``` python\n>>> test_df._get_numeric_data()\n  test_ind_1 test_ind_2\n0      False      False\n1      False       True\n2       True       True\n3       True      False\n4      False      False\n5       True       True\n6       True       True\n7      False       True\n>>> test_df.select_dtypes(include=[np.number])\nEmpty DataFrame\nColumns: []\nIndex: [0, 1, 2, 3, 4, 5, 6, 7]\n```\n\nI'm working on a fix, but I'm worried I may be getting too deep into the internals of pandas...\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "renamed",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 58,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/generic.py",
      "pandas/tests/frame/test_analytics.py",
      "pandas/tests/series/test_analytics.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13901,
    "reporter": "starhel",
    "created_at": "2016-08-04T11:11:55+00:00",
    "closed_at": "2016-09-22T10:34:46+00:00",
    "resolver": "agraboso",
    "resolved_in": "ebc4ac101b8dab001d5d711075a2a50c5eead088",
    "resolver_commit_num": 5,
    "title": "Groupby doesn't accept level=[0] for Index.",
    "body": "Today I've found weird behaviour of level parameter of groupby. I need to groupby two dataframes by index and this code is working as long as I'm using Multiindex. \n\n\n\nUnfortunately when I'm using simple Index I get:\n\n\n\nIn other way if I passed 0 instead of [0], everything is working properly. It's weird and cause that it's hard to write reusable code. \n#### INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.12.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.2.0-42-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_GB.UTF8\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 20.3\nCython: None\nnumpy: 1.11.1\nscipy: None\nstatsmodels: None\nxarray: None\nIPython: 4.2.0\nsphinx: None\npatsy: None\ndateutil: 2.4.1\npytz: 2016.4\nblosc: None\nbottleneck: 1.1.0\ntables: 3.2.2\nnumexpr: 2.6.0\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: None\nxlwt: 1.1.2\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.13\npymysql: None\npsycopg2: None\njinja2: None\nboto: None\npandas_datareader: None\n",
    "labels": [
      "Bug",
      "Groupby",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "Using `range` as level may work but not intended. It doesn't work PY3 at least.\n",
      "I suppose this is legit, as `[0]` de-facto is equivalent to a scalar.\n",
      "ok, then the fix should include the support of `range` in py3.\n",
      "ah sorry you refer to one length list, not `range`, @jreback?\n",
      "yes @sinhrks we don't accept non-list-likes anywhere (meaning range) for things like this. They must be a list/tuple/array (or scalar in this case)\n",
      "@sinhrks Sorry for the confusion. In python2 range returns list and I totally forgot about the change in py3. \n",
      "In Python 3 `range` is an iterable &mdash; equivalent to Python 2's `xrange`. It fits the `is_list_like` definition, so my PR does accept them.\n"
    ],
    "events": [
      "referenced",
      "cross-referenced",
      "referenced",
      "commented",
      "labeled",
      "labeled",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "referenced",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 58,
    "deletions": 9,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13904,
    "reporter": "PeterKucirek",
    "created_at": "2016-08-04T15:39:32+00:00",
    "closed_at": "2017-02-15T15:25:57+00:00",
    "resolver": "jreback",
    "resolved_in": "e351ed0fd211a204f960b9116bc13f75ed1f97c4",
    "resolver_commit_num": 4232,
    "title": "Example of High Memory Usage of MultiIndex ",
    "body": "This is a dupe of #1752, in which @jreback recommended a new issue be opened. I have a (hopefully) reproduce-able example which shows how using MultiIndex can explode RAM usage.\n\nAt the top I define a function to get the memory of the current Python process. This will be called as I create arrays to get the \"true\" space of each object.\n\n\n\nI need some functions to construct a plausible MultiIndex which matches roughly the actual data I'm using.\n\n\n\nConstructing the actual MultiIndex:\n\n\n\nBased on this, my fake trip table is taking up about 171.7 - 121.7 = 50MB of RAM. I haven't done any fancy indexing, just initialized the table. But, when I call `trips.info()` (which appears to instantiate the array of PyTuples) this happens:\n\n\n\nMy process's memory usage balloons to 723MB!. Doing the math, the cached indexer takes up 723.6 - 171.7 = 551 MB, a **tenfold** increase over the actual DataFrame!\n\nFor this fake dataset, this is not so much of a problem, but my production code is 20x the size and I soak up 27 GB of RAM when I as much as look at my trips table. \n\nAny performance tips would be appreciated; the only thing I can think of right now is \"don't use a MultiIndex\". It sounds like a 'proper' fix for this lies deep in the indexing internals which is far above my pay grade. But I wanted to at least log this issue with the community.\n\nOutput of `pd.show_versions()`:\n\n\n",
    "labels": [
      "Performance",
      "MultiIndex",
      "Difficulty Advanced",
      "Effort High"
    ],
    "comments": [
      "I don't know much about the indexing internals, but is it possible to design a \"fast-path\" MultiIndex when, say, level arrays are all integers? And fall back on the original behaviour when the MultiIndex gets more complicated?\n",
      "The problem here is that a array of tuples is created and cached on some operations.  But the tuples aren't needed for many things you'd do with a `MultiIndex`.  So an _ugly_ hack you could do to prevent this from happening is:\n\n```\ndef fail(self):\n    raise RuntimeError(\"Not allowing boxed MultiIndex\")\n\npd.MultiIndex.values = property(fail)\n```\n\nThen, you can still do _most_ things with the `MultiIndex`, but if you try something that would materialize the tuples, you'll get an error.  Not really recommended ... but kind of helps, or at least will let you know what step in your program is creating the tuples.\n\n```\nIn [9]: trips.head()\nOut[9]: \n                          origin  destination\nhhid pid tour_id trip_id                     \n0    0   0       0           107          122\n                 1           159          113\n                 2           115          152\n     1   0       0           178          188\n                 1           195          184\n\nIn [10]: idx = pd.IndexSlice\n\nIn [11]: trips.loc[idx[0, 1, :, 2], :]\nOut[11]: \n                          origin  destination\nhhid pid tour_id trip_id                     \n0    1   0       2           185          178\n\nIn [12]: trips.groupby(level='tour_id').sum()\nOut[12]: \n            origin  destination\ntour_id                        \n0        350398162    350323339\n1         52599924     52592203\n2         17548605     17554048\n\nIn [13]: trips.info()\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n\nRuntimeError: Not allowing boxed MultiIndex\n```\n",
      "can a simpler example be created for this? (simpler in that with no numba)\n",
      "yes, you don't _ever_ actually materalize the tuples, unless `.values` is called (at which point they are cached; a MI is immutable so this is feasible)\n",
      "the act of printing a frame is probably the culprit here\n\nsomewhere `.values` is called on the index when printing (and its prob _then_ sliced), this materializes it.\n",
      "Actually one could try disabling the caching of `.values` altogether (or maybe only do it below some certain length in order to not sacrifice perf on 'small/medium' sized things).\n\nand see if perf suite suffers.\n",
      "@jreback If dependence on Numba is a problem, then the `@nb.jit()` decorator can be removed and it should just work fine as a regular Python function, albeit a lot slower.\n\n@chris-b1 I tried your trick, but unfortunately it just breaks all sorts of things. \n\n~~For starters, I cannot even reproduce your index-slice (it makes a call to `values`)A `loc[]` lookup breaks if a column indexer is not specified. A call to `is_monotonic_increasing` also breaks.~~\n\nEDIT: Only `is_monotonic_increasing` breaks.\n",
      "I'm on 0.18.1 as well - the slice I wrote doesn't call `.values` but if you slice with a tuple it will.\n\n`is_monotonic_increasing` feels like it could/should be implemented without `.values`, but you're right, it does fail.\n",
      "I've also found that `values` gets called when using `MultiIndex.equals()`, which is another method that I feel could be implemented without the tuples.\n",
      "Yeah, this is actually more problematic than I realized - it's the same problem noted in the original issue - that the tuples are what's being used for the underlying hash table - it's just that some ops, like groupby and slice, are smart enough to not need it.\n",
      "It looks to me that, because of this problem, MultiIndex is fundamentally broken for large arrays. They just take up too much RAM to be useful in production code. Either MutliIndex needs be refactored so that PyTuples aren't used for `values` OR a number of methods need to be refactored to not call `values` at all. The former is more 'pure' and it sounds like @njsmith was looking into it when the original issue was raised (and @wesm agreed). \n\nI'm under a time crunch so the most I can do for the moment is refactor my code to drop as many MultiIndex's as possible (of which I am using many). But I'm interested in learning about the indexing internals so I may revisit this.\n",
      "I agree that the underlying implementation just doesn't scale, and probably was always intended to be replaced.  \n\nI'm sure a well thought out PR fixing the internals would be accepted, but given the [discussion](https://mail.python.org/pipermail/pandas-dev/2016-July/000512.html) around internals refactoring, this might be the type of the thing that is punted to \"pandas 2.0\"\n",
      "actually this could / should be done independtly or pandas 2.0. This is clearly an area that could have improvement, but the API is not touched.\n\nso @PeterKucirek @chris-b1 welcome improvements here in the current environment.\n",
      "The previous thread contains a link to [ a stale branch which looks to implement a fix](https://github.com/njsmith/pandas/commits/smaller-multi-index). So anyone working on this may not have to start from zero.\n",
      "Totally with you on this. The complicated part is hashing, so we would need to devise a custom hash table implementation that does not rely on materialized PyTuple objects\n",
      "so @PeterKucirek if you are interested in trying this out (still a WIP):\r\n\r\nhttps://github.com/pandas-dev/pandas/compare/master...jreback:mi\r\n\r\nthis uses the new-ish hashing algos ``pandas.tools.hashing`` to hash the values of the MI, directly as ``uint64`` (allowing the hashtable to be entirely done in a compact repr and w/o the gil), just like the other hashtables.\r\n\r\n- some tests still broken\r\n- doesn't handle non-unique MI's.\r\n- it doesn't *appear* to use that much less memory (its about 60% of prior), but then again ``.values`` *can* still be called (to instantiate tuples), it just isn't called on populating the hash table (but can be elsewhere, which is *still* problematic, but one step at a time).\r\n- this is about 2.5-3x faster for indexing than before. (and releases gil)",
      "Here's my simple test (all done in separate sessions)\r\n0.19.2\r\n\r\n```\r\nIn [1]: i = MultiIndex.from_product([np.arange(1000), np.arange(10), np.arange(10), np.arange(10), np.arange(3)])\r\n\r\nIn [3]: len(i)\r\nOut[3]: 3000000\r\n\r\nIn [2]: %time i.get_loc((0,0,1,0,1))\r\nCPU times: user 2.04 s, sys: 238 ms, total: 2.27 s\r\nWall time: 2.27 s\r\nOut[2]: 31\r\n\r\nIn [2]: %memit i.get_loc((0,0,1,0,1))\r\npeak memory: 583.14 MiB, increment: 486.66 MiB\r\n```\r\n\r\nThis PR\r\n\r\n```\r\nIn [2]: %time i.get_loc((0,0,1,0,1))\r\nCPU times: user 632 ms, sys: 286 ms, total: 918 ms\r\nWall time: 924 ms\r\nOut[2]: 31\r\n\r\nIn [3]: %memit i.get_loc((0,0,1,0,1))\r\npeak memory: 423.96 MiB, increment: 318.61 MiB\r\n```",
      "Ok, I will try it out and report back."
    ],
    "events": [
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 22,
    "additions": 605,
    "deletions": 125,
    "changed_files_list": [
      "asv_bench/benchmarks/indexing.py",
      "asv_bench/benchmarks/reindex.py",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/algorithms.py",
      "pandas/core/frame.py",
      "pandas/hashtable.pxd",
      "pandas/index.pyx",
      "pandas/indexes/base.py",
      "pandas/indexes/multi.py",
      "pandas/io/pytables.py",
      "pandas/src/algos_common_helper.pxi.in",
      "pandas/src/hashtable_class_helper.pxi.in",
      "pandas/tests/frame/test_mutate_columns.py",
      "pandas/tests/frame/test_repr_info.py",
      "pandas/tests/groupby/test_groupby.py",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tests/indexing/test_multiindex.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tests/tools/test_hashing.py",
      "pandas/tests/tools/test_join.py",
      "pandas/tools/hashing.py",
      "pandas/types/cast.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13923,
    "reporter": "ozak",
    "created_at": "2016-08-06T17:30:47+00:00",
    "closed_at": "2016-08-10T22:37:44+00:00",
    "resolver": "bashtage",
    "resolved_in": "257ac884d61a74990d1cb4d72c48b1c9003298d5",
    "resolver_commit_num": 27,
    "title": "Stata read categoricals gives ValueError: Categorical categories must be unique",
    "body": "I am trying to open some Stata files generated in [IPUMS International](), but I am getting a `ValueError: Categorical categories must be unique`. I opened in Stata and could not find a repeated category for the column I am trying to import. I had similar issues with other datasets from the same source, which seemed to be generated by missing values, but that does not seem to be the case here. Here's the [link to the file](?id=0By-h7HPv1NhVVllUUEFkc1JucGs) I am trying to read.\n#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "IO Stata",
      "Error Reporting",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "Here's the [link to a file](https://drive.google.com/open?id=0By-h7HPv1NhVbXI3aGJTVENqTkk) with a column with only categoricals (non-repeated) and the same issue arises.\n",
      "cc @bashtage \ncc @kshedden \n",
      "The value labels for the data posted a\n\n```\n{101: 'bainouk',\n 102: 'badiaranke',\n ...\n 111: 'diola',\n 112: 'fulani',\n 113: 'wolof',\n 114: 'laobe',\n 115: 'lebou',\n ...\n 128: 'tandanke',\n 129: 'toucouleur',\n 130: 'wolof',\n 131: 'khassonke',\n ...\n 398: 'other countries',\n 999: 'unknown'}\n```\n\nNote 130 and 113: `wolof`.  This is the problem. I'm not sure what the correct behavior is here since there are two numeric values that make to same name.  Categoricals don't understand this since there must be a 1-to-1 mapping between the underying numeric store and the labels.\n",
      "I suppose the error could be trapped and a more meaningful error possibly with a report could be returned.\n",
      "I thought it was having trouble due to possibly repeated numbering of the categories. Not sure why it should care if the label values (strings) are repeated, since I imagine the categories should work regardless of the value of the label value, no? Or am I missing something? I agree that a better error message and even a print out of the repeated categories would be very useful. \n\nThanks for the help!\n",
      "Another possibility would be to allow Pandas to read the categories as strings.\n",
      "Stata stores value labeled variables as labels and some number.  Your data has 2 values that correspond to the same number.  In pandas, a label is as good as its underlying integer data type, and so there is no way for a categorical to have 2 values with the same label.  Stata value labels are not equivalent to pandas categoricals, only close.  This is a case where the difference matters.\n",
      "BTW, you can use `convert_categoricals=False` to read the data and return the integer values.  You can also use `StataReader` to access the value labels.  From these you can anything you want with the data.\n",
      "so this looks like buggy Stata behavior? @bashtage \n\nyeah I would prob raise here (if convert_categoricals is set), let the user figure it out. (I suppose you could just turn categorical conversion off and show a warning).\n",
      "Thanks for the suggestions @bashtage ...do you have an example on how one would access those labels?\n",
      "It is not buggy Stata behavior so much as just different.  In Stata value\nlabels are just labels and are not actually values.  Pandas doesnt have the\nconcept of a labeled Series, and so there is no way to map between the two\nperfectly.\n\nOn Tue, Aug 9, 2016, 12:10 AM Jeff Reback notifications@github.com wrote:\n\n> so this looks like buggy Stata behavior? @bashtage\n> https://github.com/bashtage\n> \n> yeah I would prob raise here (if convert_categoricals is set), let the\n> user figure it out. (I suppose you could just turn categorical conversion\n> off and show a warning).\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/pydata/pandas/issues/13923#issuecomment-238405327,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AFU5RTT5FYLD0Q50948r1gtpbD9sRKPgks5qd7dXgaJpZM4JeU3B\n> .\n",
      "> Thanks for the suggestions @bashtage ...do you have an example on how one would access those labels?\n\nUsing your short file:\n\n``` python\nimport pandas as pd\ndf = pd.read_stata('ipumsi_00014_ethn.dta',convert_categoricals=False)\nsr = pd.io.stata.StataReader('ipumsi_00014_ethn.dta')\nvl = sr.value_labels()\nsr.close()\n```\n\nThen you can do whatever you need to with `vl` and `df`.\n",
      "@bashtage Cool! Thanks!\n"
    ],
    "events": [
      "commented",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 17,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/io/stata.py",
      "pandas/io/tests/data/stata15.dta",
      "pandas/io/tests/test_stata.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13932,
    "reporter": "jreback",
    "created_at": "2016-08-08T13:43:09+00:00",
    "closed_at": "2016-08-09T21:53:08+00:00",
    "resolver": "agraboso",
    "resolved_in": "4a805216d99b37955c97625d304980eff10cab56",
    "resolver_commit_num": 3,
    "title": "CLN: resource warnings",
    "body": "-ci.org/pydata/pandas/jobs/150623749\n\nso we have warnings enabled on the 3.5/dev build. Lots of resource warnings....\n",
    "labels": [
      "Difficulty Intermediate",
      "Effort Medium",
      "Clean",
      "IO CSV",
      "IO Stata"
    ],
    "comments": [
      "@gfyoung an odd one here.\n\n```\n................................./home/travis/build/pydata/pandas/pandas/io/parsers.py:1904: FutureWarning: split() requires a non-empty pattern match.\n  yield pat.split(line.strip())\n```\n",
      "That's due to a `regex` matching the empty string somewhere in one of the tests.  Can try to patch that.\n\nAs for the other `ResourceWarnings`, I think it's because files are being closed before being destroyed, but in this instance, not sure why that would be happening given our test construction.\n",
      "these only happen in PY3 fyi. Maybe its more strict.\n",
      "Yes, it's because that warning was introduced in Python 3.  Again, not 100% sure why they are occurring, since that implies that the file object is getting deleted before it is closed, and we're using `with` statements everywhere in the tests.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 14,
    "additions": 121,
    "deletions": 48,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/io/common.py",
      "pandas/io/parsers.py",
      "pandas/io/sas/sas7bdat.py",
      "pandas/io/sas/sas_xport.py",
      "pandas/io/sas/sasreader.py",
      "pandas/io/stata.py",
      "pandas/io/tests/parser/common.py",
      "pandas/io/tests/parser/python_parser_only.py",
      "pandas/io/tests/parser/test_textreader.py",
      "pandas/io/tests/sas/test_sas7bdat.py",
      "pandas/io/tests/sas/test_xport.py",
      "pandas/io/tests/test_common.py",
      "pandas/tests/series/test_io.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13937,
    "reporter": "johngu",
    "created_at": "2016-08-08T15:45:10+00:00",
    "closed_at": "2017-03-02T13:44:58+00:00",
    "resolver": "amolkahat",
    "resolved_in": "f000a4eac361737c6524ca2273c158e8d3b04ab2",
    "resolver_commit_num": 0,
    "title": "BUG: DataFrame.to_records() bug in converting datetime64 index with timezone",
    "body": "#### Fix\n\nin to_records(), use pandas.core.common.is_datetime64_any_dtype instead of pandas.core.common.is_datetime64_dtype to check to see if the index is in fact of the datetime64 type.\n#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\ndata\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.2.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.6.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 20.3\nCython: 0.23.4\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.1.2\nsphinx: 1.3.5\npatsy: 0.4.0\ndateutil: 2.5.1\npytz: 2016.2\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.5\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.8.4\nlxml: 3.6.0\nbs4: 4.4.1\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.39.0\npandas_datareader: None\n",
    "labels": [
      "Bug",
      "Reshaping",
      "Timezones",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "note that this should return an object array with tz-aware objects (as numpy has no clu about non-naive datetimes).\n\ne.g. similar to this (though this is tz-naive)\n\n```\nIn [35]: df\nOut[35]: \nEmpty DataFrame\nColumns: []\nIndex: [2016-08-08 15:50:08.058674, 2016-08-08 15:50:08.058699, 2016-08-08 15:50:08.058713, 2016-08-08 15:50:08.058716, 2016-08-08 15:50:08.058719, 2016-08-08 15:50:08.058722, 2016-08-08 15:50:08.058725, 2016-08-08 15:50:08.058728, 2016-08-08 15:50:08.058732, 2016-08-08 15:50:08.058735]\n\nIn [36]: df.to_records()\nOut[36]: \nrec.array([(datetime.datetime(2016, 8, 8, 15, 50, 8, 58674),),\n (datetime.datetime(2016, 8, 8, 15, 50, 8, 58699),),\n (datetime.datetime(2016, 8, 8, 15, 50, 8, 58713),),\n (datetime.datetime(2016, 8, 8, 15, 50, 8, 58716),),\n (datetime.datetime(2016, 8, 8, 15, 50, 8, 58719),),\n (datetime.datetime(2016, 8, 8, 15, 50, 8, 58722),),\n (datetime.datetime(2016, 8, 8, 15, 50, 8, 58725),),\n (datetime.datetime(2016, 8, 8, 15, 50, 8, 58728),),\n (datetime.datetime(2016, 8, 8, 15, 50, 8, 58732),),\n (datetime.datetime(2016, 8, 8, 15, 50, 8, 58735),)], \n          dtype=[('datetime', 'O')])\n```\n\npull-requests welcome. I don't think this is highly tested. Most people don't really use numpy arrays directly once they need something (e.g. tz) that they don't provide.\n",
      "note this code has been changed quite a in master. But the fix is essentially the same .\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "renamed",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 21,
    "deletions": 8,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/frame.py",
      "pandas/tests/frame/test_convert_to.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13947,
    "reporter": "sstanovnik",
    "created_at": "2016-08-09T12:10:33+00:00",
    "closed_at": "2016-08-12T15:44:54+00:00",
    "resolver": "sinhrks",
    "resolved_in": "29d9e24f4c778b0c9ebe9288bfc217808d2c6edb",
    "resolver_commit_num": 377,
    "title": "Unified common dtype discovery",
    "body": "Common dtype discovery (like `np.find_common_type`) should be unified into an internal function, with proper handling of `pandas` dtypes. As of #13917, there are two such implementations: \n1. `pandas/types/common.py:_lcd_dtypes`, which works on `numpy` dtypes only, but differently than `np.find_common_type`.\n2. `pandas/types/cast.py:_find_common_type`, which just uses `np.find_common_type` internally. \n\nExchanging the first one with the second (which is, judging by the code, more proper), breaks some tests. \n\nProposed tasks:\n- [ ] Convert usages of `_lcd_dtypes` into `_find_common_types`.\n- [ ] Extend `_find_common_types` to properly evaluate `pandas` dtypes.  \n\nOccurences of `_lcd_dtypes`, the one in `internals.py` is removed by #13917:\n\n\n",
    "labels": [
      "Dtypes",
      "Compat",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "`_lcd_dtypes` is used only in `combine` and looks buggy (it's called from `combine_first`):\n\n```\ndf1 = pd.DataFrame({'a': [pd.Timestamp('2011-01-01'), pd.NaT]})\ndf2 = pd.DataFrame({'a': [1, 2]})\ndf1.combine_first(df2)\n#                               a\n# 0 2011-01-01 00:00:00.000000000\n# 1 1970-01-01 00:00:00.000000002\n```\n",
      "that might be related to  #10567 \n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 7,
    "additions": 410,
    "deletions": 227,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/frame.py",
      "pandas/tests/frame/test_combine_concat.py",
      "pandas/tests/frame/test_operators.py",
      "pandas/tests/types/test_cast.py",
      "pandas/types/cast.py",
      "pandas/types/common.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13965,
    "reporter": "chrisaycock",
    "created_at": "2016-08-11T15:50:49+00:00",
    "closed_at": "2017-04-13T11:39:14+00:00",
    "resolver": "carlosdanielcsantos",
    "resolved_in": "73222392f389f918272a9d96c5f623f0b13966eb",
    "resolver_commit_num": 0,
    "title": "Exact matches in time-based .rolling()",
    "body": "I would like to `allow_exact_matches` in time-based `.rolling()`. Using an example similar to the one found in the documentation:\n\n\n\nI can compute the three-second rolling sum:\n\n\n\nBut note that `09:00:03` does not include `09:00:00`, `09:00:05` does not include `09:00:02`, and `09:00:06` does not include `09:00:03`. If I were to include these timestamps, then I would have:\n\n\n\nA quick-and-dirty way of getting these values is with:\n\n\n\n`pd.merge_asof()` has the parameter `allow_exact_matches` to permit this. Would it be possible to add this to `.rolling()`?\n\nI can try this one myself.\n",
    "labels": [
      "Enhancement",
      "Resample",
      "Reshaping",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "For similarity with resampling, would this make sense as a parameter `closed={'right', 'left', 'both'}`, where IIUC, `'right'` is the default and `'both'` is the option you're adding?\n",
      "Although `closed='left'` maybe doesn't make any sense.\n",
      "@chris-b1 The choice is rather binary: either we want all timestamps _strictly greater than_ the lowest bound, or we want all timestamps _greater than or equal to_ the lowest bound. `.rolling()` always defines the last row as the current one under consideration.\n",
      "Yep, fair point.  I might be off base, but I find `allow_exact_matches` hard to parse in this context, since here it's about including the earliest date in a window, where in `merge_asof` it's about including the latest date (right?).  \n\nThat said, regardless of how it's spelled, I do think it'd be a nice addition!\n",
      "@chris-b1 Fair point about first in `.rolling()` vs last in `pd.merge_asof()`. I do like reusing the parameter name since it's already around, but I'm open to anything that makes sense.\n",
      "Agreed with @chris-b1 that it would be a nice addition, but for me `allow_exact_matches` is not a very clear name as well (in the sense that I wouldn't directly think of that use case when I see the name). \n\nI think it is actually about an open vs closed interval? So maybe using `closed=True/False` would also be an option, given that 'right'/'left' is not the correct meaning here (as it is always right closed, and it's only the left that can be open or closed). Although that name is also not directly meaningful from its name alone.\n",
      "How about `left_closed`? #13968\n",
      "@chrisaycock re-reading this, is there a suggestion to change ``allow_exact_matches`` to a ``closed='left'|'both'`` parameter in ``merge_asof`` as well? (we can create a separate issue for that).",
      "@jreback Since there are only two possible values for that parameter, I'd rather that it be a boolean."
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "referenced",
      "cross-referenced",
      "commented",
      "referenced",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced"
    ],
    "changed_files": 6,
    "additions": 222,
    "deletions": 51,
    "changed_files_list": [
      "doc/source/computation.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/generic.py",
      "pandas/core/window.py",
      "pandas/core/window.pyx",
      "pandas/tests/test_window.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13967,
    "reporter": "bklaas",
    "created_at": "2016-08-11T18:45:43+00:00",
    "closed_at": "2016-08-20T13:25:01+00:00",
    "resolver": "OXPHOS",
    "resolved_in": "5c78ee6b457439c9521aa3d17113b519740a9cdc",
    "resolver_commit_num": 3,
    "title": "DOC: read_excel() documentation for na_values should show default na values",
    "body": "I'd like to see read_excel show the same valid information for na_values as read_csv does. This ended up taking a lot of digging to find because it was unclear what the defaults were with read_excel().\n#### read_csv() documentation shows the following for na_values\n\nna_values : str or list-like or dict, default None\nAdditional strings to recognize as NA/NaN. If dict passed, specific per-column NA values. By default the following values are interpreted as NaN: \u2018\u2019, \u2018#N/A\u2019, \u2018#N/A N/A\u2019, \u2018#NA\u2019, \u2018-1.#IND\u2019, \u2018-1.#QNAN\u2019, \u2018-NaN\u2019, \u2018-nan\u2019, \u20181.#IND\u2019, \u20181.#QNAN\u2019, \u2018N/A\u2019, \u2018NA\u2019, \u2018NULL\u2019, \u2018NaN\u2019, \u2018nan\u2019.\n#### read_excel() documentation does not include default NaN values\n\nna_values : list-like, default None\nList of additional strings to recognize as NA/NaN\n",
    "labels": [
      "Docs",
      "Difficulty Novice",
      "IO Excel",
      "Effort Low"
    ],
    "comments": [
      "Thx for the suggestion. PR is appreciated!\n",
      "I wish to work on the issue. How do I start?\n",
      "First, look at the contributing docs (http://pandas.pydata.org/pandas-docs/stable/contributing.html) which explains how to set-up a development environment and to make a pull request. \n\nSpecifically for this change, you can take a look at the `read_csv` docstring, which includes the correct values for `na_values`. So you could just copy the explanation from there.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "labeled",
      "renamed",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 7,
    "additions": 120,
    "deletions": 97,
    "changed_files_list": [
      "pandas/io/common.py",
      "pandas/io/excel.py",
      "pandas/io/parsers.py",
      "pandas/io/tests/data/test5.xls",
      "pandas/io/tests/data/test5.xlsm",
      "pandas/io/tests/data/test5.xlsx",
      "pandas/io/tests/test_excel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13977,
    "reporter": "kernc",
    "created_at": "2016-08-12T18:54:42+00:00",
    "closed_at": "2016-08-26T20:04:20+00:00",
    "resolver": "kernc",
    "resolved_in": "042b6f00ad691345812e61bb7e86e52476805602",
    "resolver_commit_num": 0,
    "title": "df.iterrows() constructs Series instead of its proper subclass",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n`row` would be of proper overriden `_constructor_sliced` type (i.e. `SubclassedSeries`).\n",
    "labels": [
      "Bug",
      "Dtypes"
    ],
    "comments": [],
    "events": [
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 12,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/frame.py",
      "pandas/tests/frame/test_subclass.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 13993,
    "reporter": "agraboso",
    "created_at": "2016-08-13T18:36:50+00:00",
    "closed_at": "2016-08-14T00:15:47+00:00",
    "resolver": "agraboso",
    "resolved_in": "a0d05dbf7477df863486d82a0cdd4e3e93023864",
    "resolver_commit_num": 4,
    "title": "groupby cumsum with axis=1 computes cumprod",
    "body": "#### Code sample\n\n\n#### Output\n\n\n#### Expected Output\n\n\n#### The culprit\n\nA [single line](#L1400)  in `pandas.core.groupby`:\n\n\n\n(the result of a copy-paste oversight in [this commit](#diff-720d374f1a709d0075a1f0a02445cd65))\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Bug",
      "Groupby"
    ],
    "comments": [],
    "events": [
      "cross-referenced",
      "labeled",
      "labeled",
      "milestoned"
    ],
    "changed_files": 3,
    "additions": 10,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14012,
    "reporter": "spillz",
    "created_at": "2016-08-16T17:35:10+00:00",
    "closed_at": "2016-08-17T10:26:53+00:00",
    "resolver": "gfyoung",
    "resolved_in": "cb43b6c5a1e66343fcd8696402677de98012d6e0",
    "resolver_commit_num": 70,
    "title": "read_csv treats \\x00 as EOL instead of null value",
    "body": "Not sure if this is a bug, but it took me a long time to figure out what was going on in a much bigger datafile than the sample one below. \n#### Code Sample, a copy-pastable example if possible\n\nimport pandas\nimport StringIO\n\ndata='''var1,var2,var3\n1,2,0\n2,\\x00,0\n3,4,0\n4,5,0\n'''\n\nprint pandas.read_csv(StringIO.StringIO(data))\n#### Expected Output\n\nA table with 4 rows instead of 5, or an error.\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 62 Stepping 4, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 20.3\nCython: 0.23.4\nnumpy: 1.11.0\nscipy: 0.17.0\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.1.2\nsphinx: 1.3.5\npatsy: 0.4.0\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.5\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.8.4\nlxml: 3.6.0\nbs4: 4.4.1\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.39.0\npandas_datareader: None\n",
    "labels": [
      "Bug",
      "IO CSV"
    ],
    "comments": [
      "I don't see an error on `master`:\n\n``` python\n>>> from pandas import read_csv\n>>> from pandas.compat import StringIO\n>>> data=\"\"\"var1,var2,var3\n1,2,0\n2,\\x00,0\n3,4,0\n4,5,0\n\"\"\"\n>>> df = read_csv(StringIO(data))\n>>> df\n   var1  var2  var3\n0   1.0   2.0   0.0\n1   2.0   NaN   NaN\n2   NaN   0.0   NaN\n3   3.0   4.0   0.0\n4   4.0   5.0   0.0\n```\n",
      "It should be:\n\n   var1  var2  var3\n0   1.0   2.0   0.0\n1   2.0   NaN   0.0\n2   3.0   4.0   0.0\n3   4.0   5.0   0.0\n\nOn Aug 16, 2016 10:57 PM, \"gfyoung\" notifications@github.com wrote:\n\n> I don't see an error on master:\n> \n> > > > from pandas import read_csv>>> from pandas.compat import StringIO>>> data=\"\"\"var1,var2,var31,2,02,\\x00,03,4,04,5,0\"\"\">>> df = read_csv(StringIO(data))>>> df\n> > > >    var1  var2  var30   1.0   2.0   0.01   2.0   NaN   NaN2   NaN   0.0   NaN3   3.0   4.0   0.04   4.0   5.0   0.0\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/pydata/pandas/issues/14012#issuecomment-240300583,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAFd5VMOaYZqpvfkorqtIybWXjB8-Kjhks5qgnh9gaJpZM4JlqYQ\n> .\n",
      "@spillz : Sorry, I was meaning to write more to clarify my comment.  In the meantime, could you add that (the expected output) to your original issue?\n",
      "@spillz , @jreback : Actually, my PR speaks for itself here in terms of \"expanding\" on my comment <a href=\"https://github.com/pydata/pandas/issues/14012#issuecomment-240300583\">above</a>.  In short, this is a bug.\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "milestoned"
    ],
    "changed_files": 3,
    "additions": 37,
    "deletions": 13,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/io/tests/parser/c_parser_only.py",
      "pandas/src/parser/tokenizer.c"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14015,
    "reporter": "WindJunkie",
    "created_at": "2016-08-16T21:50:27+00:00",
    "closed_at": "2017-04-07T15:18:24+00:00",
    "resolver": "jreback",
    "resolved_in": "f478e4f4b0a353fa48ddb19e70cb9abe5b36e1b5",
    "resolver_commit_num": 4359,
    "title": "BUG: multi-indexing sorting on axis=1 on >0 levels",
    "body": "In a DataFrame with MultiIndex, sorting on the level with date values does not do anything (order remains unchanged). This happens for both row and column indexes. In my case I am starting with a string index, converting that to datetime index, have not tried with datetime values at the start.\n#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\nColumns ordered by date is the expected output. \n\n\n\nActual output are columns in the original sort order (not even lexicographically sorted).\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Windows\nOS-release: 10\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.1\nsetuptools: 20.3\nCython: 0.23.4\nnumpy: 1.11.0\nscipy: 0.17.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.1.2\nsphinx: 1.3.1\npatsy: 0.4.0\ndateutil: 2.5.1\npytz: 2016.2\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.6.0\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.8.4\nlxml: 3.6.0\nbs4: 4.4.1\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.39.0\npandas_datareader: None\n",
    "labels": [
      "Bug",
      "Indexing",
      "MultiIndex",
      "Difficulty Advanced",
      "Effort Low"
    ],
    "comments": [
      "xref #13431 \n",
      "I seem to remember this exact issue, but can't find ATM.\n",
      "As discussed in some of those issues, MultiIndex sorting is based on the ordering in the levels.  The levels are sorted on construction, but not on re-assignment.  So a couple workarounds would be to construct the mi with the converted levels, or the index also has a `sort_values()` method that sorts by values.\n\n```\nIn [22]: df.reindex(columns = df.columns.sort_values())\nOut[22]: \nl1            0           \nDate 2016-08-09 2016-08-11\n0             2          1\n1             7          6\n```\n",
      "> MultiIndex sorting is based on the ordering in the levels.\n\nIs this something we could consider changing?\nIn my opinion, this behaviour does not make much sense from a user perspective. If you want such behaviour, you can now use explicitly a CategoricalIndex. For most users of MultiIndex, the fact that it is implemented with label/levels (codes/categories) is only an implementation detail.\n",
      "Rather than basing MultiIndex sorting on something other than sorted levels, what about requiring that each `level` always be sorted? I believe this is already done by default in every case where pandas constructs the MultiIndex levels, so this would only breaks cases where levels are provided explicitly in the `MultiIndex` constructor or set using `set_levels`.\n",
      "That seems fairly reasonable, although the sorting behavior is [documented](http://pandas.pydata.org/pandas-docs/stable/advanced.html#the-need-for-sortedness-with-multiindex), and I'm sure it would break somebody's code, though probably not too painful to detect/deprecate in 0.19, fix in 0.20 / 1.0?\n",
      "Yes, assuredly someone relies on the existing behavior, but we could probably deprecate it. In my opinion the fact that levels and labels can be sorted differently is a major source of confusion.\n\nOne option with `set_level` would be to automatically factorize new levels and change the underlying integer codes, too. That's probably not a good idea if someone explicitly wrote `MultiIndex(levels, labels)`, though.\n",
      "@chris-b1 : are you referring to the sentence \"the present implementation of `MultiIndex` requires that the labels be sorted for some of the slicing\"? Actually, I think most people (including me, until few minutes ago) interpret \"the labels be sorted\" as \"the MultiIndex be sorted\", not \"the initialization arrays for `.levels` be sorted\". And if instead they give the second interpretation, well then they should already be following the design proposed by @shoyer .\n\nSo if I'm not missing anything, it would be possible and great to have already in 0.19 the following behaviour: for each component of `.levels` passed at initialization, if it is not sorted, sort it, rearranging the correspondent component of `.labels`.\n\nBy the way: another case in which bad things currently happen is in a `.join()` of unsorted dataframes:\n\n```\nIn [1]: import pandas as pd\n   ...: labels = ['c', 'b']\n   ...: comp = pd.DataFrame(index=pd.MultiIndex.from_product([labels, labels],\n   ...:                                names =['uid', 'oth']))\n   ...: idists = pd.Series(0, index=labels, name='Charles')\n   ...: idists.index.name = 'uid'\n   ...: cc = comp.join(idists, how='inner').sort_index()\n   ...: len(cc.index.levels), cc.index.lexsort_depth, cc.index.is_monotonic\n   ...: \nOut[1]: (2, 0, True)\n\n```\n",
      "I meant this note at the end of paragraph, though I agree that isn't super clear either.\n\n>  ... labels are grouped and sorted by the original ordering of the associated factor at that level. Note that this does not necessarily mean the labels will be sorted lexicographically!\n",
      "Yes, it is rather hidden in the docs that sorting does sort according to the order of the levels.\n\nI am +1 to change the behaviour of sort to actually sort. But, if we do this by sorting the levels (on initialization, or when sorting), how many people would rely on the actual order of the levels? Eg if you use `set_levels`, you implicitly rely on the order ...\n",
      "Looking at the MultiIndex docs, it looks like `sort_index()` was originally written to ensure that a MultiIndex is \"lexsorted\" in the way that MultiIndex needs for efficient operations (sorted integer labels a.k.a. codes #13443). I would be much happier using something more explicit like `sort_index_codes()` for that, though, and reserving `sort_index()` for actually sorting the index.\n\nI'm less certain now that it's the right thing to always require that levels be sorted. There are some cases where this lets you do different types of indexing efficiently, and right now we expose most of the MultiIndex implementation directly as public API, so people are indeed probably making use of this.\n",
      "While I understand that both behaviours can be useful, to my eyes it is a bit unnecessary to provide another way (than `Categorical`s) to impose an order on labels in a level. I'd rather have nice parameters that, for instance, create the required `Categorical`s on the fly when creating a `MultiIndex.from`, if a specific ordering is desired.\n\nBut if supporting the two different behaviours is considered worth the effort, then maybe a parameter (e.g. `codes_sort=False`) in `sort_index()` is sufficient, rather than adding a new method?\n\nWhat I strongly agree on is that the default behaviour should be changed, despite the backwards incompatibility.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "renamed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 15,
    "additions": 593,
    "deletions": 57,
    "changed_files_list": [
      "asv_bench/benchmarks/timeseries.py",
      "doc/source/advanced.rst",
      "doc/source/api.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/core/sorting.py",
      "pandas/indexes/multi.py",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tests/series/test_analytics.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tests/tools/test_hashing.py",
      "pandas/tests/tools/test_pivot.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14021,
    "reporter": "hnykda",
    "created_at": "2016-08-17T13:56:40+00:00",
    "closed_at": "2016-08-26T20:50:24+00:00",
    "resolver": "conquistador1492",
    "resolved_in": "0e61847e111a3ba181f16c8b9b974c74d360ad2e",
    "resolver_commit_num": 2,
    "title": "Better error message when fillna is used with NaN with category dtype",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n\nand the result is:\n\n\n\nWhen trying the above with float, it works as expected (nans are \"replaced\" by nans). \n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Missing-data",
      "Categorical",
      "Bug",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "hmm, something funny is going on there. welcome to have you take a look.\n",
      "its legit to fill with a `np.nan` (or `pd.NaT` if datetimelike) with a categorical. So may not be hitting the right method.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 37,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/categorical.py",
      "pandas/tests/test_categorical.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14065,
    "reporter": "schodge",
    "created_at": "2016-08-22T19:27:03+00:00",
    "closed_at": "2016-08-31T16:10:31+00:00",
    "resolver": "gfyoung",
    "resolved_in": "5db52f0d3000cb78322ffb148b3f94b3c883bb26",
    "resolver_commit_num": 81,
    "title": "Unicode char as delimiter won't use C engine",
    "body": "I have the following code:\n\n`dfEL = pd.read_csv(IN_PATH, delimiter='\\\\u00A7', encoding='utf-8')`\n\nwhich I've also tried with other ways of writing the delimiter, e.g.:\n\n`dfEL = pd.read_csv(IN_PATH, delimiter='\u00a7', encoding='utf-8')`\n\nThese other methods don't work, and generate a `UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 10: unexpected end of data`.\n\nThe first method, though, won't use the C regex engine:\n\n\n\nShouldn't this only be considered one character and still use (the I presume faster) C engine?\n\nSample data - there's a lot of messiness in the rightmost column, which is why an unusual separator was used:\n\n\n\n\n",
    "labels": [
      "IO CSV",
      "Unicode",
      "Effort Medium",
      "Difficulty Intermediate"
    ],
    "comments": [
      "Copy-pastable example (python3)\n\n```\nimport pandas as pd\nfrom io import StringIO\ns = \"a\u00a7b\\n1\u00a72\\n3\u00a74\"\n\npd.read_csv(StringIO(s), sep='\u00a7')\n```\n",
      "By the way, for you first example, I think you want `delimiter='\\u00A7'` (you had an extra backslash).\n",
      "Apologies for not including a cut and paste example.\n\nActually, I do need the delimiter with two backslashes. In fact, your copy-and-paste example doesn't work for me as written:\n\n```\nimport pandas as pd\nfrom io import StringIO\ns = \"a\u00a7b\\n1\u00a72\\n3\u00a74\"\n\npd.read_csv(StringIO(s), sep='\u00a7')\n\n---------------------------------------------------------------------------\nUnicodeDecodeError                        Traceback (most recent call last)\n<ipython-input-1-43a4e6b0419c> in <module>()\n      3 s = \"a\u00a7b\\n1\u00a72\\n3\u00a74\"\n      4 \n----> 5 pd.read_csv(StringIO(s), sep='\u00a7')\n      6 \n\n<cutting>\n\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 1: unexpected end of data\n```\n\nWith the doubled form:\n\n```\nimport pandas as pd\nfrom io import StringIO\ns = \"a\u00a7b\\n1\u00a72\\n3\u00a74\"\n\npd.read_csv(StringIO(s), sep='\\\\u00A7')\n\nC:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\nOut[2]: \n   a  b\n0  1  2\n1  3  4\n\nimport pandas as pd\nfrom io import StringIO\ns = \"a\u00a7b\\n1\u00a72\\n3\u00a74\"\n```\n\nAnd with single form:\n\n```\npd.read_csv(StringIO(s), sep='\\u00A7')\n\n---------------------------------------------------------------------------\nUnicodeDecodeError                        Traceback (most recent call last)\n<ipython-input-3-e789e47e41b2> in <module>()\n      3 s = \"a\u00a7b\\n1\u00a72\\n3\u00a74\"\n      4 \n----> 5 pd.read_csv(StringIO(s), sep='\\u00A7')\n      6 \n\n<cutting>\n\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 1: unexpected end of data\n```\n",
      "That's odd, I wonder if we escape something improperly... Compare these two:\n\n``` python\nIn [22]: '\\u00A7'\nOut[22]: '\u00a7'\n\nIn [24]: '\\\\u00A7'\nOut[24]: '\\\\u00A7'\n```\n\nWhen you have the double `\\`, the first backslash escapes the second,  so `Out[24]` is the literal string `\\u00A7`.\n",
      "So those work:\n\n```\n'\\u00A7'\nOut[2]: '\u00a7'\n\n'\\\\u00A7'\nOut[3]: '\\\\u00A7'\n```\n\nRight - I don't think I've seen this behavior with unicode outside of pandas, but I rarely venture into unicode.\n",
      "The reason for the error is that the data is getting encoded as `utf-8` (see <a href=\"https://github.com/pydata/pandas/blob/be61825986ba565bc038beb2f5df2750fc1aca30/pandas/parser.pyx#L647\">here</a>), which \"destroys\" the delimiter in the data:\n\n``` python\n>>> data = \"a\u00a7b\\n1\u00a72\\n3\u00a74\"\n>>> data.encode('utf-8')\nb'a\\xc2\\xa7b\\n1\\xc2\\xa72\\n3\\xc2\\xa74'\n```\n\nHowever, `ord(\u00a7) == 167`, which is `\\xa7`.  This causes the data to split improperly with the first element of the header being `a\\xc2`, which leads to the error that you're seeing.\n\nNow technically, we should be splittng by `\\xc2\\xa7`, but the C engine doesn't support splitting of that kind (we only support single character splitting for now).\n\nIn the long run, the solution would be to somehow support multi-char delimiters (tricky since we parse byte by byte with the C engine).  In the short-term, I think we should check the separator to see if it would be a multi-char when encoded, and if so, raise an error.\n\nThoughts?\n",
      "Trying to detect such separators and raising an informative message sounds fine IMO\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 21,
    "deletions": 8,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/io/parsers.py",
      "pandas/io/tests/parser/test_unsupported.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14068,
    "reporter": "mathause",
    "created_at": "2016-08-22T23:11:42+00:00",
    "closed_at": "2016-09-27T10:43:45+00:00",
    "resolver": "gfyoung",
    "resolved_in": "977b384cc7cb0ee0d403730ca4b34e32b388ac0c",
    "resolver_commit_num": 89,
    "title": "TimedeltaIndex + Timestamp -> no overflow error",
    "body": "When adding a TimedeltaIndex and a Timestamp this overflows instead of raising (pydata/xarray#975).\n\n\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.12.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.13.0-24-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: None\npip: 8.1.2\nsetuptools: 25.1.6\nCython: None\nnumpy: 1.11.1\nscipy: None\nstatsmodels: None\nxarray: 0.7.2-73-g584e703\nIPython: 5.1.0\nsphinx: None\npatsy: None\ndateutil: 2.5.3\npytz: 2016.6.1\nblosc: None\nbottleneck: 1.1.0\ntables: None\nnumexpr: None\nmatplotlib: None\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\npandas_datareader: None\n\nedit: 'D' as unit\n",
    "labels": [
      "Difficulty Novice",
      "Timedelta",
      "Effort Low",
      "Bug"
    ],
    "comments": [
      "@mathause your code is missing the value of `delta`\n",
      "thanks & apologies, 'twas already late - I edited my comment above\n",
      "xref https://github.com/pydata/pandas/issues/12534\n\nyeah this is detectable I think. you can just check the sign of the result. e.g. (should be the same as the sign of the timedelta), as numpy doesn't raise on overflow :<\n\n```\nIn [63]: pd.to_timedelta([106580], 'D').values + pd.Timestamp('2000').value\nOut[63]: array([-8291547273709551616], dtype='timedelta64[ns]')\n```\n"
    ],
    "events": [
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 5,
    "additions": 56,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/nanops.py",
      "pandas/tests/test_nanops.py",
      "pandas/tseries/tdi.py",
      "pandas/tseries/tests/test_timedeltas.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14088,
    "reporter": "RobertasA",
    "created_at": "2016-08-25T19:46:12+00:00",
    "closed_at": "2016-09-08T10:35:40+00:00",
    "resolver": "conquistador1492",
    "resolved_in": "d8cd33b148daba78f1450c19c69cfe4896cfb98c",
    "resolver_commit_num": 3,
    "title": "Cannot subtract tz-aware datetime.datetime from tz-aware datetime64 series.",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n\n\nNote, doing \n\n\n\nor \n\n\n\nworks.\n",
    "labels": [
      "Bug",
      "Timeseries",
      "Timezones",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "so in master only [18] fails. not getting coerced to Timestamp first. pull-requests welcome.\n\n```\nIn [12]: foo = pd.Series(datetime.datetime(2016, 8, 23, 12, tzinfo=pytz.utc))\nIn [13]: foo\nOut[13]:\n0   2016-08-23 12:00:00+00:00\ndtype: datetime64[ns, UTC]\n\nIn [14]: dt = datetime.datetime(2016, 8, 22, 12, tzinfo=pytz.utc)\n\nIn [15]: dt\nOut[15]: datetime.datetime(2016, 8, 22, 12, 0, tzinfo=<UTC>)\n\nIn [16]: foo\nOut[16]:\n0   2016-08-23 12:00:00+00:00\ndtype: datetime64[ns, UTC]\n\nIn [17]: foo-pd.Timestamp(dt)\nOut[17]:\n0   1 days\ndtype: timedelta64[ns]\n\nIn [18]: foo-dt\nValueError: Tz-aware datetime.datetime cannot be converted to datetime64 unless utc=True\n```\n\nnote that `.astype(datetime.datetime)` is quite non-idiomatic as you are coercing to an object array.\n",
      "`.astype(datetime.datetime)` should be a TypeError?\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 94,
    "deletions": 82,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/ops.py",
      "pandas/tests/series/test_operators.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14095,
    "reporter": "sdementen",
    "created_at": "2016-08-27T05:33:15+00:00",
    "closed_at": "2016-08-31T16:05:06+00:00",
    "resolver": "tom-bird",
    "resolved_in": "b2a73b8ee7456ec684daad1ecfb9d0fded13b7ec",
    "resolver_commit_num": 1,
    "title": "BUG: resolvers in pandas.eval should accept list-like",
    "body": "I think the resolvers argument of pandas.eval requires a tuple and not a list\n\n\n\nas with a list I get a\n\n\n",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "Yes, although rather than changing the docs, it should be pretty straightforward to convert any list-like into a tuple.\n\nProbably [here](https://github.com/pydata/pandas/blob/042b6f00ad691345812e61bb7e86e52476805602/pandas/core/frame.py#L2269) and somewhere around [here](https://github.com/pydata/pandas/blob/042b6f00ad691345812e61bb7e86e52476805602/pandas/computation/eval.py#L250)\n",
      "note that this is in reality a 'private'-ish kw, not sure what an outside of pandas user would do with it.\n",
      "For the use case, I use it to transform timeseries according to expressions given by the user and I want to add some \"variables\" that are columns of specific dataframes  (a bit like the query method)\n",
      "I'll have a look at fixing\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "commented",
      "commented",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 12,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/frame.py",
      "pandas/tests/frame/test_query_eval.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14125,
    "reporter": "jzwinck",
    "created_at": "2016-08-31T08:49:58+00:00",
    "closed_at": "2017-01-24T23:13:38+00:00",
    "resolver": "jeffcarey",
    "resolved_in": "64d7670d99a11ec4e263da73f2bb1335cb4290d3",
    "resolver_commit_num": 2,
    "title": "BUG: read_csv() crashes with engine='c'",
    "body": "Here's the code (input data is at the end of this message):\n\n\n\nIt fails with:\n\n\n\nSmall perturbations of the input file (adding or removing characters) makes it work, as does `engine='python'`.  Note that while one row (or more) of the file contains \"extra\" columns, I have only asked Pandas to read column 0, which it should well be able to do since that column has a consistent, short length.\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.2.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.13.0-85-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: None\npip: 8.1.2\nsetuptools: 25.1.6\nCython: 0.24.1\nnumpy: 1.11.1\n## DATA\n\n\n",
    "labels": [
      "IO CSV",
      "Bug"
    ],
    "comments": [
      "this was likely fixed by: https://github.com/pydata/pandas/pull/13788\n\ncan you try.\n",
      "This still gives an error for me with latest master."
    ],
    "events": [
      "commented",
      "labeled",
      "commented",
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 16,
    "deletions": 5,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/io/tests/parser/c_parser_only.py",
      "pandas/src/parser/tokenizer.c"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14143,
    "reporter": "jreback",
    "created_at": "2016-09-02T23:49:40+00:00",
    "closed_at": "2016-09-03T14:50:08+00:00",
    "resolver": "jreback",
    "resolved_in": "e9c5c2d2c550a5f8ae47b0c4348fb359c93ab8f1",
    "resolver_commit_num": 4076,
    "title": "TST: blosc>1.4.1 failing",
    "body": "xref: \n\nseems this update just hit and failing 1 tests [here](-ci.org/pydata/pandas/jobs/157218567)\nworked [here](-ci.org/pydata/pandas/jobs/157068571)\n\nseems like a back-compat issue (new blosc not reading older blosc?)\n\n\n",
    "labels": [
      "Testing",
      "Compat"
    ],
    "comments": [
      "cc @kawochen \ncc @llllllllll \n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 1,
    "additions": 1,
    "deletions": 1,
    "changed_files_list": [
      "ci/requirements-2.7.pip"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14171,
    "reporter": "ponomarevvl90",
    "created_at": "2016-09-07T08:50:42+00:00",
    "closed_at": "2016-09-09T22:29:16+00:00",
    "resolver": "josh-howes",
    "resolved_in": "289cd6d0df66b812921ff4c5cbade937b875406d",
    "resolver_commit_num": 0,
    "title": "Series.str.contains doesn't correct if series contains only nan values.",
    "body": "#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.10.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.2.0-42-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.17.1\nnose: None\npip: 8.0.3\nsetuptools: 18.0.1\nCython: 0.23.4\nnumpy: 1.10.1\nscipy: None\nstatsmodels: None\nIPython: 4.0.1\nsphinx: None\npatsy: None\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: None\ntables: 3.2.2\nnumexpr: 2.4.6\nmatplotlib: None\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\nJinja2: None\n",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "Strings",
      "Effort Low"
    ],
    "comments": [
      "Here's a simpler example. This is casting only when `na` flag is passed.\n\n```\nIn [20]: Series([np.nan, np.nan, np.nan],dtype='object').str.contains('foo')\nOut[20]: \n0   NaN\n1   NaN\n2   NaN\ndtype: float64\n\nIn [21]: Series([np.nan, np.nan, np.nan],dtype='object').str.contains('foo',na=False)\nOut[21]: \n0    0.0\n1    0.0\n2    0.0\ndtype: float64\n\nIn [22]: Series([np.nan, np.nan, np.nan],dtype='object').str.contains('foo',na=True)\nOut[22]: \n0    1.0\n1    1.0\n2    1.0\ndtype: float64\n```\n\nworks fine if not all-nan `object` dtype.\n\n```\nIn [23]: Series([np.nan, np.nan, np.nan, 'a'],dtype='object').str.contains('foo')\nOut[23]: \n0      NaN\n1      NaN\n2      NaN\n3    False\ndtype: object\n\nIn [25]: Series([np.nan, np.nan, np.nan, 'a'],dtype='object').str.contains('foo', na=False)\nOut[25]: \n0    False\n1    False\n2    False\n3    False\ndtype: bool\n\nIn [26]: Series([np.nan, np.nan, np.nan, 'a'],dtype='object').str.contains('foo', na=True)\nOut[26]: \n0     True\n1     True\n2     True\n3    False\ndtype: bool\n```\n",
      "pull-requests to fix are welcome!\n",
      "First time contributor here.  I'll take this on.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 24,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/strings.py",
      "pandas/tests/test_strings.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14173,
    "reporter": "jreback",
    "created_at": "2016-09-07T10:11:28+00:00",
    "closed_at": "2016-09-10T14:30:49+00:00",
    "resolver": "chris-b1",
    "resolved_in": "ef2098074a973cfa4ddee66333f5739ac025c65e",
    "resolver_commit_num": 47,
    "title": "BUG: union_categoricals w/Series & CategoricalIndex",
    "body": "\n\nI don't see why this shouldn't work. This should return a Categorical.\n\n\n\nactual categoricals are combinable\n\n\n\nThis works fine\n\n\n\nBut this is broken\n\n\n",
    "labels": [
      "Bug",
      "Categorical",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "cc @chris-b1 \n@sinhrks @jorisvandenbossche \n",
      "xref to #13767 though this is independent\n",
      "further I think we need a doc-note at the end of the unionining section in categorical.rst. Showing that when unioned categoricals are re-coded. Even though this is technically a dtype implementation detail that the categoricals hide, unioning re-codes, so the user should be aware.\n\n```\nIn [16]: c1 = pd.Categorical(['a', 'b'])\n    ...: c2 = pd.Categorical(['b', 'c'])\n\nIn [17]: c1\nOut[17]: \n[a, b]\nCategories (2, object): [a, b]\n\n# in c1, b is coded to 1\nIn [20]: c1.codes\nOut[20]: array([0, 1], dtype=int8)\n\nIn [21]: c2\nOut[21]: \n[b, c]\nCategories (2, object): [b, c]\n\n# in c2, b is coded to 0\nIn [22]: c2.codes\nOut[22]: array([0, 1], dtype=int8)\n\nIn [23]: from pandas.types.concat import union_categoricals\n\n# b is now re-coded to 1 (same as c2, but not the same as c1)\nIn [24]: union_categoricals([c1,c2])\nOut[24]: \n[a, b, b, c]\nCategories (3, object): [a, b, c]\n\nIn [25]: union_categoricals([c1,c2]).codes\nOut[25]: array([0, 1, 1, 2], dtype=int8)\n```\n\nnote that some users, @mrocklin (dask), actually like/want this!\n",
      "Yep, I agree that `union_categoricals` should handle categorical serieses\n",
      "How should the index be handled in the `Series` case - thrown away, or add an `ignore_index` kw to support what concat does?\n\n``` python\nIn [1]: pd.concat([pd.Series([1,2,3]), pd.Series([3,4,5])])\nOut[1]: \n0    1\n1    2\n2    3\n0    3\n1    4\n2    5\ndtype: int64\n\nIn [2]: pd.concat([pd.Series([1,2,3]), pd.Series([3,4,5])], ignore_index=True)\nOut[2]: \n0    1\n1    2\n2    3\n3    3\n4    4\n5    5\ndtype: int64\n\n```\n",
      "I would do it similar as `concat` does (although that in many cases the `ignore_index=True` case would be a better default ...)\n",
      "I disagree, I think this should always return a `Categorical`. `pd.concat` is designed to return `Series/DataFrame`, but `union_categoricals` specifically is meant for returning a `Categorical`.\n\nUntil we add this functionaility to `pd.concat`, concat of `Series` is a big tricky, but I don't think we should change the api of this function (otherwise it just becomes _another_ concat)\n",
      "Oh, you wouldn't even box it back in the original type?  I guess that makes sense as that usecase should really be handled by `concat` once the api is figured out.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 4,
    "additions": 74,
    "deletions": 9,
    "changed_files_list": [
      "doc/source/categorical.rst",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/tools/tests/test_concat.py",
      "pandas/types/concat.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14190,
    "reporter": "chris-b1",
    "created_at": "2016-09-08T23:51:56+00:00",
    "closed_at": "2016-09-09T19:33:37+00:00",
    "resolver": "chris-b1",
    "resolved_in": "939a22118a531d71a667456daf46964265c79d1e",
    "resolver_commit_num": 46,
    "title": "BUG: Categorical constructor is not idempotent with datetime with tz",
    "body": "This isn't a huge deal, but popped up adding tests for #14173\n\nWorks fine with plain datetimes\n\n\n\nDoes not with a tz\n\n\n",
    "labels": [
      "Dtypes",
      "Categorical"
    ],
    "comments": [
      "I'll take a look at this. \n",
      "@rforgione #14191 seems to have this covered\n",
      "ah got it, cool -- thanks @jreback . \n"
    ],
    "events": [
      "cross-referenced",
      "commented",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned"
    ],
    "changed_files": 3,
    "additions": 18,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/core/categorical.py",
      "pandas/tests/test_categorical.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14203,
    "reporter": "gfyoung",
    "created_at": "2016-09-11T17:54:38+00:00",
    "closed_at": "2016-12-16T23:32:48+00:00",
    "resolver": "gfyoung",
    "resolved_in": "dd8cba27677895e80768f22f6992e58046dcc3bf",
    "resolver_commit_num": 109,
    "title": "BUG: Accept column indices for na_values in read_csv",
    "body": "\r\n\r\nThis behaviour is slightly inconsistent with what we do with `usecols` for example, so it would be nice to be able to process column indices with `na_values` too.\r\n\r\nxref #7119.\r\n",
    "labels": [
      "API Design",
      "IO CSV"
    ],
    "comments": [
      "Reclassifying as a bug because the doc makes it sound like that this behaviour should be possible."
    ],
    "events": [
      "cross-referenced",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 62,
    "deletions": 14,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.2.txt",
      "pandas/io/parsers.py",
      "pandas/io/tests/parser/na_values.py",
      "pandas/parser.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14204,
    "reporter": "jgerardsimcock",
    "created_at": "2016-09-11T23:32:41+00:00",
    "closed_at": "2016-10-24T22:24:18+00:00",
    "resolver": "chris-b1",
    "resolved_in": "192b1cd510948cefff6d8b6c34655ca4828bc95b",
    "resolver_commit_num": 56,
    "title": "pandas-0.19.0rc1: 'global name tempita is not defined'",
    "body": "![screen shot 2016-09-11 at 4 27 54 pm](-783c-11e6-8120-e33a450e85be.png)\n\nI am running a Travis CI build in a python 2.7 min environment. Not sure why it is pulling from an unreleased version of pandas but it looks like the tempita needs to be imported. \n",
    "labels": [
      "Build",
      "Regression"
    ],
    "comments": [
      "you must have a really really old version of cython (or maybe cython is not installed). you are building from source so you need to have the development requirements. http://pandas.pydata.org/pandas-docs/stable/install.html#installing-from-source\n\nThis is not recommended unless you are developing pandas. there are wheels and conda packages available.\n",
      "acutally this should be a better reported error (meaning setup should fail)\n\nwhen cython is NOT installed (as is required), we cannot run tempita.\nSo need to put a check in.\n\n```\n[Sun Sep 11 19:47:16 ~/pandas]$ rm pandas/src/algos_groupby_helper.pxi\n[Sun Sep 11 19:47:36 ~/pandas]$ make\npython setup.py build_ext --inplace\nrunning build_ext\nTraceback (most recent call last):\n  File \"setup.py\", line 94, in <module>\n    if cython:\n  File \"/Users/jreback/miniconda/lib/python2.7/distutils/core.py\", line 151, in setup\n    dist.run_commands()\n  File \"/Users/jreback/miniconda/lib/python2.7/distutils/dist.py\", line 953, in run_commands\n    self.run_command(cmd)\n  File \"/Users/jreback/miniconda/lib/python2.7/distutils/dist.py\", line 972, in run_command\n    cmd_obj.run()\n  File \"/Users/jreback/miniconda/lib/python2.7/distutils/command/build_ext.py\", line 339, in run\n    self.build_extensions()\n  File \"setup.py\", line 368, in build_extensions\n    build_ext.build_extensions(self)\n  File \"setup.py\", line 140, in build_extensions\n    pyxcontent = tempita.sub(tmpl)\nNameError: global name 'tempita' is not defined\nmake: *** [tseries] Error 1\n```\n",
      "It's unclear to me why this was implemented with the equivalent of an assertion in setup.py rather than adding cython to the setup_requires list. Pandas 0.19.0 upgrades via pip on Linux (due to lack of Linux wheels support on PyPI) collapse catastrophically, requiring someone to manually pip install cython into the env first.\n",
      "@fungi not sure what you are taking about\n\nthis issue is fixed on 0.19.0\nand wheels are supoorted on linux \n",
      "Wheels are supported on Linux, but Linux wheels (the manylinux1 platform PEP aside) are not supported on PyPI. Thus, `pip install pandas` on Linux throws \"ImportError: Building pandas requires cython\" instead of installing cython automatically. I'm in the process of opening a separate issue now for this and will gladly submit a PR updating setup.py to use setup_requires for cython unless there's a good reason it shouldn't.\n",
      "@fungi Have a look at PyPI, the linux wheels are definitely there: https://pypi.python.org/pypi/pandas. Maybe you need to update pip to download them?\n",
      "@fungi before you open an issue\nplease show an authoritative source that says setup_requires is a good idea -  \nand is a standard way to do this\n\ndevelopment building has always required cython\n",
      "Those are the manylinux1 platform wheels to which I was referring. Support for manylinux1 was first added in pip 8.1.0 (March 2016), so yes this should work for anyone using a pip release from the past 7 months. I'm just curious why this was not added as a setup_requires so that it can work on a broader set of platforms which either may not automatically retrieve manylinux1 wheels or need to operate on sdists for other reasons. In contrast, you already have a setup_requires declared on numpy rather than simply aborting setup.py when you don't see it available in the environment.\n",
      "hmm, we are using `setup_requires` already. ok, then guess would take a PR for adding cython there. Not really sure why we don't have that already, though we do have a slightly different process than some other libraries. IOW, we _don't_ checkin in `.c` and instead build them in the dev version. \n",
      "As to \"development building has always required cython,\" prior to 0.19.0 development building must not have included installs from sdist since `pip install 'pandas<0.19.0'` on a system with, e.g., pip 7.1.0 successfully retrieves pandas-0.18.1.tar.gz from PyPI and installs it into the environment.\n",
      "FWIW, we're hitting this in our various builds and are not sure how to treat. We're seeing the same error for both our internal jenkins & public usages, e.g., https://readthedocs.org/projects/pygraphistry/builds/4425847/ .\n",
      "The error message can be improved (but that is fixed in 0.19.0, which is released in the meantime), but, as explained in the above comments, this just means that you are trying to build pandas from source, which requires cython to be installed. And this has not changed in comparison to previous versions, this has always been the case.\n\nHowever, normally there should be wheels available. Not sure why readthedocs is downloading the zip source instead of the binary wheels. \n",
      "I think there's probably still some confusion here. The \"fix\" in 0.19.0 makes it impossible to install the pandas sdist (.tar.gz or .zip) packages published on PyPI without manually preinstalling cython in the environment. The sdists of prior releases up to and including 0.18.1 could be installed without cython present. Digging deeper, it seems this worked because you were (and still are, in fact) shipping the cython-generated files in your sdists, which made them installable without cython present. So yes, _creating_ the sdist does require cython (and this is what I assume you mean by \"building from source,\" e.g. `setup.py install` or `setup.py sdist` or similar), but _installing_ the created sdist did not require cython until the additional raise ImportError was added in 461e0e9. I get that it made \"installs from source\" fail with a slightly more obvious error, but it also broke \"installs from sdist\" at the same time.\n",
      "There is no wheel for Alpine Linux, so installing Pandas 0.19.0 from PyPi without Cython installed manually, the exception is raised and the installation breaks: https://hub.docker.com/r/frolvlad/alpine-python-machinelearning/builds/bfih8p7usyqibefhnsjk5be/\n",
      "@fungi Aha, thanks, that clarifies the issue. We should try to solve that for 0.19.1\n"
    ],
    "events": [
      "commented",
      "closed",
      "labeled",
      "labeled",
      "milestoned",
      "unlabeled",
      "commented",
      "reopened",
      "referenced",
      "closed",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "reopened",
      "milestoned",
      "demilestoned",
      "labeled",
      "cross-referenced"
    ],
    "changed_files": 2,
    "additions": 20,
    "deletions": 17,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.1.txt",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14218,
    "reporter": "jreback",
    "created_at": "2016-09-14T10:50:22+00:00",
    "closed_at": "2017-01-18T16:12:00+00:00",
    "resolver": "jreback",
    "resolved_in": "99afdd9aecbeb6be532f3c9d1d3c430241a9b579",
    "resolver_commit_num": 4168,
    "title": "DEPR: deprecate .ix",
    "body": "enough said.\n",
    "labels": [
      "Indexing",
      "Deprecate"
    ],
    "comments": [
      "What is the suggested replacement for the deprecated `.ix`? Is it `.loc`?\n\nFor me `.ix` works 5-10% faster than `.loc`:\n\n``` python\n>>> df.shape\n(10000, 211)\n\n>>> df.index\nCategoricalIndex(['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A',\n                  ...\n                  'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'],\n                 categories=['A', 'B', 'C'], ordered=False, dtype='category', length=10000)\n\n>>> df.loc[['C']].shape\n(8000, 211)\n\n>>> %timeit df.loc['C']\n100 loops, best of 3: 5.61 ms per loop\n\n>>> %timeit df.ix['C']\n100 loops, best of 3: 5.37 ms per loop\n```\n\nBTW, passing a list into the indexer adds another 25-50% overhead:\n\n``` python\n>>> %timeit df.loc['C']\n100 loops, best of 3: 5.61 ms per loop\n\n>>> %timeit df.loc[['C']]\n100 loops, best of 3: 9.97 ms per loop\n\n>>> %timeit df.ix['C']\n100 loops, best of 3: 5.37 ms per loop\n\n>>> %timeit df.ix[['C']]\n100 loops, best of 3: 7.57 ms per loop\n```\n",
      "yes `.loc` and `.iloc` are the expected replacements. Timings are expected to eventually be faster, though a single sub-millisecond access difference is pretty meaningless in any real usecase.\n",
      "@jreback Having terabytes of data and processing it with a help of Dask DataFrame which uses Pandas DataFrames as chunks turns \"milliseconds\" into minutes...\n",
      "@frol doesn't matter how much data you have. you are almost certainly ineffeciently using indexing operations.\n",
      "@frol the indexing code paths are going to be rewritten in C/C++ as part of the pandas 2.0 effort, so the microperformance should improve by a factor of 10 or more. Some refactoring or Cythonization may be able to give some quick perf wins in .loc or .iloc\n",
      "Question on .ix deprecation-- suppose you want to set the first row of a DataFrame in a particular column  with a value (assume that the index is not an Int64Index). Then you can currently use:\n\n`\ndf.ix[0, 'colname'] = 5\n`\n\nIn the future can you safely do:\n\n`\ndf.iloc[0].loc['colname'] = 5\n`\n\n(this seems to beg for SettingWithCopyWarning)? Or is the only proper option going to be\n`\ndf.loc[df.index[0], 'colname'] = 5\n`\n?\n",
      "Our experience has been that mixing positional and label indexing has been a significant source of problems for users. Here you might want to do `df['colname'][0]`\n",
      "unambigously safe setting (may be better syntactically nicer in 2.0)\r\n\r\n```\r\ndf.iloc[0, df.columns.get_loc('colname')] = 5\r\n```\r\n\r\nor \r\n\r\n```\r\ndf.loc[df.index[0], 'colname'] = 5\r\n```\r\n",
      "@jreback Thanks, makes sense. \n",
      "@jreback  I think you have a typo with square brackets used instead of parens?  \r\n\r\n    df.iloc[0, df.columns.get_loc['colname']] = 5\r\n\r\nshould be\r\n\r\n    df.iloc[0, df.columns.get_loc('colname')] = 5",
      "@johne13 yes that was a typo, thanks!"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 90,
    "additions": 1657,
    "deletions": 1399,
    "changed_files_list": [
      "doc/source/advanced.rst",
      "doc/source/api.rst",
      "doc/source/basics.rst",
      "doc/source/categorical.rst",
      "doc/source/computation.rst",
      "doc/source/cookbook.rst",
      "doc/source/gotchas.rst",
      "doc/source/indexing.rst",
      "doc/source/io.rst",
      "doc/source/merging.rst",
      "doc/source/missing_data.rst",
      "doc/source/reshaping.rst",
      "doc/source/sparse.rst",
      "doc/source/visualization.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/indexing.py",
      "pandas/core/panel.py",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/io/pytables.py",
      "pandas/io/tests/json/test_pandas.py",
      "pandas/io/tests/parser/common.py",
      "pandas/io/tests/parser/parse_dates.py",
      "pandas/io/tests/test_date_converters.py",
      "pandas/io/tests/test_excel.py",
      "pandas/io/tests/test_html.py",
      "pandas/io/tests/test_packers.py",
      "pandas/io/tests/test_pytables.py",
      "pandas/io/tests/test_sql.py",
      "pandas/io/tests/test_stata.py",
      "pandas/sparse/tests/test_frame.py",
      "pandas/sparse/tests/test_indexing.py",
      "pandas/sparse/tests/test_series.py",
      "pandas/stats/tests/test_ols.py",
      "pandas/tests/formats/test_format.py",
      "pandas/tests/frame/test_alter_axes.py",
      "pandas/tests/frame/test_analytics.py",
      "pandas/tests/frame/test_apply.py",
      "pandas/tests/frame/test_asof.py",
      "pandas/tests/frame/test_axis_select_reindex.py",
      "pandas/tests/frame/test_block_internals.py",
      "pandas/tests/frame/test_combine_concat.py",
      "pandas/tests/frame/test_constructors.py",
      "pandas/tests/frame/test_indexing.py",
      "pandas/tests/frame/test_misc_api.py",
      "pandas/tests/frame/test_missing.py",
      "pandas/tests/frame/test_nonunique_indexes.py",
      "pandas/tests/frame/test_operators.py",
      "pandas/tests/frame/test_replace.py",
      "pandas/tests/frame/test_reshape.py",
      "pandas/tests/frame/test_sorting.py",
      "pandas/tests/frame/test_subclass.py",
      "pandas/tests/frame/test_timeseries.py",
      "pandas/tests/frame/test_to_csv.py",
      "pandas/tests/groupby/test_filters.py",
      "pandas/tests/groupby/test_groupby.py",
      "pandas/tests/indexes/test_datetimelike.py",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tests/indexing/test_callable.py",
      "pandas/tests/indexing/test_categorical.py",
      "pandas/tests/indexing/test_floats.py",
      "pandas/tests/indexing/test_indexing.py",
      "pandas/tests/plotting/test_datetimelike.py",
      "pandas/tests/plotting/test_frame.py",
      "pandas/tests/series/test_analytics.py",
      "pandas/tests/series/test_constructors.py",
      "pandas/tests/series/test_indexing.py",
      "pandas/tests/series/test_repr.py",
      "pandas/tests/series/test_subclass.py",
      "pandas/tests/test_categorical.py",
      "pandas/tests/test_generic.py",
      "pandas/tests/test_internals.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_panel4d.py",
      "pandas/tests/test_panelnd.py",
      "pandas/tests/test_strings.py",
      "pandas/tests/test_window.py",
      "pandas/tools/tests/test_concat.py",
      "pandas/tools/tests/test_join.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tools/tests/test_merge_ordered.py",
      "pandas/tools/tests/test_pivot.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_resample.py",
      "pandas/tseries/tests/test_timedeltas.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14233,
    "reporter": "patricktokeeffe",
    "created_at": "2016-09-16T01:16:44+00:00",
    "closed_at": "2017-01-24T18:46:27+00:00",
    "resolver": "Dr-Irv",
    "resolved_in": "84bc3b2498d89ffd84977c6a5337fb9bb63be176",
    "resolver_commit_num": 9,
    "title": "BUG: KeyError from resample().median() with duplicate column names",
    "body": "I start with a dataframe (`df`) containing staggered measurements (select->copy->`from_clipboard()`):\n\n\n\nWhen I try to aggregate measurements into 5-second intervals using `df.resample('5s').median()`, I get this traceback:\n\n\n\nThe other documented dispatching methods (`sum`, `mean`, `std`, `sem`, `max`, `min`, `first`, `last`) work just fine (except for `ohlc`, which produces an `InvalidIndexError`).\n\nI can work around the problem like so: `df.resample('5s').apply(lambda x: x.median())`. But it seems like dispatching should work here...\n#### output of `pd.show_versions()`\n\n\n",
    "labels": [
      "Bug",
      "Resample"
    ],
    "comments": [
      "Reproducible example:\n\n```\nIn [21]: df = pd.DataFrame(np.random.randn(20,3), columns=list('abc'), index=pd.date_range('2012-01-01', periods=20, freq='s'))\n\nIn [23]: df.resample('5s').median()\nOut[23]: \n                            a         b         c\n2012-01-01 00:00:00 -0.209421 -0.649436 -0.857474\n2012-01-01 00:00:05  0.304136  0.335305  0.639129\n2012-01-01 00:00:10 -0.228682 -0.803259 -0.615048\n2012-01-01 00:00:15  0.121994 -0.214258  0.520752\n\nIn [24]: df = pd.DataFrame(np.random.randn(20,3), columns=list('aaa'), index=pd.date_range('2012-01-01', periods=20, freq='s'))\n\nIn [25]: df.resample('5s').median()\n...\nKeyError: 'median'\n```\n\nSo it is caused by the duplicate column names. \nThis seems like a bug, but in any case you can for now solve for you by renaming the columns as a work-around.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 15,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/groupby.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14241,
    "reporter": "wisefool769",
    "created_at": "2016-09-17T22:57:06+00:00",
    "closed_at": "2016-09-22T10:20:26+00:00",
    "resolver": "chris-b1",
    "resolved_in": "14a1c80fd01d20a3ce951c5956f030ccf1c56b16",
    "resolver_commit_num": 49,
    "title": "DataFrame Query Rounding Error",
    "body": "#### Code Sample, a copy-pastable example if possible\n\nimport pandas as pd\n\ndf = pd.DataFrame([{\"A\": 1000000000.0099}])\ncutoff = 1000000000.006\n\nlocal_res = \"works\" if df.query(\"A < @cutoff\").empty else \"fails\"\nprint(\"query by local var %s\" % local_res)\n\nexplicit_res = \"works\" if df.query(\"A < %.3f\" % cutoff).empty else \"fails\"\nprint(\"explicit query %s\" % explicit_res)\n#### Expected Output\n\nquery by local var works\nexplicit query fails\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.12.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.2.0-42-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 20.3\nCython: 0.23.4\nnumpy: 1.11.1\nscipy: 0.18.0\nstatsmodels: 0.6.1\nxarray: None\nIPython: 5.0.0\nsphinx: 1.3.5\npatsy: 0.4.1\ndateutil: 2.5.1\npytz: 2016.2\nblosc: None\nbottleneck: 1.1.0\ntables: 3.2.3.1\nnumexpr: 2.6.1\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.8.4\nlxml: 3.6.0\nbs4: 4.4.1\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.13\npymysql: None\npsycopg2: 2.6.1 (dt dec pq3 ext)\njinja2: 2.8\nboto: 2.39.0\npandas_datareader: None\n",
    "labels": [
      "Bug",
      "Compat"
    ],
    "comments": [
      "Yeah, something going wrong here - most likely the literal being cast to a 32 bit float.  Doesn't seem to be a numexpr bug, and works correctly on python 3, PRs welcome!\n\n```\nimport numexpr as ne\na, b = 1000000000.0099, 1000000000.006\n\nne.evaluate('a < %.3f' % b)\nOut[57]: array(False, dtype=bool)\n\npd.eval(\"a <  %.3f\" % b)\nOut[58]: True\n```\n"
    ],
    "events": [
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "cross-referenced",
      "milestoned",
      "labeled"
    ],
    "changed_files": 5,
    "additions": 59,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/computation/ops.py",
      "pandas/computation/pytables.py",
      "pandas/computation/tests/test_eval.py",
      "pandas/io/tests/test_pytables.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14266,
    "reporter": "ssanderson",
    "created_at": "2016-09-21T08:28:34+00:00",
    "closed_at": "2016-09-22T10:03:24+00:00",
    "resolver": "jreback",
    "resolved_in": "3c9644277b10bde301025733402c67310a28cdf0",
    "resolver_commit_num": 4083,
    "title": "Large Monotonic Index Objects Always Allocate Hash Tables on get_loc",
    "body": "Historically, large monotonically-increasing `Index` objects would attempt to avoid creating a large hash table on `get_loc` calls. In service of this goal, many `IndexEngine` methods have guards like the one in `DatetimeEngine.get_loc`:\n\n\n\nSince at least 5eecdb2dd94719e1fd097ce3fb046697445a3d7f, `self.is_unique` has been implemented as a property that would force a hash table to be created **unless the index had already been marked as unique**.  Until , the `is_monotonic_increasing` property would perform a check that would [sometimes set `self.unique` to True](#diff-4f3357a9f53e943087e2778134494905L236), which would prevent the large hash table allocation.  After the commit linked above, however, the only code path that ever sets `IndexEngine.unique` is in [`IndexEngine.initialize`](#L269-L270), which [unconditionally creates a hash table before setting the unique flag.](#L266-L267).\n#### Code Sample, a copy-pastable example if possible\n\n\n#### Output (Old Pandas):\n\n\n#### Output (Pandas 0.18.1)\n\n\n\nFor some context, I found this after the internal Jenkins build for Zipline (which makes heavy use of large minutely `DatetimeIndex`es to represent trading calendars) started failing with memory errors after merging \n\nAssuming that the memory-saving behavior of older pandas is still desired, I think the right immediate fix for this is to change `IndexEngine._do_unique_check` to actually do a uniqueness check instead of just forcing a hash table creation.  Reading through the code, however, there are a bunch of ways that large `Index`es could still hit code paths that trigger hash table allocations.  For example, `DatetimeEngine.__contains__` guards against `self.over_size_threshold`, but none of the other `IndexEngine` subclasses do.  A more significant refactor is probably needed to provide a meaningful guarantee that indices don't consume too much memory.\n#### output of `pd.show_versions()`\n\n<details>\nIn [4]: pd.show_versions()\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.10.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.2.0-16-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.16.1\nnose: 1.3.7\nCython: 0.22.1\nnumpy: 1.9.2\nscipy: 0.15.1\nstatsmodels: 0.6.1\nIPython: 5.1.0\nsphinx: 1.3.4\npatsy: 0.4.0\ndateutil: 2.4.2\npytz: 2015.4\nbottleneck: 1.0.0\ntables: None\nnumexpr: 2.4.6\nmatplotlib: 1.4.3\nopenpyxl: None\nxlrd: 0.9.4\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.8\npymysql: None\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\n</details>\n",
    "labels": [
      "Indexing",
      "Performance"
    ],
    "comments": [
      "xref #12272 for the general usecase (frequency based index) makes this vastly more memory efficient (IOW it already knows `is_unique`, `is_monotonic_increasing` etc.)\n",
      "Unfortunately, I don't think a range-based implementation will work for Zipline's specific use-case, since there are lots of irregularities to real-world trading schedules, but a datetime-based range index definitely seems useful for folks with nicely regular data.\n",
      "yeah I suspect what you actually want is a sparse range (IOW a range based, with a mask which is much more memory efficient)\n",
      "https://github.com/pydata/pandas/pull/13594 does not seem to matter here (though maybe it did originally)\n",
      "so [this](https://github.com/jreback/pandas/commit/4cca9439978ae6490c968795c82cd8b0b1d8cb2e) fixes the issue (it breaks some other things, but those just need a trivial `_ensure_mapping` to fixup.\n\nIIRC quite some time ago we changed the is_monotonic check. It _used_ to _also_ compute uniqueness when it actually is unique, so this is a necessary but not sufficient condition.\n\nHowever we are not using that information and recomputing (and constructing the mapping which is memory heavy).\n\nThis uses the check where possible (and does the re-initialization if needed).\n\njust needs a little fixup and I think will solve the issue.\n",
      "@jreback thanks for the quick response.  Does your branch also plan to update the other code paths that can trigger hash table creation ?  At a glance, it looks like `get_indexer` does this, as well as `__contains__` for everything except `DatetimeEngine`, which has custom special-case logic.\n",
      "would need some other test cases\n\nthis by definition IS special cased\n",
      "@ssanderson by-definition `get_indexer` MUST populate the hash table, so for `__contains__` this also seems reasonable (e.g. even if its unique, how do you find the indexer?)\n",
      "> so for `__contains__` this also seems reasonable\n\nI'm not sure if you're saying that `__contains__` should always populate, or if you're saying it should avoid populating when possible.  To be clear, `DatetimeEngine` [already implements](https://github.com/pydata/pandas/blob/a7469cf98275a183ad2e4bfafa9706a1ef8d035e/pandas/index.pyx#L517-L524) `__contains__` without a hash table by doing a `searchsorted` and then comparing the value at the located index with the whose containment status was requested.\n",
      "and the DTE will avoid populating for the cases we are talking. \n\nFor non-large (e.g. < cutoff) it has always used the existing logic (which does populate). separate / independent whether that should be profiled.\n",
      "> and the DTE will avoid populating for the cases we are talking.\n\nRight.  My original question was whether we should make the other engines have the same behavior as the datetime engine.  It seemed odd to me the different index types would want to make different choices about whether to populate the hash table, but it's possible that that's by design for reasons I missed?\n",
      "so it IS possible for int64 index as `.searchsorted` would be faster. I _dont't_ think this should be the default for `Index` as I suspect the hashtable impl is much faster that `.searchsorted`. But that could/should be another issue.\n\nBenchmarks on this behavior would be welcome though to make a decision. Profiling is key here.\n",
      "> by-definition `get_indexer` MUST populate the hash table\n\nI'm not sure what you mean when you say that `get_indexer` \"by definition\" has to populate the hash table.  \n\nAs I understand it, `get_indexer` is essentially just a vectorized version of `get_loc`, so assuming we can implement `get_loc` without a hash table (which we've already done for the special case of of a monotonic index), in the worst case we could implement `get_indexer` in terms of a for-loop that calls `get_loc`.  In practice, you could probably do much better than the naive for-loop, especially if the target indexer is also monotonic.\n\nI certainly could see the argument that such a change to `get_indexer` is large enough that it deserves to be a separate PR/discussion.\n",
      "> so it IS possible for int64 index as searchsorted would be faster. I dont't think this should be the default for Index as I suspect the hashtable impl is much faster that searchsorted. But that could/should be another issue.\n\nFair enough.  Another thing to think about here is that there are workloads where the user might accept a slower searchsorted-based index in exchange for memory savings.  In our production Zipline deployments, for example, our bottleneck is almost always RAM, not CPU, so we'd likely be willing to take a performance hit in exchange for an extra couple hundred MB.  The design here is tricky though b/c different users and use-cases will be willing to make different tradeoffs here.\n",
      "let me clarify, yes I agree `.get_indexer` could have the same treatment, though it doesn't ATM. So separate issue for that. I wasn't attempting to change things which weren't already implemented the large-sorted-unique behavior (which is a useful special case)\n",
      "Cool, sounds like we're in agreement.  If I find the time to work on it, would you be interested in a separate PR that extends the no-hash-table algorithms in some of the cases outlined above?  I can't promise for sure that I'll be able to devote a ton of time to it, but zipline leans pretty heavily on DatetimeIndex, so optimizations here can be pretty big wins for us.\n",
      "yes I think that would be great. Please create an issue in any event.\n",
      "Opened as https://github.com/pydata/pandas/issues/14273.\n"
    ],
    "events": [
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "renamed",
      "commented",
      "cross-referenced",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "referenced",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 89,
    "deletions": 55,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/index.pyx",
      "pandas/src/algos_common_helper.pxi",
      "pandas/src/algos_common_helper.pxi.in",
      "pandas/tseries/index.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14291,
    "reporter": "mbochk",
    "created_at": "2016-09-23T14:04:17+00:00",
    "closed_at": "2016-10-24T22:51:28+00:00",
    "resolver": "paul-mannino",
    "resolved_in": "2e77536bdf90ef20fefd4eab751447918e07668f",
    "resolver_commit_num": 1,
    "title": "BUG: DataFrame.insert with allow_duplicates=True fails when already duplicates present",
    "body": "upon DataFrame.insert option allow_duplicates works, but only only once.\nWhen i have 2 columns with same name, additon of third throws\n\n> ValueError: Wrong number of items passed 2, placement implies 1\n#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n   zxc  qwe  qwe\n0    1    1    1\n1    2    2    2\n2    3    3    3\n3    4    4    4\n#### output of `pd.show_versions()`\n\n<details>\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.12.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: ru_RU\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 25.1.6\nCython: 0.24.1\nnumpy: 1.11.1\nscipy: 0.18.0\nstatsmodels: 0.6.1\nxarray: None\nIPython: 5.1.0\nsphinx: 1.4.6\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.6.1\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.6.1\nmatplotlib: 1.5.3\nopenpyxl: 2.3.2\nxlrd: 1.0.0\nxlwt: 1.1.2\nxlsxwriter: 0.9.2\nlxml: 3.6.4\nbs4: 4.4.1\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.13\npymysql: 0.7.6.None\npsycopg2: None\njinja2: 2.8\nboto: 2.40.0\npandas_datareader: None\n<\\details>\n",
    "labels": [
      "Bug",
      "Reshaping",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "@mbochk That looks like a bug indeed. Thanks for reporting\n",
      "I looked into this and discovered that the problem is in frame.py, in the _sanitize_column method. Here's the relevant code:\n\n``` python\n# broadcast across multiple columns if necessary\nif key in self.columns and value.ndim == 1:\n    if (not self.columns.is_unique or\n            isinstance(self.columns, MultiIndex)):\n        existing_piece = self[key]\n        if isinstance(existing_piece, DataFrame):\n            value = np.tile(value, (len(existing_piece.columns), 1))\n```\n\nOn the third time `insert` is called, the `existing_piece` is a 2d array consisting of the previous values. I'm not sure how to fix this though as I don't understand why the values are being broadcast in the first place. Any thoughts?\n",
      "What happens here is needed when you are setting a certain column (eg `df[key] = value`). If `key` then is a duplicate column name, the `value` has to be broadcasted to fit in those multiple columns. \nBut of course this part of `_sanitize_column` is not needed for an insert operation.\n"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "renamed",
      "labeled",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 56,
    "deletions": 8,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.1.txt",
      "pandas/core/frame.py",
      "pandas/sparse/frame.py",
      "pandas/tests/frame/test_nonunique_indexes.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14293,
    "reporter": "mrocklin",
    "created_at": "2016-09-23T23:55:38+00:00",
    "closed_at": "2016-09-27T22:10:32+00:00",
    "resolver": "jreback",
    "resolved_in": "71df09cfb3110cebf5598e3eb1646f246b483f51",
    "resolver_commit_num": 4085,
    "title": "Performance of pandas.algos.groupby_int64",
    "body": "For dask.dataframe shuffle operations (groupby.apply, merge), when running with multiple threads per process, I sometimes find my computations dominated by `pandas.algos.groupby_int64`.  Looking at the source code for this it looks like it's using dynamic pure python objects from Cython.  I'm curious if there are ways to accelerate this function, particularly in multi-threaded situations (releasing the GIL).  \n\nOne solution that comes to mind would be to do a single pass over `labels`, pre-compute the length of each `members` list in `results` and then pre-allocate these as arrays.  This might allow better GIL-releasing behavior.\n\nThoughts?\n",
    "labels": [
      "Groupby",
      "Performance"
    ],
    "comments": [
      "can u show a particular example that you have been timing (so all on the same page)\n",
      "Those `groupby_*` functions are a real bummer. \n\nThe right way to do this that avoids the GIL (assuming that `labels` does not contain Python objects):\n- Specialize on the labels array type\n- Use a native hash table rather than a Python dict\n- Do not use Python lists\n- Compute categories, then do a O(n) stable counting sort (depending on the cardinality, if large cardinality do a merge sort), then you can do the groupby.apply take operations using the counting sort array.\n\nFor example, if the labels are:\n\n`[a, b, c, a, b, c, a, b, c]`\n\nThen you factorize (which will cause GIL contention if object dtype) to get\n\n`[0, 1, 2, 0, 1, 2, 0, 1, 2]`\n\nStable argsort (either by counting sort or mergesort) yields\n\n`[0, 3, 6, 1, 4, 7, 2, 5, 8]`\n\nNow, you iterate through this array to delimit each contiguous group.\n- 0, 3, 6 are all 0's\n- when you hit 1 (index 3 in the argsort array) you know you have reached the end of the group\n- and so on\n",
      "Note that in pandas 2.0, factorizing strings (assuming we implement https://pydata.github.io/pandas-design/strings.html) will not require the GIL -> multicore happiness. \n",
      "@jreback \n\n``` python\nimport numpy as np\nimport pandas as pd\ns = pd.Series(np.random.randint(0, 100, size=2**24))\ns.groupby(s).groups\n```\n\n```\n         412 function calls (408 primitive calls) in 5.913 seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        1    5.120    5.120    5.120    5.120 {pandas.algos.groupby_int64}\n        1    0.792    0.792    5.913    5.913 <string>:1(<module>)\n        1    0.000    0.000    5.120    5.120 base.py:2355(groupby)\n```\n",
      "@wesm FWIW I only care about `groupby_int*`.  For me the index here is the eventual partition number so labels are always in `0..n`.  Obviously you don't want to assume this for general pandas groupby algorithms but it might be useful to pull out the argsort-partition logic into a separate function.\n",
      "master\n\n```\nIn [1]: np.random.seed(1234)\n   ...: \n   ...: s = pd.Series(np.random.randint(0, 100, size=2**24))\n   ...: \n\nIn [2]: %timeit -n 1 -r 1 s.groupby(s).groups\n1 loop, best of 1: 5.26 s per loop\n\nIn [3]: s.groupby(s).groups[0][-10:]\nOut[3]: \n[16776669,\n 16776672,\n 16776713,\n 16776752,\n 16776875,\n 16777047,\n 16777110,\n 16777131,\n 16777139,\n 16777165]\n\nIn [4]: pd.__version__\nOut[4]: '0.19.0rc1+31.gd9e51fe'\n```\n\npatch\n\n```\nIn [1]: np.random.seed(1234)\n   ...: \n   ...: s = pd.Series(np.random.randint(0, 100, size=2**24))\n   ...: \n\nIn [2]: %timeit -n 1 -r 1 s.groupby(s).groups\n1 loop, best of 1: 742 ms per loop\n\nIn [3]: s.groupby(s).groups[0][-10:]\nOut[3]: \narray([16776669, 16776672, 16776713, 16776752, 16776875, 16777047,\n       16777110, 16777131, 16777139, 16777165])\n\nIn [4]: pd.__version__\nOut[4]: '0.19.0rc1+32.g6256f2f'\n```\n\n```\n[jreback-~/pandas] git diff master\ndiff --git a/pandas/algos.pyx b/pandas/algos.pyx\nindex 8710ef3..0a1e806 100644\n--- a/pandas/algos.pyx\n+++ b/pandas/algos.pyx\n@@ -1024,16 +1024,17 @@ def groupby_indices(dict ids, ndarray[int64_t] labels,\n         result[ids[i]] = arr\n         vecs[i] = <int64_t*> arr.data\n\n-    for i from 0 <= i < n:\n-        k = labels[i]\n+    with nogil:\n+        for i from 0 <= i < n:\n+            k = labels[i]\n\n-        # was NaN\n-        if k == -1:\n-            continue\n+            # was NaN\n+            if k == -1:\n+                continue\n\n-        loc = seen[k]\n-        vecs[k][loc] = i\n-        seen[k] = loc + 1\n+            loc = seen[k]\n+            vecs[k][loc] = i\n+            seen[k] = loc + 1\n\n     free(vecs)\n     return result\ndiff --git a/pandas/indexes/base.py b/pandas/indexes/base.py\nindex f430305..16fe13c 100644\n--- a/pandas/indexes/base.py\n+++ b/pandas/indexes/base.py\n@@ -2366,7 +2366,11 @@ class Index(IndexOpsMixin, StringAccessorMixin, PandasObject):\n         groups : dict\n             {group name -> group labels}\n         \"\"\"\n-        return self._groupby(self.values, _values_from_object(to_groupby))\n+        from pandas import Categorical\n+        from pandas.core.groupby import _groupby_indices\n+        result = _groupby_indices(Categorical(to_groupby))\n+\n+        return result\n\n     def map(self, mapper):\n         \"\"\"\n```\n",
      "I had added a special case for categorical grouping because `merge_asof` needed it (it make things a lot faster). So just converting to categoricals and using it here is a pretty good speedup.\n\na couple of minor tests fail with this (edge case with all nan categories), but no big deal to fix.\n\nThis _still_ hits a dictionary encoding path (in cython), but _could_ have the GIL released for part of it.\n",
      "you should get gil releasing in the factorize and now the groupby_indices (these are about 2/3 of the time), rest is python-ish\n",
      "That performance gain would definitely resolve my immediate needs and likely move Pandas well away from being a bottleneck.\n",
      "I pushed it up: https://github.com/jreback/pandas/tree/groupby\n\n(as s I said, running some perf numbers and a couple of edge cases), but give it a go\n",
      "```\n  [d9e51fe7] [3da4a8d7]\n+  610.20\u03bcs     2.54ms      4.17  groupby.groupby_ngroups_float_100.time_sum\n+    2.91ms    11.70ms      4.02  groupby.groupby_ngroups_float_10000.time_count\n+   12.49ms    45.73ms      3.66  groupby.groupby_ngroups_float_100.time_unique\n+    1.50ms     5.24ms      3.50  groupby.groupby_ngroups_float_100.time_tail\n+  484.40ms      1.48s      3.06  groupby.groupby_multi_index.time_groupby_multi_index\nSOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.\n```\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "referenced",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 16,
    "additions": 201,
    "deletions": 482,
    "changed_files_list": [
      "asv_bench/benchmarks/gil.py",
      "asv_bench/benchmarks/groupby.py",
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/algos.pyx",
      "pandas/core/categorical.py",
      "pandas/core/groupby.py",
      "pandas/indexes/base.py",
      "pandas/indexes/numeric.py",
      "pandas/src/algos_common_helper.pxi",
      "pandas/src/algos_common_helper.pxi.in",
      "pandas/tests/indexes/test_base.py",
      "pandas/tests/indexes/test_numeric.py",
      "pandas/tests/test_groupby.py",
      "pandas/tests/types/test_inference.py",
      "pandas/tseries/base.py",
      "pandas/types/common.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14316,
    "reporter": "boncelet",
    "created_at": "2016-09-28T21:32:17+00:00",
    "closed_at": "2016-09-30T10:08:02+00:00",
    "resolver": "chris-b1",
    "resolved_in": "ad92aeec3a0d48ec7b68f34aa3081fb8e02cd086",
    "resolver_commit_num": 51,
    "title": "Slicing with reversed datetime index",
    "body": "I'm working with a time series data file from EIA.gov.  It's a CSV file with the dates in reverse order (most recent first).  Slicing on the dates wasn't working right.  I believe these snippets illustrate the problem:\n\nCreate dataframe and date reversed dataframe.\n\n\n\nFirst the normal order: Slice on dates, getting what we expect:\n\n\n\nNow the reverse order: I expect to get exactly the same answer (same slice on the same dataset), perhaps with rows reversed, but don't:\n\n\n\nPerhaps if I reverse the slice.  Note only two rows are returned:\n\n\n\nThis makes sense, but it's still confusing:\n\n\n\nI could sort (or reverse) my index, but I had no reason to believe I needed to.  Furthermore, other operations, such as plotting, work as expected (the time axis is correct).  \n#### Output of `pd.show_versions()`\n\n<details>\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.2.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 23 Stepping 6, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 23.0.0\nCython: 0.24\nnumpy: 1.11.1\nscipy: 0.17.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.2.0\nsphinx: 1.3.1\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: 1.1.0\ntables: 3.2.2\nnumexpr: 2.6.0\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 1.0.0\nxlwt: 1.1.2\nxlsxwriter: 0.9.2\nlxml: 3.6.0\nbs4: 4.4.1\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.13\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.40.0\npandas_datareader: None\n\n</details>\n",
    "labels": [
      "Bug",
      "Timeseries"
    ],
    "comments": [
      "Sorry to comment on my own issue, but here are two more examples. I get that the rows are different orders, but shouldn't both return the same number of rows?\n\n``` python\nprint(rdf['2016-01-01':'2016-01-03':-1])\n            Col\n2016-01-02    2\n2016-01-03    3\n```\n\n``` python\nprint(df['2016-01-03':'2016-01-01':-1])\n            Col\n2016-01-03    3\n2016-01-02    2\n2016-01-01    1\n```\n",
      "> Now the reverse order: I expect to get exactly the same answer (same slice on the same dataset), perhaps with rows reversed, but don't:\n> \n> ```\n> >> print(rdf['2016-01-01':'2016-01-03'])\n> Empty DataFrame\n> Columns: [Col]\n> Index: []\n> ```\n\nThis part at least is working as intended. Slicing a descending order index requires a descending slice. This mirrors how slicing an ascending order index requires an ascending slice.\n\nDatetime indexing with strings involves some implicit rounding to match all appropriate times. It's possible that the logic is not quite right for reversed indexes.\n",
      "The reason I said \" I expect to get exactly the same answer (same slice on the same dataset)\" is that I expected a slice to behave like a query on a database: give me all rows whose dates are in the range (inclusive).  Clearly, the implementation will be much faster if the index is ordered (internally), but the query (slice) should still work as \"expected\" even if not.\n",
      ">  I expected a slice to behave like a query on a database: give me all rows whose dates are in the range (inclusive).\n\nThis is a reasonable intuition, but it's the wrong mental model for slicing in pandas.  A better model is: give me the continuous slice of the data enclosed by these bounds. Slicing in pandas doesn't work on indexes that are not either monotonic increasing or monotonic decreasing.\n",
      "Should label slicing with a step maybe just raise?  I might be missing a valid usecase, but I'm not even sure if these are bugs, because I don't really know what to expect.\n\n```\nIn [8]: rdf['2016-01-01':'2016-01-03':-1]\nOut[8]: \n            Col\n2016-01-02    2\n2016-01-03    3\n\nIn [9]: df['2016-01-03':'2016-01-01':-1]\nOut[9]: \n            Col\n2016-01-03    3\n2016-01-02    2\n2016-01-01    1\n```\n",
      "I guess there are tests here covering this behavior, so it is well defined, if a little strange.\n\nhttps://github.com/pydata/pandas/blob/master/pandas/tests/indexing/test_indexing.py#L5140\n",
      "@boncelet - of these cases, the only one that looks buggy to me is this one - it should include the ending label.  The others seem to just be symptoms of slicing semantics like @shoyer mentioned.  Thanks for the report.\n\n```\nIn [22]: rdf.loc['2016-01-03':'2016-01-01', :]\nOut[22]: \n            Col\n2016-01-03    3\n2016-01-02    2\n```\n",
      "I think the issue is in [`_maybe_cast_slice_bound`](https://github.com/pydata/pandas/blob/5033a4a799c77fdc7e868a9f332384eabcc332b8/pandas/tseries/index.py#L1421). It needs to flip `side` for monotonic decreasing, like [`_searchsorted_monotonic`](https://github.com/pydata/pandas/blob/5033a4a799c77fdc7e868a9f332384eabcc332b8/pandas/indexes/base.py#L3042)\n",
      "Thanks.  Part of my concern is it wasn't obvious to me I needed to worry about the order of the data in the original CSV file.  Same data, whether it's sorted first to last or last to first.  \n\nIt turns out I was wrong about plots.  At least some plots give the x-axis (time) reversed.  Normally this is not what we want.  \n\nUnfortunately, this slicing (and plotting) behavior means user code has to check the order of the data and sort it if necessary.  For modest datasets (like those I'm looking at) no problem.  But I can imagine huge datasets where sorting will be a burden.  \n\nIt might be nice to have a user settable flag to reverse the index without actually reversing the index, analogously to a numpy stride (which can be negative).  That way we can assume the index is increasing even if internally it is decreasing.  It'd be even nicer if the various dataframe creators set the flag automatically, thereby relieving the user of the necessity to check the order.\n\nSincerely, thanks to all you developers who've worked so hard to create useful tools like pandas.\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned"
    ],
    "changed_files": 3,
    "additions": 23,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.0.txt",
      "pandas/tests/indexing/test_indexing.py",
      "pandas/tseries/index.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14320,
    "reporter": "jluttine",
    "created_at": "2016-09-29T08:27:01+00:00",
    "closed_at": "2017-03-03T13:16:14+00:00",
    "resolver": "sahildua2305",
    "resolved_in": "0b07b07da7d5de06a414af467f9f5667835c150e",
    "resolver_commit_num": 2,
    "title": "BUG: Frequency not set for empty Series",
    "body": "Pandas doesn't set frequency properly for empty series. This is probably related to #14313.\n#### A small, complete example of the issue\n\nIt works for non-empty series:\n\n\n\nBut for empty series, `freq` is `None`:\n\n\n#### Expected Output\n\nI expected the following output:\n\n\n#### Output of `pd.show_versions()`\n\n<details>\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.2.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.7.5-gnu-1\nmachine: x86_64\nprocessor: \nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\nLOCALE: en_US.UTF-8\n\npandas: 0.19.0rc1\nnose: 1.3.6\npip: 8.1.2\nsetuptools: 27.3.0\nCython: 0.24.1\nnumpy: 1.11.1\nscipy: 0.18.1\nstatsmodels: None\nxarray: None\nIPython: 5.0.0\nsphinx: 1.4.5\npatsy: None\ndateutil: 2.5.3\npytz: 2016.6.1\nblosc: None\nbottleneck: None\ntables: 3.2.3.1\nnumexpr: 2.6.1\nmatplotlib: 1.5.2\nopenpyxl: None\nxlrd: 1.0.0\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: 4.5.1\nhtml5lib: None\n 0.9.2\napiclient: 1.5.1\nsqlalchemy: 1.0.15\npymysql: None\npsycopg2: 2.6.2 (dt dec pq3 ext lo64)\njinja2: 2.8\nboto: None\npandas_datareader: None\n</details>\n",
    "labels": [
      "Bug",
      "Timeseries",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "https://github.com/pydata/pandas/blob/master/pandas/tseries/resample.py#L1347\n\nshould do: `obj._shallow_copy(freq=to_offset(freq))`\n",
      "@jreback This will require defining `_shallow_copy` for `Series` right? Because as of now it gives error -\n\n```\nAttributeError: 'Series' object has no attribute '_shallow_copy'\n```\n",
      "no this is for an Index\n",
      "I tried `obj._shallow_copy(freq=to_offset(freq))` but it gets the above mentioned error. \n",
      "create a copy of the obj then assign the series with a shallow copy \n",
      "I got this one! I will write tests to verify the expected behavior and create a PR. Thanks \ud83d\ude04 \n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "renamed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 21,
    "deletions": 8,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/tests/series/test_timeseries.py",
      "pandas/tseries/resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14323,
    "reporter": "Liam3851",
    "created_at": "2016-09-29T16:24:34+00:00",
    "closed_at": "2016-10-24T22:34:21+00:00",
    "resolver": "Liam3851",
    "resolved_in": "bee90a7c50576b0160db55fb325908040233e92d",
    "resolver_commit_num": 0,
    "title": "DatetimeIndex union fails in 0.19rc1 when constructed from differences in DatetimeIndexes",
    "body": "When constructing a union of 2 DatetimeIndex objects that themselves were constructed from differences from a third DatetimeIndex, the union operator is ignored. This appears to be new behavior in 0.19rc1; the code functioned correctly under 0.18.1.\n\n\n#### Output of `pd.show_versions()`\n\n<details>\n## INSTALLED VERSIONS                                                                                  \n\ncommit: None  \npython: 2.7.11.final.0  \npython-bits: 64  \nOS: Windows  \nOS-release: 7  \nmachine: AMD64  \nprocessor: Intel64 Family 6 Model 62 Stepping 4, GenuineIntel  \nbyteorder: little  \nLC_ALL: None  \nLANG: None  \nLOCALE: None.None                                                                                   \n\npandas: 0.19.0rc1+0.g497a3bc.dirty  \nnose: 1.3.7  \npip: 8.1.2  \nsetuptools: 27.2.0  \nCython: 0.24.1  \nnumpy: 1.11.1  \nscipy: 0.18.1  \nstatsmodels: 0.6.1  \nxarray: 0.8.2  \nIPython: 5.1.0  \nsphinx: 1.4.6  \npatsy: 0.4.1  \ndateutil: 2.5.3  \npytz: 2016.6.1  \nblosc: None  \nbottleneck: 1.1.0  \ntables: 3.2.2  \nnumexpr: 2.6.1  \nmatplotlib: 1.5.3  \nopenpyxl: 2.3.2  \nxlrd: 1.0.0  \nxlwt: 1.1.2  \nxlsxwriter: 0.9.3  \nlxml: 3.6.4  \nbs4: 4.5.1  \nhtml5lib: None  \n 0.9.2  \napiclient: 1.4.2  \nsqlalchemy: 1.0.13  \npymysql: None  \npsycopg2: None  \njinja2: 2.8  \nboto: 2.42.0  \npandas_datareader: 0.2.1  \n</details>\n",
    "labels": [
      "Regression",
      "Bug",
      "Timeseries",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "@Liam3851 Thanks for the report! I can confirm it is indeed a bug/regression.\n\nSeems it has something to do with `offset` being now defined:\n\n```\nIn [36]: pd.__version__\nOut[36]: '0.19.0rc1+34.gc128626'\n\nIn [37]: a\nOut[37]: \nDatetimeIndex(['2016-09-21', '2016-09-22', '2016-09-25', '2016-09-26',\n               '2016-09-27', '2016-09-28', '2016-09-29', '2016-09-30'],\n              dtype='datetime64[ns]', freq='D')\n\nIn [38]: a.offset\nOut[38]: <Day>\n```\n\nvs\n\n```\nIn [12]: pd.__version__\nOut[12]: '0.18.1'\n\nIn [13]: a\nOut[13]: \nDatetimeIndex(['2016-09-21', '2016-09-22', '2016-09-25', '2016-09-26',\n               '2016-09-27', '2016-09-28', '2016-09-29', '2016-09-30'],\n              dtype='datetime64[ns]', freq=None)\n\nIn [15]: a.offset is None\nOut[15]: True\n```\n",
      "It looks to me like this was introduced with #13514. `Index.difference` now returns a shallow copy of the original index with the differenced values:\n\n`return this._shallow_copy(the_diff, name=result_name)`\n\nBefore, this code returned a new DatetimeIndex:\n\n`return Index(theDiff, name=result_name)`\n\nI'm not too familiar with this code, but it looks to me like _shallow_copy is copying all the attributes, including the freq (which is no longer valid after the differencing operation). The `Index` constructor version would have re-computed the frequency during the construction of the new Index (thus determining there was no valid frequency).\n",
      "@Liam3851 I think that is a perfect assessment of the situation. We could use `_simple_new` instead of `_shallow_copy` (which does not retain the attributes), but then eg the timezone of a DatetimeIndex would get lost.\n\n@sinhrks do you think of a generic approach without adding code to specifically invalidate the `freq` in case of a DatetimeIndex\n",
      "you can just pass freq=None to _shallow_copy\ndon't directly use _simple_new\n",
      "Ah, I thought that would not work for PeriodIndex (which wants to keep its `freq`), but apparently it does.\n\n@Liam3851 Wants to do a PR for this change?\n",
      "I'm doing a PR (sorry, still a newb at Git and pandas building). Just want to get the requirements straight for the unit tests:\n1. DatetimeIndex and TimedeltaIndex should be None freq after the differencing, not an inferred frequency (it appears this was the old behavior, counter to what I said above).\n2. PeriodIndex should retain its frequency after differencing, even without the missing datapoints (because the Periods themselves have frequency)\n3. All three should have a union of differences matching a union of indexes.\n\nThat about right?\n"
    ],
    "events": [
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 77,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.1.txt",
      "pandas/indexes/base.py",
      "pandas/tests/indexes/test_datetimelike.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14336,
    "reporter": "xflr6",
    "created_at": "2016-10-03T13:45:59+00:00",
    "closed_at": "2017-01-23T13:41:25+00:00",
    "resolver": "xflr6",
    "resolved_in": "a1b6587153ebf81b02272fc8a717d2b6c9d0c4aa",
    "resolver_commit_num": 2,
    "title": "API: add DataFrame.nunique() and DataFrameGroupBy.nunique()",
    "body": "When exploring a data set, I often need to `df.apply(pd.Series.nunique)` or `df.apply(lambda x: x.nunique())`. How about adding this as `nunique()`-method parallel to `DataFrame.count()` (`count` and `unique` are also the two most basic infos displayed by `DataFrame.describe()`)?\n\nI think there are also use cases for this as a `groupby`-method, for example when checking a candidate primary key for different lines (values):\n\n\n",
    "labels": [
      "Enhancement",
      "Groupby",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "Agreed, I think this would be welcome functionality.\n",
      "Note that these are already defined for Series.\n\n```\nIn [9]: \n   ...: df.groupby('id').value.nunique()\nOut[9]: \nid\neggs    1\nspam    2\nName: value, dtype: int64\n\nIn [10]: \n    ...: df.groupby('id').value.unique()\nOut[10]: \nid\neggs       [5]\nspam    [1, 2]\nName: value, dtype: object\n```\n",
      "Of course, extending the `groupby`-example:\n\n``` python\n>>> df = pd.DataFrame({'id': ['spam', 'eggs', 'eggs', 'spam', 'ham', 'ham'],\n                       'value1': [1, 5, 5, 2, 5, 5], 'value2': list('abbaxy')})\n>>> df\n     id  value1 value2\n0  spam       1      a\n1  eggs       5      b\n2  eggs       5      b\n3  spam       2      a\n4   ham       5      x\n5   ham       5      y\n>>> df.groupby('id').filter(lambda g: (g.apply(pd.Series.nunique) > 1).any())\n     id  value1 value2\n0  spam       1      a\n3  spam       2      a\n4   ham       5      x\n5   ham       5      y\n```\n",
      "Any news?"
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "commented"
    ],
    "changed_files": 7,
    "additions": 161,
    "deletions": 5,
    "changed_files_list": [
      "asv_bench/benchmarks/frame_methods.py",
      "asv_bench/benchmarks/groupby.py",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/tests/frame/test_analytics.py",
      "pandas/tests/groupby/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14345,
    "reporter": "MattRijk",
    "created_at": "2016-10-04T15:25:20+00:00",
    "closed_at": "2016-10-06T10:23:45+00:00",
    "resolver": "MattRijk",
    "resolved_in": "58542e83367973be15fe22e0906907edab35813f",
    "resolver_commit_num": 1,
    "title": "TST: failing test at tseries/tests/test_base.py TestDatetimeIndexOps test_nat",
    "body": "#### A small, complete example of the issue\n\n\n#### Expected Output\n# failing test\n\nidx = pd.DatetimeIndex(['2011-01-01', '2011-01-02'], tz=tz)\ntm.assert_numpy_array_equal(idx._nan_idxs, np.array([], dtype=np.int64))\n\nidx = pd.DatetimeIndex(['2011-01-01', 'NaT'], tz=tz)\ntm.assert_numpy_array_equal(idx._nan_idxs, np.array([1], dtype=np.int64))\n# test passes\n\ntm.assert_numpy_array_equal(idx._nan_idxs, np.array([], dtype=np.int32))\n\nIn Ipython on 32bit the values look fine.\n\nIn [3]: idx = pd.DatetimeIndex(['2016-01-01', 'NaT'], tz=None)\n\nIn [4]: idx.dtype\nOut[4]: dtype('<M8[ns]')\n\nIn [5]: idx._nan_idxs\nOut[5]: array([], dtype=int64)\n\nIn [6]: arry = np.array([], dtype=np.int64)\n\nIn [7]: arry\nOut[7]: array([], dtype=int64) \n\nIn [8]: import pandas.util.testing as tm\n\nIn [9]: tm.assert_numpy_array_equal(idx._nan_idxs, arry))\nOut[9]: True\n\nThis fixes it but I think theres more going on.\n\nimport platform\nif platform.architecture()[0] == '32bit':\n    tm.assert_numpy_array_equal(idx._nan_idxs, np.array([], dtype=np.int32))\nelse:\n    tm.assert_numpy_array_equal(idx._nan_idxs, np.array([], dtype=np.int64))\n#### Output of `pd.show_versions()`\n\n<details>\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.12.final.0\npython-bits: 32\nOS: Linux\nOS-release: 3.13.0-92-generic\nmachine: i686\nprocessor: i686\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\nLOCALE: None.None\n\npandas: 0.19.0\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 27.2.0\nCython: 0.24.1\nnumpy: 1.11.1\nscipy: 0.18.1\nstatsmodels: None\nxarray: None\nIPython: 5.1.0\nsphinx: 1.4.6\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.6.1\nblosc: None\nbottleneck: 1.1.0\ntables: 3.2.3.1\nnumexpr: 2.6.1\nmatplotlib: 1.5.3\nopenpyxl: 2.4.0\nxlrd: 1.0.0\nxlwt: 1.1.2\nxlsxwriter: 0.9.3\nlxml: 3.6.4\nbs4: 4.3.2\nhtml5lib: 0.999\n None\napiclient: None\nsqlalchemy: 1.0.13\npymysql: 0.7.6.None\npsycopg2: None\njinja2: 2.8\nboto: None\npandas_datareader: 0.2.1\n\n</details>\n",
    "labels": [
      "Testing",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "Thanks for the report, I think the test just needs written with an expectation of `np.intp` rather than `np.int64`.  We don't run CI on 32 bit Linux, so some of these kind of things sneak in.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 1,
    "additions": 6,
    "deletions": 6,
    "changed_files_list": [
      "pandas/tseries/tests/test_base.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14364,
    "reporter": "maxencedb",
    "created_at": "2016-10-06T07:58:45+00:00",
    "closed_at": "2016-10-24T22:20:45+00:00",
    "resolver": "jorisvandenbossche",
    "resolved_in": "fe2ebc15d696f02dc3137c0d0c318c7bca6abb7c",
    "resolver_commit_num": 616,
    "title": "Inconsistent behaviour on empty RangeIndex intersection",
    "body": "## Description of the issue\n\nTwo empty RangeIndex objects with same starts and stops will have a non-null intersection.\n## Code example\n\n\n### Expected output\n\n\n### Output\n\n\n## Output of `pd.show_versions()`\n\nNot the most recent version of Pandas, but I believe this behaviour is still present in the master branch\nafter browsing the code. If somebody could confirm.\n\n<details>\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.10.final.0\npython-bits: 64\nOS: Linux\nOS-release: None\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nsetuptools: 24.0.2\nnumpy: 1.10.1\ndateutil: 1.5\npytz: 2015.4\n</details>\n",
    "labels": [
      "Bug",
      "Indexing",
      "Regression"
    ],
    "comments": [
      "Actually, with pandas 0.19.0, it is even worse: first example gives an error, the second is still the wrong output:\n\n```\nIn [2]: a.intersection(c)\n...\nTypeError: RangeIndex(...) must be called with integers\n\nIn [3]: a.intersection(b)\nOut[3]: RangeIndex(start=0, stop=1, step=1)\n\nIn [4]: a.intersection(b).values\nOut[4]: array([0])\n```\n\nThanks for the report!\n"
    ],
    "events": [
      "renamed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 35,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.1.txt",
      "pandas/indexes/range.py",
      "pandas/tests/indexes/test_range.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14369,
    "reporter": "AlbertDeFusco",
    "created_at": "2016-10-06T19:55:48+00:00",
    "closed_at": "2016-10-15T19:59:53+00:00",
    "resolver": "brandonmburroughs",
    "resolved_in": "fd3be00bc46b416437e8cfafcf5661ec57385e2f",
    "resolver_commit_num": 0,
    "title": "concat with axis='rows'",
    "body": "#### A small, complete example of the issue\n\nIs there something special going on here that `pd.concat` seems to interpret `axis='rows'` as `axis=1`?\n\n\n#### Expected Output\n\nI would think that `axis='rows'` and `axis=0` do the same thing.\n#### Output of `pd.show_versions()`\n\n<details>\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.2.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.6.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\nLOCALE: en_US.UTF-8\n\npandas: 0.19.0\nnose: None\npip: 8.1.2\nsetuptools: 27.2.0\nCython: None\nnumpy: 1.11.2\nscipy: None\nstatsmodels: None\nxarray: None\nIPython: 5.1.0\nsphinx: None\npatsy: None\ndateutil: 2.5.3\npytz: 2016.7\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: None\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\npandas_datareader: None\n\n</details>\n",
    "labels": [
      "Bug",
      "Reshaping",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "It seems that the conversion from 'rows'/'columns' to 0/1 does not happen in `concat`, so any string will be evaluated as True in a boolean check ..\n",
      "@jreback  I use pandas pretty extensively in most of my projects. I'd like to make some contributions back. \ud83d\ude03  Mind if I take a stab at this?\n",
      "sure the more the merrier\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 67,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.1.txt",
      "pandas/tests/frame/test_combine_concat.py",
      "pandas/tools/merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14372,
    "reporter": "rahulporuri",
    "created_at": "2016-10-07T07:05:48+00:00",
    "closed_at": "2017-03-20T13:47:14+00:00",
    "resolver": "pankajp",
    "resolved_in": "b1e29dba26ff86b826fe0f866182466ae42c0bc5",
    "resolver_commit_num": 0,
    "title": "BUG : Check for and Use QApplication instance : clipboard",
    "body": "reading data from clipboard fails on Linux because pandas doesn't check if there's already a `QApplication` running - #L247\n\n\n#### Output of `pd.show_versions()`\n\n<details>\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.4.0-31-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_IN\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 23.1.0\nCython: 0.24\nnumpy: 1.10.4\nscipy: 0.17.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.1.2\nsphinx: 1.4.1\npatsy: 0.4.1\ndateutil: 2.5.2\npytz: 2016.3\nblosc: None\nbottleneck: None\ntables: 3.2.2\nnumexpr: 2.6.0\nmatplotlib: 1.5.1\nopenpyxl: 2.3.4\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: 3.6.0\nbs4: 4.4.1\nhtml5lib: 0.999\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\n</details>\n",
    "labels": [
      "Data IO"
    ],
    "comments": [],
    "events": [
      "cross-referenced",
      "labeled",
      "referenced",
      "cross-referenced",
      "milestoned"
    ],
    "changed_files": 2,
    "additions": 3,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/util/clipboard/clipboards.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14373,
    "reporter": "aixtools",
    "created_at": "2016-10-07T10:49:09+00:00",
    "closed_at": "2016-10-08T11:31:08+00:00",
    "resolver": "aixtools",
    "resolved_in": "794f79295484298525dbc8dd3b8ab251ad065e61",
    "resolver_commit_num": 0,
    "title": "problem with building pandas - not using gcc as compiler",
    "body": "\"\"\"the example is merely\npip install pandas - which fails, so download and run\n\npython ./setup.py build\n\n\n",
    "labels": [
      "Build"
    ],
    "comments": [
      "please let me know how to attach a text or zip file - if interested!\n",
      "pls show architecture, version of cython & python. as well as how you got the source.\n\nand version of gcc.\n",
      "AIX 5.3, (not gcc, but xlc), 64-bit\nroot@x064:[/]pip list\nCython (0.24.1)\nnumpy (1.11.2)\npip (8.1.1)\npython-dateutil (2.5.3)\npytz (2016.7)\nrequests (2.11.1)\nsetuptools (20.10.1)\nsix (1.10.0)\n\npip install pandas\n\npip download pandas\n\nroot@x065:[/data/prj/python/pipbuilds]ls *.gz\nnumpy-1.11.2.tar.gz     pandas-0.19.0.tar.gz\n",
      "I repeated the process, on AIX 6.1 - 32-bit mode, python built using gcc rather xlc.\n\nAIX 6.1, python-2.7.12, gcc-4.7.4\nroot@x065:[/data/prj/python/pipbuilds]pip list\nCython (0.24.1)\nnumpy (1.11.2)\npip (8.1.1)\npython-dateutil (2.5.3)\npytz (2016.7)\nsetuptools (20.10.1)\nsix (1.10.0)\n\nThis ends one(?) file earlier - I think the first .cpp (_packer.cpp), with different warnings:\n    gcc -pthread -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -D_LARGE_FILES -D__BIG_ENDIAN__=1 -Ipandas/src/msgpack -Ipandas/src/klib -Ipandas/src -I/opt/lib/python2.7/site-packages/numpy/core/include -I/opt/include/python2.7 -c pandas/msgpack/_packer.cpp -o build/temp.aix-6.1-2.7/pandas/msgpack/_packer.o -Wno-unused-function\n    cc1plus: warning: command line option '-Wstrict-prototypes' is valid for C/ObjC but not for C++ [enabled by default]\n    pandas/msgpack/_packer.cpp: In function 'Py_ssize_t __Pyx_PyIndex_AsSsize_t(PyObject*)':\n    pandas/msgpack/_packer.cpp:7517:9: error: call of overloaded 'abs(const Py_ssize_t&)' is ambiguous\n    pandas/msgpack/_packer.cpp:7517:9: note: candidates are:\n    In file included from /opt/include/python2.7/Python.h:42:0,\n                     from pandas/msgpack/_packer.cpp:4:\n    /opt/lib/gcc/powerpc-ibm-aix5.3.7.0/4.7.4/include-fixed/stdlib.h:309:14: note: int abs(int)\n    In file included from /opt/include/python2.7/pyport.h:325:0,\n                     from /opt/include/python2.7/Python.h:58,\n                     from pandas/msgpack/_packer.cpp:4:\n    /opt/lib/gcc/powerpc-ibm-aix5.3.7.0/4.7.4/include-fixed/math.h:947:14: note: float abs(float)\n    /opt/lib/gcc/powerpc-ibm-aix5.3.7.0/4.7.4/include-fixed/math.h:977:20: note: long double abs(long double)\n    error: command 'gcc' failed with exit status 1\n\nThe \"error\" being this line:\n   pandas/msgpack/_packer.cpp:7517:9: error: call of overloaded 'abs(const Py_ssize_t&)' is ambiguous\n\nFYI: when using xlc - _packer.o completed.\n\n45482770  116 -rw-r-----  1 root      system      117901 Oct  7 10:46 ./pandas-0.19.0/build/temp.aix-5.3-2.7/pandas/msgpack/_packer.o\nroot@x065:[/data/prj/python/pipbuilds]oslevel\n6.1.0.0\n\nNote the directory above is build/temp.aix5.3-2.7 not aix6.1-2.7\n",
      "we don't support aix directly, meaning that it's not tested at all\nso would accept compatible patches\n\nwe also make no guarantees about big endian (powerpc)\nagain would accept patches \n"
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "commented",
      "commented",
      "milestoned"
    ],
    "changed_files": 1,
    "additions": 1,
    "deletions": 1,
    "changed_files_list": [
      "pandas/src/msgpack/unpack_template.h"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14402,
    "reporter": "adamschultz",
    "created_at": "2016-10-12T20:39:34+00:00",
    "closed_at": "2016-10-13T10:14:19+00:00",
    "resolver": "jreback",
    "resolved_in": "a40e185bd1c9bef6ddc6221cd70a4ca13f4a4676",
    "resolver_commit_num": 4091,
    "title": "AmbiguousTimeError: Cannot infer dst time from Timestamp('2015-11-01 01:00:03'), try using the 'ambiguous' argument",
    "body": "In Pandas 0.19.0, there seems to be an error when setting the timezone for times close to the dst cutoff. \n\nThis issue appears very similar to #11619 and #11626, both of which have been closed. The traceback is below. \n\n\n\nHere is some relevant information on the data frame. \n\n\n\nI also had this same issue in Pandas 0.18.0 before upgrading to 0.19.0.\n",
    "labels": [
      "Timezones",
      "Bug"
    ],
    "comments": [
      "can you show a copy-pastable example. e.g. construct a frame exactly like what you need. Then apply operations to show an error.\n",
      "```\nIn [25]: Timestamp('2015-11-01 01:00:03').tz_localize('US/Central',ambiguous=False)\nOut[25]: Timestamp('2015-11-01 01:00:03-0600', tz='US/Central')\n\nIn [26]: Timestamp('2015-11-01 01:00:03').tz_localize('US/Central',ambiguous=True)\nOut[26]: Timestamp('2015-11-01 01:00:03-0500', tz='US/Central')\n\n```\n",
      "```\nIn [45]: pd.to_datetime(df.iloc[:,1] + ' ' + df.iloc[:, 2])\nOut[45]: \n0   2016-01-04 07:10:03\n1   2016-01-04 07:20:04\n2   2016-01-04 07:30:03\n3   2016-01-04 07:40:03\n4   2016-01-04 07:50:03\ndtype: datetime64[ns]\n\nIn [46]: pd.to_datetime(df.iloc[:,1] + ' ' + df.iloc[:, 2]).dt.tz_localize('US/Central')\nOut[46]: \n0   2016-01-04 07:10:03-06:00\n1   2016-01-04 07:20:04-06:00\n2   2016-01-04 07:30:03-06:00\n3   2016-01-04 07:40:03-06:00\n4   2016-01-04 07:50:03-06:00\ndtype: datetime64[ns, US/Central]\n\n```\n",
      "Thanks for the quick reply. Here's the traceback from a minimal, reproducible example with a one-row data frame. \n\n```\nimport pandas as pd\n\ndf2 = pd.DataFrame({'Timestamp Date': '11-01-2015', \n                   'Timestamp Time': '01:00:03'}, \n                    index=range(1))\ndf2['Timestamp'] = df2['Timestamp Date'] + \" \" + df2['Timestamp Time']\ndf2['Timestamp'] = pd.to_datetime(df2['Timestamp']).dt.tz_localize('US/Central')\n\n---------------------------------------------------------------------------\nAmbiguousTimeError                        Traceback (most recent call last)\n in ()\n      5 df2.head()\n      6 df2['Timestamp'] = df2['Timestamp Date'] + \" \" + df2['Timestamp Time']\n----> 7 df2['Timestamp'] = pd.to_datetime(df2['Timestamp']).dt.tz_localize('US/Central')\n\n/Users/adam/anaconda/lib/python2.7/site-packages/pandas/core/base.pyc in f(self, *args, **kwargs)\n    208 \n    209             def f(self, *args, **kwargs):\n--> 210                 return self._delegate_method(name, *args, **kwargs)\n    211 \n    212             f.__name__ = name\n\n/Users/adam/anaconda/lib/python2.7/site-packages/pandas/tseries/common.pyc in _delegate_method(self, name, *args, **kwargs)\n    130 \n    131         method = getattr(self.values, name)\n--> 132         result = method(*args, **kwargs)\n    133 \n    134         if not is_list_like(result):\n\n/Users/adam/anaconda/lib/python2.7/site-packages/pandas/util/decorators.pyc in wrapper(*args, **kwargs)\n     89                 else:\n     90                     kwargs[new_arg_name] = new_arg_value\n---> 91             return func(*args, **kwargs)\n     92         return wrapper\n     93     return _deprecate_kwarg\n\n/Users/adam/anaconda/lib/python2.7/site-packages/pandas/tseries/index.pyc in tz_localize(self, tz, ambiguous, errors)\n   1823             new_dates = tslib.tz_localize_to_utc(self.asi8, tz,\n   1824                                                  ambiguous=ambiguous,\n-> 1825                                                  errors=errors)\n   1826         new_dates = new_dates.view(_NS_DTYPE)\n   1827         return self._shallow_copy(new_dates, tz=tz)\n\npandas/tslib.pyx in pandas.tslib.tz_localize_to_utc (pandas/tslib.c:70327)()\n\nAmbiguousTimeError: Cannot infer dst time from Timestamp('2015-11-01 01:00:03'), try using the 'ambiguous' argument\n```\n",
      "@adamschultz ok fixed in #14405 ; this was a bug in interpreting a passed boolean. Note that you still MUST pass the ambiguous argument, as this fundamentally IS an ambiguous time.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "commented",
      "commented",
      "commented",
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 58,
    "deletions": 11,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.1.txt",
      "pandas/tseries/tests/test_timezones.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14408,
    "reporter": "joshowen",
    "created_at": "2016-10-12T22:53:20+00:00",
    "closed_at": "2016-10-15T20:02:41+00:00",
    "resolver": "jreback",
    "resolved_in": "7cad3f16bccd1d4702ef9d038b1ee0db33b9bb94",
    "resolver_commit_num": 4093,
    "title": "PERF: to_json very slow with lines=True",
    "body": "#### A small, complete example of the issue\n\n\n\nAs discussed in \n",
    "labels": [
      "Performance",
      "IO JSON",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "cc @aterrel \n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 43,
    "deletions": 21,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.1.txt",
      "pandas/io/json.py",
      "pandas/lib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14423,
    "reporter": "waqarmalik",
    "created_at": "2016-10-14T04:13:53+00:00",
    "closed_at": "2017-03-16T12:08:59+00:00",
    "resolver": "gwpdt",
    "resolved_in": "37e5f78b4e9ff03cbff4dea928445cc3b1f707c8",
    "resolver_commit_num": 0,
    "title": "BUG: unwanted numeric coercion after groupby-apply",
    "body": "xref #14873 (boolean casts)\r\nxref #14849 (datetime)\r\n\r\n#### A small, complete example of the issue\r\n\r\n\r\n#### Actual Output:\r\n\r\n\r\n#### Expected Output\r\n\r\npred1 and pred2 should have the same values in column p1. \r\npred1 is correct whereas pred2 is changing type to float64.\r\n#### Output of `pd.show_versions()`\r\n\r\n<details>\r\n## INSTALLED VERSIONS\r\n\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 14.5.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.0\r\nnose: None\r\npip: 8.1.2\r\nsetuptools: 27.2.0\r\nCython: None\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.2.3.1\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: 1.1.0\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Groupby",
      "Difficulty Novice",
      "Dtypes",
      "Effort Low"
    ],
    "comments": [
      "So I think we a duplicate of this already, need to search for it. In any event I think its doing a coercing conversion. This should strictly be a soft-conversion from object -> numeric. So the following works (though I _think_ the existing code should actually work correctly, maybe something is not getting passed thru).\n\n```\ndiff --git a/pandas/core/groupby.py b/pandas/core/groupby.py\nindex 3c376e3..a86e6d6 100644\n--- a/pandas/core/groupby.py\n+++ b/pandas/core/groupby.py\n@@ -10,6 +10,7 @@ from pandas.compat import(\n     zip, range, long, lzip,\n     callable, map\n )\n+import pandas as pd\n from pandas import compat\n from pandas.compat.numpy import function as nv\n from pandas.compat.numpy import _np_version_under1p8\n@@ -3446,7 +3447,7 @@ class NDFrameGroupBy(GroupBy):\n                 # as we are stacking can easily have object dtypes here\n                 so = self._selected_obj\n                 if (so.ndim == 2 and so.dtypes.isin(_DATELIKE_DTYPES).any()):\n-                    result = result._convert(numeric=True)\n+                    result = result.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n                     date_cols = self._selected_obj.select_dtypes(\n                         include=list(_DATELIKE_DTYPES)).columns\n                     date_cols = date_cols.intersection(result.columns)\n```\n\na pull-request with tests would be welcome.\n\nas an aside, what you are doing in side the `.apply` is completely inefficient and non-idiomatic.\n",
      "Tested and the suggested change works on a much larger data set too.\n\nAs an aside, I'd like to find better ways to do it -- groupby followed by extracting key parameters from each group. I couldn't devise a way to make aggregate work. Could you provide some suggestion on improving this? I've setup another page on stackoverflow for the discussion.\n\nhttp://stackoverflow.com/questions/40032039/pandas-groupby-apply-weird-behavior-with-series\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 54,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/groupby.py",
      "pandas/tests/groupby/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14435,
    "reporter": "MMCMA",
    "created_at": "2016-10-16T21:24:19+00:00",
    "closed_at": "2016-12-04T17:22:51+00:00",
    "resolver": "chris-b1",
    "resolved_in": "27fcd811f5b5df89eeede049cd048d94a65e7ff4",
    "resolver_commit_num": 60,
    "title": "HDF Store with multi-index problems when using data_columns=True",
    "body": "#### \n\nNot sure if this is supposed to be like this. When I store a multi-index along with the option `data_column=True`, I cannot query along the dataframe indexes only the columns are valid references. However, it works when I store them with the option `data_columns=[...]`. \n\n\n#### Output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.2.final.0\npython-bits: 64\nOS: Windows\nOS-release: 10\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 78 Stepping 3, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\nLOCALE: None.None\n\npandas: 0.19.0\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 23.0.0\nCython: 0.24\nnumpy: 1.11.2\nscipy: 0.17.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.2.0\nsphinx: 1.3.1\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.7\nblosc: None\nbottleneck: 1.1.0\ntables: 3.2.2\nnumexpr: 2.6.0\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 1.0.0\nxlwt: 1.1.2\nxlsxwriter: 0.9.2\nlxml: 3.6.0\nbs4: 4.4.1\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.13\npymysql: 0.7.9.None\npsycopg2: None\njinja2: 2.8\nboto: 2.40.0\npandas_datareader: 0.2.1\n",
    "labels": [
      "Bug",
      "IO HDF5",
      "MultiIndex"
    ],
    "comments": [
      "Thanks for the example, yes, I think this should work - PR welcome if you're interested.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 15,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.2.txt",
      "pandas/io/pytables.py",
      "pandas/io/tests/test_pytables.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14440,
    "reporter": "eoincondron",
    "created_at": "2016-10-17T13:42:57+00:00",
    "closed_at": "2017-03-05T16:26:59+00:00",
    "resolver": "mroeschke",
    "resolved_in": "5067708f0199a0b614586dbbc1a1536fa4442b65",
    "resolver_commit_num": 34,
    "title": "Floating point accuracy problems in `DatetimeIndex.round`",
    "body": "#### A small, complete example of the issue\n\nThere is a slight problem when using the rounding methods of `DatetimeIndex` (`round, floor, ceil`) to high frequencies as illustrated by this example:\n\n\n\nThe problem is here in the `TimelikeOps._round` method: \n\n\n\n`rounder(values / float(unit))` returns an array of `floats` containing the multiples of `unit` required. However, although the values look like `ints`, when multiplied by `unit` the result can be off due to floating point accuracy. Replacing it with \n\n\n\nShould fix the problem. I'm willing to do a PR to fix it. \n#### Output of `pd.show_versions()`\n\npandas: 0.19.0\nnose: None\npip: 8.1.2\nsetuptools: 27.2.0\nCython: None\nnumpy: 1.11.1\nscipy: None\nstatsmodels: None\nxarray: None\nIPython: 5.1.0\nsphinx: None\npatsy: None\ndateutil: 2.5.3\npytz: 2016.6.1\nblosc: None\nbottleneck: None\ntables: 3.2.3.1\nnumexpr: 2.6.1\nmatplotlib: 1.5.3\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.13\npymysql: 0.7.6.None\npsycopg2: None\njinja2: 2.8\nboto: None\n",
    "labels": [
      "Bug",
      "Timeseries"
    ],
    "comments": [
      "Thanks for the report - if you want to submit a PR with that fix that would be great.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 5,
    "additions": 24,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/tests/indexes/datetimes/test_ops.py",
      "pandas/tests/scalar/test_timestamp.py",
      "pandas/tseries/base.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14448,
    "reporter": "AnkurDedania",
    "created_at": "2016-10-18T22:19:12+00:00",
    "closed_at": "2016-10-24T22:28:17+00:00",
    "resolver": "keshavramaswamy",
    "resolved_in": "18fba53089fdfa3075cb9faa1f3ac57a2146be9b",
    "resolver_commit_num": 0,
    "title": "DOC/BUG: pd.to_datetime example produces different results from documentation",
    "body": "#### A small, complete example of the issue\n\nreferring too -docs/stable/generated/pandas.to_datetime.html\n\nThe example provides different results.\n\n\n\nraises...\n\n\n#### Expected Output\n\n(according to example in docs)\n\n\n#### Output of `pd.show_versions()`\n\n<details>\n\n> > > pd.show_versions()\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.2.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 63 Stepping 2, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\nLOCALE: None.None\n\npandas: 0.19.0\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 28.0.0\nCython: 0.24.1\nnumpy: 1.11.2\nscipy: 0.18.1\nstatsmodels: 0.6.1\nxarray: 0.8.2\nIPython: 5.1.0\nsphinx: 1.4.8\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.7\nblosc: 1.4.1\nbottleneck: 1.1.0\ntables: None\nnumexpr: 2.6.0\nmatplotlib: 1.5.3\nopenpyxl: 2.4.0\nxlrd: 1.0.0\nxlwt: 1.1.2\nxlsxwriter: 0.9.3\nlxml: 3.6.4\nbs4: 4.5.0\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.1.0\npymysql: 0.7.9.None\npsycopg2: None\njinja2: 2.8\nboto: None\npandas_datareader: None\n\n</details>\n",
    "labels": [
      "Timeseries",
      "Docs",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "I'm guessing this was a side effect of #10674.  I'm actually in favor of the new behavior - makes it much more obvious there is a problem, but I suppose this behavior might have been useful in some cases.\n",
      "the doc-string needs to be updated to use `ignore`\n\n```\nIn [2]: import pandas as pd\n   ...: pd.to_datetime('13000101', format='%Y%m%d',errors='ignore')\nOut[2]: datetime.datetime(1300, 1, 1, 0, 0)\n```\n",
      "@jreback - oh, I see, I had tried without the format, which I guess never returned a plain datetime, so agree it's just docs\n\n```\nIn [26]: pd.to_datetime('13000101', errors='ignore')\nOut[26]: '13000101'\n```\n",
      "It is a bit inconsistent that the return type changes on whether `format` is specified or not:\n\n```\nIn [1]: pd.to_datetime('13000101', errors='ignore')\nOut[1]: '13000101'\n\nIn [2]: pd.to_datetime('13000101', errors='ignore', format='%Y%m%d')\nOut[2]: datetime.datetime(1300, 1, 1, 0, 0)\n```\n"
    ],
    "events": [
      "renamed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "labeled",
      "unlabeled",
      "unlabeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "commented",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 1,
    "additions": 12,
    "deletions": 5,
    "changed_files_list": [
      "pandas/tseries/tools.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14457,
    "reporter": "pirsquared",
    "created_at": "2016-10-19T21:40:43+00:00",
    "closed_at": "2016-10-25T10:55:04+00:00",
    "resolver": "jreback",
    "resolved_in": "f99f050aaf29e4b7e9190488904c12bb719f8210",
    "resolver_commit_num": 4094,
    "title": "groupby transform producing different results depending on whether a lambda function was passed or a numpy function was passed",
    "body": "stackoverflow\n\n\n\n#### Output of `pd.show_versions()`\n\n<details>\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.2.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\nLOCALE: en_US.UTF-8\n\npandas: 0.19.0\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 27.2.0\nCython: 0.24.1\nnumpy: 1.11.1\nscipy: 0.18.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 5.1.0\nsphinx: 1.4.6\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.6.1\nblosc: None\nbottleneck: 1.1.0\ntables: 3.2.3.1\nnumexpr: 2.6.1\nmatplotlib: 1.5.3\nopenpyxl: 2.3.2\nxlrd: 1.0.0\nxlwt: 1.1.2\nxlsxwriter: 0.9.3\nlxml: 3.6.4\nbs4: 4.5.1\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.13\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.42.0\npandas_datareader: 0.2.1\n</details>\n",
    "labels": [
      "Bug",
      "Groupby"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 31,
    "deletions": 6,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.1.txt",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14477,
    "reporter": "larssono",
    "created_at": "2016-10-23T01:24:45+00:00",
    "closed_at": "2016-10-26T22:31:40+00:00",
    "resolver": "gfyoung",
    "resolved_in": "6130e77fb7c9d44fde5d98f9719bd67bb9ec2ade",
    "resolver_commit_num": 94,
    "title": "read_csv incompatible with newstr and future",
    "body": "When upgrading the pandas-0.19 I have several tests failing on a package I maintain.  These packages are using several imports from future to work with both py2 and py3.  It seems there is an issue with using `from __future__ import unicode_literals`\n#### A small, complete example of the issue\n\n\n\nThe first reading works the second does not and throws the stack trace attached. (\"TypeError: \"quotechar\" must be string, not unicode\")\nThe example file \n[simple.txt](-dev/pandas/files/546124/simple.txt)\n#### Expected Output\n#### Output of `pd.show_versions()`\n\n<details>\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.10.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.6.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\nLOCALE: None.None\n\npandas: 0.19.0\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 26.0.0\nCython: None\nnumpy: 1.11.2\nscipy: 0.16.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 5.1.0\nsphinx: 1.3.1\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.7\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: 2.4.6\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: 0.9.4\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: 4.4.1\nhtml5lib: 0.9999999\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.42.0\npandas_datareader: None\n\n\n\n</details>\n",
    "labels": [
      "IO CSV",
      "Regression"
    ],
    "comments": [
      "@larssono Thanks for the report! \n\ncc @gfyoung \n",
      "@jorisvandenbossche : Might it be best to just add a `unicode` class to `pandas.compat`?  I think that should patch this issue IINM i.e.\n\n``` python\ntry:\n    unicode\nexcept NameError:\n    unicode = str\n```\n",
      "FYI, for future reference, here's a slightly easier way to reproduce (<b>Note</b>: Python 2.x required):\n\n``` python\n>>> from pandas import read_csv\n>>> from pandas.compat import StringIO, u\n>>>\n>>> data = 'a\\n1'\n>>> read_csv(StringIO(data), quotechar=u('\"'))\n...\nTypeError: \"quotechar\" must be string, not unicode\n```\n",
      "@gfyoung unicode needs to be very explicit\n",
      "@jreback : Right...but what do you think of the patch I proposed <a href=\"https://github.com/pandas-dev/pandas/issues/14477#issuecomment-255693939\">above</a>, and we can then add the class to the allowed string types in parser.pyx?\n",
      "well it's not explicit \nso -1\n",
      "In pandas.compat:\n\n``` python\ntry:\n    unicode\nexcept NameError:\n    unicode = str\n...\n```\n\nIn parser.pyx:\n\n``` python\nif not isinstance(quote_char, (str, bytes, compat.unicode)) and quote_char is not None:\n...\n```\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 20,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.1.txt",
      "pandas/io/parsers.py",
      "pandas/io/tests/parser/quoting.py",
      "pandas/parser.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14483,
    "reporter": "darindillon",
    "created_at": "2016-10-24T14:06:12+00:00",
    "closed_at": "2016-11-23T13:06:09+00:00",
    "resolver": "jreback",
    "resolved_in": "8b404539b8b8f2ce2eaf38c7cd2f7f3925c6e171",
    "resolver_commit_num": 4377,
    "title": "Enhancement: add \"sum\" to pandas.describe()",
    "body": "Feature request: can we add \"sum\" to the pandas.describe() method? Not every dataset cares about sum, of course, but enough datasets do care about it that it seems worth adding to the default describe() method. (For instance, financial applications where you have multiple columns representing different scenarios, and you want to see how much profit would be earned of each scenario). \n",
    "labels": [
      "Enhancement",
      "Reshaping",
      "Numeric"
    ],
    "comments": [
      "xref https://github.com/pandas-dev/pandas/issues/7014\n\nI am a bit - on this ATM as `.aggregate/.agg` will be easily able to do this pretty generically.\n",
      "I also leaning to \"not needed to add it\" (you already have many statistics, adding one more makes the output more complex).\n\n(for reference, R's `summary` also does not include `sum`, not that we cannot deviate from that of course)\n",
      "Well yes, you can always call .describe() to get some of the statistics and also call .agg() to get other statistics and alsocall something else to get other statistics and then merge them all together; but of course that's more work. Obviously, .describe() can't contain everything, it should contains the \"commonly used\" ones; and what's \"commonly used\" for you may be different than for me. \n   But I would argue that (especially for financial applications) \"sum\" is a very common metric it's worth including. (If this were an exotic metric like \"kurtosis\" that most people don't understand, we wouldn't include it; but \"sum\" is useful to lots of people).\n\nIf I did all the work to add this (including test cases, etc) and submitted a PR, would you be willing to consider including it? \n",
      "It is difficult to assess, as I am also biased by the type of data I am working with, but I think `sum` is less generally applicable as the others. \nFor example, I work with time series data of concentrations. For such a case, all current statistics are useful (or at least mean something), while a sum makes no sense. And I have the feeling that this can be generalized, meaning that the current (distribution) statistics almost always make some sense, while for sum it much more depends on the type of data. \n",
      "I'm also -0 on this. I think focusing on `DataFrame.agg` would be more useful than modifying `describe`.\n",
      "@darindillon I am going to close this issue, as there does not seem much enthusiasm to add this. But have a look at #14668 as a possible way to more easily define a custom aggregation function."
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 14,
    "additions": 877,
    "deletions": 45,
    "changed_files_list": [
      "doc/source/api.rst",
      "doc/source/basics.rst",
      "doc/source/computation.rst",
      "doc/source/groupby.rst",
      "doc/source/timeseries.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/base.py",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/series.py",
      "pandas/tests/frame/test_apply.py",
      "pandas/tests/groupby/test_aggregate.py",
      "pandas/tests/groupby/test_value_counts.py",
      "pandas/tests/series/test_apply.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14488,
    "reporter": "ragesz",
    "created_at": "2016-10-25T10:02:35+00:00",
    "closed_at": "2017-05-18T10:30:53+00:00",
    "resolver": "jbschiratti",
    "resolved_in": "539de79692704c735a38975988ffe7293f6c2583",
    "resolver_commit_num": 0,
    "title": "Adding optional pickle protocol version argument to pandas.to_pickle()",
    "body": "Sometimes pickle files saved in Python v3.x are needed to read in Python v2.x. It would be nice if one can easily set pickle protocol version in `to_pickle()`.\n\nIt can be done with the following little changes:\n\nIn file `/pandas/io/pickle.py`:\n\n\n\nIn file `/pandas/core/generic.py`:\n\n\n",
    "labels": [
      "API Design",
      "Compat",
      "Data IO",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "sure, you could add this. \n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 72,
    "deletions": 6,
    "changed_files_list": [
      "doc/source/whatsnew/v0.21.0.txt",
      "pandas/core/generic.py",
      "pandas/io/pickle.py",
      "pandas/tests/io/test_pickle.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14489,
    "reporter": "jreback",
    "created_at": "2016-10-25T10:08:45+00:00",
    "closed_at": "2016-10-26T22:19:16+00:00",
    "resolver": "jreback",
    "resolved_in": "050bf60edd9e551eb6927f2c167b974d1f8eade5",
    "resolver_commit_num": 4096,
    "title": "CI: numpy dev started failing",
    "body": "-ci.org/pandas-dev/pandas/jobs/169382808\n\nIIRC some issues w.r.t. int *\\* int. unrelated to this PR.\n",
    "labels": [
      "Compat"
    ],
    "comments": [
      "cc @shoyer \n",
      "@jreback : The breaking PR is <a href=\"https://github.com/numpy/numpy/pull/8127\">numpy/numpy #8127</a>.\n",
      "I think we should just fix the test. Negative integers to negative powers was already broken in previous versions of NumPy.\n\nThe other choice would be to coerce all arguments to floats before taking powers.\n"
    ],
    "events": [
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files": 1,
    "additions": 18,
    "deletions": 6,
    "changed_files_list": [
      "pandas/tests/indexes/test_range.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14508,
    "reporter": "jreback",
    "created_at": "2016-10-26T22:36:15+00:00",
    "closed_at": "2016-12-12T11:48:25+00:00",
    "resolver": "yarikoptic",
    "resolved_in": "14e4815391dcd8c9fe91479fed629410bf63ca33",
    "resolver_commit_num": 25,
    "title": "BLD: require more recent cython",
    "body": "see comments [here](-dev/pandas/pull/14496#issuecomment-256497307)\n\nwe currently require >= 0.19.1. So probably should move that up.\n\nNot super pressing though.\n",
    "labels": [
      "Build"
    ],
    "comments": [
      "cc @wesm \n",
      "should we suppress this warning\n\n```\n(pandas) [Thu Oct 27 06:45:37 ~/pandas]$ make\npython setup.py build_ext --inplace\n/Users/jreback/miniconda3/envs/pandas/lib/python3.5/site-packages/Cython/Distutils/old_build_ext.py:29: UserWarning: Cython.Distutils.old_build_ext does not properly handle dependencies and is deprecated.\n  \"Cython.Distutils.old_build_ext does not properly handle dependencies \"\nrunning build_ext\ncythoning pandas/hashtable.pyx to pandas/hashtable.c\nbuilding 'pandas.hashtable' extension\ncreating build\ncreating build/temp.macosx-10.6-x86_64-3.5\ncreating build/temp.macosx-10.6-x86_64-3.5/pandas\n...\n```\n"
    ],
    "events": [
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 10,
    "additions": 24,
    "deletions": 8,
    "changed_files_list": [
      "ci/install_travis.sh",
      "ci/requirements-2.7.build",
      "ci/requirements-2.7_COMPAT.build",
      "ci/requirements-2.7_LOCALE.build",
      "doc/source/install.rst",
      "doc/source/whatsnew/v0.16.1.txt",
      "doc/source/whatsnew/v0.17.1.txt",
      "doc/source/whatsnew/v0.20.0.txt",
      "doc/sphinxext/numpydoc/LICENSE.txt",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14522,
    "reporter": "golobor",
    "created_at": "2016-10-27T15:45:30+00:00",
    "closed_at": "2016-12-30T21:43:21+00:00",
    "resolver": "nathalier",
    "resolved_in": "0252385a71f8b8738c3223dcc44af001baa79b10",
    "resolver_commit_num": 1,
    "title": "Categorical.searchsorted() uses lexical order instead of the provided categorical order",
    "body": "Hi,\nit seems that the searchsorted() method of Categorical series does not take into account the specific order of the categories.\n#### An slightly modified example from the API documentation\n\n\n#### Output\n\nUnsorted: [cheese, apple, bread, bread, milk]\nCategories (4, object): [cheese < milk < apple < bread]\nSorted: [cheese, milk, apple, bread, bread]\nCategories (4, object): [cheese < milk < apple < bread]\nSearchsorted apple: [0]\nSearchsorted milk: [5]\n\nAs you can see, \"apple\" is inserted at the beginning (b/c it starts with an 'a') and \"milk\" is inserted at the end, even though the order of both categories is between \"cheese\" and \"bread\". \nUnfortunately, the API documentation does not specify is this is the expected behavior, but it seems inconsistent with the fact that .sort_values() does use the categorical order. \n#### Output of `pd.show_versions()`\n\n<details>\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.2.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.4.0-34-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\nLOCALE: en_US.UTF-8\n\npandas: 0.19.0\nnose: None\npip: 8.1.2\nsetuptools: 25.1.6\nCython: 0.24.1\nnumpy: 1.11.1\nscipy: 0.18.0\nstatsmodels: 0.6.1\nxarray: None\nIPython: 5.0.0\nsphinx: None\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.6.1\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: 2.6.1\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: 1.0.0\nxlwt: 1.1.2\nxlsxwriter: None\nlxml: None\nbs4: 4.5.1\nhtml5lib: None\n 0.9.2\napiclient: 1.5.1\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\npandas_datareader: None\n</details>\n",
    "labels": [
      "Bug",
      "Indexing",
      "Categorical",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "yeah this may not be respecting `ordered`. a PR to fix would be welcome.\n",
      "this `searchsorted` seems having more problem than lexical order.\n\n```\nIn [35]: c1\nOut[35]:\n[apple, bread, bread, cheese, milk]\nCategories (4, object): [apple < bread < cheese < milk]\n\nIn [36]: c1.searchsorted([\"eggs\"])\nOut[36]: array([4])\n\nIn [37]: c1.searchsorted([\"milk\"])\nOut[37]: array([4])\n```\n\nis this result right?\n",
      "I'd like to work on it.\nWhat behavior is expected if some of values are not in categories?\nShould:\na) exception be raised\nb) -1 be returned for such values\nc) the leftmost or the rightmost indices returned without notifications? \n\nb) option looks to be the most convenient. What do you think? \n",
      "I would say that raising an Exception is the more logical thing to do, as you can never insert such a non-category in the categorical (this would also raise an error). \n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 53,
    "deletions": 46,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/categorical.py",
      "pandas/tests/test_categorical.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14554,
    "reporter": "jburroni",
    "created_at": "2016-11-01T23:51:17+00:00",
    "closed_at": "2016-12-19T23:46:27+00:00",
    "resolver": "clham",
    "resolved_in": "3ccb50131b698bccee21780c660a70bed87396d0",
    "resolver_commit_num": 6,
    "title": "CLN/BUG: remove bare excepts",
    "body": "Using KeyError may prevent the system to show system error, and thus make the error hard to find\r\n\r\n\r\n#### Expected Output\r\nMax recursion limit exception. Got key error\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: 1.3.7\r\npip: None\r\nsetuptools: None\r\nCython: None\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.4.1\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n\r\n</details>",
    "labels": [
      "Difficulty Novice",
      "Error Reporting",
      "Effort Low"
    ],
    "comments": [
      "what exactly is a `KeyError`?\n\nThis evaluates correctly to an infinite recursion.\n",
      "This is the KeyError (there was a little bug in the code snippet).\nI'm using python 2.7\n    /../Library/Python/2.7/lib/python/site-packages/pandas/core/indexing.pyc in _multi_take(self, tup)\n        838             return o.reindex(**d)\n        839         except:\n    --> 840             raise self._exception\n        841 \n        842     def _convert_for_reindex(self, key, axis=0):\n\n```\nKeyError: \n```\n",
      "and? \n",
      "That there is no key error. The exception should be \"max recursion limit\",\nbut it was hid by the indexer. This makes the situation very hard to debug\n\nOn Tuesday, November 1, 2016, Jeff Reback notifications@github.com wrote:\n\n> and?\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/pandas-dev/pandas/issues/14554#issuecomment-257748799,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AChv6gnd2sEqEckdZXhvrNYTDKsDKmYEks5q5-lJgaJpZM4Kmt1b\n> .\n\n## \n\n\" To be is to do \" ( Socrates )\n\" To be or not to be \" ( Shakespeare )\n\" To do is to be \" ( Sartre )\n\" Do be do be do \" ( Sinatra )\n",
      "I am still not clear what exactly is the issue. you have a recursion issue. This pandas call returns a valid value. What do you think this should do?\n",
      "Part of the issue is in here:\nhttps://github.com/pandas-dev/pandas/blob/master/pandas/core/indexing.py#L55\nwhere the default exception is set to KeyError\nThen, in\nhttps://github.com/pandas-dev/pandas/blob/master/pandas/core/indexing.py#L850\n_all_ exception are caught and converted to KeyError. In the example above, the previous line will rise a RuntimeError with 'maximum recursion depth exceeded' but it will be caught and transformed in KeyError, which is incorrect (and misleading).\nThere are 6 extra 'except:' in that file. (And those are not recommended)\n",
      "sure. bare excepts are not recommended\nsubmitting a pr is the best way to fix\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 2,
    "additions": 2,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.2.txt",
      "pandas/core/indexing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14561,
    "reporter": "jorisvandenbossche",
    "created_at": "2016-11-02T12:56:16+00:00",
    "closed_at": "2016-11-17T17:41:06+00:00",
    "resolver": "jreback",
    "resolved_in": "b52dda8fa4fa3818730f7e3493a6fe1108684918",
    "resolver_commit_num": 4105,
    "title": "Invalid datetime string conversion giving SystemError with Python 3.6.0b3",
    "body": "From #14552, using Python 3.6.0b3 and pandas 0.19.0:\r\n\r\n\r\n\r\ngives a SystemError (which gives problems in the indexing code etc), while with python < 3.6, you get a ValueError:\r\n\r\n\r\n",
    "labels": [
      "Compat"
    ],
    "comments": [
      "is it an abnormal change that we should report ? to numpy ? to python.org ? to ... ?\n",
      "Possibly, but that depends on the underlying reason. The error occurs in the `_string_to_dts` C function, but I am not familiar enough with that code to be able to directly pinpoint the reason. \n\nSo it someone want to take a deeper look at this, very welcome!\n",
      "well, as it was during python-3.6.0b3 testing, I reported to python-3.6 team. A false alarm is better than no alarm http://bugs.python.org/issue28636 \n",
      "> while with python < 3.6, you get a ValueError:\n\nPython 3.6 probably implements new checks on exceptions.\n\n> SystemError: <class 'str'> returned a result with an error set\n\nI suggest to modify pandas to check that you \"leak\" an exception when calling Python code. Try to add assert(!PyErr_Occurred()); near \"pandas/tslib.pyx in pandas.tslib.convert_str_to_tsobject (pandas/tslib.c:26851)()\".\n"
    ],
    "events": [
      "cross-referenced",
      "labeled",
      "cross-referenced",
      "commented",
      "commented",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 7,
    "deletions": 6,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.2.txt",
      "pandas/src/datetime.pxd",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14570,
    "reporter": "dhimmel",
    "created_at": "2016-11-02T20:26:55+00:00",
    "closed_at": "2016-12-13T23:11:38+00:00",
    "resolver": "dhimmel",
    "resolved_in": "37614485a9740df1c55e7f0da2d32216e2561af1",
    "resolver_commit_num": 2,
    "title": "Reading bz2, zip, and xz-compressed files from URL fails",
    "body": "Previously, we've discussed reading tables form gzip-compressed URLs (see -dev/pandas/issues/8685 and -dev/pandas/pull/10649). This is an essential feature as data storage increasingly migrates to the cloud.\r\n\r\nI now realize that the previous work only added support for gzipped URLs and not bz2, zip, or xz compression. However, [the documentation](-docs/version/0.19.0/generated/pandas.read_csv.html) doesn't reflect this. See [this notebook](-bug.ipynb) to see the errors that are generated.\r\n\r\nSo :+1: for allowing URL locations for all supported compression formats.\r\n",
    "labels": [
      "Testing",
      "Data IO",
      "Duplicate",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "https://github.com/pandas-dev/pandas/issues/12688 is the base issue which needs fixing (there is a PR open on it, which is almost there). Once that is fixed almost everything else is just testing.\n",
      "Thanks @jreback for the update. Excited for the pull request (#13340)!\n",
      "that PR is a bit stalled - if u want to fix up and submit would be great\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 7,
    "additions": 166,
    "deletions": 217,
    "changed_files_list": [
      "pandas/formats/format.py",
      "pandas/io/common.py",
      "pandas/io/json.py",
      "pandas/io/parsers.py",
      "pandas/io/s3.py",
      "pandas/io/tests/parser/compression.py",
      "pandas/io/tests/parser/test_network.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14580,
    "reporter": "navin-k80",
    "created_at": "2016-11-03T21:16:37+00:00",
    "closed_at": "2016-12-30T21:35:23+00:00",
    "resolver": "nathalier",
    "resolved_in": "7dd451d881964d958acbd078e8dba505906b01bf",
    "resolver_commit_num": 0,
    "title": "BUG: iloc misbehavior with pd.Series: sometimes returns pd.Categorical instead",
    "body": "#### A small, complete example of the issue\r\n\r\n\r\n\r\n#### Expected Output\r\nBoth should return a pandas Series object\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\npandas version = 0.18\r\n\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Indexing",
      "Difficulty Novice",
      "Categorical",
      "Effort Low"
    ],
    "comments": [
      "in 0.19.0\n\n```\nIn [18]: pd.Series([1,2,3]).astype('category').iloc[0:1]\nOut[18]: \n0    1\ndtype: category\nCategories (3, int64): [1, 2, 3]\n\nIn [19]: pd.Series([1,2,3]).astype('category').iloc[np.array([0, 1])]\nOut[19]: \n[1, 2]\nCategories (3, int64): [1, 2, 3]\n```\n\ncould be #12531 (in 0.18.1) or in 0.19.0\n",
      "actually, I stand corrected. This is a different treatment of slices vs list-like\nso this is a bug.\n\n```\nIn [16]: s = pd.Series([1,2,3]).astype('category')\n\nIn [17]: type(s.iloc[[0,1]])\nOut[17]: pandas.core.categorical.Categorical\n\nIn [18]: type(s.iloc[0:1])\nOut[18]: pandas.core.series.Series\n```\n"
    ],
    "events": [
      "renamed",
      "commented",
      "closed",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "reopened",
      "demilestoned",
      "milestoned",
      "labeled",
      "labeled",
      "labeled",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 54,
    "deletions": 17,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/indexing.py",
      "pandas/tests/test_categorical.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14617,
    "reporter": "kapilsh",
    "created_at": "2016-11-08T20:13:26+00:00",
    "closed_at": "2017-02-28T14:28:37+00:00",
    "resolver": "mroeschke",
    "resolved_in": "d0a281fd60a2099c932151280af88d5392ea9a84",
    "resolver_commit_num": 32,
    "title": "Columns and Index share the same numpy object underneath when pd.DataFrame.cov is used",
    "body": "#### A small, complete example of the issue\r\n\r\n\r\n#### Expected Output\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nIn [8]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.10.0-327.36.2.el7.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.utf8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.1\r\nnose: 1.3.7\r\npip: 9.0.0\r\nsetuptools: 27.2.0\r\nCython: 0.24.1\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.4.8\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: 1.1.0\r\ntables: 3.2.3.1\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.3\r\nopenpyxl: 2.3.2\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.3\r\nlxml: 3.6.4\r\nbs4: 4.5.1\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: 1.0.13\r\npymysql: None\r\npsycopg2: 2.6.2 (dt dec pq3 ext)\r\njinja2: 2.8\r\nboto: 2.42.0\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Indexing",
      "Reshaping",
      "Difficulty Novice",
      "Numeric",
      "Effort Low"
    ],
    "comments": [
      "In my use case, I am doing something like below:\n\n``` python\nIn [96]: df = pd.DataFrame({\"Value\": np.random.randn(1000), \"Kind\": map(chr, np.random.randint(65, 69, 1000))})\n\nIn [97]: df.pivot(values=\"Value\", columns=\"Kind\").ffill().diff().cov()\nOut[97]: \nKind             A             B             C             D\nKind                                                        \nA     6.094439e-01  1.864854e-06 -5.956038e-07 -1.130525e-08\nB     1.864854e-06  5.643768e-01  1.384354e-06  2.627663e-08\nC    -5.956038e-07  1.384354e-06  4.964671e-01 -1.802524e-08\nD    -1.130525e-08  2.627663e-08 -1.802524e-08  3.862837e-01\n\nIn [98]: cc = df.pivot(values=\"Value\", columns=\"Kind\").ffill().diff().cov()\n\nIn [99]: cc.index is cc.columns\nOut[99]: True\n```\n\nAs a result, \n\n```\ncc.unstack().reset_index()\n```\n\nfails.\n",
      "yeah it should shallow copy the index first rather than setting the same object so that meta data will not be shared\n\nwant to do a PR ?\n",
      "Sure! I can do a PR. Feel free to assign it to me. \n",
      "@jreback Made the changes to cov and corr. \n"
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 14,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/frame.py",
      "pandas/tests/frame/test_analytics.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14618,
    "reporter": "mverleg",
    "created_at": "2016-11-08T21:00:02+00:00",
    "closed_at": "2016-11-17T12:54:05+00:00",
    "resolver": "bashtage",
    "resolved_in": "fe555db3f178b57f1d15c6c30f7bea0ca452db68",
    "resolver_commit_num": 28,
    "title": "to_stata + read_stata results in NaNs (close to double precision limit)",
    "body": "#### Explanation\r\n\r\nSaving and loading data as stata results in a lot of NaNs.\r\n\r\nI think the code & output is pretty self-explanatory, otherwise please ask.\r\n\r\nI've not been able to test this on other systems yet.\r\n\r\nIf this is somehow expected behaviour, maybe a bigger warning would be in order.\r\n\r\n#### A small, complete example of the issue\r\n\r\n\r\n\r\n#### Expected Output\r\n\r\n\t     index          c000           c001           c002           c003  \\\r\n\t995    995  1.502566e+308  1.019238e+308 -1.169342e+308  6.845363e+307\r\n\t996    996 -3.418435e+307 -8.113486e+307  2.544741e+306  5.771775e+307\r\n\t997    997  1.507324e+308  4.610183e+307 -1.016633e+308 -1.632862e+308\r\n\t998    998 -8.138620e+307  6.312126e+307 -6.557370e+307  6.342690e+307\r\n\t999    999 -1.179032e+308  1.554709e+308 -1.175680e+308  1.921731e+307\r\n\t\r\n\t              c004           c005           c006           c007  \\\r\n\t995  1.611898e+308 -5.171776e+307 -8.918000e+307 -5.322720e+307\r\n\t996  3.693405e+307 -1.480267e+308  1.586053e+308  7.489689e+306\r\n\t997  1.060605e+308 -6.826590e+307  1.644990e+308 -1.379562e+308\r\n\t998  1.379642e+308  1.005632e+307 -1.206948e+308 -1.198931e+308\r\n\t999 -5.965607e+307  8.844623e+307  2.727894e+307 -5.433995e+307\r\n\t\r\n\t              c008           c009      ...                 c390  \\\r\n\t995 -6.580851e+306  1.284482e+308      ...       -1.770789e+308\r\n\t996 -9.312612e+307 -1.778315e+308      ...        7.410784e+307\r\n\t997 -9.415141e+307  9.058828e+307      ...       -5.451829e+305\r\n\t998  1.651712e+308  4.435415e+307      ...        5.220773e+307\r\n\t999 -1.747738e+308 -1.603248e+308      ...        1.415798e+307\r\n\t\r\n\t              c391           c392           c393           c394  \\\r\n\t995  7.360232e+307 -3.850417e+307  1.453624e+308  5.690363e+307\r\n\t996 -6.943490e+307  1.047268e+308  4.026712e+307  9.161669e+305\r\n\t997  4.406343e+306  1.617739e+308  4.218585e+307  1.573892e+307\r\n\t998 -2.390131e+307 -6.649416e+307  6.548489e+307  1.000078e+307\r\n\t999 -1.239203e+308 -5.038284e+307 -1.340608e+307 -1.193758e+308\r\n\t\r\n\t              c395           c396           c397           c398           c399\r\n\t995  8.371989e+307  3.491895e+307  7.344525e+307 -9.260950e+307  1.032120e+308\r\n\t996  9.200510e+307 -1.729595e+308  4.021503e+307  2.274318e+307  5.856302e+307\r\n\t997 -7.624901e+307 -1.206386e+308 -6.164537e+306 -7.634148e+307 -1.462809e+308\r\n\t998 -9.399560e+307  9.697224e+307 -6.963726e+307 -1.655656e+308  1.513218e+308\r\n\t999 -1.476121e+308  1.187603e+308  1.402195e+308 -1.584051e+308 -1.232190e+308\r\n\t\r\n\t[5 rows x 401 columns]\r\n\r\n#### Actual Output\r\n\r\n\t     index           c000           c001           c002           c003  \\\r\n\t995    995            NaN            NaN -1.169342e+308  6.845363e+307\r\n\t996    996 -3.418435e+307 -8.113486e+307  2.544741e+306  5.771775e+307\r\n\t997    997            NaN  4.610183e+307 -1.016633e+308 -1.632862e+308\r\n\t998    998 -8.138620e+307  6.312126e+307 -6.557370e+307  6.342690e+307\r\n\t999    999 -1.179032e+308            NaN -1.175680e+308  1.921731e+307\r\n\t\r\n\t              c004           c005           c006           c007  \\\r\n\t995            NaN -5.171776e+307 -8.918000e+307 -5.322720e+307\r\n\t996  3.693405e+307 -1.480267e+308            NaN  7.489689e+306\r\n\t997            NaN -6.826590e+307            NaN -1.379562e+308\r\n\t998            NaN  1.005632e+307 -1.206948e+308 -1.198931e+308\r\n\t999 -5.965607e+307  8.844623e+307  2.727894e+307 -5.433995e+307\r\n\t\r\n\t              c008      ...                 c390           c391  \\\r\n\t995 -6.580851e+306      ...       -1.770789e+308  7.360232e+307\r\n\t996 -9.312612e+307      ...        7.410784e+307 -6.943490e+307\r\n\t997 -9.415141e+307      ...       -5.451829e+305  4.406343e+306\r\n\t998            NaN      ...        5.220773e+307 -2.390131e+307\r\n\t999 -1.747738e+308      ...        1.415798e+307 -1.239203e+308\r\n\t\r\n\t              c392           c393           c394           c395  \\\r\n\t995 -3.850417e+307            NaN  5.690363e+307  8.371989e+307\r\n\t996            NaN  4.026712e+307  9.161669e+305            NaN\r\n\t997            NaN  4.218585e+307  1.573892e+307 -7.624901e+307\r\n\t998 -6.649416e+307  6.548489e+307  1.000078e+307 -9.399560e+307\r\n\t999 -5.038284e+307 -1.340608e+307 -1.193758e+308 -1.476121e+308\r\n\t\r\n\t              c396           c397           c398           c399\r\n\t995  3.491895e+307  7.344525e+307 -9.260950e+307            NaN\r\n\t996 -1.729595e+308  4.021503e+307  2.274318e+307  5.856302e+307\r\n\t997 -1.206386e+308 -6.164537e+306 -7.634148e+307 -1.462809e+308\r\n\t998            NaN -6.963726e+307 -1.655656e+308            NaN\r\n\t999            NaN            NaN -1.584051e+308 -1.232190e+308\r\n\t\r\n\t[5 rows x 401 columns]\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-45-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 26.1.0\r\nCython: None\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.6.1\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Numeric",
      "IO Stata",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "these look like out of bounds floats (or very close to the limit)\ncan you actually generate this with stata ?\ni am not sure how much precision it has anyhow \n",
      "Yeah it's close to the limit, but should be within an IEEE double precision float I think. I don't know if Stata deals with those somehow differently.\n**EDIT**: but yes, it's based on the magnitude of numbers. It's the biggest ones that become NaNs, and in the visible output it's exclusively positive ones.\n",
      "the floats might have overflowed and so when round tripped they have an undefined behavior\n\nwelcome to have a look \n\nthough using numbers close to the limit can easily cause issues - any particular reason you are trying to do this?\n",
      "If they're overflowing (which seems likely), happens in either to_stata or read_stata. It's not overflowed in the script yet; I compared other methods (npy, csv, etc) which don't give NaNs.\n\nI'm using it for a benchmark, so I'm using random data that uses the full range to test compression. I guess most people don't use such data and I don't urgently need it, so this is probably a low-priority bug. But it seems like a bug nonetheless.\n",
      "csv is not a valid comparison the floats get stringified\n\nsure could be either in to or from stata - i'll mark it but would take a community pull request to fix\n",
      "cc @bashtage \n",
      "Yeah CSV doesn't win the benckmark :-)\n\nThanks, I'll use smaller data for now, hope no one else runs into it!\n",
      "Stata has a maximum value for doubles and uses the very largest values to indicate coded values\n\nFrom the dta spec:\n\n```\n                minimum nonmissing    -1.798e+308 (-1.fffffffffffffX+3ff)\n                maximum nonmissing    +8.988e+307 (+1.fffffffffffffX+3fe)\n                code for .                        (+1.0000000000000X+3ff)\n                code for .a                       (+1.0010000000000X+3ff)\n                code for .b                       (+1.0020000000000X+3ff)\n                ...\n                code for .z                       (+1.01a0000000000X+3ff)\n```\n",
      " Would probably be best to warn/error when values like these are encountered for float and double. Right now integers are promoted to a larger type if possible to avoid this issue.\n",
      "I don't think there is any promise to correctly round trip to_state/read_stata, especially for edge cases.  The most important cases are to read data saved by Stata with `read_stata` and generate files Stata will correctly read in with `to_stata`.\n",
      "Also, for performance measurement, at its core to_stata uses `ndarray.tofile`.  You should just use this rather than going through `to_stata`.  `tofile` is very fast since it just dumps the memory contents of an ndarray to disk and sis usually limited by disk read/write speed.\n",
      "Ah I guess it's related to the encoding thing. It's probably good to use NaNs rather than just returning `.a` as a number. So the current behaviour is, in a sense, the desirable way.\n\nA warning would be useful though. If the performance penalty is worth it, which I'm not sure of.\n\nAlso thanks for the benchmark hint.\n",
      "@jreback I assume #14631 is causing these segfaults.  Otherwise I think I'm happy with it.\n",
      "I think this is ready unless you see something.\n",
      "Couldn't get cython to work to test it, but the source looks good!\n"
    ],
    "events": [
      "commented",
      "commented",
      "renamed",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "referenced",
      "commented",
      "referenced",
      "commented",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 66,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.2.txt",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/io/stata.py",
      "pandas/io/tests/test_stata.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14621,
    "reporter": "davidslac",
    "created_at": "2016-11-09T07:07:02+00:00",
    "closed_at": "2016-11-12T15:59:51+00:00",
    "resolver": "jreback",
    "resolved_in": "f8bd08e9c2fc6365980f41b846bbae4b40f08b83",
    "resolver_commit_num": 4100,
    "title": "dateutil 2.6 gives segfault in normalizing timestamp with datetutil timezone",
    "body": "Newly release dateutil 2.6.0 breaks some of the tests related to the use of dateutil timezones (travis is therefore currently failing)\r\n\r\n---\r\n\r\nOriginal report:\r\n\r\n#### A small, complete example of the issue\r\nI maintain central installs of miniconda environments that include pandas. My previous environment with pandas 0.19.0,  if I did this \r\n\r\nit worked. Now with pandas 0.19.1, it seg faults. Other packages may have been updated in the new environment. \r\n\r\nBelow are details - first the failure in my ana-1.0.5 environment, it is clearly segfaulting on a test maybe 2/3 the way through? Then the success in my ana-1.0.4 environment, then the pd.get_versions() in the working old environment, and finally in the newer environment where it fails:\r\n\r\n<details>\r\n\r\n</details>",
    "labels": [
      "Bug",
      "Compat",
      "Timezones"
    ],
    "comments": [
      "Can you run the test with verbose mode so you can see for which test it segfaults? `pd.test(verbose=10)`\n",
      "Some of our PR builds are segfaulting as well, e.g. https://travis-ci.org/pandas-dev/pandas/jobs/174412414\n\nNo failures on master yet though. Haven't had a chance to dig in yet.\n",
      "On that PR, it are all the builds using python 3.5 that fail with a segfault\n",
      "If I compare the installed versions before and after the moment tests started failing, it is `dateutil` that changed from 2.5.3 to 2.6.0\n",
      "https://pypi.python.org/pypi/python-dateutil/2.6.0\n\nshocker that this breaks something \nthey don't have a good history of back compat\n",
      "OK, small reproducible example:\n\n```\nimport pandas as pd\nimport datetime\n\ndt = datetime.datetime(2011, 1, 1, 9, 0)\noffset = pd.offsets.Day()\npd.Timestamp(dt, tz='dateutil/Asia/Tokyo')\npd.Timestamp(dt, tz='dateutil/Asia/Tokyo') + offset\noffset2 = pd.offsets.Day(normalize=True)\npd.Timestamp(dt, tz='dateutil/Asia/Tokyo') + offset2\n```\n\ngives\n\n```\n>>> import pandas as pd\n>>> import datetime\n>>> \n>>> dt = datetime.datetime(2011, 1, 1, 9, 0)\n>>> offset = pd.offsets.Day()\n>>> pd.Timestamp(dt, tz='dateutil/Asia/Tokyo')\nTimestamp('2011-01-01 09:00:00+0900', tz='dateutil//usr/share/zoneinfo/Asia/Tokyo')\n>>> pd.Timestamp(dt, tz='dateutil/Asia/Tokyo') + offset\nTimestamp('2011-01-02 09:00:00+0900', tz='dateutil//usr/share/zoneinfo/Asia/Tokyo')\n>>> offset2 = pd.offsets.Day(normalize=True)\n>>> pd.Timestamp(dt, tz='dateutil/Asia/Tokyo') + offset2\nSegmentation fault (core dumped)\n```\n",
      "So, it's related to using their timezones and normalizing offsets, so rather a specific use case that won't affect to much people I think. \nJeff, a lot of people can say the same about pandas :-)\n",
      "Trimmed down a bit further:\n\n```\nIn [1]: dt = pd.Timestamp('2016-01-01 09:00:00', tz='dateutil/Asia/Tokyo')\n\nIn [2]: dtpy = dt.to_pydatetime()\n\nIn [3]: dtpy\nOut[3]: datetime.datetime(2016, 1, 1, 9, 0, tzinfo=tzfile('/usr/share/zoneinfo/Asia/Tokyo'))\n\nIn [4]: dtpy.replace(hour=0)\nOut[4]: datetime.datetime(2016, 1, 1, 0, 0, tzinfo=tzfile('/usr/share/zoneinfo/Asia/Tokyo'))\n\nIn [5]: dt.replace(hour=0)\nSegmentation fault (core dumped)\n```\n",
      "> Jeff, a lot of people can say the same about pandas :-)\n\nwell maybe though we really really are thoughtful /. try hard /. give notice\n",
      "> well maybe though we really really are thoughtful /. try hard /. give notice\n\nIf you have problems with backwards compatibility, feel free to kick off builds against `dateutil`'s master branch and notify me of issues _before_ a release. It's not like we're changing interfaces willy-nilly and backwards compatibility is essentially an overriding goal of the project, but it's mostly just me working on the project and I can't cross-test against every downstream user or know the weird, undocumented behaviors that apparently people are relying on.\n",
      "@pganssle\n\nwe have been biten by downstream things before (by other deps)\njust trying minimize disruptions to upstream\n\nyes we could test against master but that makes our matrix even bigger\nin any event this is prob just a small easily correctable issue\n",
      "@pganssle all respect for your hard work on dateutil! That's why I pointed out to Jeff that people could say the same for pandas, because even when you care about backwards compatibility, it always happen that people are relying on your project in a way you did not expect or don't want to. And that for sure happens in case of pandas as well.\n\nApart from including dateutil master in our builds (as @jreback said, our test matrix is also already huge), what would also be helpful is getting notified of an upcoming release (not sure if you do a release candidate? or if there is a communication channel for such things? (where there is not too much other noise))\n",
      "@jorisvandenbossche I have set up a python-dateutil mailing list, but no one has joined it, so I do not usually announce releases there. Usually I just create a Release issue before at least a major release and tag in it everyone who has submitted an issue or PR that was included in the release and leave that open for a day or so (unless it's a critical bugfix).\n\nI think going forward I'll also announce it on the mailing list (you can join [here](https://mail.python.org/mailman/listinfo/dateutil)). At the moment, no one has ever sent a message to it, so I would consider it \"low noise\".\n",
      "Thanks for educating me on the verbose flag, I was wondering how I could tell you which test failed. Looks like (from later posts in the thread) the problem has been figured out, but when I add the flag I get:\n\n(ana-1.0.5) psanaphi106: ~/rel/slaclab_conda/anarel-manage/recipes/psana $ python -c \"import pandas as pd; pd.test('fast', verbose=10)\"\nRunning unit tests for pandas\npandas version 0.19.1\nnumpy version 1.11.2\npandas is installed in /reg/g/psdm/sw/conda/inst/miniconda2-dev-rhel7/envs/ana-1.0.5/lib/python2.7/site-packages/pandas\nPython version 2.7.12 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:42:40) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\nnose version 1.3.7\nnose.config: INFO: Ignoring files matching ['^\\.', '^_', '^setup\\.py$']\n/reg/g/psdm/sw/conda/inst/miniconda2-dev-rhel7/envs/ana-1.0.5/lib/python2.7/site-packages/nose/importer.py:94: FutureWarning: The pandas.rpy module is deprecated and will be removed in a future version. We refer to external packages like rpy2. \nSee here for a guide on how to port your code to rpy2: http://pandas.pydata.org/pandas-docs/stable/r_interface.html\n  mod = load_module(part_fqname, fh, filename, desc)\ntest_api (pandas.api.tests.test_api.TestApi) ... ok\ntest_deprecation_access_func (pandas.api.tests.test_api.TestDatetools) ... ok\n\n...\n\ntest_week_of_month_index_creation (pandas.tseries.tests.test_offsets.TestCaching) ... ok\ntest_add (pandas.tseries.tests.test_offsets.TestCommon) ... Segmentation fault (core dumped)\n\nso that is probably the datetime issue. If it is simple, can you tell me how to re-run a specific test? Maybe it is worth my downgrading the datetime package to get the more recent pandas.\n\nbest,\n\nDavid Schneider\nSLAC/LCLS\n\n---\n\nFrom: Joris Van den Bossche [notifications@github.com]\nSent: Wednesday, November 9, 2016 12:11 AM\nTo: pandas-dev/pandas\nCc: Schneider, David A.; Author\nSubject: Re: [pandas-dev/pandas] nose tests fail with 0.19.1, but succeed with 0.19.0 (#14621)\n\nCan you run the test with verbose mode so you can see for which test it segfaults? pd.test(verbose=10)\n\n\ufffd\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/pandas-dev/pandas/issues/14621#issuecomment-259355600, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AQAThZynbKT5VHNk93370U8itkoNcqe9ks5q8YAkgaJpZM4KtPTG.\n",
      "It was indeed the `pandas.tseries.tests.test_offsets.TestCommon.test_add` test that failed (or at least the first that fails, possibly others fail as well). \nSee http://stackoverflow.com/questions/3704473/how-do-i-run-a-single-test-with-nose-in-pylons for how to run a single test (that is not possible with the `pd.test()` function).\n\n> Maybe it is worth my downgrading the datetime package to get the more recent pandas.\n\nNote that it is `dateutil` (or python-dateutil depending on the source). Up to you to decide whether you want to downgrade or not, but it is in any case a rather specific application where this comes up (using dateutil timezones in not the default in pandas).\n",
      "Based on this:\n\n``` python\n>>> import pandas as pd\n>>> from datetime import datetime\n>>> dt = pd.Timestamp('2016-01-01 09:00:00')\n>>> datetime.replace(dt, hour=0)\nSegmentation fault (core dumped)\n```\n\nAnd the fact that the problem came about because the way dateutil calculates timestamps under the hood has changed to a function that uses `replace(tzinfo=None)` (as opposed to the old method, which just calculates the timestamp from, essentially, dt.timetuple()), I suspect the real issue is that `pandas.Timestamp.replace` is broken, so I wouldn't be surprised if it caused more issues later.\n\nAlso, I am not sure what `pytz` is planning to do, but in my experience their approach to time zones, while not changed by [PEP 495](https://www.python.org/dev/peps/pep-0495/) (because they make it a point to not support ambiguous `tzinfo` zones), is also not easily updated to support it without significant behavioral changes. `dateutil` now has a backwards-compatible PEP-495 interface, so I wouldn't be surprised if more people wanted to start using dateutil-provided zones in the future.\n",
      "so this is hitting this issue now: https://github.com/pandas-dev/pandas/issues/7825\n\nwe are using the datetime.datetime.replace (iirc) and should simply override and fix it\n"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "renamed",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned"
    ],
    "changed_files": 7,
    "additions": 188,
    "deletions": 27,
    "changed_files_list": [
      "ci/requirements-3.5_OSX.pip",
      "doc/source/whatsnew/v0.19.2.txt",
      "pandas/tseries/offsets.py",
      "pandas/tseries/tests/test_offsets.py",
      "pandas/tseries/tests/test_timezones.py",
      "pandas/tseries/tests/test_tslib.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14626,
    "reporter": "evectant",
    "created_at": "2016-11-09T21:21:39+00:00",
    "closed_at": "2016-11-12T16:08:53+00:00",
    "resolver": "MykolaGolubyev",
    "resolved_in": "3552dc0c4533a5eafafe859f5afd29a7ce063e03",
    "resolver_commit_num": 0,
    "title": "TST: indexes/test_base.py:TestIndex.test_format is flaky",
    "body": "`indexes/test_base.py:TestIndex.test_format` compares `index.format()` and `str(index[0])`, where `index = Index([datetime.now()])`. These won't match if the current timestamp ends with zeros:\r\n\r\n\r\n\r\nSame thing for real: -ci.org/pandas-dev/pandas/jobs/173440088#L1355",
    "labels": [
      "Difficulty Novice",
      "Effort Low",
      "Testing"
    ],
    "comments": [
      "yeah this does occasionally fail\n\nit's not a great test, but is testing the default behavior so no easy way to control the formatting\nmaybe put this in a while loop and get a new time if the current time ends with 000 \n\nput a short explanation there \n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 1,
    "additions": 13,
    "deletions": 1,
    "changed_files_list": [
      "pandas/tests/indexes/test_base.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14636,
    "reporter": "denfromufa",
    "created_at": "2016-11-10T17:25:03+00:00",
    "closed_at": "2017-07-24T23:49:21+00:00",
    "resolver": "toobaz",
    "resolved_in": "9e6bb42fa50df808bffd60a665bf921e49b87032",
    "resolver_commit_num": 31,
    "title": "API: set_axis does not have inplace=False",
    "body": "-docs/version/0.19.1/generated/pandas.DataFrame.set_axis.html",
    "labels": [
      "Difficulty Novice",
      "Effort Low",
      "Enhancement",
      "Indexing",
      "Reshaping"
    ],
    "comments": [
      "this would be a trivial impl, e.g.\n\n```\nobj = obj.copy()\nobj.set_axis(...)\nreturn obj\n```\n\nso if you'd like to do a PR (keeping `inplace=True` as the default) would be ok.\n",
      "I would not be upset by making this `inplace=None` and having a warning if not specified, so that we could make `inplace=False` the default (e.g. return a copy). We did a similar change in `.eval` IIRC in 0.18.0\n",
      "@jreback Ok, I started the change to set_axis, why is flake not running in CI?\n\nHere is eval change you mentioned that I'm going to use as a guide:\n\nhttps://github.com/pandas-dev/pandas/pull/11149/files\n",
      "https://travis-ci.org/pandas-dev/pandas/jobs/175772192\n\nhmm, lint IS running, but its not failing the build....hmmm\n\n@jorisvandenbossche \n",
      "I think it was removed by accident here: https://github.com/pandas-dev/pandas/pull/14516/files, but you did already put in back https://github.com/pandas-dev/pandas/commit/52241a7d8a8be2cc1ae40cbde054b0983ac1f429 in the meantime?\n",
      "yes lint should be good now \n"
    ],
    "events": [
      "renamed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "renamed",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 7,
    "additions": 201,
    "deletions": 11,
    "changed_files_list": [
      "doc/source/api.rst",
      "doc/source/whatsnew/v0.21.0.txt",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/reshape/pivot.py",
      "pandas/tests/frame/test_alter_axes.py",
      "pandas/tests/series/test_alter_axes.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14641,
    "reporter": "Ty-WDFW",
    "created_at": "2016-11-12T08:44:56+00:00",
    "closed_at": "2017-03-15T16:05:50+00:00",
    "resolver": "jzwinck",
    "resolved_in": "68212918a65accffb33e0db6d986ad8f080e67ed",
    "resolver_commit_num": 2,
    "title": "import pandas hanging Flask 0.11.1 / Apache 2.4.18",
    "body": "pandas 0.19.1 is hanging apache on the import in the python script, the website times out. Downgrading to 0.18.1 resolves the issue. Tested this on a fresh EC2 instance Ubuntu 16.04.\r\n\r\n \r\nApache log:\r\n- [Sat Nov 12 03:05:18.784672 2016] [core:warn] [pid 23426:tid 139710063925120] AH00045: child process 23563 still did not exit, sending a SIGTERM\r\n- [Sat Nov 12 03:05:20.786946 2016] [core:warn] [pid 23426:tid 139710063925120] AH00045: child process 23563 still did not exit, sending a SIGTERM\r\n- [Sat Nov 12 03:05:22.789238 2016] [core:warn] [pid 23426:tid 139710063925120] AH00045: child process 23563 still did not exit, sending a SIGTERM\r\n- [Sat Nov 12 03:05:24.791547 2016] [core:error] [pid 23426:tid 139710063925120] AH00046: child process 23563 still did not exit, sending a SIGKILL\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-45-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.1\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 28.8.0\r\nCython: None\r\nnumpy: 1.11.2\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n\r\n</details>\r\n",
    "labels": [
      "Build",
      "Usage Question"
    ],
    "comments": [
      "pls show a code example\n",
      "``` python\nfrom flask import Flask\nimport pandas as pd\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello():\n    return 'Hello World!'\n\nif __name__ == \"__main__\":\n    app.run()\n```\n\nWhen import pandas as pd is commented out the script will run fine and the website will load. Interesting enough when pandas imported in the python console it works flawlessly -- this is purely an issue with pandas 0.19.1 and apache. In the example here I'm using a environment, I've also tested this outside of a environment and still have the same issue.\n\nHere's my apache configuration file:\n\n``` html\n<VirtualHost *:80>\n        # The ServerName directive sets the request scheme, hostname and port that\n        # the server uses to identify itself. This is used when creating\n        # redirection URLs. In the context of virtual hosts, the ServerName\n        # specifies what hostname must appear in the request's Host: header to\n        # match this virtual host. For the default virtual host (this file) this\n        # value is not decisive as it is used as a last resort host regardless.\n        # However, you must set it for any further virtual host explicitly.\n        #ServerName www.example.com\n\n        ServerAdmin webmaster@localhost\n        DocumentRoot /var/www/html\n\n        WSGIDaemonProcess flaskapp user=flask group=www threads=5\n        WSGIScriptAlias / /var/www/html/flaskapp/flaskapp.wsgi\n\n        <Directory flaskapp>\n            WSGIProcessGroup flaskapp\n            WSGIApplicationGroup %{GLOBAL}\n            Order deny,allow\n            Allow from all\n        </Directory>\n```\n\nHere's the .wsgi file:\n\n``` python\nimport os\nimport sys\nimport site\n\n# Add virtualenv site packages\nsite.addsitedir(os.path.join(os.path.dirname(__file__), 'env/local/lib64/python2.7/site-packages'))\n\n# Path of execution\nsys.path.append('/var/www/html/flaskapp')\n\n# Fired up virtualenv before include application\nactivate_env = os.path.expanduser(os.path.join(os.path.dirname(__file__), 'env/bin/activate_this.py'))\nexecfile(activate_env, dict(__file__=activate_env))\n\nfrom main import app as application\n```\n",
      "you are doing odd path manipulation\nyou shouldn't do any of that in the python program ; activation needs to occur before the app starts\n\nyou are probably picking up different versions of pandas from different envs (and/or it's deps)\n\nso you need to fix that\n\nclosing as not a pandas issue\n",
      "Same issue here with pandas >= 0.19.0. However, in this case I don't have different versions of pandas from different envs (and/or it's deps), because I'm running this web server inside a docker container freshly built each time. \n\nCommenting out `import pandas` in the following solves the issue, as well as using pandas 0.18.1. Pandas 0.19.0 or 0.19.1 is making Apache hang.\n\n``` python\nfrom flask import Flask\nimport pandas\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello():\n    return 'Hello World!'\n\nif __name__ == \"__main__\":\n    app.run()\n```\n\nThe _.wsgi_ file is:\n\n``` python\nimport sys\nsys.path.insert(0, '/app')\nfrom myapplication import app as application\n```\n",
      "not really sure what you are actually running. But this works fine.\n\nIf you can show a reproducible example, pls comment.\n\n```\nPython 3.5.2 |Continuum Analytics, Inc.| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)]\nType \"copyright\", \"credits\" or \"license\" for more information.\n\nIPython 5.1.0 -- An enhanced Interactive Python.\n?         -> Introduction and overview of IPython's features.\n%quickref -> Quick reference.\nhelp      -> Python's own help system.\nobject?   -> Details about 'object', use 'object??' for extra details.\n\nIn [1]: from flask import Flask\n   ...: import pandas\n   ...:\n   ...: app = Flask(__name__)\n   ...:\n   ...: @app.route(\"/\")\n   ...: def hello():\n   ...:     return 'Hello World!'\n   ...:\n\nIn [2]: app.run()\n * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n127.0.0.1 - - [14/Nov/2016 16:30:11] \"GET / HTTP/1.1\" 200 -\n127.0.0.1 - - [14/Nov/2016 16:30:11] \"GET /favicon.ico HTTP/1.1\" 404 -\n\nIn [3]: pandas.__version__\nOut[3]: '0.19.0'\n\nIn [5]: import flask\n\nIn [6]: flask.__version__\nOut[6]: '0.11.1'\n```\n",
      "The above script has to be run by Apache, with mod_wsgi, exactly like it was first reported. I've included this piece\n\n``` python\nif __name__ == \"__main__\":\n    app.run()\n```\n\nto show that the script runs fine outside of Apache, but makes it hang otherwise.\n",
      "> However, in this case I don't have different versions of pandas from different envs (and/or it's deps)\n\nJust to clarify, I was using a brand new instance with only 0.19.1 freshly installed in my environment. There were no other versions of pandas or even environments installed system-wide. \n",
      "you can try running with `python -v` to see what is happening, otherwise no idea.\n",
      "It'll take someone with more technical knowledge than a salmon biologist to figure out how to make mod_wsgi run python in verbose. :disappointed:. Since I first posted this I've tested this on multiple clean instances, the only resolution is to downgrade to 18.1.",
      "I have also had this issue.\r\nIt can be solved by adding WSGIApplicationGroup %{GLOBAL}.\r\nunfortunately that may lead to more issues down the road if packages ever share names. \r\n\r\nI found the solution here http://stackoverflow.com/questions/25782912/pandas-and-numpy-thread-safety but have only ever had issues for the latest version of pandas if that helps the salmon people figure it out. ",
      "Hi all, in case it's useful.\r\n\r\nI had a similar issue: \r\nDjango==1.10.3 \r\npandas==0.19.1\r\nPython 3.4\r\nApache with mod_wsgi on AWS Elastic Beanstalk\r\n\r\nMy solution was to move the imports from the top of my views.py file and into the functions that needed them and all was well. `pandas` was already being used by this project in django management commands with the imports at the top of the module it was the addition into views.py that gave the problem.\r\n\r\nHowever, it may be worth noting that I think bokeh also had the same problem, and bokeh doesn't have a dependency on pandas any more. I will need to confirm this though if it's a useful avenue.",
      "@jreback, you closed this issue on Nov 12 with the reason \"closing as not a pandas issue\" - i don't know where else I'd file this bug and look for progress/insight on it - suggestions welcome.",
      "@birdsarah I would try doing your import of pandas IN the function you need it (rather than at the top of the module). If you have ``numexpr`` installed this would make a difference (and make sure you have latest versions of thing)",
      "I do not have numexpr installed but will try moving the imports when I get a chance. Any idea if the problem comes from numpy instead of pandas? ",
      "@Qblack no idea. it *sounds* like an initialization problem. ",
      "> I would try doing your import of pandas IN the function you need it\r\n\r\nThat's what I'm doing - and it works. But feels like a workaround as I've got commented pandas imports dotted all around my codebase - it's not ideal.\r\n\r\nWill have a look at `numexpr` - thanks",
      "I have encountered a similar hang, when doing `import pandas as pd` at the top level of a Python file which I import using `boost::python::exec()`.  I'm using Pandas 0.19.2 and Python 3.5.  It hangs when it imports indexing.py which does this:\r\n\r\n    _eps = np.finfo('f4').eps\r\n\r\nAnd indeed, if I just import numpy and do that myself instead of importing Pandas, it hangs as well, seemingly trying to manage the GIL.\r\n\r\n@Ty-WDFW if you have a chance, perhaps you can try replacing the above-mentioned line in indexing.py with this approximation:\r\n\r\n    _eps = 1.1920929e-07\r\n\r\nAnd see if that fixes the hang.  Or just try doing `np.finfo('f4').eps` in your own script before you import pandas and see if it hangs there.\r\n\r\nIn my case, the problem seems to be that `Py_Initialize()` was called in one thread during startup, then the actual Python code was executed in a different thread later.  The GIL ends up being held by the first thread and `np.finfo()` tries to acquire it in the second.  One solution to this is to call `PyEval_InitThreads(); PyEval_SaveThread();` after `Py_Initialize()`, then acquire the GIL explicitly before each call into the Python C API.",
      "@jzwinck Thanks for looking into that! \r\nDo I understand you correctly that this is then something that should be reported to numpy? (as it is triggered by just calling `np.finfo('f4').eps`)",
      "@jorisvandenbossche No, it is not a NumPy bug, though it is pretty strange/annoying that `np.finfo()` does anything with the GIL.\r\n\r\nThe only true bug that I believe exists here is in the outer application, which in my case and probably in all the cases here called `Py_Initialize()` in one thread, then ran Python code in another thread without the Python C API mandated calls to `PyEval_InitThreads()` and so on.  In other words, this is a classic deadlock caused by lock precondition violation.\r\n\r\nIf Pandas wants to make life easier on future folks who could get screwed up by this, it is probably possible to check if the GIL is held by the thread which imports pandas, by importing a small Cython module at the top of pandas.py with code as here: http://stackoverflow.com/questions/11366556/how-can-i-check-whether-a-thread-currently-holds-the-gil\r\n\r\nIt could really save some people a lot of time (as evidenced by this issue; it took me perhaps two hours to debug my own occurrence of the same).  And it isn't a lot of code...just needs a careful hand to get it right, and a willingness to add a bit more Cython, which I have previously been told is not desirable in Pandas generally.\r\n\r\nThen again, this diagnostic could equally be applied to Python's very own `import` mechanism (because importing anything while the GIL is not held is an error, and importing anything is already not high-performance).  Or to the various `PyRun_*()` functions, all of which run Python code so must never be called from C when the GIL is not held.",
      "@jreback and @jorisvandenbossche: I just noticed that the NumPy docs explicitly say that `finfo()` should not be cached at the module level (when developing NumPy itself).  I admit that `import pandas` is already far slower than `import numpy`, but if Pandas wishes to follow NumPy's edict, it might be a good idea to move Pandas' `finfo()` call into the one function which needs it, `is_index_slice()`.",
      "@jzwinck yes, I think that is certainly OK (want to do a PR?)\r\n\r\nWould that actually solve this issue at the same time? Or would it just postpone the hanging until an indexing operation is done?",
      "@jzwinck we could just hard code this. it is barely used."
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "closed",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "reopened",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "milestoned"
    ],
    "changed_files": 2,
    "additions": 2,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/indexing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14647,
    "reporter": "gfyoung",
    "created_at": "2016-11-13T07:41:56+00:00",
    "closed_at": "2016-11-23T20:40:40+00:00",
    "resolver": "gfyoung",
    "resolved_in": "e4413c425e1fafe0b92b3b04dc49efb57d21af0b",
    "resolver_commit_num": 98,
    "title": "BUG: to_dense is broken",
    "body": "As of <a href=\"-dev/pandas/tree/1d6dbb41b26a39121ec8c4f19f5da78bb0ab4af7/pandas\">1d6dbb</a>, the behaviour, documentation, and signatures for `to_dense` are widely inconsistent:\r\n\r\n<a href=\"-dev/pandas/blob/1d6dbb41b26a39121ec8c4f19f5da78bb0ab4af7/pandas/sparse/array.py#L382\">SparseArray</a>: documentation is incorrect (we are not converting a `SparseSeries`), and the `fill` parameter is not even respected if you trace the code.  Not sure why it was ever there in the first place.\r\n\r\n<a href=\"-dev/pandas/blob/1d6dbb41b26a39121ec8c4f19f5da78bb0ab4af7/pandas/sparse/series.py#L529\">SparseSeries</a>: What is the purpose of `sparse_only`?  Not only is it not documented, but it also is inconsistent with `SparseArray` behaviour.\r\n\r\n<a href=\"-dev/pandas/blob/1d6dbb41b26a39121ec8c4f19f5da78bb0ab4af7/pandas/sparse/frame.py#L231\">SparseDataFrame</a>: Perhaps the most intuitive of them all.  Documentation is correct, and it has no unnecessary parameters like the other two do.\r\n\r\nI propose that we change the `to_dense` behaviour to match that of `SparseDataFrame`.  Take no arguments and just convert to a dense object with no questions asked.",
    "labels": [
      "Sparse",
      "API Design",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "yes I agree these should all be consistent as `.to_dense()`\n",
      "It's nice to once organize related definitions, including `.values`, `.get_values()`, `internal_values()` and `external_values()`. maybe `internals.rst`?\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 5,
    "additions": 43,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/sparse/array.py",
      "pandas/sparse/series.py",
      "pandas/sparse/tests/test_array.py",
      "pandas/sparse/tests/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14652,
    "reporter": "luca-s",
    "created_at": "2016-11-14T12:21:48+00:00",
    "closed_at": "2016-11-17T12:49:43+00:00",
    "resolver": "luca-s",
    "resolved_in": "2fc0c68ace1cb447f1fa6f016295575a2024db3d",
    "resolver_commit_num": 0,
    "title": "BUG: pandas.cut and negative values",
    "body": "Here is an example of pandas.cut ran on a pandas.Series with only one positive element and then on a pandas.Series with only one negative element. In the second scenario pandas.cut is not able to insert the single value on the only one bin.\r\n\r\nI might be wrong but I expected pandas.cut to behave on negative values the same as with positive \r\nvalues. \r\n\r\n\r\n#### A small, complete example of the issue\r\n\r\n\r\n\r\n#### Expected Output\r\n\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-45-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: 1.3.1\r\npip: 1.5.4\r\nsetuptools: 28.0.0\r\nCython: 0.20.1post0\r\nnumpy: 1.11.1\r\nscipy: 0.18.0\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.4.8\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.6.1\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999\r\n 0.8\r\napiclient: None\r\nsqlalchemy: 1.0.14\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: 0.2.1\r\n\r\n\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "Effort Low",
      "Groupby",
      "Reshaping"
    ],
    "comments": [
      "hmm, that does look wrong.\n\nwelcome for you to have a look!\n",
      "Ok, I had a look and the problem is probably in [here](https://github.com/pandas-dev/pandas/blob/master/pandas/tools/tile.py#L101), the code should be:\n`mn -= .001 * abs(mn)`\nand also [here](https://github.com/pandas-dev/pandas/blob/master/pandas/tools/tile.py#L102), the code  should be:\n`mx += .001 * abs(mx)`\n\nShould I submit a PR? I am not familiar with Pandas development\n",
      "@luca-s perfect. if you'd like to do a PR would be great.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "renamed",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 17,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.2.txt",
      "pandas/tools/tests/test_tile.py",
      "pandas/tools/tile.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14659,
    "reporter": "jreback",
    "created_at": "2016-11-14T18:56:19+00:00",
    "closed_at": "2016-11-14T21:55:23+00:00",
    "resolver": "jreback",
    "resolved_in": "52241a7d8a8be2cc1ae40cbde054b0983ac1f429",
    "resolver_commit_num": 4102,
    "title": ".pyx linting NOT failing the builds......",
    "body": "-ci.org/pandas-dev/pandas/jobs/175345297",
    "labels": [
      "Style"
    ],
    "comments": [],
    "events": [
      "labeled",
      "milestoned",
      "referenced"
    ],
    "changed_files": 2,
    "additions": 13,
    "deletions": 5,
    "changed_files_list": [
      "ci/lint.sh",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14682,
    "reporter": "j-santander",
    "created_at": "2016-11-17T22:22:17+00:00",
    "closed_at": "2016-11-22T11:29:39+00:00",
    "resolver": "jsantander",
    "resolved_in": "9f2e45378cbce5532a8edf2484d62a802369634e",
    "resolver_commit_num": 0,
    "title": "AmbiguousTimeError on groupby when including a DST change",
    "body": "#### A small, complete example of the issue\r\n\r\n\r\n#### Expected Output\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n# Paste the output here pd.show_versions() here\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-47-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.1\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 28.6.1\r\nCython: 0.25.1\r\nnumpy: 1.11.2\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: None\r\nsphinx: 1.4.8\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: 2.6.1\r\nmatplotlib: None\r\nopenpyxl: 2.2.6\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: 3.5.0\r\nbs4: 4.4.1\r\nhtml5lib: 0.999\r\n 0.9.1\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n\r\nThe above code raises an `AmbiguousTimeError` exception, when grouping by a time-date series including a DST change. In the above example the unix timestamps are for the recent DST change in Europe.\r\n\r\nThe stack trace is:\r\n\r\n\r\nCode works if the series does not include a DST change (e.g. one day earlier):\r\n\r\n\r\n\r\ngets:\r\n\r\n",
    "labels": [
      "Bug",
      "Difficulty Advanced",
      "Effort Low",
      "Groupby",
      "Timezones"
    ],
    "comments": [
      "xref https://github.com/pandas-dev/pandas/issues/10668 (though this looks separate).\n\nyeah, prob need to specify `ambiguous` when creating the bins. a pull-request to fix would make the fix happen sooner.\n",
      "I've been trying to debug the above issue. \n\nTried adding the ambiguous keyword to the constructor of the Timestamps... but I wasn't sure how to set it (as `infer`) didn't seem to be a valid option.\n\nThe code raising the exception seems to have been modified with commit dcc68d7c5a06df85fe9fec568566bee1e9936b10 where the `_adjust_dates_anchored()` function at `pandas.tseries.resample` module first drops the tz information at the beginning of the function and then adds it back on the return statement.\n\nI've modified the code to not do that... but then I had to modify an assert at pandas.tseries.index.py that it is checking for equality of time zones... but it turns that Europe/Madrid on DST is considered different from Europe/Madrid not on DST.\n\nI'll try to create a pull request with my changes so that you can comment.\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 40,
    "deletions": 7,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.2.txt",
      "pandas/tseries/index.py",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14692,
    "reporter": "dragonator4",
    "created_at": "2016-11-18T21:16:24+00:00",
    "closed_at": "2016-11-18T22:32:54+00:00",
    "resolver": "discort",
    "resolved_in": "d02ef6f04466e4a74f67ad584cf38cdc6df56e42",
    "resolver_commit_num": 1,
    "title": "BUG: HDF5 Files cannot be read concurrently",
    "body": "#### A small, complete example of the issue\r\n\r\n\r\n\r\nThe above code either fails loudly with the following error:\r\n\r\n\r\n\r\nOr with the following error:\r\n\r\n\r\nBut in this case, `object 7` clearly exists in the table. Any help?\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-47-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.0\r\nnose: None\r\npip: 8.1.2\r\nsetuptools: 27.2.0\r\nCython: 0.25.1\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: 0.8.2\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: 1.1.0\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: 0.9.3\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n\r\n</details>\r\n",
    "labels": [
      "Duplicate",
      "IO HDF5"
    ],
    "comments": [
      "duplicate https://github.com/pandas-dev/pandas/issues/12236\n\npull-request to fix are welcome. This is not actually that hard to fix.\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned"
    ],
    "changed_files": 3,
    "additions": 19,
    "deletions": 5,
    "changed_files_list": [
      "doc/source/whatsnew/v0.21.0.txt",
      "pandas/core/resample.py",
      "pandas/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14699,
    "reporter": "yarikoptic",
    "created_at": "2016-11-20T03:48:06+00:00",
    "closed_at": "2016-12-12T11:48:25+00:00",
    "resolver": "yarikoptic",
    "resolved_in": "14e4815391dcd8c9fe91479fed629410bf63ca33",
    "resolver_commit_num": 25,
    "title": "test_quotechar_unicode on Debian jessie (stable) ",
    "body": "Seems to happen only with python3 (passes on python2)\r\n\r\n\r\nFWIW also happens on ubuntu 15.04 .  Passes on later releases.\r\nAdvice on where/how to dig would be appreciated\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-rc2+\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: C\r\nLANG: C\r\nLOCALE: None.None\r\n\r\npandas: 0.19.1\r\nnose: 1.3.4\r\npip: None\r\nsetuptools: 20.10.1\r\nCython: 0.21.1\r\nnumpy: 1.8.2\r\nscipy: 0.14.0\r\nstatsmodels: None\r\nxarray: None\r\nIPython: None\r\nsphinx: 1.2.3\r\npatsy: None\r\ndateutil: 2.2\r\npytz: 2012c\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.2.1\r\nnumexpr: 2.4.3\r\nmatplotlib: 1.4.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.3.2\r\nhtml5lib: 0.999\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.7.3\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "IO CSV",
      "Unicode"
    ],
    "comments": [
      "@yarikoptic : Here are your relevant files:\r\n\r\n1) <a href=\"https://github.com/pandas-dev/pandas/blob/c045e1d6774aaa32ce13def79901f1a1ad8792bf/pandas/parser.pyx#L569\">parser.pyx</a>: this file is where we set the quoting for the CParser\r\n2) <a href=\"https://github.com/pandas-dev/pandas/blob/6130e77fb7c9d44fde5d98f9719bd67bb9ec2ade/pandas/io/tests/parser/quoting.py#L142\">quoting.py</a>: this is file is where you are getting the failing test\r\n\r\nI suspect it is a compatibility issue in that the character (which is valid FYI) that we chose for the test cannot be converted to `char` for some reason or another.  The patch I think is just changing the `unicode` value chosen <a href=\"https://github.com/pandas-dev/pandas/blob/6130e77fb7c9d44fde5d98f9719bd67bb9ec2ade/pandas/io/tests/parser/quoting.py#L152\">here</a> so that it doesn't fail for your machine (a lower number most likely).",
      "@yarikoptic : Do you have any follow-up regarding this?  At this point, it's a little difficult for us to patch without confirmation since I can't reproduce myself.  @jreback ?",
      "I will check tomorrow... Already stepped away from the keyboard. I suspect my locale setting during running tests",
      "reproduced... and I think it is a cython issue and requires higher min version... on jessie stock cython is 0.21.1.  When I forced to have at least 0.23 (thus use pregenerated by cython 0.23.4 files) -- test seems to pass!  I will boost up cython version requirement on my end and see how it changes the situation overall ;)",
      "Awesome!  I think then we can close this issue so long as everything is working for you then.",
      "or we could \"fix\" it ;)  https://github.com/pandas-dev/pandas/pull/14831",
      "@yarikoptic : Ah, I see.  I didn't realize our minimum Cython version was `0.19.1` :smile:"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "milestoned",
      "referenced",
      "referenced"
    ],
    "changed_files": 10,
    "additions": 24,
    "deletions": 8,
    "changed_files_list": [
      "ci/install_travis.sh",
      "ci/requirements-2.7.build",
      "ci/requirements-2.7_COMPAT.build",
      "ci/requirements-2.7_LOCALE.build",
      "doc/source/install.rst",
      "doc/source/whatsnew/v0.16.1.txt",
      "doc/source/whatsnew/v0.17.1.txt",
      "doc/source/whatsnew/v0.20.0.txt",
      "doc/sphinxext/numpydoc/LICENSE.txt",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14721,
    "reporter": "dnspies",
    "created_at": "2016-11-23T16:03:30+00:00",
    "closed_at": "2016-12-20T13:46:00+00:00",
    "resolver": "gfyoung",
    "resolved_in": "b35c68996d4dfcd565dfbd7e27b53b392efe14cf",
    "resolver_commit_num": 114,
    "title": "Series.unique converts uint64 to int64 (with overflow)",
    "body": "\r\n\r\nSeries.unique should preserve whatever type the values are.  But instead it changes from uint64 to int64.\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\n...\r\npython: 3.4.3.final.0\r\npython-bits: 64\r\n...\r\n\r\n...\r\npandas: 0.17.1\r\nnumpy: 1.10.4\r\n...",
    "labels": [
      "Bug",
      "Dtypes",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "this is a sympton of #4471 in general ``uint64`` support is not great. PR's are welcome to fix that!",
      "@jreback : IIUC, your statement is incorrect.  The bug traces back as follows:\r\n\r\nWhen you call `Series.unique()`, it calls the super-class implementation <a href=\"https://github.com/pandas-dev/pandas/blob/4143b323f5f653c3f7168cef013f7b8ceeb5e27d/pandas/core/base.py#L966\">here</a>.  That method then calls `unique1d()` <a href=\"https://github.com/pandas-dev/pandas/blob/4143b323f5f653c3f7168cef013f7b8ceeb5e27d/pandas/core/nanops.py#L790\">here</a>.  The erroneous check is <a href=\"https://github.com/pandas-dev/pandas/blob/4143b323f5f653c3f7168cef013f7b8ceeb5e27d/pandas/core/nanops.py#L806\">here</a>, which catches `np.uint64` but then incorrectly converts them to `int64`.\r\n\r\nThe correct patch I think is to change the check to `np.signedinteger` and then have a separate one for `np.unsignedinteger`.  However, I cannot find a proper implementation of a `uint64` hashtable.  In addition, things seem to get insanely complicated as we move to `hashtable.pyx` (and related helper `.pxi.in` files) `khash.pxd`, and `khash.h`, which setup all of the hashtable functionality.\r\n\r\nThe easy way out but less desirable way I think is to modify this branch <a href=\"https://github.com/pandas-dev/pandas/blob/4143b323f5f653c3f7168cef013f7b8ceeb5e27d/pandas/core/nanops.py#L809\">here</a> and convert the resulting array back to the original dtype.\r\n\r\nThoughts?",
      "@gfyoung I was pointing to a 'generic' issue with uint64. it doesn't work in lots of places!",
      "@jreback : Ah, fair enough.  But thoughts about this?",
      ">However, I cannot find a proper implementation of a uint64 hashtable. In addition, things seem to get insanely complicated as we move to hashtable.pyx (and related helper .pxi.in files) khash.pxd, and khash.h, which setup all of the hashtable functionality.\r\n\r\nhmm\r\n",
      "@jreback : If someone knows how to navigate through that swamp of hashtable code, then by all means, patch it! :smile: But yes, it's just a matter of adding another hashtable class.",
      "you can use python hashing, no?\r\n\r\n```\r\nIn [9]: hash(np.uint64(2**63+1))\r\nOut[9]: 5\r\n\r\nIn [10]: hash(np.int64(2**63-1))\r\nOut[10]: 3\r\n```",
      "@jreback : Perhaps, but then what's going on with this entire hashtable class?  Why was it implemented that way if Python hashing is feasible?",
      "prob not that hard to write some klib type stuff in cython",
      "klib is >> faster than python for the types of things we do (and doesn't have as much overhead for pyobjects)",
      "or just add to the klib stuff (might be easier)",
      "@jreback : I think adding to the klib is the best solution, but I don't really understand how all of it works.",
      "me neither :<",
      "its just template stuff in a header only library",
      "you can prob copy int64 and just do a replace. (maybe)",
      "@jreback : True that file is, but I have no idea how it gets populated."
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files": 10,
    "additions": 59,
    "deletions": 7,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/algorithms.py",
      "pandas/hashtable.pxd",
      "pandas/src/algos_common_helper.pxi.in",
      "pandas/src/hashtable_class_helper.pxi.in",
      "pandas/src/hashtable_func_helper.pxi.in",
      "pandas/src/khash.pxd",
      "pandas/src/klib/khash.h",
      "pandas/tests/test_algos.py",
      "pandas/types/common.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14776,
    "reporter": "jreback",
    "created_at": "2016-11-30T21:56:38+00:00",
    "closed_at": "2016-12-04T17:34:59+00:00",
    "resolver": "jreback",
    "resolved_in": "f23010aa930e4301a6e70efce92ed1afc50dfaaa",
    "resolver_commit_num": 4122,
    "title": "BUG: group.apply with non-lexsorted levels and sort=True",
    "body": "",
    "labels": [
      "Bug",
      "Difficulty Intermediate",
      "Effort Low",
      "Groupby",
      "MultiIndex"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 31,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.2.txt",
      "pandas/core/groupby.py",
      "pandas/tests/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14778,
    "reporter": "erikcs",
    "created_at": "2016-11-30T23:31:15+00:00",
    "closed_at": "2016-12-13T23:30:35+00:00",
    "resolver": "erikcs",
    "resolved_in": "86233e15193c3bcd0f646915891ca6c7892335d9",
    "resolver_commit_num": 0,
    "title": "ENH/DOC: wide_to_long performance and docstring clarification",
    "body": "I had to massage some messy data recently, and a big bottleneck turned out to be [wide_to_long](-dev/pandas/blob/4814823903b862c411caf527271e384df0d0d7e7/pandas/core/reshape.py#L878)\r\n\r\n\r\nSome test data with many id variables and time variables\r\n\r\n\r\n\r\nReshaping with `wide_to_long` takes around 2 secs.\r\n\r\n\r\n\r\nI modified `wide_to_long` slightly (regex on categorical column / avoid copying many \"idvariables\", postpone type coercion) and the runtime is now \r\n\r\n\r\n\r\nThe result is the same\r\n\r\n\r\n\r\n#### Docstring clarification\r\n\r\nThe `wide_to_long` docstring also contains an unused last parameter `stubend : str`, which should be removed. \r\n\r\nA docstring `Note` addtion about escaping special characters (with for example `re.escape`) in `stubnames` could also perhaps be informative, since if the user passes a dataframe with messy stubnames, the function fails with a pretty uninformative error message. \r\n\r\nI can send a PR for this if that is wanted?\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 06f26b51e97a0e81e8bd7fca4bba18e57659d963\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.1.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.0+124.g06f26b5.dirty\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.24.1\r\nnumpy: 1.11.1\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.4.6\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.6.1\r\nblosc: None\r\nbottleneck: 1.1.0\r\ntables: 3.2.3.1\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.3\r\nopenpyxl: 2.3.2\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.3\r\nlxml: 3.6.4\r\nbs4: 4.5.1\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: 1.0.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.42.0\r\npandas_datareader: None\r\n\r\n</details>\r\n",
    "labels": [
      "Docs",
      "Performance",
      "Reshaping"
    ],
    "comments": [
      "@nuffe sure that would be great! (also you could add your example in the docs if that is useful).",
      "Thanks, sorry for the premature PR, it turned out I had some edges cases that broke the routine. I will get back to this and fix this later. Sorry. "
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 5,
    "additions": 402,
    "deletions": 39,
    "changed_files_list": [
      "asv_bench/benchmarks/reshape.py",
      "doc/source/api.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/reshape.py",
      "pandas/tests/test_reshape.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14784,
    "reporter": "brandonmburroughs",
    "created_at": "2016-12-01T23:28:41+00:00",
    "closed_at": "2017-04-19T22:29:29+00:00",
    "resolver": "linebp",
    "resolved_in": "dd5cef560b2fc30aaad04e74134b3f52b64425ce",
    "resolver_commit_num": 1,
    "title": "na_position doesn't work for sort_index() with MultiIndex",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n\r\n#### Problem description\r\n\r\nThe `na_position` argument isn't used in `DataFrame.sort_index()` or `Series.sort_index()` due to the way we sort the `MultiIndex`.  Whenever we create a `MultiIndex`, we store the labels as relative values.  For instance, if we have the following `MultiIndex`:  \r\n\r\nthe values get stored as \r\n\r\nwith a `NaN` placeholder of -1.\r\n\r\nThese label values are what get passed to the sorting algorithm for both [DataFrames](-dev/pandas/blob/master/pandas/core/frame.py#L3334-L3337) and [Series](-dev/pandas/blob/master/pandas/core/series.py#L1782).  Since the sorting only happens on the `labels`, it has no notion of the `NaN`.\r\n\r\nThis has been discussed in #14015 and #14672 .\r\n\r\nMy original naive solution was to change [these lines](-dev/pandas/blob/master/pandas/core/frame.py#L3337-L3338) from:\r\n\r\n\r\n\r\nto\r\n\r\n\r\n\r\nThis didn't break any tests, but it isn't necessarily the best approach.  \r\n\r\n#### Expected Output\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nIn [3]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.16.0-77-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.0\r\nnose: 1.3.4\r\npip: 9.0.0\r\nsetuptools: 27.2.0\r\nCython: 0.21\r\nnumpy: 1.11.2\r\nscipy: 0.16.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 4.0.0\r\nsphinx: 1.2.3\r\npatsy: 0.3.0\r\ndateutil: 2.5.3\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.1.1\r\nnumexpr: 2.3.1\r\nmatplotlib: 1.5.0\r\nopenpyxl: 1.8.5\r\nxlrd: 0.9.3\r\nxlwt: 0.7.5\r\nxlsxwriter: 0.5.7\r\nlxml: 3.4.0\r\nbs4: 4.3.2\r\nhtml5lib: None\r\n 0.9.2\r\napiclient: 1.5.5\r\nsqlalchemy: 0.9.7\r\npymysql: None\r\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\r\njinja2: 2.7.3\r\nboto: 2.32.1\r\npandas_datareader: None\r\n\r\n</details>\r\n",
    "labels": [
      "MultiIndex",
      "Bug",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "So basically sorting a multi-index always puts NaNs first, which can be quite annoying.\r\n\r\nIf someone wants to try to tackle this, it should be rather easy I think. `na_position` is already passed to `lexsort_indexer`, there is only a bug in its implementation where `mask = c.codes == -1` does not work because the `-1` values in the labels were turned into a a category '-1', not into a code value of '-1'.",
      "I'd like to give this a try!\r\n\r\nI have been looking at the code and when sorting the multi-index, the labels are passed to the `lexsort_indexer` function. This does not go well, because they are not the original values, but have been renamed, specifically the `NaN` values have been renamed to `-1`, so it no longer has special meaning in `Categories`.\r\n\r\nAfter a quick look I see several ways of fixing the  `mask = c.codes == -1`\r\n- Figure out which code has been assigned to the categorical value -1 and check for that instead. \r\n- Replace the -1 in the labels with `NaN` so that the code assigned will be -1 and the mask statement will work as intended\r\n- After creating the `Categories` object  from the labels, remove the -1 category if it exists, so that the mask statement will work as intended\r\n\r\nIs the check best left as is and should I fix data to make it work or should I fix the check to match the data? That bit where the `key` is checked if it is already `Categorical` worries me.\r\n\r\n",
      "you can do something like this right about [here](https://github.com/pandas-dev/pandas/blob/master/pandas/core/sorting.py#L177)\r\n\r\nThere are 2 cases when this is called. \r\n\r\n1) from MultiIndex, when you get the ``labels`` passed in, these are *already* factorized (IOW they are *almost* but not quite a categorical).\r\n\r\n2) from sorting (e.g. ``DataFrame.sort_values()`` where these are NOT factorized and must be turned into a proper categorical (this is already handled correctly).\r\n\r\nThis will correctly handle case 1), case 2) is handled by the existing code. \r\n```\r\ndiff --git a/pandas/core/sorting.py b/pandas/core/sorting.py\r\nindex 205d0d9..c62b4e2 100644\r\n--- a/pandas/core/sorting.py\r\n+++ b/pandas/core/sorting.py\r\n@@ -174,7 +174,8 @@ def lexsort_indexer(keys, orders=None, na_position='last'):\r\n \r\n         # create the Categorical\r\n         else:\r\n-            c = Categorical(key, ordered=True)\r\n+            cats = algorithms.unique(key)\r\n+            c = Categorical.from_codes(key, cats[cats != -1], ordered=True)\r\n \r\n         if na_position not in ['last', 'first']:\r\n             raise ValueError('invalid na_position: {!r}'.format(na_position))\r\n```\r\n\r\nSimply do the above in the MultiIndex code before passing to ``_lexsort_indexer``; if its a categorical it will just work. (don't actually do this in ``_lexsort_indexer`` as I did (that was just a test).",
      "Like so:\r\n```\r\n--- i/pandas/core/frame.py\r\n+++ w/pandas/core/frame.py\r\n@@ -3392,7 +3392,12 @@ it is assumed to be aliases for the column names.')\r\n             if not labels.is_lexsorted():\r\n                 labels = MultiIndex.from_tuples(labels.values)\r\n \r\n-            indexer = lexsort_indexer(labels.labels, orders=ascending,\r\n+            keys = []\r\n+            for label in labels.labels:\r\n+                cats = algorithms.unique(labels)\r\n+                keys.append(Categorical.from_codes(label, cats[cats != -1], ordered=True))\r\n+\r\n+            indexer = lexsort_indexer(keys, orders=ascending,\r\n                                       na_position=na_position)\r\n         else:\r\n             from pandas.core.sorting import nargsort\r\n```\r\nI figured the for loop was better than using either map or a list comprehension, since that is what was suggested and for loops are what is used elsewhere.\r\n\r\nI also did a few tests:\r\n```\r\n--- i/pandas/tests/frame/test_sorting.py\r\n+++ w/pandas/tests/frame/test_sorting.py\r\n@@ -58,6 +58,20 @@ class TestDataFrameSorting(tm.TestCase, TestData):\r\n         expected = df.sort_index()\r\n         assert_frame_equal(result, expected)\r\n \r\n+        # Setting up data for NaN sorting\r\n+        mi = MultiIndex.from_tuples([[1, 2], [np.nan, np.nan], [np.nan, 3], [12, 13]])\r\n+        frame = DataFrame(np.arange(16).reshape(4, 4), index=mi, columns=list('ABCD'))\r\n+\r\n+        # MI sort with NaN's first\r\n+        result = frame.sort_index(na_position='first')\r\n+        expected = frame.iloc[[1, 2, 0, 3], :]\r\n+        assert_frame_equal(result, expected)\r\n+\r\n+        # MI sort with NaN's last\r\n+        result = frame.sort_index(na_position='last')\r\n+        expected = frame.iloc[[0, 3, 2, 1], :]\r\n+        assert_frame_equal(result, expected)\r\n+\r\n     def test_sort(self):\r\n         frame = DataFrame(np.arange(16).reshape(4, 4), index=[1, 2, 3, 4],\r\n                           columns=['A', 'B', 'C', 'D'])\r\n```\r\n\r\n### When sorting on levels\r\nWhen sorting the index with the level option set, the na_position option is ignored. Is this working as intended or should the option be passed along?\r\n",
      "@linebp It is probably easier if you open a PR with the above changes (even if you are not sure of the approach, or if it is not yet finished, just indicate so in the PR), that will make discussing it easier.\r\n\r\n> When sorting the index with the level option set, the na_position option is ignored. Is this working as intended or should the option be passed along?\r\n\r\nI think ideally this should also work (so pass na_option along)"
    ],
    "events": [
      "cross-referenced",
      "renamed",
      "labeled",
      "labeled",
      "milestoned",
      "milestoned",
      "demilestoned",
      "labeled",
      "unlabeled",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 5,
    "additions": 79,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/frame.py",
      "pandas/core/indexes/multi.py",
      "pandas/core/series.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14792,
    "reporter": "rubennj",
    "created_at": "2016-12-03T17:19:21+00:00",
    "closed_at": "2016-12-30T19:27:40+00:00",
    "resolver": "gfyoung",
    "resolved_in": "a42a015263936caacfb626e68fa14f08421a55d9",
    "resolver_commit_num": 123,
    "title": "read_csv() doesn't parse correctly when `usecols` and `parse_dates` are both used",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n#### Problem description\r\n\r\nSince v0.18.1 `pd.read_csv()` doesn't parse correctly, and it occurs randomly at every run. It occurs only when `usecols` and `parse_dates` are both used.\r\n\r\n#### Expected Output\r\nAll the columns parsed as int64 and not some randomly as object.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 42 Stepping 7, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.19.1\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 29.0.1.post20161201\r\nCython: None\r\nnumpy: 1.11.2\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "Bug",
      "IO CSV",
      "Regression"
    ],
    "comments": [
      "@rubennj Thanks for the report! (I simplified the example a little bit)\r\n\r\ncc @gfyoung ",
      "@rubennj : Really weird bug.  We have tests for `parse_dates` and `usecols` <a href=\"https://github.com/pandas-dev/pandas/blob/2466ecbb717d8cdfd30cc20d5d22e5e095d9a14d/pandas/io/tests/parser/usecols.py#L179-L253\">here</a>.  However, we are evidently dealing with fewer columns than your example does.  Quick patch seems to be passing in `engine='python'`, though why that should make a difference bewilders me.",
      "Finally got some time to look at this, and your statement about it happening at random was the key.  We unfortunately have flaky behavior on the C engine side.  When we determine which columns to not convert because they're being used for `datetime` conversions and `usecols` is also passed in, the indexing in `parse_dates` is used with respect to `usecols`, except that how we do it is unstable.\r\n\r\nFirst, we initialize `self.usecols` to be a set, which you can see <a href=\"https://github.com/pandas-dev/pandas/blob/aba7d255a165bbc221be106839c59a876655c0c2/pandas/parser.pyx#L444\">here</a>.  When we proceed to index into `usecols` for `parse_dates`, we first convert to `list`, as seen <a href=\"https://github.com/pandas-dev/pandas/blob/aba7d255a165bbc221be106839c59a876655c0c2/pandas/io/parsers.py#L1557\">here</a>.  That is the flaky part, for if you run this command in the terminal:\r\n~~~\r\npython -c \"print(list(set(list('abcdefghij'))))\"\r\n~~~\r\nyou see you will get different results.\r\n\r\nThe reason why the Python engine does not see this issue is because it prunes columns early on and iterates over the column names, which is a `list`.  Hence, it is robust against this flaky `set` behavior."
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "cross-referenced",
      "milestoned",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 74,
    "deletions": 12,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/io/parsers.py",
      "pandas/io/tests/parser/usecols.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14800,
    "reporter": "jreback",
    "created_at": "2016-12-05T00:36:52+00:00",
    "closed_at": "2017-04-03T21:52:14+00:00",
    "resolver": "jreback",
    "resolved_in": "da0523a346abd9575ab05746e242ec67c1c442d4",
    "resolver_commit_num": 4353,
    "title": "DEPR: relocate exceptions in pandas.core.common",
    "body": "move these to ``pandas.api.exceptions``\r\n\r\nusing ``pandas.util.depr_module`` this should be straightforward",
    "labels": [
      "Deprecate"
    ],
    "comments": [
      "I can do this, just looking at the `_DeprecateModule` class and its use.",
      "I don't think using the existing `pandas.util.depr_module._DeprecatedModule`\r\nclass will work in this scenario.\r\n\r\n`_DeprecatedModule` is currently used to deprecate a complete module, `pandas.core.dateutils`, \r\nand in this respect it performs as expected.\r\n\r\nThis scenario involves selectively deprecating individual exceptions in one module\r\n`pandas.core.common` and redirecting calls of exceptions in `pandas.api.exceptions`\r\nto `pandas.core.common`.\r\n\r\nSo when I execute the following:\r\n\r\n```python\r\nfrom pandas.core.common import PandasError\r\n```\r\n\r\nI should get: \r\n\r\n```\r\nFutureWarning: pandas.core.common.PandasError is deprecated. Please use \r\n`pandas.api.exceptions.PandasError` instead. \r\n```\r\n\r\nIf I import `pandas.api.exceptions.PandasError` then I should *not* see any\r\nFutureWarning message. \r\n\r\nThis is how I'm using `_DeprecatedModule`, it could of course be incorrect: \r\n\r\n```python\r\n# module: pandas.api.exceptions\r\nfrom pandas.util.depr_module import _DeprecatedModule\r\n\r\nexceptions = _DeprecatedModule('pandas.core.common')\r\n\r\nPandasError = exceptions.PandasError\r\n\r\n```\r\n\r\nWhen I import `pandas.api.exceptions.PandasError` then I get the following:\r\n\r\n```\r\nFutureWarning: pandas.core.common.PandasError is deprecated. Please use pandas.core.common.PandasError instead.\r\n  PandasError = exceptions.PandasError\r\n```\r\n\r\nSeveral problems with the above:\r\n\r\n1. The warning message should not appear when I import `pandas.api.exceptions.PandasError` only when I import `pandas.core.common.PandasError`.\r\n2. The warning message should say `Please use pandas.api.exceptions.PandasError instead.` not `Please use pandas.core.common.PandasError instead.`\r\n\r\nThe only way I could I could see fixing (2) is by passing in the name of the module where the \r\nexceptions are being moved, in this case that would be `pandas.api.exceptions`.\r\n\r\nI'm trying to think of solution which satisfies the following criteria:\r\n\r\n1. Not break existing code which uses exceptions from `pandas.common.core`\r\n2. Issues a warning message when an attempt is made to import an exception from `pandas.common.core`\r\n3. Does not issue a warning message when importing an exception from `pandas.api.exceptions`\r\n4. Avoids any code duplication between `pandas.common.core` and `pandas.api.exceptions`",
      "We had quite a long discussion in https://github.com/pandas-dev/pandas/pull/14479 about deprecating an Exception (and the conclusion was more or less that it is not possible), but not sure if it is fully similar situation, as there it was about renaming a warning, where here it is about moving it.",
      "@m-charlton you might be able to easily do this by:\r\n\r\na) moving the exception definition to ``pandas.api.exceptions``\r\nb) in ``pandas.core.common`` put a similar wrapper to what we do for the function deprecations",
      "Had a look at the way functions are deprecated in `pandas.core.common` and this\r\napproach will not work for classes, I suspect that the function already exists\r\nat import time and there is something to decorate. \r\n\r\nThe function deprecation warnings would appear every time an attempt to invoke \r\none of these functions was made, as expected.\r\n\r\nWhen deprecating an exception you'd want the deprecation warning to appear on\r\neach import, not each time the exception was raised. \r\n\r\nIf warnings were added at import time: \r\n\r\n```python\r\n# module: pandas.core.common \r\n\r\nclass PandasError(Exception):\r\n  warnings.warn('...', DeprecationWarning)\r\n  ...\r\n```  \r\n\r\nThen you'd see a warning anytime anything from the enclosing module were \r\nimported, which is not ideal.\r\n\r\nUsing the imported referenced exception in `pandas.api.exceptions`\r\n\r\n```python\r\n#module: pandas.api.exceptions\r\nimport pandas.core.common\r\n...\r\nPandasError = pandas.core.common.PandasError\r\n``` \r\n\r\nWill still result in a warning message being issued, if of course \r\nDeprecationWarning is enabled.\r\n\r\nI don't know if there is some decorator/metaclass *magic* which could be used.  \r\n\r\nShort of duplicating code in `pandas.api.exceptions`, which I think would create\r\nmore problems than it solves, I don't see an elegant solution to this problem.\r\n\r\n\r\n",
      "@m-charlton maybe was not clear here.\r\n\r\n I want to completely move all of the exceptions to ``pandas.api.exceptions``, THEN,\r\n\r\nwe want to have a warning message printed when someone does\r\n``from pandas.core.common import ParserError`` (or whatever).\r\n\r\nThis is the purpose of the ``_DeprecateModule``; it essentially puts a proxy behind the module to intercept getattr requests in the name space, which is exactly what we want. (the method I used to deprecate the functions like ``is_datetime64_dtype`` and such in ``pandas.core.common`` is 'easier' in that we can return a proxy function to provide the warning.\r\n\r\nTHEN\r\n\r\ninternaly in pandas we will always import from ``pandas.api.exceptions``. so no duplication of code and no warnings from our internal code, ONLY if someone externally imports it.\r\n\r\nThis is almost exactly what we did with ``pandas.core.datetools`` (which in addition to functions & names *inside* the module, we also deprecated the actual module itself).",
      "My bad, that makes more sense. Will start work now."
    ],
    "events": [
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 47,
    "additions": 420,
    "deletions": 236,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/__init__.py",
      "pandas/_libs/src/inference.pyx",
      "pandas/api/lib/__init__.py",
      "pandas/compat/numpy/function.py",
      "pandas/computation/align.py",
      "pandas/core/common.py",
      "pandas/core/frame.py",
      "pandas/core/indexing.py",
      "pandas/core/ops.py",
      "pandas/core/panel.py",
      "pandas/errors/__init__.py",
      "pandas/indexes/multi.py",
      "pandas/io/common.py",
      "pandas/io/excel.py",
      "pandas/io/html.py",
      "pandas/io/packers.py",
      "pandas/io/parsers.py",
      "pandas/io/parsers.pyx",
      "pandas/io/pytables.py",
      "pandas/lib.py",
      "pandas/tests/api/test_api.py",
      "pandas/tests/api/test_lib.py",
      "pandas/tests/api/test_types.py",
      "pandas/tests/computation/test_eval.py",
      "pandas/tests/frame/test_axis_select_reindex.py",
      "pandas/tests/frame/test_constructors.py",
      "pandas/tests/frame/test_to_csv.py",
      "pandas/tests/groupby/test_groupby.py",
      "pandas/tests/indexes/datetimes/test_ops.py",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tests/indexing/test_ix.py",
      "pandas/tests/indexing/test_multiindex.py",
      "pandas/tests/io/parser/common.py",
      "pandas/tests/io/parser/dialect.py",
      "pandas/tests/io/parser/dtypes.py",
      "pandas/tests/io/parser/skiprows.py",
      "pandas/tests/io/parser/test_unsupported.py",
      "pandas/tests/io/test_common.py",
      "pandas/tests/io/test_packers.py",
      "pandas/tests/test_errors.py",
      "pandas/tests/test_window.py",
      "pandas/tests/tseries/test_resample.py",
      "pandas/tseries/index.py",
      "pandas/tslib.py",
      "pandas/util/depr_module.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14813,
    "reporter": "jreback",
    "created_at": "2016-12-06T23:48:39+00:00",
    "closed_at": "2016-12-17T15:43:50+00:00",
    "resolver": "aileronajay",
    "resolved_in": "73bc6cfb35a886b18955b1f0a07f7647f1f1d409",
    "resolver_commit_num": 2,
    "title": "TST: reorg groupby tests",
    "body": "see suggestions here: -dev/pandas/pull/12607#issuecomment-265310458\r\n\r\nwe have lots and lots of groupby tests and so getting pretty unweidly\r\n\r\nso should reorg to a sub-dir setup like we have for other things\r\n\r\neg.\r\n\r\npandas/tests/groupby/.....",
    "labels": [
      "Groupby",
      "Testing"
    ],
    "comments": [
      "@jreback so this change would involve relocating pandas/tests/test_groupby.py to the hierarchy pandas/tests/groupby/ and then breaking this file into other files (having tests of similar type)?",
      "yes",
      "I have created this PR for the change, https://github.com/pandas-dev/pandas/pull/14817/commits"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 5,
    "additions": 1597,
    "deletions": 1275,
    "changed_files_list": [
      "pandas/tests/groupby/test_aggregate.py",
      "pandas/tests/groupby/test_categorical.py",
      "pandas/tests/groupby/test_filters.py",
      "pandas/tests/groupby/test_groupby.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14821,
    "reporter": "keshavramaswamy",
    "created_at": "2016-12-08T01:40:01+00:00",
    "closed_at": "2016-12-15T15:40:38+00:00",
    "resolver": "keshavramaswamy",
    "resolved_in": "033d34596f5327472007f0f15029b86050ee2592",
    "resolver_commit_num": 2,
    "title": "KDE Plot does not drop missing values",
    "body": "\r\nThe KDE Plot of a series with missing values fails, producing an empty plot whereas the histogram is able to drop the missing values.\r\n#### Expected Output\r\n* The KDE Plot is generated when the missing values are removed manually or by using `dropna()`\r\n* According to the [Pandas doc on plotting with missing data](-docs/version/0.18.1/visualization.html#plotting-with-missing-data), the default way NaNs are handled is by dropping them. But this does not happen - an empty plot is generated\r\n* I have created a PR to fix this: [#14820](-dev/pandas/pull/14820)\r\n\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n\r\n------------------\r\n\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-92-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.1\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 23.0.0\r\nCython: 0.24\r\nnumpy: 1.11.1\r\nscipy: 0.17.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 4.2.0\r\nsphinx: 1.4.1\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: 1.1.0\r\ntables: 3.2.2\r\nnumexpr: 2.6.0\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.3.2\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.2\r\nlxml: 3.6.0\r\nbs4: 4.4.1\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: 1.0.13\r\npymysql: None\r\npsycopg2: 2.6.2 (dt dec pq3 ext lo64)\r\njinja2: 2.8\r\nboto: 2.40.0\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "Visualization",
      "Missing-data"
    ],
    "comments": [],
    "events": [
      "cross-referenced",
      "labeled",
      "labeled"
    ],
    "changed_files": 3,
    "additions": 11,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.2.txt",
      "pandas/tests/plotting/test_series.py",
      "pandas/tools/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14826,
    "reporter": "ischurov",
    "created_at": "2016-12-08T09:48:33+00:00",
    "closed_at": "2016-12-20T17:44:52+00:00",
    "resolver": "ischurov",
    "resolved_in": "50930a9879b580ab4f30d8b741229391e41afa76",
    "resolver_commit_num": 0,
    "title": "Inconsistent behavior of DatetimeIndex Partial String Indexing on Series and DataFrames",
    "body": "This bugreport is related to [this](-is-pandas-not-returing-a-scalar-string-instead-of-a-series-when-accessing-a) SO question and the discussion there.\r\n\r\n#### Summary\r\nI believe that current [DatetimeIndex Partial String Indexing](-docs/stable/timeseries.html#datetimeindex-partial-string-indexing) behavior is either inconsistent or underdocumented as the result depends nontrivially on whether we are working with `Series` or `DataFrame` and whether `DateTimeIndex` is periodic or not.\r\n\r\n#### `Series` vs. `DataFrame`\r\n\r\n\r\n\r\n\r\n\r\nHere we see that the behaviour depends on what we are indexing: `Series` returns scalar while `DataFrame` raises an exception. This exception is consistent with the documentation notice:\r\n> **Warning** The following selection will raise a KeyError; otherwise this selection methodology would be inconsistent with other selection methods in pandas (as this is not a slice, nor does it resolve to one)\r\n\r\nWhy we do not get the same exception for `Series` object?\r\n\r\n#### Periodic vs. Non-periodic\r\n\r\nIn contrast with the previous example, we get an instance of `Series` here, so the same timestamp is considered as a slice, not index. Why it depends in such a way on periodicity of the index?\r\n\r\n\r\nNo exceptions here, in contrast with periodic case.\r\n\r\nIs it intended behavior? If yes, I believe that this should be clearly documented and rationale provided.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.1.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.0+157.g2466ecb\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.1\r\nnumpy: 1.11.2\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Difficulty Advanced",
      "Effort Low",
      "Indexing",
      "Timeseries"
    ],
    "comments": [
      "The first \"series vs dataframe\" issue is as expected / follows from the second issue. `__getitem__`/`[]` indexing does not work exactly the same for series vs dataframe (http://pandas.pydata.org/pandas-docs/stable/indexing.html#basics). When provided a single key, Series will get a single value, while dataframe will try to get a column. And there is no column named \"2016-12-07 09:00:00\", hence the KeyError. \r\nSo this behaviour follows from the fact that in your first example, \"2016-12-07 09:00:00\" is not interpreted as a slice, but as a single key. Given that, the behaviour is as expected.\r\n\r\nBut, you are correct there might be an inconsistency in determining whether the string is a single key or a slice between the regular and irregular datetimeindex. The problem is that it is very difficult for pandas to guess/determine this when the index has no frequency.  ",
      "For the first part, I completely agree. I forgot that `[]` for dataframe select columns if argument is not a slice (shame on me), this explains the behavior nicely. (And now I understand the rationale behind the Warning in the docs I quoted.)\r\n\r\nFor the second part, is it true that for irregular indexes any string is considered to be slice, and for regular ones only those strings that provide date-time specification with precision less then frequency is considered to be slice?\r\n\r\nWhy it is not possible to consider string a key (not a slice) if it is casted to date-time that is exactly the same as one of keys in the index for irregular indexes?",
      "> And now I understand the rationale behind the Warning in the docs I quoted.\r\n\r\nIf you have ideas to rephrase this to make it clearer, very welcome!\r\n\r\n> Why it is not possible to consider string a key (not a slice) if it is casted to date-time that is exactly the same as one of keys in the index for irregular indexes?\r\n\r\nI am not exactly sure how it is implemented in the code, but imagine the following case: you have a timeseries with index [\"2016-01-01 00:00\", \"2016-01-01 12:00\", \"2016-01-01 23:00\", \"2016-01-02 05:00\", \"2016-01-02 18:00\"] (some irregular hours over two days). \r\nIf you would index this with the key \"2016-01-01\", this can be interpreted as \"give me all the data of the 1st of January\", so in this case: a slice (which is what pandas does). But if you would also want to check if the provided key exactly matches with one of the labels of the index, pandas parses this key -> `pd.Timestamp(\"2016-01-01\")`, which gives you \"2016-01-01 00:00:00\" (because pandas does not have different resolution in its datetime data type) and this would match with one of the elements (so: not a slice). So which one of the two would the user want? \r\n(to put it in other words, pandas cannot make a distinction between `s[\"2016-01-01\"]` (logically a slice) and `s[\"2016-01-01 00:00\"]` (logically a single element))",
      "Yes, I'm going to improve the docs according to our discussion after I understand all the details.\r\n\r\nYour explanation sounds reasonable, but I cannot get why all these arguments do not apply to the case of regular index? In fact, *pandas* can detect the resolution of a string-represented timestamp, this is done in function `pandas.tseries.tools.parse_time_string()` and it allows `pandas` to distinct `s[\"2016-01-01\"]` and `s[\"2016-01-01 00:00\"]` in case `s` is indexed with regular index:\r\n\r\n```python\r\nseries = pd.Series([1, 2, 3], pd.DatetimeIndex(['2016-12-07 00:00:00',\r\n                                                '2016-12-07 01:00:00',\r\n                                                '2016-12-07 02:00:00']))\r\nprint(series[\"2016-12-07 00:00:00\"])\r\n# 1\r\nprint(series[\"2016-12-07 00:00\"])\r\n# 1\r\nprint(series[\"2016-12-07\"])\r\n# 2016-12-07 00:00:00    1\r\n# 2016-12-07 01:00:00    2\r\n# 2016-12-07 02:00:00    3\r\n# dtype: int64\r\n```",
      "Finally, it seems that I got it. The code I'm interested in is [the following](https://github.com/pandas-dev/pandas/blob/master/pandas/tseries/index.py#L1296):\r\n\r\n```python\r\n    def _partial_date_slice(self, reso, parsed, use_lhs=True, use_rhs=True):\r\n        is_monotonic = self.is_monotonic\r\n        if ((reso in ['day', 'hour', 'minute'] and\r\n             not (self._resolution < Resolution.get_reso(reso) or\r\n                  not is_monotonic)) or\r\n            (reso == 'second' and\r\n             not (self._resolution <= Resolution.RESO_SEC or\r\n                  not is_monotonic))):\r\n            # These resolution/monotonicity validations came from GH3931,\r\n            # GH3452 and GH2369.\r\n            raise KeyError\r\n```\r\n\r\nraising `KeyError` here means that the timestamp cannot be coerced to a slice. The condition basically says that if the resolution of the timestamp (that we remember from the string) is less precise than the resolution of the index, it is slice, otherwise it is not. That sounds very reasonable.\r\n\r\nI'm not sure yet, why `second` resolution uses different condition (non-strict inequality instead of strict one), but basically I understand what's going on here.\r\n\r\nI'll try to improve the docs soon and prepare PR that will refer to this issue.",
      "Btw, could anybody tell, why `second` resolution is treated in such a different way here? I tried to figure it out myself (looking at PRs mentioned near the code), but didn't succeed.",
      "I finally discovered that this was PR #3931\r\n\r\n@jreback, could you please comment on this? Why do we introduce the inconsistence like this:\r\n\r\n```python\r\nseries = pd.Series([1, 2, 3, 4], pd.DatetimeIndex(['2016-12-06 23:59:00',\r\n                                                   '2016-12-07 01:00:00',\r\n                                                   '2016-12-07 01:01:00',\r\n                                                   '2016-12-07 01:02:01']))\r\n\r\nprint(type(series[\"2016-12-07 01:01:00\"]))\r\n# <class 'pandas.core.series.Series'>\r\n\r\nseries = pd.Series([1, 2, 3, 4], pd.DatetimeIndex(['2016-12-07',\r\n                                                   '2016-12-08',\r\n                                                   '2016-12-09',\r\n                                                   '2016-12-10']))\r\nprint(type(series[\"2016-12-07\"]))\r\n\r\n# <class 'numpy.int64'>\r\n```\r\nWhy `second` frequency is treated in different way?",
      "@ischurov Thanks for digging in! So indeed, it had in the end nothing to do with the irregular/regular index (only the resolution of the index is different due to the irregularity, and this impact how the slice is determined), but the different treatment of second resolution or higher resolutions.\r\n\r\nI am not sure why this was added differently as the other resolution, and this seems rather inconsistent to me.\r\n\r\nBy the way, apart from clarification in the docs, some comprehensive tests looping over some combinations of different resolutions is also welcome",
      "if you look at the PR, DataFrames need to have a slice here (and not a single indexer). So I think this could fix the inconsistent case you enumerate above (e.g. seconds is an exact match in which case you raise ``KeyError``; this may be counter intuitive in *how* to do it, but it goes to another path that converts to a timestamp and just looks for an exact match, if its there it returns the value, otherwise raises).\r\n\r\nSo obviously this is not tested on series.\r\n\r\nBut I think you'd have to introduce some logic  to actually return a slice when selecting for a DataFrame, which in this case IS a slice.\r\n\r\nThe other resolutions, day, hour, minute, cannot by definition ever have an exact match directly (because they always have a seconds component attached which you don't know). However seconds is special in that it *could* fully represented and actually be an exact match.\r\n",
      "> if you look at the PR, DataFrames need to have a slice here (and not a single indexer).\r\n\r\nWhy is that? \r\nThe example you give at the top of that PR is:\r\n\r\n```\r\nIn [11]: df = DataFrame(randn(5,5),columns=['open','high','low','close','volume'],index=date_range('2012-01-02 18:01:00',periods=5,tz='US/Central',freq='s'))\r\n\r\nIn [12]: df\r\nOut[12]: \r\n                               open      high       low     close    volume\r\n2012-01-02 18:01:00-06:00  0.131243  0.301542  0.128027  0.804162  1.296658\r\n2012-01-02 18:01:01-06:00  0.341487  1.548695  0.703234  0.904201  1.422337\r\n2012-01-02 18:01:02-06:00 -1.050453 -1.884035  1.537788 -0.821058  0.558631\r\n2012-01-02 18:01:03-06:00  0.846885  1.045378 -0.722903 -0.613625 -0.476531\r\n2012-01-02 18:01:04-06:00  1.186823 -0.018299 -0.513886 -1.103269 -0.311907\r\n\r\nIn [14]: df['2012-01-02 18:01:02']\r\nOut[14]: \r\n                               open      high       low     close    volume\r\n2012-01-02 18:01:02-06:00 -1.050453 -1.884035  1.537788 -0.821058  0.558631\r\n```\r\n\r\nSo AFAIK the PR made possible to slice the above dataframe with that string index. However, I would argue that in this case this is no slice at all, but a single key (as both the indexer key as the index is of second resolution, so the result of such a string key will always be slice of length 1 ?)\r\n",
      "for a dataframe by definition is IS a slice always as it cannot be an exact match (wrong axis for exact matching); it can only ever be a slice\r\n\r\nwhile for a series both are possible",
      "As the issue is marked as *bug*, may I ask, what is the desired behavior?\r\n\r\nActually, I believe this is not `Series` issue, it's `DataFrame` issue as well. I believe the following logic is consistent:\r\n\r\n- If timestamp resolution is strictly greater (less precise) than index resolution, timetamp is a slice as it can (in theory) correspond to more than one elements in the index. For `Series`, `[]` should return `Series`, for `DataFrame` \u2014 `DataFrame`.\r\n- If timestamp resolution is equal to index resolution, then timestamp is considered as an attempt to get a kind of \"exact match\". For `Series`, `[]` should return scalar, for `DataFrame` \u2014 try to find column with this key (if any), and most probably raise `KeyError`.\r\n- If timestamp resolution is strictly less than index resolution, `KeyError` have to be raised in both cases.\r\n\r\nOne can argue that if the resolution is greater than `second`, no exact match possible. However, I believe that it's an implementation detail \u2014 how timestamps are presented internally \u2014 and from user's point of view if the resolution of index is e.g. hour, then \"2016-01-01 01\" is in fact exact match.",
      "@ischurov so can you show some short test cases that replicate the logic you have presented (and show what is changing from current).",
      "@jreback see PR #14856. I added several [tests](https://github.com/pandas-dev/pandas/pull/14856/files#diff-db69b090c3ef26a0c2111e1c0a5909e3) to check partial string indexing with respect to the logic stated above. With current code, only `test_partial_slice_second` fails. The proposed solution is as simple as [this](https://github.com/pandas-dev/pandas/pull/14856/files#diff-23ecb29e7ceba52109a365e447400d2e).",
      "@jreback Here is a super short summary of what's changed:\r\n\r\nLet\r\n```python\r\ndf = DataFrame({'a': [1, 2, 3]},\r\n                       DatetimeIndex(['2011-12-31 23:59:59',\r\n                                      '2012-01-01 00:00:00',\r\n                                      '2012-01-01 00:00:01']),\r\n                       dtype=np.int64)\r\n```\r\nThen `df['a']['2011-12-31 23:59:59']` should return `np.int64` object `1` (now returns `Series`) and `df['2011-12-31 23:59:59']` should raise `KeyError` (now return `DataFrame`).",
      "I reported the behaviour on StackOverflow (and @ischurov raised it here), so I believe I am not influenced by Pandas design decision/culture. As a new comer, I expected the indexing/selection to return a consistent datatype (using the example in the response above):\r\n\r\n- if the index value (eg: `'2012-01-01 00:00:00'`) has the same resolution as the values in the DataFrame's index and the index value has an exact match then return a single scalar value.\r\n- Otherwise, return a Series containing all scalar values that can match the index value's resolution. This means that if the index value is has a higher resolution than the index value then a empty series is returned.\r\n\r\nMy suggestion differs from @ischurov's [proposition](https://github.com/pandas-dev/pandas/issues/14826#issuecomment-266037484) on KeyError. IMHO, if KeyError is too be used then I would expect it to be raised when the index value is not the same resolution as the DataFrame's Index resolution.\r\n",
      "> if KeyError is too be used then I would expect it to be raised when the index value is not the same resolution as the DataFrame's Index resolution.\r\n\r\nI don't really understand this comment. If the resolution of the indexer is lower than of the Index, you get a slice, if it is higher, you get a KeyError. So KeyError is already used in certain cases where the resolutions differ.\r\n\r\n"
    ],
    "events": [
      "renamed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 173,
    "deletions": 28,
    "changed_files_list": [
      "doc/source/timeseries.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/tseries/index.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14848,
    "reporter": "dragonator4",
    "created_at": "2016-12-10T00:31:23+00:00",
    "closed_at": "2017-01-13T17:38:20+00:00",
    "resolver": "Dr-Irv",
    "resolved_in": "ab0d23653a6e8264769b50c27b3b8efc95c6ab70",
    "resolver_commit_num": 8,
    "title": "BUG: DataFrame.groupby() on tuple column works only when column name is \"key\"",
    "body": "This is the weirdest bug I have seen in Pandas. But I am guessing (hoping) the fix will not be too difficult.\r\n\r\n#### Code Sample\r\n\r\nConsider the following two code blocks:\r\n\r\n**Block 1: key column is called \"k\"**\r\n\r\n**Block 2: key column is called \"key\"**\r\n\r\nNote that the same, static data is used, so that nothing else may be different, and hence culpable.\r\n\r\n#### Problem description\r\n\r\nRunning a simple `.groupby().describe()` operation produces the following results:\r\n\r\n\r\n\r\nNote that `groupby().mean()`, `sum()`, and a few others work fine. `describe()` is the only one I think is causing the problem.\r\n\r\n#### Expected Output\r\n\r\nObviously, the expected output for `df1.groupby('k').describe()` should be the same as `df2.groupby('key').describe()`.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-53-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.1\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.1\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: 0.8.2\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: 1.1.0\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: 0.9.4\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Groupby"
    ],
    "comments": [
      "Tuples in columns in not that well supported/tested, but this looks indeed like a bug. Welcome to look into it!",
      "It was working perfect in 0.19.0. Updating to 0.19.1 broke my code and this turned out to be the cause. I usually don't  work with tuples in columns, but my current project called for it. I had the choice of using three columns to store the three values in the tuple, but since tuples are immutable, I chose to use them.\r\n\r\nI have workarounds, so it is not high priority for me. ",
      "There's a correlation between the length of the column name and the number of items in the tuples. So `'key'` worked in the example, and I think any other 3 letter name would as well because the tuples were of that length.  I will try to fix."
    ],
    "events": [
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned"
    ],
    "changed_files": 4,
    "additions": 28,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/indexes/multi.py",
      "pandas/tests/groupby/test_groupby.py",
      "pandas/tests/indexes/test_multi.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14855,
    "reporter": "smartinsightsfromdata",
    "created_at": "2016-12-11T01:30:23+00:00",
    "closed_at": "2017-04-05T21:47:34+00:00",
    "resolver": "alexandercbooth",
    "resolved_in": "ba30e3a2e376035549b009079d44ba5ca7a4c48f",
    "resolver_commit_num": 0,
    "title": "scatter_matrix `color` vs `c`",
    "body": "I reported the issue below to matplotlib [here](#issuecomment-266250371), but they have requested to report it here as it appears (to the matplotlib folks) related to pandas.\r\n\r\nThe following code gives error, but works replacing `color=` with `c=`\r\n\r\nI suspect is either a bug, or some piece of documentation missing?\r\n(matplotlib folks think it is a pandas / scatter_matrix bug - see below)\r\n\r\nI'm using:\r\nmatplotlib 1.5.3\r\npandas 0.19.1\r\npython 3.5\r\n\r\nThis is a sample code:\r\n\r\nThis is the error trace:\r\n\r\nAccording to @goyodiaz:\r\n\r\n> It looks like a pandas issue. pandas.scatter_matrix always pass c to scatter but the caller can optionally pass more keywords arguments, including color. If the caller pass only color then scatter_matrix will pass both c and color and matplotlib will raise with a message that is confusing to the user because they did not pass c. Probably scatter_matrix should either raise with a clearer message or drop the c keyword argument when the user pass color\r\n\r\n.\r\n",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "Effort Low",
      "Visualization"
    ],
    "comments": [
      "Yes, this can indeed be solved on the pandas side as @goyodiaz indicated. We can just check for the user passing `color` as a kwarg and in that case let it replace the default `c` (and docs can maybe also use an update). \r\nThis should be an easy fix.\r\n"
    ],
    "events": [
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 10,
    "deletions": 5,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/tests/plotting/test_misc.py",
      "pandas/tools/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14861,
    "reporter": "jreback",
    "created_at": "2016-12-12T01:37:45+00:00",
    "closed_at": "2016-12-12T11:16:15+00:00",
    "resolver": "gr8h",
    "resolved_in": "96b171a6593fdab6b4b20157bf4d2e8bd72c5fb2",
    "resolver_commit_num": 0,
    "title": "DOC: groupby.rst examples broken",
    "body": "the df for the groupby.resample examples are hiding the correct frame that was previously being used by the docs below \r\n\r\n-docs.github.io/pandas-docs-travis/groupby.html#dispatching-to-instance-methods",
    "labels": [
      "Difficulty Novice",
      "Docs",
      "Groupby"
    ],
    "comments": [
      "Can i contribute to this?\r\n\r\nShould i use `dff` instead `df` since `dff = pd.DataFrame({'A': np.arange(8), 'B': list('aabbbbcc')})` and we need to groupby **A**?",
      "I have renamed _resample()_ `df` to `df_re` under **New syntax to window and resample operations** section. [pull request](https://github.com/pandas-dev/pandas/pull/14863)\r\n\r\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented"
    ],
    "changed_files": 1,
    "additions": 13,
    "deletions": 13,
    "changed_files_list": [
      "doc/source/groupby.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14866,
    "reporter": "yarikoptic",
    "created_at": "2016-12-12T15:53:59+00:00",
    "closed_at": "2017-03-21T17:41:02+00:00",
    "resolver": "jreback",
    "resolved_in": "aa9d0cf7fa0061058125d79d22d86f82f69c9185",
    "resolver_commit_num": 4309,
    "title": "on 32bit -- Series is int64 but cut produces int32 series",
    "body": "#### Code Sample, a copy-pastable example if possible\r\nfailing on 32bit test\r\n\r\n#### Problem description\r\norig:  ?pkg=pandas&arch=i386&ver=0.19.1%2Bgit174-g81a2f79-1&stamp=1481436461  (still don't know about \"bad argument to internal function\" -- can't replicate locally, so ignore for this one):\r\n\r\n\r\n\r\nso I guess if Series should use int64 regardless of underlying architecture default type, then cut should also produce int64, I guess.... or should Series have arch-default int??",
    "labels": [
      "Dtypes",
      "Testing",
      "Difficulty Intermediate",
      "Effort Low",
      "32bit"
    ],
    "comments": [
      "hmm, this *should* always be ``int64``. (btw this is a new tests in 0.19.2-dev)",
      "re new tests: yeap -- trying to build current master thus version is `0.19.1+git174-g81a2f79` to correspond to 81a2f79 (count of commits 174 is off since counted since 0.19.0, since 0.19.1 was released from the release branch, so current master is not a child of it)",
      "yeah for some reason our git describe is off, not really sure why",
      "normally things like this would show up on windows, but this is ok there. ",
      "so what should we do about this one?  on my end I know only asmuch as do astype in the test to workaround ;)",
      "@yarikoptic if you want to debug and see where the 32bit is coming from, great. we don't test on 32-bit systems any longer."
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "labeled",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 34,
    "deletions": 21,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/algorithms.py",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tests/tools/test_tile.py",
      "pandas/tools/tile.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14872,
    "reporter": "brendene",
    "created_at": "2016-12-13T03:36:20+00:00",
    "closed_at": "2016-12-18T19:42:28+00:00",
    "resolver": "opensourceworkAR",
    "resolved_in": "f3c5a427cc632e89cc5d2cb0df5b5e875cb6e23b",
    "resolver_commit_num": 0,
    "title": "fillna() containing timezone aware datetime64 rounded",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n\r\n\r\n#### Problem description\r\n\r\nThis is similar to bug #6587 but difference is a timezone aware datetime.  It looks like the dtype is being stripped and the value is converted to a float and back.\r\n\r\n#### Expected Output\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\npandas: 0.19.1\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\npytz: 2016.7\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Missing-data",
      "Timeseries",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "yep, these prob just need a ``is_datetime64tz_dtype`` in the same path (see #6587 for where to add).\r\n\r\nPR's welcome!"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files": 4,
    "additions": 28,
    "deletions": 10,
    "changed_files_list": [
      "doc/source/whatsnew/v0.19.2.txt",
      "pandas/core/missing.py",
      "pandas/tests/series/test_missing.py",
      "pandas/types/common.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14882,
    "reporter": "fonnesbeck",
    "created_at": "2016-12-14T19:21:21+00:00",
    "closed_at": "2017-01-21T23:07:44+00:00",
    "resolver": "Dr-Irv",
    "resolved_in": "2fa33fbddf4ae40b2dd47e146444a41bd6bccd6e",
    "resolver_commit_num": 7,
    "title": "Incorrect index label displayed on MultiIndex DataFrame",
    "body": "I have a DataFrame with a hierarchical index as follows:\r\n\r\n\r\n\r\nNotice in particular the values of the first two levels, and that they are balanced in the labels. However, the display of the table is incorrect. Here is what it looks like:\r\n\r\n![](+)\r\n\r\nNotice the second label of the first two levels of the hierarchical index are repeated (40 and 40 instead of 40 and 50, and 12 and 12 instead of 12 and 24).\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.3.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.1\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 31.0.1\r\nCython: 0.25.2\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.5.1\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.3\r\nopenpyxl: None\r\nxlrd: 1.0.0\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n</details>",
    "labels": [
      "Bug",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "NB confirmed that this occurs in a build from the current master as well.",
      "This issue disappears if I set `notebook_repr_html` to False. Does this make it a Jupyter issue?\r\n\r\n![](http://d.pr/i/T09A+)",
      "Probably a pandas issue.\r\n\r\nI'm having trouble reproducing it with just\r\n\r\n```python\r\nfrom pandas import *\r\n\r\nidx = MultiIndex(levels=[[30, 40, 50], [6, 12, 24], ['MRIgFUS', 'ablation', 'hysterectomy', 'iud', 'myomectomy', 'none', 'uae']],\r\n           labels=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2], [0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6]],\r\n           names=['age', 'followup', 'next intervention'])\r\ndf = pd.DataFrame({\"A\": 1}, index=idx)\r\ndf\r\n```\r\n\r\nI suspect it's related to the particular sorting of your index (do you have repr issues with `df.sort_index()`?) Could you maybe paste your `df.index.values.tolist()`?\r\n",
      "It's a pandas bug (pandas creates the html that jupyter displays), and has probably something to do with the truncation code. Because when you set the number of rows higher, eg `pd.options.display.max_rows = 100`, it displays correctly.",
      "I'm attempting to chase this down. @jorisvandenbossche is correct.  It is the truncation code."
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "unlabeled",
      "labeled",
      "labeled",
      "unlabeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 576,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/formats/format.py",
      "pandas/tests/formats/test_format.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14883,
    "reporter": "jowens",
    "created_at": "2016-12-14T21:24:36+00:00",
    "closed_at": "2017-03-28T21:50:37+00:00",
    "resolver": "jreback",
    "resolved_in": "34c6bd0fb7ad58b579ba940d4248ebab0aa758bf",
    "resolver_commit_num": 4335,
    "title": "ENH: json_normalize should allow a different separator than .",
    "body": "\r\n#### Problem description\r\n\r\nThe above snippet shows that it's not ideal to have `.` as a character in a column name. (I'm running into this when using Vega for data visualization, -lite/issues/1775.) When `json_normalize` flattens a nested input JSON, it separates the nesting levels with a `.`. I believe this happens on this line:\r\n\r\n-dev/pandas/blob/7d8bc0deaeb8237a0cf361048363c78f4867f218/pandas/io/json.py#L831\r\n\r\nI'd like to see an additional argument to `json_normalize`, `separator`, with default `.`, that specified the character (string) that separated nesting levels. In the line of code above, `'.'.join(val)` would be replaced by `separator.join(val)` (if I'm reading what that line does correctly). I could use, say, `_` to use underscore instead of period.\r\n\r\nn00b at pandas, please correct me if I'm doing anything wrong.\r\n\r\n#### Expected Output\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n>>> pandas.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.3.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.US-ASCII\r\nLOCALE: None.None\r\n\r\npandas: 0.19.1\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 30.3.0\r\nCython: 0.25.2\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.5.1\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.2.3.1\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.3\r\nopenpyxl: 2.4.1\r\nxlrd: 1.0.0\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: 3.6.0\r\nbs4: 4.5.1\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "IO JSON",
      "Enhancement",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "The dot notation for accessing columns is just a convenience. You can still get the column normally using `df['c2.x']`.",
      "I understand. I'm merely offering the observation that columns with `.` in the names are perhaps not a perfect fit for everything in Pandas and having the option to use a different separator might be also useful for someone who isn't me.",
      "Sure thing. Just wanted to point that out in case it was keeping you from doing something you needed to do now since you said that you were new to pandas.",
      "Yeah, it's the vega-lite bug I filed (https://github.com/vega/vega-lite/issues/1775) that's my proximate difficulty here.",
      "this would be quite easy to add; PR's welcome."
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "labeled",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 114,
    "deletions": 72,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/io/json/normalize.py",
      "pandas/tests/io/json/test_normalize.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14894,
    "reporter": "nateyoder",
    "created_at": "2016-12-16T06:28:25+00:00",
    "closed_at": "2016-12-17T23:32:00+00:00",
    "resolver": "nateyoder",
    "resolved_in": "e503d40ace473556990e5453ed5b4c9aa96e24ff",
    "resolver_commit_num": 1,
    "title": "Improve the performance of instantiating a Series object with dictionary data and a datetimeindex",
    "body": "The current code path always results in an exception on:\r\n-dev/pandas/blob/3ac41ab2d7ae446fa04f47ec911003cd722dbd65/pandas/core/series.py#L191\r\n\r\nwhich is then caught.  Only a slight performance advantage is seen but hopefully the code change  makes it less confusing for newcomers like me.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nThe current code path always results in an exception on:\r\n-dev/pandas/blob/3ac41ab2d7ae446fa04f47ec911003cd722dbd65/pandas/core/series.py#L191\r\n\r\nwhich is then caught.  Only a slight performance advantage is seen but hopefully the code change  makes it less confusing for newcomers like me.\r\n\r\nASV output of new benchmark\r\n Running 2 total benchmarks (2 commits * 1 environments * 1 benchmarks)\r\n[  0.00%] \u00b7 For pandas commit hash 5f05fdcf:\r\n[  0.00%] \u00b7\u00b7 Building for conda-py2.7-Cython-matplotlib-numexpr-numpy-openpyxl-pytables-scipy-sqlalchemy-xlrd-xlsxwriter-xlwt.................................\r\n[  0.00%] \u00b7\u00b7 Benchmarking conda-py2.7-Cython-matplotlib-numexpr-numpy-openpyxl-pytables-scipy-sqlalchemy-xlrd-xlsxwriter-xlwt\r\n[ 50.00%] \u00b7\u00b7\u00b7 Running ...x.time_series_constructor_no_data_datetime_index      3.26s\r\n[ 50.00%] \u00b7 For pandas commit hash 3ba2cff9:\r\n[ 50.00%] \u00b7\u00b7 Building for conda-py2.7-Cython-matplotlib-numexpr-numpy-openpyxl-pytables-scipy-sqlalchemy-xlrd-xlsxwriter-xlwt...\r\n[ 50.00%] \u00b7\u00b7 Benchmarking conda-py2.7-Cython-matplotlib-numexpr-numpy-openpyxl-pytables-scipy-sqlalchemy-xlrd-xlsxwriter-xlwt\r\n[100.00%] \u00b7\u00b7\u00b7 Running ...x.time_series_constructor_no_data_datetime_index      3.77s    before     after       ratio\r\n  [3ba2cff9] [5f05fdcf]\r\n-     3.77s      3.26s      0.87  series_methods.series_constructor_dict_data_datetime_index.time_series_constructor_no_data_datetime_index",
    "labels": [
      "Performance",
      "Timeseries"
    ],
    "comments": [],
    "events": [
      "cross-referenced",
      "labeled",
      "labeled",
      "milestoned"
    ],
    "changed_files": 3,
    "additions": 20,
    "deletions": 3,
    "changed_files_list": [
      "asv_bench/benchmarks/series_methods.py",
      "doc/source/whatsnew/v0.19.2.txt",
      "pandas/core/series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14898,
    "reporter": "rahulporuri",
    "created_at": "2016-12-16T11:54:04+00:00",
    "closed_at": "2016-12-31T15:55:21+00:00",
    "resolver": "gfyoung",
    "resolved_in": "5d4e92cc86dbb1f7a7fa5a9a88df94cf7770d654",
    "resolver_commit_num": 111,
    "title": "DOC : setting dialect on pandas.read_table overrides a few other kwds",
    "body": "#### Problem description\r\n\r\nCorrect me if I'm wrong but [TextFileReader](-dev/pandas/blob/master/pandas/io/parsers.py#L691-L700) overwrites the `delimiter`, `quoting` etc kwds if the `dialect` kwarg is passed. But this isn't highlighted in the `pandas.read_table` documentation.\r\n\r\nI don't know if this was implicit. Sorry for the noise if this is a known issue or if it isn't actually one.",
    "labels": [
      "API Design",
      "IO CSV"
    ],
    "comments": [
      "can you give an example",
      "the data contained in the file `test_data_quoting.txt` is\r\n```\r\n1,False,1.0,\"one\",2015-01-01,\"quote space\"\r\n2,True,2.0,\"two\",2015-01-02,noquote space\r\n3,False,3.0,three,2015-01-03,\"quote space\"\r\n4,True,4.0,four,2015-01-04,\"escaped\"\"quote\"\r\n```\r\n\r\nand for an example of kwds being overridden - \r\n\r\n```python\r\nIn [24]: pandas.read_table('test_data_quoting.txt', header=None, delimiter=',', quotechar=None, quoting=csv.QUOTE_NONE)\r\nOut[24]: \r\n   0      1    2      3           4                 5\r\n0  1  False  1.0  \"one\"  2015-01-01     \"quote space\"\r\n1  2   True  2.0  \"two\"  2015-01-02     noquote space\r\n2  3  False  3.0  three  2015-01-03     \"quote space\"\r\n3  4   True  4.0   four  2015-01-04  \"escaped\"\"quote\"\r\n\r\nIn [25]: pandas.read_table('test_data_quoting.txt', header=None, delimiter=',', quotechar=None, quoting=csv.QUOTE_NONE, dialect='excel')\r\nOut[25]: \r\n   0      1    2      3           4              5\r\n0  1  False  1.0    one  2015-01-01    quote space\r\n1  2   True  2.0    two  2015-01-02  noquote space\r\n2  3  False  3.0  three  2015-01-03    quote space\r\n3  4   True  4.0   four  2015-01-04  escaped\"quote\r\n```\r\n\r\nAgain, I'm not advocating to change the behavior or anything of those sorts. I just wanted to point out that I didn't find the behavior to have been documented.",
      "hmm, ok though *maybe* we should raise / warn if these are not default parameters. That seems a bit unexpected.\r\n\r\ncc @gfyoung \r\n@jorisvandenbossche \r\n?",
      "I agree that behaviour should not be changed at this point but warn that we have conflicting options.  I sense that we may want to raise in the future though?",
      "Mentioning this in the explanation of the keyword would also already be a start. Warning seems OK for me as well (although I am not sure how simple this is, as you would need to detect whether it is passed or not for each of the keywords), not sure if raising is needed."
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "milestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 6,
    "additions": 126,
    "deletions": 51,
    "changed_files_list": [
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/io/parsers.py",
      "pandas/io/tests/parser/common.py",
      "pandas/io/tests/parser/dialect.py",
      "pandas/io/tests/parser/test_parsers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14908,
    "reporter": "pbreach",
    "created_at": "2016-12-18T19:30:38+00:00",
    "closed_at": "2016-12-19T20:30:21+00:00",
    "resolver": "pbreach",
    "resolved_in": "8e630b63010ffdb4ed3c9b47309dc237f8141fdc",
    "resolver_commit_num": 0,
    "title": "List required for `percentiles` kwarg in `DataFrame.describe` when median is not present as opposed to array-like",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nIn the documentation the kwarg `percentiles` is expecting an array-like input, however when passing in a numpy array, an attribute error is thrown as if it were expecting a list. If a list is being expected in the case that the median is not found should there be an explicit conversion to `list` before the median is appended?\r\n\r\n#### Expected Output\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nIn [7]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.19.1\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 23.0.0\r\nCython: 0.24\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: 0.8.2\r\nIPython: 4.2.0\r\nsphinx: 1.3.1\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: 1.1.0\r\ntables: 3.2.2\r\nnumexpr: 2.6.0\r\nmatplotlib: 1.5.3\r\nopenpyxl: 2.3.2\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.2\r\nlxml: 3.6.0\r\nbs4: 4.4.1\r\nhtml5lib: 0.999\r\n None\r\napiclient: None\r\nsqlalchemy: 1.0.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.40.0\r\npandas_datareader: None\r\n\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Dtypes",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "this is a bug. should have a ``list()`` conversion first.\r\n\r\npull-requests are welcome."
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 11,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/generic.py",
      "pandas/tests/test_generic.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14922,
    "reporter": "uweschmitt",
    "created_at": "2016-12-19T15:37:14+00:00",
    "closed_at": "2016-12-23T21:04:14+00:00",
    "resolver": "uweschmitt",
    "resolved_in": "6bea8275e504a594ac4fee71b5c941fb520c8b1a",
    "resolver_commit_num": 0,
    "title": "BUG: sorting with large float and multiple columns incorrect",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n\r\nproduces\r\n\r\n\r\n\r\nAs you can see the first two calls (only sorting by `int` or by `float`) sort correctly whereas `df.sort_values([\"int\", \"float\"])`  does not seem to sort at all. Row with index `1` should appear first, and then row with index `0`.\r\n\r\nUsing `pandas 0.17.0` I get the  correct result:\r\n\r\n\r\n\r\n####\r\n\r\nThis issue was discussed at -by-broken-in-pandas-0-18-0\r\n    \r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS                                                                                                                                                         [1/193]\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.1\r\nnose: None\r\npip: 7.1.2\r\nsetuptools: 18.2\r\nCython: None\r\nnumpy: 1.10.4\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Difficulty Intermediate",
      "Effort Low",
      "Reshaping"
    ],
    "comments": [
      "pls show a copy-pastable example.",
      "yes, pls update the top message. idea is that the repro is just a copy-paste",
      "also pls try to make a more useful than 'not working'  (e.g. what is the problem)",
      "Ok now ? I was in a hurry...",
      "ok, confirmed on master. ty. if you'd like to digin and see what's happening would be great."
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 50,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/algorithms.py",
      "pandas/tests/frame/test_sorting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14930,
    "reporter": "dragoljub",
    "created_at": "2016-12-20T18:05:10+00:00",
    "closed_at": "2016-12-21T11:09:54+00:00",
    "resolver": "jreback",
    "resolved_in": "07c83eedba7494ff293acd7d800190cc29ebb888",
    "resolver_commit_num": 4146,
    "title": "DataFrame __getitem__ ~100X slower on Pandas 0.19.1 vs 0.18.1 possibly getitem caching?",
    "body": "\r\n#### Problem description\r\n\r\nIt appears that  or  may have something to do with it. This affects other functionality such as  which is now also ~100X slower.\r\n\r\n#### Expected Output\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 63 Stepping 2, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.19.1\r\nnose: 1.3.7\r\npip: 8.1.1\r\nsetuptools: 20.10.1\r\nCython: 0.24.1\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: 0.7.1\r\nIPython: 4.2.0\r\nsphinx: 1.3.6\r\npatsy: 0.4.0\r\ndateutil: 2.5.0\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: 1.0.0\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.2.5\r\nxlrd: 0.9.4\r\nxlwt: 1.0.0\r\nxlsxwriter: None\r\nlxml: 3.6.4\r\nbs4: 4.4.1\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: 1.0.12\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "Indexing",
      "Performance",
      "Regression"
    ],
    "comments": [
      "I have a fix for this.",
      "I changed this https://github.com/pandas-dev/pandas/commit/3c9644277b10bde301025733402c67310a28cdf0 to make it lazy\r\nbut it keeps reinitializing which is not great",
      "@jreback  Thanks for looking into this. \ud83d\udc4d \r\n\r\nCan you  provide some info about ```over_size_threshold```. It does appear this issue does not affect all frames, just those with many columns. What is ```over_size_threshold``` currently set to? Is there a way to prevent creation of hash tables on column index?\r\n\r\nWas this regression due to something that improved performance of row-based index but possibly hurt columnar access?\r\n",
      "see the referenced issues. They explain all.",
      "@dragoljub short answer is this.\r\n\r\nWhen the ``over_size_threshold`` was triggered (> 1M len of index), a different path was followed where the hashtable was constructed for *any* type of indexing operation (once its constructed it stays till collected). The issue was that for these large indexes we actually use searchsorted rather than the hashtable to look things up. So we were needlessly constructing and using memory for this.\r\n\r\nThe solution was to do our uniqueness checking at the same time as we are already checking for sortedness( e.g. the ``is_monotonic_increasing``).\r\n\r\nHowever it seems that this was *each* time constructing the index mapping (bad).\r\n\r\ncc @llllllllll \r\ncc @ssanderson ",
      "@jreback I took a look at your proposed fix.  I *think* it looks correct, but I think there might be a simpler fix that just makes `initialized` synonymous with `self.mapping is not None` (or drops `initialized entirely in favor of the `is not None` check), and changes the gating behavior on the `is_unique` property.  It's certainly possible that I missed something though: keeping all the different flags straight in my head is tricky.",
      "@jreback Thanks for the explanation and quick response time on this. \ud83d\ude04 This will be a great last minute per fix for 0.19.2!"
    ],
    "events": [
      "milestoned",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 40,
    "deletions": 34,
    "changed_files_list": [
      "asv_bench/benchmarks/frame_methods.py",
      "doc/source/whatsnew/v0.19.2.txt",
      "pandas/index.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14940,
    "reporter": "rizac",
    "created_at": "2016-12-21T10:57:08+00:00",
    "closed_at": "2017-01-14T18:24:27+00:00",
    "resolver": "discort",
    "resolved_in": "2c81129420752827759a01a6fdd37260d66f9356",
    "resolver_commit_num": 0,
    "title": "BUG: pandas.Series.dt.round inconsistent behaviour on NAT's with different arguments?",
    "body": "#### Code Sample\r\n\r\n\r\n#### Problem description\r\n\r\nThis is the output I get:\r\n\r\n\r\n\r\nIn the first case (freq argument 's') NaT are preserved (I would say, as I expect). However, in the second and third case, NaT's are converted to some apparently weird date time. If I don't miss some particular information (which in case after I googled and browsed the docs shouldn't be that hidden), this seems to be a bug\r\n\r\n#### Expected Output\r\n0   2010-01-01 23:14:12\r\n1                   NaT\r\nName: dtime, dtype: datetime64[ns]\r\n\r\n0   2010-01-01 23:14:10.000000000\r\n1                   NaT\r\nName: dtime, dtype: datetime64[ns]\r\n\r\n0   2010-01-01 23:14:00\r\n1                   NaT\r\nName: dtime, dtype: datetime64[ns]\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.11.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 24.0.2\r\nCython: None\r\nnumpy: 1.11.1\r\nscipy: 0.17.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: 3.6.0\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: 1.0.13\r\npymysql: None\r\npsycopg2: 2.6.2 (dt dec pq3 ext lo64)\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Difficulty Intermediate",
      "Effort Medium",
      "Missing-data",
      "Timeseries"
    ],
    "comments": [
      "yep, this looks buggy. need to mask the NaT's before and replace after, use ``self.hasnan`` (this is all on ``DatetimeIndex``), though ``TimedeltaIndex`` and ``PeriodIndex`` need checking as well.\r\n\r\nFurther looks like ``Timestamp`` doesn't implement these (should return a NaT), so need to modify ``NaTType``.\r\n\r\n```\r\nIn [6]: d['dtime'][1].round('s')\r\nAttributeError: 'NaTType' object has no attribute 'round'\r\n\r\nIn [7]: d['dtime'][1].round('5s')\r\nAttributeError: 'NaTType' object has no attribute 'round'\r\n```\r\n\r\nprob also buggy for ``.floor`` and ``.ceil``\r\n\r\nPR's welcome!",
      "I was passing the example code though debugger and investigated the next. The `_round` method\r\n\r\n```\r\n    def _round(self, freq, rounder):\r\n\r\n        from pandas.tseries.frequencies import to_offset\r\n        unit = to_offset(freq).nanos\r\n\r\n        # round the local times\r\n        values = _ensure_datetimelike_to_i8(self)\r\n\r\n        result = (unit * rounder(values / float(unit))).astype('i8')\r\n        attribs = self._get_attributes_dict()\r\n        if 'freq' in attribs:\r\n            attribs['freq'] = None\r\n        if 'tz' in attribs:\r\n            attribs['tz'] = None\r\n        return self._ensure_localized(\r\n            self._shallow_copy(result, **attribs))\r\n\r\n    @Appender(_round_doc % \"round\")\r\n    def round(self, freq, *args, **kwargs):\r\n        return self._round(freq, np.round)\r\n ```\r\n\r\nLets consider the first example `d['dtime'].dt.round('s')`\r\n\r\n```\r\n>>> self\r\nDatetimeIndex(['2010-01-01 23:14:12.000599', 'NaT'], dtype='datetime64[ns]', name='dtime', freq=None)\r\n```\r\n \r\n `unit` variable equals `1000000000` and `result=array([ 1262387652000000000, -9223372036854775808])`. We are interested in the second item 'NaT'.\r\n At the second, `d['dtime'].dt.round('5s')` unit variable equals `5000000000` and `result=array([ 1262387650000000000, -9223372035000000512])`.\r\n We can see that in both examples values which before this were `Nat` are different.\r\n\r\n Deeper investigation gave next results\r\n 1) 1st example\r\n ```\r\n >>> values[1] / unit\r\n-9223372036.8547764\r\n>>> rounder(values[1] / unit)\r\n-9223372037.0\r\n```\r\n2) 2nd example\r\n```\r\n>>> values[1] / unit\r\n-1844674407.3709552\r\n>>> rounder(values[1] / unit)\r\n-1844674407.0\r\n```\r\n\r\nIn the second case, the value isn't rounding which is correct cause we use math round. But I think we definitely use much smarter round, because we are using the large numbers and could neglect by math round.\r\nI tried to round using `np.floor` and got the needed result `array([ 1262387645000000000, -9223372036854775808])`.",
      "@discort you don't need to worry about the calculation on the NaT at all. simply use ``self._maybe_mask_results(result)`` which will make the location of where the NaT were back into NaT's"
    ],
    "events": [
      "renamed",
      "renamed",
      "renamed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 27,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/tseries/base.py",
      "pandas/tseries/tests/test_timeseries.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14941,
    "reporter": "gryBox",
    "created_at": "2016-12-21T11:54:27+00:00",
    "closed_at": "2016-12-31T15:45:30+00:00",
    "resolver": "gfyoung",
    "resolved_in": "5353e59447a9b39cc15c0fbc70cc8cc49fe44213",
    "resolver_commit_num": 124,
    "title": "pd.to_numeric(series, downcast='integer') does not prpoerly handle floats over 10,000",
    "body": "Hi - I came across this issue in [stackoverflow ]()while testing pd.to_numeric()\r\n\r\n> If all floats in column are over 10000 it loses precision and converts them to integers.\r\n\r\n\r\n![image](-c741-11e6-9000-3eea48faa7ec.png)\r\n\r\nThis doesn't seem like the desired behavior.",
    "labels": [
      "Numeric",
      "Bug",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "so this is correct, it is effectively doing an ``.astype(int)``. though I suppose this is too aggressive and the downcast from float -> int should only succeed if they are actually equal.\r\n\r\n@gfyoung thoughts",
      "Just so I am clear, this is the behavior expected? Not this?\r\n\r\n```\r\ntst_df = pd.DataFrame({'colA':['a','b','c','a','z', 'q'],\r\n                      'colB': pd.date_range(end=datetime.datetime.now() , periods=6),\r\n                      'colC' : ['a1','b2','c3','a4','z5', 'q6'],\r\n                      'colD': [1000.0, 2000, 3000, 4000.36, 5000, 50000.00]})\r\n\r\npd.to_numeric(tst_df['colD'],  downcast='integer')\r\n```\r\n![image](https://cloud.githubusercontent.com/assets/16763201/21393444/11dd10d8-c75a-11e6-8f53-6b7abed54e93.png)\r\n",
      "hmm, this looks buggy. if you want to have a look see inside and see what's going on would be great.",
      "@jreback  I will try - I am sort of new to this.  Is this the module I should be looking at?\r\n[pandas/pandas/tools/util.py](https://github.com/pandas-dev/pandas/blob/5faf32a62b52912bc0c4a26622bfc3d72b5121ff/pandas/tools/util.py)",
      "yes",
      "@gryBox : First of all, thanks for pointing this out!  If you follow the code from where you started, the bug traces <a href=\"https://github.com/pandas-dev/pandas/blob/5faf32a62b52912bc0c4a26622bfc3d72b5121ff/pandas/types/cast.py#L117\">here</a>. You can see here yourself:\r\n~~~python\r\n>>> import numpy as np\r\n>>>\r\n>>> arr = [1000000]\r\n>>> arr2 = [1000000.5]\r\n>>>\r\n>>> np.allclose(arr, arr2)  # This is what we do now\r\nTrue\r\n>>> np.allclose(arr, arr2, rtol=0)  # This is what we probably should do\r\nFalse\r\n~~~\r\nI think if passing `rtol=0`, you should be able to patch this behavior.  It also explains why it breaks with large numbers (the numbers are so large `rtol` isn't small enough to indicate that they're not close).",
      "@gfyoung : The credit goes to [tworec](http://stackoverflow.com/users/3805131/tworec) at stack. \r\n\r\n\r\n",
      "@gryBox : I'm not sure there is such a method.  Also, `numpy` `dtype` checking is not 100% compatible with all of our `pandas` objects (deliberate), hence we prefer to stay away from such methods in `numpy`."
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 16,
    "deletions": 13,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/tools/tests/test_util.py",
      "pandas/tools/util.py",
      "pandas/types/cast.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14946,
    "reporter": "jdeschenes",
    "created_at": "2016-12-21T21:26:42+00:00",
    "closed_at": "2017-01-26T15:01:14+00:00",
    "resolver": "mroeschke",
    "resolved_in": "be328521b3a5525175b08cb740bb970ccf75727e",
    "resolver_commit_num": 27,
    "title": "ValueError when using loc with TimedeltaIndex",
    "body": "This snippet will work correctly:\r\n\r\n\r\n\r\nBut when using a TimedeltaIndex:\r\n\r\n\r\n\r\npandas 0.18.1 has a different problem, but there is a workaround. Simple use the underlying numpy array:\r\n\r\n\r\n\r\n### Ouput\r\n\r\n---------------------------------------------------------------------\r\nValueError                          Traceback (most recent call last)\r\n<ipython-input-8-8732af012025> in <module>()\r\n----> 1 df.loc[cond, 'x'] = 10\r\n\r\n~/pandas/pandas/core/indexing.py in __setitem__(self, key, value)\r\n    138         else:\r\n    139             key = com._apply_if_callable(key, self.obj)\r\n--> 140         indexer = self._get_setitem_indexer(key)\r\n    141         self._setitem_with_indexer(indexer, value)\r\n    142 \r\n\r\n~/pandas/pandas/core/indexing.py in _get_setitem_indexer(self, key)\r\n    120 \r\n    121         if isinstance(key, tuple) and not self.ndim < len(key):\r\n--> 122             return self._convert_tuple(key, is_setter=True)\r\n    123         if isinstance(key, range):\r\n    124             return self._convert_range(key, is_setter=True)\r\n\r\n~/pandas/pandas/core/indexing.py in _convert_tuple(self, key, is_setter)\r\n    182         else:\r\n    183             for i, k in enumerate(key):\r\n--> 184                 idx = self._convert_to_indexer(k, axis=i, is_setter=is_setter)\r\n    185                 keyidx.append(idx)\r\n    186         return tuple(keyidx)\r\n\r\n~/pandas/pandas/core/indexing.py in _convert_to_indexer(self, obj, axis, is_setter)\r\n   1149         # if we are a label return me\r\n   1150         try:\r\n-> 1151             return labels.get_loc(obj)\r\n   1152         except LookupError:\r\n   1153             if isinstance(obj, tuple) and isinstance(labels, MultiIndex):\r\n\r\n~/pandas/pandas/tseries/tdi.py in get_loc(self, key, method, tolerance)\r\n    675         \"\"\"\r\n    676 \r\n--> 677         if isnull(key):\r\n    678             key = tslib.NaT\r\n    679 \r\n\r\n~/pandas/pandas/core/generic.py in __nonzero__(self)\r\n    915         raise ValueError(\"The truth value of a {0} is ambiguous. \"\r\n    916                          \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\r\n--> 917                          .format(self.__class__.__name__))\r\n    918 \r\n    919     __bool__ = __nonzero__\r\n\r\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\r\n\r\n#### Versions\r\nHere is my pd.show_versions(). It also has been tested on windows7 64 bits with version 0.19\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n--------------------------------\r\n\r\ncommit: f79bc7a9d128187f3a93a3dae84ff03f4f4a62f4\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-57-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_CA.UTF-8\r\nLOCALE: en_CA.UTF-8\r\n\r\npandas: 0.19.0+243.gf79bc7a\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.2\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\ns3fs: None\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "Effort Low",
      "Indexing",
      "Timedelta"
    ],
    "comments": [
      "this is quite subtle. When a boolean array is presented, ``.get_loc()`` should raise a ``TypeError`` if it cannot handle (and so this hits a different path). However, looks like the ``.get_loc`` on a ``TimedeltaIndex`` is wrong.\r\n\r\nThis patch makes it work. If you could do a PR with some tests and a 1-liner about why this is needed, would be great. The basic issue is that ``.get_loc`` should raise a ``TypeError`` if it cannot handle the type of the key (rather than erroring with a ValueError in this case).\r\n\r\n```\r\ndiff --git a/pandas/tseries/tdi.py b/pandas/tseries/tdi.py\r\nindex 1585aac..1e4986a 100644\r\n--- a/pandas/tseries/tdi.py\r\n+++ b/pandas/tseries/tdi.py\r\n@@ -14,7 +14,8 @@ from pandas.types.common import (_TD_DTYPE,\r\n                                  _ensure_int64)\r\n from pandas.types.missing import isnull\r\n from pandas.types.generic import ABCSeries\r\n-from pandas.core.common import _maybe_box, _values_from_object\r\n+from pandas.core.common import (_maybe_box, _values_from_object,\r\n+                                is_bool_indexer)\r\n \r\n from pandas.core.index import Index, Int64Index\r\n import pandas.compat as compat\r\n@@ -674,6 +675,9 @@ class TimedeltaIndex(DatetimeIndexOpsMixin, TimelikeOps, Int64Index):\r\n         loc : int\r\n         \"\"\"\r\n \r\n+        if is_bool_indexer(key):\r\n+            raise TypeError\r\n+\r\n         if isnull(key):\r\n             key = tslib.NaT\r\n```\r\n"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 24,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/tests/indexing/test_indexing.py",
      "pandas/tseries/tdi.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14973,
    "reporter": "sadruddin",
    "created_at": "2016-12-23T17:12:08+00:00",
    "closed_at": "2016-12-30T19:14:33+00:00",
    "resolver": "kamal94",
    "resolved_in": "298b241be32365e3e2c8d53fbad098a05de044c9",
    "resolver_commit_num": 0,
    "title": "Using the python power operator on numerical Index objects yields unexpected results",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nUsing the pow '**' operator on an index object results in a behaviour different than when using the '*' operator. The pow result implies that the values being used are not index.values, which one would expect given the behaviour of other operators (*, -, +, / ...).\r\nThis problem only arises when the index object is the second argument.\r\n\r\n#### Expected Output\r\nexpected output would be the result of 2.0**index.values\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\ncommit: None\r\npython: 2.7.13.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 42 Stepping 7, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.19.1\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 32.2.0\r\nCython: None\r\nnumpy: 1.12.0rc1\r\nscipy: 0.18.1\r\nstatsmodels: 0.8.0rc1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.4.8\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.3\r\nopenpyxl: 2.4.1\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: None\r\nlxml: 3.7.0\r\nbs4: None\r\nhtml5lib: 0.9999999\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\n\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "Effort Low",
      "Numeric"
    ],
    "comments": [
      "https://github.com/pandas-dev/pandas/blob/master/pandas/indexes/base.py#L3537\r\n\r\n``rpow`` needs to be a separate function (with ``reversed=True``) as this is not a communative operation (unlike mul).\r\n\r\nwant to do a PR?",
      "I can work on this if no one is..?",
      "That would be great! I wouldn't have been able to look at it to try and fix it until the next couple of weeks."
    ],
    "events": [
      "renamed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 13,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/indexes/base.py",
      "pandas/tests/indexes/test_numeric.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14982,
    "reporter": "gfyoung",
    "created_at": "2016-12-25T07:30:19+00:00",
    "closed_at": "2016-12-30T18:53:08+00:00",
    "resolver": "gfyoung",
    "resolved_in": "17d7ddbc307e275343e8ad5e075b29612a505ebb",
    "resolver_commit_num": 121,
    "title": "BUG: maybe_convert_numeric fails with uint64",
    "body": "`master` at <a href=\"-dev/pandas/commit/aba7d255a165bbc221be106839c59a876655c0c2\">aba7d2</a>:\r\n~~~python\r\n>>> from pandas import lib, np\r\n>>> arr = np.array([2**63], dtype=object)\r\n>>> lib.maybe_convert_numeric(arr, set())\r\n...\r\nOverflowError: Python int too large to convert to C long\r\n~~~\r\nAt first, I thought the patch would be similar to #14916, but there are several complications here:\r\n\r\n* Handling an array with negative and `uint64` values: in `maybe_convert_objects`, we would just return `object` to avoid data truncation.  However, this function currently only returns numeric arrays.  And if we do decide to return non-numeric, should we then raise if `coerce_numeric=True` then?\r\n* `uint64` has no convenient `na_value`, so what happens if a `uint64` is in `na_values`?",
    "labels": [
      "Bug",
      "Dtypes",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "your two suggestions are fine\r\n\r\n- a ``uint64`` array with negative values (or in ``na_values``) must be converted to ``object``.\r\n\r\nso unfortunately ``maybe_convert_numeric`` gets more complicated :>"
    ],
    "events": [
      "cross-referenced",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 265,
    "deletions": 24,
    "changed_files_list": [
      "asv_bench/benchmarks/inference.py",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/io/tests/parser/common.py",
      "pandas/src/inference.pyx",
      "pandas/tests/types/test_inference.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 14992,
    "reporter": "gcbeltramini",
    "created_at": "2016-12-26T16:59:31+00:00",
    "closed_at": "2016-12-30T19:41:21+00:00",
    "resolver": "gcbeltramini",
    "resolved_in": "859c80f327cf4ac25b3aaac8bfb05de34bc3ef8e",
    "resolver_commit_num": 0,
    "title": "Reindex function applied to DataFrame over columns ignores method",
    "body": "#### Code Sample\r\n\r\n\r\n\r\n#### Problem description\r\n\r\nI want to apply the `reindex` function to the columns using the `\"ffill\"` method, but the holes are filled with NaN. The workaround is to transpose the DataFrame, apply the function and then transpose again.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.1.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: None\r\nnumpy: 1.11.1\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.38.0\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "Effort Low",
      "Missing-data",
      "Reshaping"
    ],
    "comments": [
      "```\r\nIn [3]: df.reindex(columns=range(7)).ffill(axis=1)\r\nOut[3]: \r\n    0     1     2     3     4     5     6\r\n1 NaN  11.0  11.0  12.0  12.0  13.0  13.0\r\n3 NaN  21.0  21.0  22.0  22.0  23.0  23.0\r\n5 NaN  31.0  31.0  32.0  32.0  33.0  33.0\r\n```\r\nis an idiomatic way to do this,\r\n\r\nbut I see that the ``method`` kwarg is not getting propogated to the reindexing methods for columns. Pull requests to fix are welcome!"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 5,
    "additions": 121,
    "deletions": 9,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/frame.py",
      "pandas/sparse/frame.py",
      "pandas/sparse/tests/test_frame.py",
      "pandas/tests/frame/test_axis_select_reindex.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15002,
    "reporter": "adrien-pain-01",
    "created_at": "2016-12-28T10:28:36+00:00",
    "closed_at": "2016-12-28T10:56:53+00:00",
    "resolver": "wcwagner",
    "resolved_in": "b2cdc02a9fbbcf0efbfbb6195cc4d2fb69348f5a",
    "resolver_commit_num": 2,
    "title": "Different behavior of DataFrame.resample() between pandas 0.16.2 and pandas 0.18.1",
    "body": "I just upgraded my projects from pandas 0.16.2 to pandas 0.18.1, but I have a difference on some `DataFrame.resample` operations, with Weekly frequencies.\r\n\r\nFor instance, I want to resample daily data to weekly data (`W-SUN`), and apply an offset to the results, so each row will be a **MONDAY** and not a SUNDAY.\r\n\r\n#### Code Sample\r\n\r\n\r\n#### Problem description\r\n\r\nWith `pandas 0.16.2`, I got this correct output:\r\n\r\nie, each row is a weekly aggregation of the data, and each date is a **MONDAY**\r\n\r\nResulting index is as follow:\r\n\r\n\r\nHowever, with `pandas 0.18.1`, I got this output:\r\n\r\neach row is correctly aggregated, but the dates are **SUNDAY**s\r\n\r\nResulting index is as follow:\r\n\r\n\r\n<observations>\r\n\r\nIt's like the `loffset` parameter is not correctly taken into account in  the new `DataFrame.resample()` implementation.\r\n\r\nie, the output with `pandas 0.18.1` is the same as the one using `pandas 0.16.2`, but calling `a.resample(\"1W\", how={\"value\": \"sum\"}, `**loffset=None**`, closed=\"right\", label=\"right\")`\r\n\r\n</observations>\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\npandas `0.16.1` installation:\r\n\r\n<details>\r\n\r\n\r\n\r\n</details>\r\n\r\npandas `0.18.2` installation:\r\n<details>\r\n\r\n\r\n</details>\r\n",
    "labels": [],
    "comments": [
      "duplicate of https://github.com/pandas-dev/pandas/issues/13218\r\n\r\nbeing addressed in #14213 expected to be in 0.20.0"
    ],
    "events": [
      "commented",
      "closed",
      "reopened",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 63,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/tseries/resample.py",
      "pandas/tseries/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15015,
    "reporter": "jreback",
    "created_at": "2016-12-30T16:57:40+00:00",
    "closed_at": "2017-04-14T15:37:14+00:00",
    "resolver": "jreback",
    "resolved_in": "8b404539b8b8f2ce2eaf38c7cd2f7f3925c6e171",
    "resolver_commit_num": 4377,
    "title": "API: handle exceptions in particular columns when .agg",
    "body": "followup to #14668 \r\n\r\nWhen you have an exception in *some* aggregation columns, need to be more friendly. We exclude them in .groupby aggs so need to do that here.\r\n\r\n\r\n\r\nso [4] should prob not happen",
    "labels": [
      "API Design",
      "Dtypes",
      "Reshaping"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "assigned",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 14,
    "additions": 877,
    "deletions": 45,
    "changed_files_list": [
      "doc/source/api.rst",
      "doc/source/basics.rst",
      "doc/source/computation.rst",
      "doc/source/groupby.rst",
      "doc/source/timeseries.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/base.py",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/series.py",
      "pandas/tests/frame/test_apply.py",
      "pandas/tests/groupby/test_aggregate.py",
      "pandas/tests/groupby/test_value_counts.py",
      "pandas/tests/series/test_apply.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15022,
    "reporter": "jorisvandenbossche",
    "created_at": "2016-12-31T10:26:53+00:00",
    "closed_at": "2017-03-22T18:47:11+00:00",
    "resolver": "jorisvandenbossche",
    "resolved_in": "1a266ee5809990244f1fe6daeb717878d06cf783",
    "resolver_commit_num": 671,
    "title": "API: let DatetimeIndex date/time components return a new Index instead of array",
    "body": "#### Problem description\r\n\r\nCurrently the date and time components on a DatetimeIndex return a numpy array. I would propose to let them return a new Index object (and in this way retaining all the interesting added functionality of an Index object).\r\n\r\nRelated to -dev/pandas/pull/14506 that changed `Index.map` to return an Index instead of an array.\r\n\r\n#### Code Sample\r\n\r\n\r\n\r\nWhen the above would return an index, something that would become possible is this:\r\n\r\n\r\n\r\nwhich is now not possible as an array has no isin method.\r\n\r\n\r\nUsing pandas master, 0.19.0+289.g1bf94c8",
    "labels": [
      "API Design",
      "Timeseries"
    ],
    "comments": [
      "want to take this."
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 15,
    "additions": 156,
    "deletions": 78,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/tests/indexes/datetimes/test_misc.py",
      "pandas/tests/indexes/period/test_construction.py",
      "pandas/tests/indexes/period/test_period.py",
      "pandas/tests/indexes/timedeltas/test_timedelta.py",
      "pandas/tests/scalar/test_timestamp.py",
      "pandas/tests/tools/test_pivot.py",
      "pandas/tests/tools/test_util.py",
      "pandas/tests/tseries/test_timezones.py",
      "pandas/tseries/common.py",
      "pandas/tseries/converter.py",
      "pandas/tseries/index.py",
      "pandas/tseries/period.py",
      "pandas/tseries/tdi.py",
      "pandas/tseries/util.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15048,
    "reporter": "jreback",
    "created_at": "2017-01-03T22:21:33+00:00",
    "closed_at": "2017-01-11T13:22:03+00:00",
    "resolver": "mroeschke",
    "resolved_in": "0fe491db358204544814aa2371fed4ea947532ab",
    "resolver_commit_num": 25,
    "title": "PERF: groupby-cummax,cummin",
    "body": "these could be implemented in cython, should be very straightforward as we already have much templated code\r\n\r\n",
    "labels": [
      "Difficulty Intermediate",
      "Effort Low",
      "Groupby",
      "Performance"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 173,
    "deletions": 14,
    "changed_files_list": [
      "asv_bench/benchmarks/groupby.py",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/groupby.py",
      "pandas/src/algos_groupby_helper.pxi.in",
      "pandas/tests/groupby/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15096,
    "reporter": "nkorlin",
    "created_at": "2017-01-10T06:09:11+00:00",
    "closed_at": "2017-01-13T20:10:08+00:00",
    "resolver": "rouzazari",
    "resolved_in": "1a18420d97f1988b8628e438ad83f44bdb618465",
    "resolver_commit_num": 0,
    "title": "to_json() line separation broken by backslash in content",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nI noticed this issue when I saw that a ~8500 row dframe became a ~3400 line file, even though in theory \"lines=True\" should mean that the number of rows and number of lines are equal. I dug into the code a bit, and it turns out that convert_json_to_lines() does not correctly insert newlines if the json contains a backslash before a double quote, even if the backslash itself is escaped.\r\n(-dev/pandas/blob/v0.19.2/pandas/lib.pyx#L1114)\r\n\r\nHere are the test files I used with the code sample above, and the outputs I got:\r\n\r\n\r\n#### Expected Output\r\n\r\n\"-out\" files should be identical to originals.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-68-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.1\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: None\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: 2.43.0\r\npandas_datareader: None\r\n\r\n</details>",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "Effort Low",
      "IO JSON",
      "Output-Formatting"
    ],
    "comments": [
      "so there is a PR #14693 which fixes this, though it was closed by its author. would welcome to pick it up."
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 15,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/io/tests/json/test_pandas.py",
      "pandas/lib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15109,
    "reporter": "jreback",
    "created_at": "2017-01-11T14:50:40+00:00",
    "closed_at": "2017-01-25T19:05:22+00:00",
    "resolver": "mroeschke",
    "resolved_in": "ba057443d1b69bb3735a4b18a18e4e4897231867",
    "resolver_commit_num": 26,
    "title": "BUG: groupby.cummin/max changing passed values to nan uncesessarily",
    "body": "after #15053 is failing. (puzzled why its not actually failing on master though).\r\n\r\nThis is local in osx (3.5), so same as the travis test (though that used numpy 1.10.4), its possible that is the cause.\r\n\r\n\r\nbecause the inf values are == iNaT, and we convert these on exit from the function as we are using these to mark NaT in all int64, not just for datetimelike.\r\n\r\n fixes, but is kludgey. I pre-check for iNaT an if so, convert to float.\r\n\r\nA better soln is to pass in the ``is_datetimelike`` flag so that the check could be done internally in cython. IOW, if is_datetimelike==True, then go ahead and treat iNaT as null, otherwise ignore it. Ideally we would actually be able to return a correctly dtyped array in the first place, but the templating code won't allow this, e.g. we have a 1-1 mapping between in-out and they are done a-priori.\r\n\r\npart of this fix could also remove the need for passing in ``accum`` as its wholly internal to the group by cython functions (as part of this could remove the fused typed, ``numeric`` and replace with templated code).",
    "labels": [
      "Bug",
      "Dtypes",
      "Groupby",
      "Missing-data",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "cc @mroeschke \r\n@chris-b1 ",
      "cc @mroeschke turns out we weren't testing groupby on travis after the refactor :<\r\n\r\nI merged in #15127 which now will run these tests, but I skip the portions where this issue is causing things to fail.",
      "Ah so that's why that test didn't trip during the build. Thanks for keeping me posted @jreback \r\n\r\nI'll uncomment those tests once I look into the fix. "
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "referenced",
      "cross-referenced",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced"
    ],
    "changed_files": 4,
    "additions": 47,
    "deletions": 42,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/groupby.py",
      "pandas/src/algos_groupby_helper.pxi.in",
      "pandas/tests/groupby/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15116,
    "reporter": "simon-b",
    "created_at": "2017-01-12T14:20:02+00:00",
    "closed_at": "2017-01-18T16:12:00+00:00",
    "resolver": "jreback",
    "resolved_in": "99afdd9aecbeb6be532f3c9d1d3c430241a9b579",
    "resolver_commit_num": 4168,
    "title": "Reindex versus ix gotchas documentation text does not match example code",
    "body": "In the gotchas documentation, in the section \"Reindex versus ix gotchas\", there is the following example:\r\n\r\n\r\n\r\nThe example shows both `ix` and `reindex` behaving the same. The subsequent text, however, states that they are expected to give different results:\r\n\r\n> Because the index in this case does not contain solely integers, `ix` falls back on integer indexing. By contrast, `reindex` only looks for the values passed in the index, thus finding the integers `0` and `1`.\r\n\r\nI am not sure if the indexing behaviour is correct and the documentation is wrong, or vice versa. It appears that in this case `ix` does not fall back on integer indexing while the text states that it does. ",
    "labels": [
      "Docs",
      "Indexing"
    ],
    "comments": [
      "@simon-b Thanks for the catch!\r\n\r\nOnce this was correct, see eg the old docs of pandas 0.8 with this example: http://pandas.pydata.org/pandas-docs/version/0.8.0/gotchas.html#reindex-versus-ix-gotchas, but apparently that behaviour was changed (the docs of 0.9.0 already show the current behaviour) but the docs never updated.\r\n\r\nPersonally, I would expect `ix` to fallback to integer indexing in this case (certainly given the explanation in the docs on ix http://pandas.pydata.org/pandas-docs/stable/indexing.html#different-choices-for-indexing, which is actually also wrong). But the complexity and unpredictability of `.ix` is one of the reasons we are going to deprecate it (see https://github.com/pandas-dev/pandas/issues/14218). \r\nGiven that, we should probably just remove this whole section.\r\n",
      "yep i will remove this section as well "
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "commented",
      "milestoned",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 90,
    "additions": 1657,
    "deletions": 1399,
    "changed_files_list": [
      "doc/source/advanced.rst",
      "doc/source/api.rst",
      "doc/source/basics.rst",
      "doc/source/categorical.rst",
      "doc/source/computation.rst",
      "doc/source/cookbook.rst",
      "doc/source/gotchas.rst",
      "doc/source/indexing.rst",
      "doc/source/io.rst",
      "doc/source/merging.rst",
      "doc/source/missing_data.rst",
      "doc/source/reshaping.rst",
      "doc/source/sparse.rst",
      "doc/source/visualization.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/indexing.py",
      "pandas/core/panel.py",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/io/pytables.py",
      "pandas/io/tests/json/test_pandas.py",
      "pandas/io/tests/parser/common.py",
      "pandas/io/tests/parser/parse_dates.py",
      "pandas/io/tests/test_date_converters.py",
      "pandas/io/tests/test_excel.py",
      "pandas/io/tests/test_html.py",
      "pandas/io/tests/test_packers.py",
      "pandas/io/tests/test_pytables.py",
      "pandas/io/tests/test_sql.py",
      "pandas/io/tests/test_stata.py",
      "pandas/sparse/tests/test_frame.py",
      "pandas/sparse/tests/test_indexing.py",
      "pandas/sparse/tests/test_series.py",
      "pandas/stats/tests/test_ols.py",
      "pandas/tests/formats/test_format.py",
      "pandas/tests/frame/test_alter_axes.py",
      "pandas/tests/frame/test_analytics.py",
      "pandas/tests/frame/test_apply.py",
      "pandas/tests/frame/test_asof.py",
      "pandas/tests/frame/test_axis_select_reindex.py",
      "pandas/tests/frame/test_block_internals.py",
      "pandas/tests/frame/test_combine_concat.py",
      "pandas/tests/frame/test_constructors.py",
      "pandas/tests/frame/test_indexing.py",
      "pandas/tests/frame/test_misc_api.py",
      "pandas/tests/frame/test_missing.py",
      "pandas/tests/frame/test_nonunique_indexes.py",
      "pandas/tests/frame/test_operators.py",
      "pandas/tests/frame/test_replace.py",
      "pandas/tests/frame/test_reshape.py",
      "pandas/tests/frame/test_sorting.py",
      "pandas/tests/frame/test_subclass.py",
      "pandas/tests/frame/test_timeseries.py",
      "pandas/tests/frame/test_to_csv.py",
      "pandas/tests/groupby/test_filters.py",
      "pandas/tests/groupby/test_groupby.py",
      "pandas/tests/indexes/test_datetimelike.py",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tests/indexing/test_callable.py",
      "pandas/tests/indexing/test_categorical.py",
      "pandas/tests/indexing/test_floats.py",
      "pandas/tests/indexing/test_indexing.py",
      "pandas/tests/plotting/test_datetimelike.py",
      "pandas/tests/plotting/test_frame.py",
      "pandas/tests/series/test_analytics.py",
      "pandas/tests/series/test_constructors.py",
      "pandas/tests/series/test_indexing.py",
      "pandas/tests/series/test_repr.py",
      "pandas/tests/series/test_subclass.py",
      "pandas/tests/test_categorical.py",
      "pandas/tests/test_generic.py",
      "pandas/tests/test_internals.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tests/test_panel.py",
      "pandas/tests/test_panel4d.py",
      "pandas/tests/test_panelnd.py",
      "pandas/tests/test_strings.py",
      "pandas/tests/test_window.py",
      "pandas/tools/tests/test_concat.py",
      "pandas/tools/tests/test_join.py",
      "pandas/tools/tests/test_merge.py",
      "pandas/tools/tests/test_merge_ordered.py",
      "pandas/tools/tests/test_pivot.py",
      "pandas/tseries/tests/test_period.py",
      "pandas/tseries/tests/test_resample.py",
      "pandas/tseries/tests/test_timedeltas.py",
      "pandas/tseries/tests/test_timeseries.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15125,
    "reporter": "adbull",
    "created_at": "2017-01-13T16:40:13+00:00",
    "closed_at": "2017-01-15T16:11:15+00:00",
    "resolver": "rouzazari",
    "resolved_in": "0e219d79dcc6815ed651c2fc6d1b85e1e0d71bd6",
    "resolver_commit_num": 1,
    "title": "Series constructor ignores copy=True when dtype agrees",
    "body": "#### Code Sample\r\n\r\n\r\n\r\n#### Problem description\r\n\r\nConstructing a Series with copy=True should return a copy of the original, but when the dtype argument agrees with the source dtype, no copy is performed.\r\n\r\n#### Expected Output\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.1.13-100.fc21.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: C\r\nLANG: C\r\nLOCALE: None.None\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.1\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 4.2.0\r\nsphinx: 1.4.8\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: 1.1.0\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.3\r\nopenpyxl: 2.4.0\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.3\r\nlxml: 3.6.4\r\nbs4: 4.5.1\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: 1.1.3\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.43.0\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Dtypes",
      "Reshaping",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "We only seem to have a single test for ``copy=True`` and ``dtype=`` (and that's for actually changing a dtype. So yes I agree this should copy. \r\n\r\nwant to do a PR? (tests are in tests/series/test_constructor).",
      "I have a working fix for this. I'll submit a PR shortly."
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 19,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/series.py",
      "pandas/tests/series/test_constructors.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15130,
    "reporter": "jreback",
    "created_at": "2017-01-13T21:23:22+00:00",
    "closed_at": "2017-01-20T12:21:53+00:00",
    "resolver": "jreback",
    "resolved_in": "14d3a98742a7c92e5bebc4aa4cd493c57c24c764",
    "resolver_commit_num": 4180,
    "title": "ERR: too strict validation on groupby.rolling with time-aware freq",
    "body": "-pandas-rolling-aggregation-over-date-range-by-group-python-2-7-windo/41643179?noredirect=1#comment70486923_41643179\r\n\r\n\r\n\r\n\r\nThis check doesn't need to occur when we are grouping\r\n\r\n\r\nThis is ok\r\n",
    "labels": [
      "Bug",
      "Difficulty Intermediate",
      "Effort Low",
      "Error Reporting",
      "Groupby",
      "Reshaping"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 59,
    "deletions": 14,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/window.py",
      "pandas/tests/test_window.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15132,
    "reporter": "Bigbrd",
    "created_at": "2017-01-14T22:02:23+00:00",
    "closed_at": "2017-01-19T13:54:35+00:00",
    "resolver": "rouzazari",
    "resolved_in": "7d5c354953aa798080a8976b7d6b6d15b4975981",
    "resolver_commit_num": 2,
    "title": "pd.read_json(file, lines=True) does not work if json has quotes inside it",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nList has quotes inside the json data. Expected to read this data line by line, but we get a UnicodeDecodeError at the position of that inner quote in the description\r\n\r\n#### Expected Output\r\nread successful\r\n\r\n#### Output:\r\n\r\nTraceback (most recent call last):\r\n    data = pd.read_json(fileName, lines=True)\r\n  File \"/usr/local/lib/python2.7/site-packages/pandas/io/json.py\", line 275, in r\r\nead_json\r\n    json = u'[' + u','.join(lines) + u']'\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 4924: ordina\r\nl not in range(128)\r\n\r\n",
    "labels": [
      "IO JSON",
      "2/3 Compat",
      "Difficulty Intermediate",
      "Effort Low",
      "Unicode"
    ],
    "comments": [
      "pd.show_versions() as the issue tracker indicates ",
      "updated.",
      "JSON validator says that isn't valid JSON.",
      "Agree on the JSON provided by @Bigbrd not being valid; however, I tried it with a simple valid JSON string and received the same results.\r\n\r\n### Potential UnicodeDecodeError for non-ascii strings in Python 2.7\r\n\r\nContents of ``/tmp/test.json`` (note the ``0xe2`` character after the first \"foo\"):\r\n```JSON\r\n{\"a\": \"foo\u201d.\", \"b\": \"bar.\"}\r\n{\"a\": \"foo\", \"b\": \"bar\"}\r\n```\r\n\r\nResult:\r\n```python\r\nimport pandas as pd\r\npd.read_json('/tmp/test.json', lines=True)\r\n...\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)\r\n```\r\n---\r\n### Why does this happen?\r\n\r\nThis can happen in Python 2.7 if the default encoding is  set to ``ascii`` (check ``sys.getdefaultencoding()``). ``StringIO`` will convert the input string to ``ascii`` when ``lines=True``, resulting in a ``UnicodeDecodeError`` because of mixing ``utf-8`` and ``ascii`` strings.\r\n\r\n```python\r\n    if lines:\r\n        lines = list(StringIO(json.strip()))  # <<- converts 'utf-8' to 'ascii'\r\n        json = u'[' + u','.join(lines) + u']'  # <<- mixes 'utf-8' and 'ascii'\r\n```\r\n\r\nNote that a user can set the default encoding using ``sys.setdefaultencoding('utf-8')`` but this is discouraged and is a no-op in Python 3 ([see SO article here](http://stackoverflow.com/questions/3828723/why-should-we-not-use-sys-setdefaultencodingutf-8-in-a-py-script/6742053))\r\n[](url)\r\n\r\n---\r\n\r\n### Next steps\r\n\r\nI would consider myself a novice on fixing this type of issue, but one (hack-y) solution could be to decode the string to ``utf-8`` before calling ``StringIO``:\r\n\r\n```python\r\n    if lines:\r\n        if compat.PY2:\r\n            json = json.decode(encoding or 'utf-8')\r\n        lines = list(StringIO(json.strip()))\r\n        json = u'[' + u','.join(lines) + u']'\r\n```\r\n\r\nInterestingly, ``nosetests`` has the default encoding set to ``utf-8`` (I checked ``sys.getdefaultencoding()`` while running ``nosetests pandas/io/tests/json/``), so I'm not sure how to write a test for this other than changing the default encoding to ``ascii``.",
      "Yes please ignore my crappy fake json, @rouzazari covered my scenario perfectly. \r\n\r\nThe solution proposed with switching to utf-8 before StringIO does sound like an efficient solution for my scenario. +1 if we can implement that or something like it\r\n\r\n\r\n",
      "interesting. So py27 is broken again. ok @Bigbrd or @rouzazari a PR would be welcome.\r\n\r\nwe have a context manager ``pandas.util.testing.stdin_encoding`` which I think will allow you to check this in testing (or maybe need to create a similar one to save / change the system encoding / restore the original).\r\n",
      "Seems like this has been a compatibility issue since ``lines=True`` was introduced with 6efd743c. \r\n\r\nI'm submitting a PR with my recommended change, which is simply to **remove the forced conversion to unicode**, as it seems like ``FrameParser`` and ``SeriesParser`` handle this just fine (and it is not an issue for non-lines JSON files)."
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "commented",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 22,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/io/json.py",
      "pandas/io/tests/json/test_pandas.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15140,
    "reporter": "Rufflewind",
    "created_at": "2017-01-16T05:16:01+00:00",
    "closed_at": "2017-01-20T12:31:32+00:00",
    "resolver": "Rufflewind",
    "resolved_in": "681e6a9b07271a0955e6780e476ab2d7101e549c",
    "resolver_commit_num": 0,
    "title": "Segmentation fault with `read_csv(io.StringIO(\"a\\na\"), float_precision=\"round_trip\")`",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n\r\nThe input needs to be at least two lines and must contain non-numerical data.\r\n\r\nExperienced this problem on Arch Linux with Python 3.\r\n\r\n#### Problem description\r\n\r\n*Why is the current behaviour a problem?* (1) I can't parse a CSV file containing text with `round_trip` precision (2) Possible security vulnerability (3) It fills up my hard drive with core dumps\r\n\r\n#### Expected Output\r\n\r\nNothing.\r\n\r\n#### Actual Output\r\n\r\n<details>\r\n<pre>\r\n#0  0x00007ffff7440350 in PyErr_Restore () from /usr/lib/libpython3.6m.so.1.0\r\n#1  0x00007ffff7440a62 in PyErr_FormatV () from /usr/lib/libpython3.6m.so.1.0\r\n#2  0x00007ffff7440b24 in PyErr_Format () from /usr/lib/libpython3.6m.so.1.0\r\n#3  0x00007ffff73c2c60 in PyOS_string_to_double ()\r\n   from /usr/lib/libpython3.6m.so.1.0\r\n#4  0x00007fffe5359a73 in ?? ()\r\n   from /usr/lib/python3.6/site-packages/pandas/parser.cpython-36m-x86_64-linux-gnu.so\r\n#5  0x00007fffe53675c6 in ?? ()\r\n   from /usr/lib/python3.6/site-packages/pandas/parser.cpython-36m-x86_64-linux-gnu.so\r\n#6  0x00007fffe535c74a in ?? ()\r\n   from /usr/lib/python3.6/site-packages/pandas/parser.cpython-36m-x86_64-linux-gnu.so\r\n#7  0x00007fffe537db89 in ?? ()\r\n   from /usr/lib/python3.6/site-packages/pandas/parser.cpython-36m-x86_64-linux-gnu.so\r\n#8  0x00007ffff74220b6 in PyCFunction_Call () from /usr/lib/libpython3.6m.so.1.0\r\n#9  0x00007fffe534fe29 in ?? ()\r\n   from /usr/lib/python3.6/site-packages/pandas/parser.cpython-36m-x86_64-linux-gnu.so\r\n#10 0x00007fffe536f78e in ?? ()\r\n   from /usr/lib/python3.6/site-packages/pandas/parser.cpython-36m-x86_64-linux-gnu.so\r\n#11 0x00007fffe5344cf4 in ?? ()\r\n   from /usr/lib/python3.6/site-packages/pandas/parser.cpython-36m-x86_64-linux-gnu.so\r\n#12 0x00007ffff7421ddc in _PyCFunction_FastCallDict ()\r\n   from /usr/lib/libpython3.6m.so.1.0\r\n#13 0x00007ffff740c21f in ?? () from /usr/lib/libpython3.6m.so.1.0\r\n#14 0x00007ffff73cd067 in _PyEval_EvalFrameDefault ()\r\n   from /usr/lib/libpython3.6m.so.1.0\r\n#15 0x00007ffff740ade9 in ?? () from /usr/lib/libpython3.6m.so.1.0\r\n#16 0x00007ffff740bf9a in ?? () from /usr/lib/libpython3.6m.so.1.0\r\n#17 0x00007ffff740c303 in ?? () from /usr/lib/libpython3.6m.so.1.0\r\n#18 0x00007ffff73cd067 in _PyEval_EvalFrameDefault ()\r\n   from /usr/lib/libpython3.6m.so.1.0\r\n#19 0x00007ffff740aaa1 in ?? () from /usr/lib/libpython3.6m.so.1.0\r\n#20 0x00007ffff740bf9a in ?? () from /usr/lib/libpython3.6m.so.1.0\r\n#21 0x00007ffff740c303 in ?? () from /usr/lib/libpython3.6m.so.1.0\r\n#22 0x00007ffff73cd067 in _PyEval_EvalFrameDefault ()\r\n   from /usr/lib/libpython3.6m.so.1.0\r\n#23 0x00007ffff740bd4a in ?? () from /usr/lib/libpython3.6m.so.1.0\r\n#24 0x00007ffff740c303 in ?? () from /usr/lib/libpython3.6m.so.1.0\r\n#25 0x00007ffff73cd067 in _PyEval_EvalFrameDefault ()\r\n   from /usr/lib/libpython3.6m.so.1.0\r\n#26 0x00007ffff740ade9 in ?? () from /usr/lib/libpython3.6m.so.1.0\r\n#27 0x00007ffff740bf9a in ?? () from /usr/lib/libpython3.6m.so.1.0\r\n#28 0x00007ffff740c303 in ?? () from /usr/lib/libpython3.6m.so.1.0\r\n#29 0x00007ffff73cde87 in _PyEval_EvalFrameDefault ()\r\n   from /usr/lib/libpython3.6m.so.1.0\r\n#30 0x00007ffff740c757 in PyEval_EvalCodeEx () from /usr/lib/libpython3.6m.so.1.0\r\n#31 0x00007ffff73ccd4b in PyEval_EvalCode () from /usr/lib/libpython3.6m.so.1.0\r\n#32 0x00007ffff74ae112 in ?? () from /usr/lib/libpython3.6m.so.1.0\r\n#33 0x00007ffff74b097d in PyRun_FileExFlags () from /usr/lib/libpython3.6m.so.1.0\r\n#34 0x00007ffff74b0b67 in PyRun_SimpleFileExFlags ()\r\n   from /usr/lib/libpython3.6m.so.1.0\r\n#35 0x00007ffff74a4a91 in Py_Main () from /usr/lib/libpython3.6m.so.1.0\r\n#36 0x0000000000400a5d in main ()\r\n</pre>\r\n</details>\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n<pre>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.0.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.8.13-1-ARCH\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8<br/>\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 33.0.0\r\nCython: 0.25.2\r\nnumpy: 1.11.3\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 2.0.0rc2+2914.g1fa4dd705.dirty\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n 0.9.2\r\napiclient: 1.5.5\r\nsqlalchemy: 1.1.4\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.4\r\nboto: None\r\npandas_datareader: None\r\n</pre>\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Difficulty Intermediate",
      "Effort Low",
      "IO CSV"
    ],
    "comments": [
      "It appears to have died [while trying to read the thread state](https://github.com/python/cpython/blob/a967fce5f634e5c90e3951887d27cfd3bb70e2f2/Python/errors.c#L42):\r\n\r\n~~~c\r\noldtype = tstate->curexc_type;\r\n~~~\r\n\r\n`tstate` is null because [the global interpreter lock has been released](https://github.com/pandas-dev/pandas/blob/52aee10acc390fc230eead4287e3341692a3500a/pandas/src/parser/tokenizer.c#L2292-L2300):\r\n\r\n~~~py\r\nwith nogil:\r\n    error = _try_double_nogil(parser, col, line_start, line_end,\r\n                              na_filter, na_hashset, use_na_flist,\r\n                              na_fset, NA, data, &na_count)\r\n~~~\r\n\r\nThis is problematic, as the `round_trip` converter does [call back into Python](https://github.com/pandas-dev/pandas/blob/52aee10acc390fc230eead4287e3341692a3500a/pandas/src/parser/tokenizer.c#L1774-L1781):\r\n\r\n~~~c\r\ndouble round_trip(const char *p, char **q, char decimal, char sci, char tsep,\r\n                  int skip_trailing) {\r\n#if PY_VERSION_HEX >= 0x02070000\r\n    return PyOS_string_to_double(p, q, 0);\r\n#else\r\n    return strtod(p, q);\r\n#endif\r\n}\r\n~~~\r\n\r\nThe funny thing is that even if I delete `with nogil:`, it still fails with a Python exception, because the code in [_try_double_nogil](https://github.com/pandas-dev/pandas/blob/7ad6c65c3ce6c8fd5e3b82306c028d4fb115483c/pandas/parser.pyx#L1744) was never even designed to handle Python exceptions to begin with.",
      "This appears to work around the problem (kept the GIL and silenced the Python exception):\r\n\r\n~~~diff\r\ndiff --git a/pandas/parser.pyx b/pandas/parser.pyx\r\nindex bd793c98e..dc0292e5b 100644\r\n--- a/pandas/parser.pyx\r\n+++ b/pandas/parser.pyx\r\n@@ -1699,10 +1699,9 @@ cdef _try_double(parser_t *parser, int col, int line_start, int line_end,\r\n     result = np.empty(lines, dtype=np.float64)\r\n     data = <double *> result.data\r\n     na_fset = kset_float64_from_list(na_flist)\r\n-    with nogil:\r\n-        error = _try_double_nogil(parser, col, line_start, line_end,\r\n-                                  na_filter, na_hashset, use_na_flist,\r\n-                                  na_fset, NA, data, &na_count)\r\n+    error = _try_double_nogil(parser, col, line_start, line_end,\r\n+                              na_filter, na_hashset, use_na_flist,\r\n+                              na_fset, NA, data, &na_count)\r\n     kh_destroy_float64(na_fset)\r\n     if error != 0:\r\n         return None, None\r\ndiff --git a/pandas/src/parser/tokenizer.c b/pandas/src/parser/tokenizer.c\r\nindex 87e17fe5f..77c36ef8a 100644\r\n--- a/pandas/src/parser/tokenizer.c\r\n+++ b/pandas/src/parser/tokenizer.c\r\n@@ -1774,7 +1774,9 @@ double precise_xstrtod(const char *str, char **endptr, char decimal, char sci,\r\n double round_trip(const char *p, char **q, char decimal, char sci, char tsep,\r\n                   int skip_trailing) {\r\n #if PY_VERSION_HEX >= 0x02070000\r\n-    return PyOS_string_to_double(p, q, 0);\r\n+    double r = PyOS_string_to_double(p, q, 0);\r\n+    PyErr_Clear();\r\n+    return r;\r\n #else\r\n     return strtod(p, q);\r\n #endif\r\n~~~",
      "yeah this looks to be violating gil holding. Welcome for you to add a test / fix.\r\n\r\nprob simply best just hold the gil if float precision is specified, since this is not the default.",
      "I made a pull request here: https://github.com/pandas-dev/pandas/pull/15148"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented"
    ],
    "changed_files": 5,
    "additions": 49,
    "deletions": 17,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/io/tests/parser/c_parser_only.py",
      "pandas/parser.pyx",
      "pandas/src/parser/tokenizer.c",
      "pandas/src/parser/tokenizer.h"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15155,
    "reporter": "watercrossing",
    "created_at": "2017-01-18T18:36:35+00:00",
    "closed_at": "2017-01-19T20:21:53+00:00",
    "resolver": "watercrossing",
    "resolved_in": "4c65d5fc79ea435bc4e47d8af2914cba324117bd",
    "resolver_commit_num": 0,
    "title": "Groupby level fails to enumerate groups",
    "body": "#### Code Sample\r\n\r\n\r\n#### Problem description\r\n\r\ninstead of returning the data as expected, an error is thrown:\r\n\r\n\r\n#### Expected Output\r\n\r\nI guess an alternative way to get this output is `test.loc[\"a\"]` - which yields the expected output below:\r\n\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.10.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.32-431.11.2.el6.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: en_GB.utf8\r\nLANG: en_GB.utf8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 32.3.1\r\nCython: None\r\nnumpy: 1.11.3\r\nscipy: 0.18.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.5.1\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.9999999\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.4\r\nboto: None\r\npandas_datareader: None\r\n\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Categorical",
      "Groupby",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "This is a bug, which is hidden because of this missing method on ``CategoricalIndex``. If you would like to do a PR which adds this (and tests of course).\r\n\r\n```\r\ndiff --git a/pandas/indexes/category.py b/pandas/indexes/category.py\r\nindex 2c89f72..e3ffa40 100644\r\n--- a/pandas/indexes/category.py\r\n+++ b/pandas/indexes/category.py\r\n@@ -255,6 +255,9 @@ class CategoricalIndex(Index, base.PandasDelegate):\r\n     def ordered(self):\r\n         return self._data.ordered\r\n \r\n+    def _reverse_indexer(self):\r\n+        return self._data._reverse_indexer()\r\n+\r\n     def __contains__(self, key):\r\n         hash(key)\r\n         return key in self.values\r\n```"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 24,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/indexes/category.py",
      "pandas/tests/groupby/test_categorical.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15160,
    "reporter": "jnothman",
    "created_at": "2017-01-19T04:51:05+00:00",
    "closed_at": "2017-02-16T17:47:46+00:00",
    "resolver": "jeffcarey",
    "resolved_in": "9b5d8488e8184da0507c09482f23ebfff34ecc43",
    "resolver_commit_num": 3,
    "title": "to_excel should (optionally) freeze panes",
    "body": "`DataFrame.to_excel()` is currently helpful in styling column and row headers distinctly from values. One other simple feature Excel provides to manage headers is \"freeze panes\". Panes can be frozen such that the column and row headers are fixed. This is achieved in openpyxl with `worksheet.freeze_panes = top_left_value_cell` and in xlwt with `worksheet.freeze_panes(bottommost_header_row, leftmost_header_col)`.\r\n\r\nI think making this available, as an option or by default, would give a valuable usability boost to the exported Excel spreadsheets.",
    "labels": [
      "Enhancement",
      "IO Excel",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "sure *lots* of thinks that pandas could do with the updates in the openpyxl style API. love to have PR's!",
      "Here's the same for `xlsxwriter`.  My preference would be not have it as a default, but agree it would be a nice addition.\r\n\r\nhttp://xlsxwriter.readthedocs.io/example_panes.html\r\n",
      "The other header display feature that might be woeth having as an option is\nan AutoFilter.\n\nOn 20 January 2017 at 23:39, chris-b1 <notifications@github.com> wrote:\n\n> Here's the same for xlsxwriter. My preference would be not have it as a\n> default, but agree it would be a nice addition.\n>\n> http://xlsxwriter.readthedocs.io/example_panes.html\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pandas-dev/pandas/issues/15160#issuecomment-274063115>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAEz6wVGhKQXvPiugDQpcP_STKumNVrpks5rUKsGgaJpZM4Lns3W>\n> .\n>\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 6,
    "additions": 74,
    "deletions": 12,
    "changed_files_list": [
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/io/excel.py",
      "pandas/tests/io/test_excel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15190,
    "reporter": "jreback",
    "created_at": "2017-01-22T17:18:09+00:00",
    "closed_at": "2017-01-30T14:15:09+00:00",
    "resolver": "jreback",
    "resolved_in": "b1c3c4838b168ae84c16853967af9f82cae4d556",
    "resolver_commit_num": 4195,
    "title": "BLD: remove warnings for building parser / hashtable extensions",
    "body": "-ci.org/jreback/pandas/jobs/194236743\r\n\r\nsee what we can do about some of these warnings, I think *some* are the result of the uint64 hashtable code. this would be after -dev/pandas/pull/15188 (merging shortly)\r\n\r\nin parser.pyx\r\n\r\n\r\nin hashtable.pyx\r\n",
    "labels": [
      "Build",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "cc @gfyoung ",
      "Some of these I noticed are due to annoying `size_t` vs `int` conversions in the Cython world.  Quite cumbersome to remove at times.",
      "actually @gfyoung #15259 fixes all these. it was just a couple of changes.",
      "@gfyoung if you want a real challenge, then removing the warnings from windows..... https://ci.appveyor.com/project/jreback/pandas-465/build/1.0.3533/job/s3uiyh1s9r95mn9s"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 7,
    "additions": 86,
    "deletions": 59,
    "changed_files_list": [
      "pandas/parser.pyx",
      "pandas/src/algos_groupby_helper.pxi.in",
      "pandas/src/algos_rank_helper.pxi.in",
      "pandas/src/hashtable_class_helper.pxi.in",
      "pandas/src/hashtable_func_helper.pxi.in",
      "pandas/src/parser/io.c",
      "pandas/src/parser/tokenizer.c"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15197,
    "reporter": "jreback",
    "created_at": "2017-01-23T17:22:32+00:00",
    "closed_at": "2017-01-24T14:20:30+00:00",
    "resolver": "jreback",
    "resolved_in": "dc400587a3e792d90f5db0be6caff7a4e12574c0",
    "resolver_commit_num": 4186,
    "title": "PERF: DataFrame.groupby.nunique is non-performant",
    "body": "xref #14376 \r\n\r\n\r\n\r\n``Series.groupby.nunique`` has a very performant implementation, but the way the ``DataFrame.groupby.nunique`` is implemented (via ``.apply``) it ends up in a python loop over the groups, which nullifies this.\r\n\r\nshould be straightforward to fix this. need to make sure to test with ``as_index=True/False``",
    "labels": [
      "Difficulty Intermediate",
      "Effort Low",
      "Groupby",
      "Reshaping",
      "Performance"
    ],
    "comments": [
      "cc @xflr6 "
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 33,
    "deletions": 11,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/groupby.py",
      "pandas/tests/groupby/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15199,
    "reporter": "jreback",
    "created_at": "2017-01-23T20:37:45+00:00",
    "closed_at": "2017-01-24T18:52:39+00:00",
    "resolver": "jreback",
    "resolved_in": "2c45ed37530722541ebf720f5748ad92c392f305",
    "resolver_commit_num": 4187,
    "title": "BLD: numpy-dev causing some issues",
    "body": "-ci.org/pandas-dev/pandas/jobs/194558937\r\n\r\n was just merged, which broke our tests \r\n\r\n:>\r\n",
    "labels": [
      "Build",
      "Compat"
    ],
    "comments": [
      "cc @jcrist",
      "Looks like the issue is this line: https://github.com/pandas-dev/pandas/blob/master/pandas/core/ops.py#L453.\r\n\r\nBefore that numpy PR, that would always evaluate `lib.infer_dtype`, since `bool(scalar_dtype)` was always False.",
      "yep, going to put a patch in shortly."
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 6,
    "additions": 34,
    "deletions": 29,
    "changed_files_list": [
      "ci/install_travis.sh",
      "ci/requirements-3.5_NUMPY_DEV.build.sh",
      "pandas/compat/numpy/__init__.py",
      "pandas/core/ops.py",
      "pandas/tseries/index.py",
      "pandas/tseries/tdi.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15234,
    "reporter": "david-hoffman",
    "created_at": "2017-01-26T05:01:56+00:00",
    "closed_at": "2017-02-01T20:45:37+00:00",
    "resolver": "david-hoffman",
    "resolved_in": "48fc9d613323ada9702a7d5c78c23eb0e8cae8a8",
    "resolver_commit_num": 0,
    "title": "BUG: groupby.count fails on windows with large categorical index",
    "body": "#### Problem description\r\n\r\nOn windows machines the native `int` is `int32` which causes an overflow error in `cartesian_product` in `tools.util`. The problem line is `lenX = np.fromiter((len(x) for x in X), dtype=int)`\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 63 Stepping 2, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.2\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: 0.8.2\r\nIPython: 5.1.0\r\nsphinx: 1.3.1\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.2.2\r\nnumexpr: 2.6.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.4.1\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 0.9.6\r\nlxml: 3.7.2\r\nbs4: 4.5.3\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: 1.1.5\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.4\r\nboto: 2.45.0\r\npandas_datareader: None\r\n\r\n</details>\r\n",
    "labels": [
      "Groupby",
      "Windows",
      "Bug",
      "Difficulty Intermediate",
      "Effort Low",
      "Categorical"
    ],
    "comments": [
      "show a reproducible example",
      "can you show a specific example where this fails?",
      "```python\r\nln[1]: import pandas as pd\r\n\r\nIn [2]: import numpy as np\r\n\r\nIn [3]: data = np.random.randn(1000000, 3)\r\n\r\nIn [4]: rangex = np.linspace(-2, 2, 2048)\r\n\r\nIn [5]: rangey = np.linspace(-1, 1, 1024)\r\n\r\nIn [6]: rangez = np.linspace(-3, 3, 4096)\r\n\r\nIn [7]: np.iinfo(int).max > rangex.size * rangey.size * rangez.size\r\nOut[8]: False\r\n\r\nIn [8]: df = pd.DataFrame(data, columns=[\"x\", \"y\", \"z\"])\r\n\r\nIn [9]: grouped = df.groupby([pd.cut(df.x, rangex), pd.cut(df.y, rangey), pd.cut(df.z, rangez)])\r\n\r\nIn [10]: grouped.count()\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-11-220cef394518> in <module>()\r\n----> 1 grouped.count()\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py in count(self)\r\n   3886         blk = map(make_block, map(counter, val), loc)\r\n   3887\r\n-> 3888         return self._wrap_agged_blocks(data.items, list(blk))\r\n   3889\r\n   3890\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py in _wrap_agged_blocks(self, items, blocks)\r\n   3801             result = result.T\r\n   3802\r\n-> 3803         return self._reindex_output(result)._convert(datetime=True)\r\n   3804\r\n   3805     def _reindex_output(self, result):\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py in _reindex_output(self, result)\r\n   3823         levels_list = [ping.group_index for ping in groupings]\r\n   3824         index, _ = MultiIndex.from_product(\r\n-> 3825             levels_list, names=self.grouper.names).sortlevel()\r\n   3826\r\n   3827         if self.as_index:\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\multi.py in from_product(cls, iterables, sortorder, names)\r\n   1023\r\n   1024         labels, levels = _factorize_from_iterables(iterables)\r\n-> 1025         labels = cartesian_product(labels)\r\n   1026\r\n   1027         return MultiIndex(levels=levels, labels=labels, sortorder=sortorder,\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\tools\\util.py in cartesian_product(X)\r\n     70     return [np.tile(np.repeat(np.asarray(com._values_from_object(x)), b[i]),\r\n     71                     np.product(a[i]))\r\n---> 72             for i, x in enumerate(X)]\r\n     73\r\n     74\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\tools\\util.py in <listcomp>(.0)\r\n     70     return [np.tile(np.repeat(np.asarray(com._values_from_object(x)), b[i]),\r\n     71                     np.product(a[i]))\r\n---> 72             for i, x in enumerate(X)]\r\n     73\r\n     74\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py in repeat(a, repeats, axis)\r\n    394     except AttributeError:\r\n    395         return _wrapit(a, 'repeat', repeats, axis)\r\n--> 396     return repeat(repeats, axis)\r\n    397\r\n    398\r\n\r\nValueError: negative dimensions are not allowed\r\n```",
      "ok thanks. I think this is the *same* as #14942, though on windows this fails before getting to the *eats-all-memory* part.\r\n\r\nwelcome for you to do a PR to fix this.",
      "Is it possible to do a PR without forking the whole repository?",
      "@david-hoffman theoretically (for a very very simple thing, you can do it via github), but not for this, this will require some investigation, debugging, and tests.\r\n\r\ncontribution docs are here: http://pandas.pydata.org/pandas-docs/stable/contributing.html"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced"
    ],
    "changed_files": 2,
    "additions": 2,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/tools/util.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15240,
    "reporter": "sdementen",
    "created_at": "2017-01-27T05:15:45+00:00",
    "closed_at": "2017-01-28T18:29:11+00:00",
    "resolver": "sdementen",
    "resolved_in": "66d8c41db4e1cb39e06c9e1f712f7d1f105c4478",
    "resolver_commit_num": 0,
    "title": "Timestamp.replace raises ValueError instead of TypeError when given wrong argument",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nThe reimplementation of Timestamp.replace (-dev/pandas/commit/f8bd08e9c2fc6365980f41b846bbae4b40f08b83#diff-05de2950b2c86c9828531957e02d1b87L662) raises a ValueError when a keyword argument does not exist.\r\nPrevious implementation used datetime.replace that raises a TypeError when an argument is not valid.\r\nThis is a regression and it is probably better to keep API compatibility with the datetime.replace function (modulo extra functionalities).\r\nFor autodocumentation/autocomplete purposes, it could also be useful to have explicitly in the function signature the keywords, so instead of::\r\n\r\n`  def replace(self, **kwds):\r\n`\r\nhave::\r\n \r\n`  def replace(self, year=None, month=None, day=None, hour=None, minute=None, second=None, microsecond=None, nanosecond=None, tzinfo=None)\r\n`\r\n\r\n#### Actual Output\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"pandas\\tslib.pyx\", line 715, in pandas.tslib.Timestamp.replace (pandas\\tslib.c:14832)\r\nValueError: invalid name apple passed\r\n\r\n#### Expected Output\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"pandas\\tslib.pyx\", line 715, in pandas.tslib.Timestamp.replace (pandas\\tslib.c:14832)\r\nTypeError: 'apple' is an invalid keyword argument for this function\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 61 Stepping 4, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: None\r\nnumpy: 1.11.3\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.5.2\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.4\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "Difficulty Novice",
      "Effort Low",
      "Error Reporting",
      "Timeseries"
    ],
    "comments": [
      "if you want to do a PR wouldn be great",
      "I am having a try :-)\r\nCould you tell me where I can add an additional test on Timestamp.replace ? I have a hard time locating the tests...",
      "I can't really test my patch on my machine (as I have not the tool suite necessary to compile pandas). Can I start a PR (even unfinished) to test via the Travis/Appveyor tests ? or is it \"bad practice\" ?",
      "@sdementen the tests are exactly in the above link, IOW in pandas/tseries/test_timezones.py\r\n\r\nI would recommend you install a proper dev version of pandas to test: http://pandas.pydata.org/pandas-docs/stable/contributing.html",
      "@jreback I can't find any test of the Timestamp.replace method (except tests testing the replace(tzinfo=xxx)). Are there specific tests for Timestamp.replace ? I was expecting/looking for tests like \r\n```\r\ndt = pandas.Timestamp(\"2016\")\r\ntm.assert(dt.hour,0)\r\ntm.assert(dt.replace(hour=3).hour,3)\r\n```",
      "https://github.com/pandas-dev/pandas/blob/master/pandas/tseries/tests/test_timezones.py#L1170\r\n\r\nyou could add some in ``test_timeseries.py`` if you want (and just leave these for the actual timezone replacement), but doesn't really matter.",
      "Hi All, I've been looking to contribute to pandas for a while, so I've made a PR that I think closes this. followed the guide as best I could."
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "commented",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 41,
    "deletions": 38,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/tseries/tests/test_timezones.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15244,
    "reporter": "jcrist",
    "created_at": "2017-01-27T19:01:08+00:00",
    "closed_at": "2017-03-23T19:10:26+00:00",
    "resolver": "kernc",
    "resolved_in": "56ccad8229824584678e22815f4f180a91309c9d",
    "resolver_commit_num": 5,
    "title": "DOC: groupby aligns Series when passed as groupands",
    "body": "xref #15338 \r\n\r\nCalling groupby with an unaligned index on the same frame in multiple threads can result in incorrect results. The following example demonstrates:\r\n\r\n\r\n\r\nOn my machine, running `python test.py` results in:\r\n\r\n\r\n\r\nA few notes:\r\n\r\n- Calling groupby once beforehand fixes everything. (see comment in `build_frame`). To me this indicates that the groupby call sets up some state (that's cached) that may not be threadsafe. Also, the second call to `pool.map` always returns correct results, only the first calls fail.\r\n- Doing this without threading results in the same answer every time.\r\n- Without filtering by amount, the result is also always correct.\r\n\r\nTested with pandas master, as well as pandas 0.19.0 and up.\r\n",
    "labels": [
      "Groupby",
      "Docs"
    ],
    "comments": [
      "```\r\ndef f(x):\r\n    df = x[x.amount < 20]\r\n    return df.groupby(df.id).amount.count()\r\n```\r\n\r\nin your original function, you are passing ``x.id`` which certainly can change as the filtered frame will be new, but the grouping index is the original. That is very unsafe in general (though pandas may take it)\r\n\r\n\r\n",
      "I can't seem to reproduce it on my machine.\r\n\r\nHowever, you are passing a grouper with a different shape. I actually didn't know that pandas is aligning the grouper. You can also pass the string: `df.groupby('id').amount.count()`",
      "I think @jorisvandenbossche has it right.\r\n\r\n``.groupby`` is aligning the group-and (``x.id``). So this 'works', but I suspect this is in general an unsafe operation (from a thread point of view).\r\n\r\nIf you dont't align then this will be ok I think (IOW, if you pass in the group-and from the same frame).\r\n\r\ncertainly no guarantees :<",
      "Yeah, I think this is why this didn't show up until now - I didn't know pandas supported unaligned indices passed to groupby. Showed up in a user bug https://github.com/dask/dask/issues/1876.\r\n\r\nI can fix this in dask by manually aligning beforehand, but the threadsafe issue still stands. Out of curiousity, why does this fail only on the first call? Some index structure being built up and then cached on later calls?",
      "The passed series (`df.id`) gets reindexed (https://github.com/pandas-dev/pandas/blob/ba057443d1b69bb3735a4b18a18e4e4897231867/pandas/core/groupby.py#L2580), but that still shouldn't modify the original object?",
      "Hmmm, well something is getting modified, as it only fails the first time.",
      "its *shouldn't* ever modify the original object, only the groupby object itself has state, but that *could* be the problem. IOW, this is a cached_property, which I suppose could be interrupted and if the groupby object is shared......",
      "This fails in the code above though, where only the original frame is shared (neither the groupby or the filtered frame is shared). But `df.id` is the same object for all calls - could that be modified by the reindex? Or am I misunderstanding what's being cached here?",
      "This seems to only fail if the index is longer than the grouped frame. Swapping the filter onto the index passes every time (not that this is recommended):\r\n\r\n```python\r\ndef f(x):\r\n    return x.groupby(x[x.amount < 20].id).amount.count()\r\n```"
    ],
    "events": [
      "renamed",
      "cross-referenced",
      "labeled",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "labeled",
      "milestoned",
      "renamed",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 1,
    "additions": 6,
    "deletions": 3,
    "changed_files_list": [
      "pandas/core/generic.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15262,
    "reporter": "KevinBaudin",
    "created_at": "2017-01-30T15:29:29+00:00",
    "closed_at": "2017-02-24T20:18:20+00:00",
    "resolver": "Dr-Irv",
    "resolved_in": "595580464a256fb883e8baa5b6e62f2013f0cf1a",
    "resolver_commit_num": 10,
    "title": "BUG in MultiIndex truncated repr with integer level names",
    "body": "Reproducible example:\r\n\r\n\r\n\r\nSo the truncated repr shows incorrectly the first index level (with integer level name `1`) again for the second level.\r\n\r\n\r\n---\r\nOriginal post:\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\n<table border=\"1\" class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th></th>       <th>merged</th>     </tr>     <tr>       <th>1</th>       <th>2</th>       <th></th>     </tr>   </thead>   <tbody>     <tr>       <th>a.</th>       <th>a.</th>       <td>2</td>     </tr>     <tr>       <th>abel</th>       <th>abel</th>       <td>1</td>     </tr>     <tr>       <th>agnes</th>       <th>agnes</th>       <td>2</td>     </tr>     <tr>       <th rowspan=\"2\" valign=\"top\">alain</th>       <th>alain</th>       <td>8</td>     </tr>     <tr>       <th>alain</th>       <td>2</td>     </tr>   </tbody> </table>\r\n\r\nI have created a multi-index based on 2 columns . \r\nThose two columns wont appear properly, index_column \"2\" being duplicated from \"1\"\r\nWhen displaying up to the 60th first rows of dataframe, it's fine, then it duplicates again the column 1 in the column 2.\r\n\r\n#### Expected Output\r\n\r\n<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th></th>      <th>merged</th>    </tr>    <tr>      <th>1</th>      <th>2</th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>a.</th>      <th>masson-dubois</th>      <td>2</td>    </tr>    <tr>      <th>abel</th>      <th>pinchard</th>      <td>1</td>    </tr>    <tr>      <th>agnes</th>      <th>paquet</th>      <td>2</td>    </tr>    <tr>      <th rowspan=\"2\" valign=\"top\">alain</th>      <th>corcia</th>      <td>8</td>    </tr>    <tr>      <th>hudelot-noellat</th>      <td>2</td>    </tr>  </tbody></table>\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.9.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.4-moby\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.19.1\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 28.8.0\r\nCython: None\r\nnumpy: 1.11.2\r\nscipy: 0.18.0\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.5.0\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.5.1\r\nhtml5lib: None\r\n 0.9.2\r\napiclient: 1.5.1\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Output-Formatting"
    ],
    "comments": [
      "pls show a copy-pastable example that doesn't rely on opening your file",
      "Sorry, I didn't manage to find a simple reproducible example.\r\nThe problem only appeared on this particular dataframe at the moment. \r\n\r\nPS : It's only a 22Kb file",
      "this is not reproducible. you can try ``.sort_index()``",
      "I could reproduce it but `.sort_index()` did not work.\r\nThe data seem not corrupted, only displaying is wrong",
      "@KevinBaudin The file is not available anymore?",
      "@jorisvandenbossche edited with new link, sorry.",
      "@KevinBaudin The cause of the issue are the index level names ([1, 2]). If you set those to something else, you will see that the issue is resolved:\r\n\r\n```\r\nIn [6]: df\r\nOut[6]: \r\n                                 merged\r\n1               2                      \r\na.              a.                    2\r\nabel            abel                  1\r\nagnes           agnes                 2\r\n...\r\n[100 rows x 1 columns]\r\n\r\nIn [7]: df.index.names = ['a', 'b']\r\n\r\nIn [8]: df\r\nOut[8]: \r\n                                   merged\r\na               b                        \r\na.              masson-dubois           2\r\nabel            pinchard                1\r\nagnes           paquet                  2\r\n...\r\n[100 rows x 1 columns]\r\n```\r\n\r\nThe reason for this is the integer level names (confusion between integer number of first (0) or second (1) level, or the level names (1 and 2)). \r\nSo it seems that the repr used in `.head()` is correctly dealing with this distinction, but the general repr not.",
      "Smaller reproducible example:\r\n\r\n```\r\nIn [10]: df = pd.DataFrame({'col': range(9)}, index=pd.MultiIndex.from_product([['A0', 'A1', 'A2'], ['B0', 'B1', 'B2']], names=[1,2]))\r\n\r\nIn [11]: df\r\nOut[11]: \r\n       col\r\n1  2      \r\nA0 B0    0\r\n   B1    1\r\n   B2    2\r\nA1 B0    3\r\n   B1    4\r\n   B2    5\r\nA2 B0    6\r\n   B1    7\r\n   B2    8\r\n\r\nIn [12]: pd.options.display.max_rows = 4\r\n\r\nIn [13]: df\r\nOut[13]: \r\n       col\r\n1  2      \r\nA0 A0    0\r\n   A0    1\r\n...    ...\r\nA2 A2    7\r\n   A2    8\r\n\r\n[9 rows x 1 columns]\r\n```\r\n\r\nSo it is the truncated repr that has this issue.",
      "@jorisvandenbossche \u2764\ufe0f \ud83d\udc4d \r\n",
      "I've started looking at this.  Seems to be an issue in `pd.concat()`:\r\n```\r\nIn [2]: df = pd.DataFrame({'col': range(9)}, index=pd.MultiIndex.from_product([ ['A0', 'A1', 'A2'], ['B0', 'B1', 'B2']], names=[1,2]))\r\n\r\nIn [3]: df.iloc[:2,:]\r\nOut[3]:\r\n       col\r\n1  2\r\nA0 B0    0\r\n   B1    1\r\n\r\nIn [4]: df.iloc[-2:,:]\r\nOut[4]:\r\n       col\r\n1  2\r\nA2 B1    7\r\n   B2    8\r\n\r\nIn [5]: pd.concat((df.iloc[:2,:],df.iloc[-2:,:]))\r\nOut[5]:\r\n       col\r\n1  2\r\nA0 A0    0\r\n   A0    1\r\nA2 A2    7\r\n   A2    8\r\n```\r\n\r\nThat last result is incorrect.  Should the name of this issue be changed? (@jorisvandenbossche)\r\n",
      "I think this is a dupe of: https://github.com/pandas-dev/pandas/issues/12223\r\n\r\nif this is the case, just use an example from there as well in tests."
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "closed",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "reopened",
      "labeled",
      "labeled",
      "unlabeled",
      "commented",
      "renamed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned"
    ],
    "changed_files": 10,
    "additions": 46,
    "deletions": 22,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/reshape.py",
      "pandas/formats/format.py",
      "pandas/indexes/base.py",
      "pandas/indexes/multi.py",
      "pandas/io/sql.py",
      "pandas/tests/frame/test_combine_concat.py",
      "pandas/util/doctools.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15289,
    "reporter": "ExpHP",
    "created_at": "2017-02-02T04:41:49+00:00",
    "closed_at": "2017-02-02T20:26:55+00:00",
    "resolver": "ExpHP",
    "resolved_in": "f6cfaabad9b9de6d0382e51a77b080723f84d778",
    "resolver_commit_num": 0,
    "title": "series.replace with empty dictlike raises ValueError: not enough values to unpack",
    "body": "#### Problem description\r\n\r\n`DataFrame.replace` and `Series.replace` fail unceremoniously on an empty Series or dict argument; I would expect it to simply do nothing.\r\n\r\nWhile there is a fair bit of type-based wizardry going on in this function, I don't suppose there is any actual ambiguity in how this edge case should behave?\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n\r\n#### Output\r\n\r\n\r\n\r\n#### Expected Output\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-59-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 8.1.2\r\nsetuptools: 27.2.0\r\nCython: 0.24.1\r\nnumpy: 1.11.1\r\nscipy: 0.18.0\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.6.1\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: None\r\nnumexpr: 2.6.2\r\nmatplotlib: 1.5.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: 0.7.3\r\nlxml: None\r\nbs4: 4.4.1\r\nhtml5lib: 0.999\r\n 0.9.1\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "Difficulty Novice",
      "Effort Low",
      "Missing-data",
      "Reshaping"
    ],
    "comments": [
      "yep, this look like a bug. pull-requests to fix welcome!",
      "I am on it"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "renamed",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 20,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/generic.py",
      "pandas/tests/frame/test_replace.py",
      "pandas/tests/series/test_replace.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15328,
    "reporter": "jesrael",
    "created_at": "2017-02-07T07:55:50+00:00",
    "closed_at": "2017-02-16T17:41:27+00:00",
    "resolver": "abaldenko",
    "resolved_in": "c7300ea9ccf6c8b4eeb5a4ae59dc2419753c9b18",
    "resolver_commit_num": 0,
    "title": "BUG: Concat with inner join and empty DataFrame",
    "body": "Function `concat` with parameter `join='inner'` not return empty `DataFrame`:\r\n\r\n    DF_empty = pd.DataFrame()\r\n    DF_a = pd.DataFrame({'a': [1, 2]}, index=[0, 1])\r\n    print (DF_empty)\r\n    Empty DataFrame\r\n    Columns: []\r\n    Index: []\r\n    \r\n    print (DF_a)\r\n       a\r\n    0  1\r\n    1  2\r\n    \r\n    print(pd.concat([DF_empty, DF_a], axis=1, join='inner'))\r\n       a\r\n    0  1\r\n    1  2\r\n\r\nBut `merge` works nice:\r\n\r\n    print (pd.merge(DF_empty, DF_a, left_index=True, right_index=True))\r\n    Empty DataFrame\r\n    Columns: [a]\r\n    Index: []\r\n\r\n[SO question](-concat-inner-join-a-join-with-an-empty-dataframe-does-not-yield-an-empty)\r\n\r\n---\r\n\r\n    print (pd.show_versions())\r\n\r\n<details>\r\n    INSTALLED VERSIONS\r\n    ------------------\r\n    commit: None\r\n    python: 3.5.1.final.0\r\n    python-bits: 64\r\n    OS: Windows\r\n    OS-release: 7\r\n    machine: AMD64\r\n    processor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\r\n    byteorder: little\r\n    LC_ALL: None\r\n    LANG: sk_SK\r\n    LOCALE: None.None\r\n    \r\n    pandas: 0.19.2\r\n    nose: 1.3.7\r\n    pip: 9.0.1\r\n    setuptools: 20.3\r\n    Cython: 0.23.4\r\n    numpy: 1.11.0\r\n    scipy: 0.17.0\r\n    statsmodels: 0.6.1\r\n    xarray: None\r\n    IPython: 4.1.2\r\n    sphinx: 1.3.1\r\n    patsy: 0.4.0\r\n    dateutil: 2.5.1\r\n    pytz: 2016.2\r\n    blosc: None\r\n    bottleneck: 1.0.0\r\n    tables: 3.2.2\r\n    numexpr: 2.5.2\r\n    matplotlib: 1.5.1\r\n    openpyxl: 2.3.2\r\n    xlrd: 0.9.4\r\n    xlwt: 1.0.0\r\n    xlsxwriter: 0.8.4\r\n    lxml: 3.6.0\r\n    bs4: 4.4.1\r\n    html5lib: 0.999\r\n     None\r\n    apiclient: None\r\n    sqlalchemy: 1.0.12\r\n    pymysql: None\r\n    psycopg2: None\r\n    jinja2: 2.8\r\n    boto: 2.39.0\r\n    pandas_datareader: 0.2.1\r\n    None\r\n<\\details>",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "Effort Low",
      "Reshaping"
    ],
    "comments": [
      "ok, this is a bug, we normally filter out completely empty frames, but in this case we need to skip that condition\r\n\r\n```diff --git a/pandas/tools/merge.py b/pandas/tools/merge.py\r\nindex 3fbd83a..5bf0546 100644\r\n--- a/pandas/tools/merge.py\r\n+++ b/pandas/tools/merge.py\r\n@@ -1689,7 +1689,7 @@ class _Concatenator(object):\r\n                            if sum(obj.shape) > 0 or isinstance(obj, Series)]\r\n \r\n             if (len(non_empties) and (keys is None and names is None and\r\n-                                      levels is None and join_axes is None)):\r\n+                                      levels is None and join_axes is None and not self.intersect)):\r\n                 objs = non_empties\r\n                 sample = objs[0]\r\n \r\n```\r\n\r\npull-request to add some tests  (there are *very* little tests for using ``join=`` with ``concat``) and fix would be great.",
      "happy to work on that.  does it amount to fixing `/pandas/tools/merge.py` and writing some related tests in `pandas/pandas/tests/types/test_concat.py`?",
      "tests are in ``pandas/tools/tests/test_concat.py``\r\n\r\nand (shortly) this code is in ``pandas/tools/concat.py`` (just moving it)."
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "commented",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 4,
    "additions": 22,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/tests/tools/test_concat.py",
      "pandas/tests/tools/test_merge.py",
      "pandas/tools/concat.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15337,
    "reporter": "erewok",
    "created_at": "2017-02-07T21:58:33+00:00",
    "closed_at": "2017-04-05T19:18:44+00:00",
    "resolver": "gfyoung",
    "resolved_in": "e4e87ec55765d31e59e97d89c71ed5a3fa2f3d38",
    "resolver_commit_num": 159,
    "title": "Segfault when passing a mock.Mock instance to pandas.read_csv (it was an accident!)",
    "body": "#### Code Sample\r\n\r\n\r\n#### Problem description\r\n\r\nI wrote a test where I accidentally ended up passing a `mock.Mock` object into `pandas.read_csv` and it segfaults.\r\n\r\n#### Expected Output\r\n\r\nWell, it would nice if it threw an error telling me that I screwed up, instead of segfaulting.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nIn [6]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.3.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.0\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 24.0.2\r\nCython: 0.24.1\r\nnumpy: 1.10.4\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: 1.0.0\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: 1.0.9\r\npymysql: None\r\npsycopg2: 2.6.1 (dt dec pq3 ext)\r\njinja2: 2.8\r\nboto: None\r\n\r\n</details>\r\n",
    "labels": [
      "Compat",
      "IO CSV"
    ],
    "comments": [
      "Couldn't help exploring it a bit because the test-case is so easy. \r\n\r\nI installed the develop branch of pandas from source just now and traced the bug back to where it enters the parser defined in [cython](https://github.com/pandas-dev/pandas/blob/master/pandas/parser.pyx#L281) at this point in the `read_csv` process: https://github.com/pandas-dev/pandas/blob/master/pandas/io/parsers.py#L1534\r\n\r\nSeems like a sound strategy would be to maybe rule out that whatever is getting passed to that parser inside `parser.pyx` is definitely not a string or file handle or buffer and if so to reject it?",
      "sure it must be a string, bytes, or a file-like (which mainly involves having ``.read``, ``.seek`` (maybe ``tell``) I think might be enough).",
      "Yeah, but I am suspicious that pandas is testing for `read` and friends in the wrong place or that simply asking for those attributes is too fuzzy.\r\n\r\nFor instance, I isolated the error to this chunk of code: \r\nhttps://github.com/pandas-dev/pandas/blob/master/pandas/parser.pyx#L700\r\n\r\nIf you comment this `elif hasattr...` block out, it throws the following, which is way better than a segfault:\r\n\r\n```\r\nOSError: Expected file path name or file-like object, got <class 'unittest.mock.Mock'> type\r\n```\r\n\r\nHowever, I think the test inside `TextReader` here is little bit too soft: my mock object goes through because it pretends to have a `read` attribute, but it might also be possible to craft something malicious with a `read` attribute, and it seems like that thing-with-a-read-object would make it all the way through to this cython parser. \r\n\r\nAnyway, I believe that the test for whether it really is a `filepath_or_buffer` should happen much earlier in the call to `read_csv` and I am not sure testing whether there's a `read` or `seek` attribute is good enough?\r\n\r\n**Edit:** I may have misread your comment, a bit. Apologies for any confusion.",
      "@pellagic-puffbomb sure, you can test earlier. We have a fairly complicated opening process (because of many many possibilities of what it could be, encoding, from url etc). So it may not be possible to do it earlier (though it certainly might be).\r\n\r\nI wouldn't worry about malicious things, that is the resonsibility of the user. We necessarily have a string to a file (which may or may not exist), or a file-like object which we read bytes. If someone wants to give a duck-like that is just fine.\r\n\r\nIf you can check earlier and have everything passing that would be great. We DO have a fairly extensive test suite for things like this.",
      "Alright, well, I have an idea that may help a bit.",
      "Hmm...seems like this thread has gone a bit stale.  Reading through this, I do agree that we can do a better job if we encounter invalid objects for parsing.  For example, specifying the Python engine leads to this:\r\n~~~python\r\n>>> read_csv(mock.Mock(), engine=\"python\")\r\n...\r\nTypeError: argument 1 must be an iterator\r\n~~~\r\n\r\nThat comes from Python's native `csv` library.  Not really that useful either.\r\n\r\nThe problem with attribute checking is that `mock` objects have overloaded the `__getattr__` method so that most attributes will exist.  However, one that does catch my attention is this:\r\n\r\n~~~python\r\n>>> mock.Mock().__iter__\r\n...\r\nAttributeError: __iter__\r\n~~~\r\n\r\nIf you take a look at the C-code for Python's `csv` library <a href=\"https://hg.python.org/cpython/file/tip/Modules/_csv.c#l967\">here</a>, that's how they catch that `mock.Mock()` is an invalid file object.",
      "it *might* be enough to check if a file-like (IOW not a string) ``is_iterator`` and otherwise raise a nice error (before passing to to anything else). xref to #15862.\r\n\r\n> sure it must be a string, bytes, or a file-like (which mainly involves having .read, .seek (maybe tell) I think might be enough).\r\n\r\nfrom above, we might need a more sophisticated test (which could use ``is_iterator``)",
      "How about adding that as a check in `pandas.io.common`?  We do plenty of file-handling there for all of our I/O file operations.  It seems like we should be able to abstract this validation logic.",
      "@gfyoung \r\n\r\nyes ``is_file_like`` would be great! (*maybe* could put in ``pandas.types.common`` alternatively). its kind of a 'type' check.\r\n\r\nThen calling in parser (and other methods would be great). "
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 8,
    "additions": 361,
    "deletions": 32,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/io/common.py",
      "pandas/io/excel.py",
      "pandas/tests/api/test_types.py",
      "pandas/tests/io/parser/common.py",
      "pandas/tests/types/test_inference.py",
      "pandas/types/api.py",
      "pandas/types/inference.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15339,
    "reporter": "kernc",
    "created_at": "2017-02-08T00:00:26+00:00",
    "closed_at": "2017-02-08T14:36:43+00:00",
    "resolver": "kernc",
    "resolved_in": "87c2c2af6150d6e8fa8cfbc017fbfd52e7f8c5c7",
    "resolver_commit_num": 1,
    "title": "df.squeeze() doesn't support axis parameter",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\n`np.squeeze` supports `axis` parameter and [this comment in the source](-dev/pandas/blob/542c9166a6ceff4a4889caae3843c3a82a2301cd/pandas/compat/numpy/function.py#L217-L219) implies it should eventually be implemented.\r\n\r\n#### Expected Output\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\npandas: 0.19.0+416.ge1390cd",
    "labels": [
      "API Design",
      "Reshaping"
    ],
    "comments": [
      "@kernc how is this actually useful though? numpy needs this because of n-dim shrinking to a lower dim. pandas generally has 2-dim max so this is not so important.",
      "Indeed, as the example shows, it is only useful to squeeze a 1\u00d71 frame into a series instead of a scalar, or to keep 1\u00d7N frame a frame. :smiley: "
    ],
    "events": [
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "commented",
      "milestoned"
    ],
    "changed_files": 4,
    "additions": 34,
    "deletions": 16,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/compat/numpy/function.py",
      "pandas/core/generic.py",
      "pandas/tests/test_generic.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15342,
    "reporter": "stephenrauch",
    "created_at": "2017-02-08T05:51:37+00:00",
    "closed_at": "2017-02-09T17:05:36+00:00",
    "resolver": "stephenrauch",
    "resolved_in": "3c9fec39d502cf7a24d4a9e16e3c5733560dc05c",
    "resolver_commit_num": 0,
    "title": "Multiline Eval broken for local variables after first line",
    "body": "#### Problem description\r\n\r\nAs discussed [here](-local-variables-with-multiple-assignments-with-pandas-eval-function), using local variables with a multi-line eval does not work for locals not on the first line. \r\n\r\nThis: \r\n\r\n\r\n\r\nfails with:\r\n\r\n    error: pandas.computation.ops.UndefinedVariableError: local variable 'y' is not defined\r\n\r\nNOTE: I will be submitting a PR shortly.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 32\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel\r\nbyteorder: little\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 8.1.2\r\nsetuptools: 18.2\r\nCython: None\r\nnumpy: 1.12.0\r\ndateutil: 2.6.0\r\npytz: 2016.6.1\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Numeric"
    ],
    "comments": [],
    "events": [
      "cross-referenced",
      "labeled",
      "labeled",
      "milestoned"
    ],
    "changed_files": 3,
    "additions": 18,
    "deletions": 7,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/computation/eval.py",
      "pandas/computation/tests/test_eval.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15344,
    "reporter": "tobgu",
    "created_at": "2017-02-08T08:03:18+00:00",
    "closed_at": "2017-02-10T14:12:04+00:00",
    "resolver": "tobgu",
    "resolved_in": "e8840725447859531ddcc4b878266f2043fb6465",
    "resolver_commit_num": 0,
    "title": "Calling DataFrame.to_json() increases data frame memory usage in Python 3.6",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n\r\nCompared to Python 2.7.12\r\n\r\n\r\n#### Problem description\r\nCalling `to_json` should not have any impact on the reported memory usage of a DataFrame. Just like in Python 2. The observed increase above is 32% which is really high.\r\n\r\nThis only seems to happen with dataframes that have strings in them.\r\n\r\nI've also tested calling `to_csv`, that does not trigger this behaviour.\r\n\r\nFurthermore it seems like the memory usage is quite a lot higher in Python 3 compared to the equivalent data frame in Python 2 (~25% in the example above). I guess this is more related to strings in Python 2 vs Python 3 than Pandas though?\r\n\r\n#### Expected Output\r\nNo change in reported memory usage after calling `to_json`.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.0.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-83-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 34.1.0\r\nCython: None\r\nnumpy: 1.12.0\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: 2.6.2\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: None\r\npandas_datareader: None\r\n\r\n</details>\r\n",
    "labels": [
      "2/3 Compat",
      "IO JSON",
      "Performance"
    ],
    "comments": [
      "Partly guessing, but I think https://www.python.org/dev/peps/pep-0393/ could be the culprit.  Python 3 can choose a compact string representation, then the C API calls the json code is using could force it into a less compact representation.\r\n\r\nhttps://docs.python.org/3/whatsnew/3.3.html#pep-393-flexible-string-representation\r\n",
      "the absolute sizes of memory used are dependent on py2/py3 as @chris-b1 indicates.\r\n\r\nSo strings are held in python space, backed by pointers from numpy. This IS using the c-api; I would hazard a guess that strings that were formerly interned are now no longer. This is pretty deep. \r\n\r\nIf anyone wants to investigate please do so. Though I would say this is out of pandas hands.",
      "It does look like `ujson` was updated to use the new C API - so if you wanted to try porting this back in, may not be too hard.\r\n\r\nours - https://github.com/pandas-dev/pandas/blob/bf1a5961a09a6f5237a681f9f1c9a698b1a13918/pandas/src/ujson/python/objToJSON.c#L402\r\n\r\ntheirs - https://github.com/esnme/ultrajson/blob/2f1d4874f4f4d2a40a460678004c80e69387c663/python/objToJSON.c#L143\r\n",
      "Yes, there seems to be quite a few updates in `uson` since the version that is present in Pandas. It does not seem to me like it has with interning to do but rater that single byte ascii characters are turned into full blown 4 byte unicode characters based on the basic experiment below.\r\n\r\nHaving a dataframe dominated by long strings would hence mean that the memory usage would be quadrupled after calling `to_json` compared to before.\r\n\r\n```python\r\n>>> df = pd.DataFrame({'a': [str(1)]})\r\n>>> df.memory_usage(index=True, deep=True)\r\nIndex    80\r\na        58\r\ndtype: int64\r\n>>> df.to_json()\r\n'{\"a\":{\"0\":\"1\"}}'\r\n>>> df.memory_usage(index=True, deep=True)\r\nIndex    80\r\na        66\r\ndtype: int64\r\n>>> df = pd.DataFrame({'a': [str(11)]})\r\n>>> df.to_json()\r\n'{\"a\":{\"0\":\"11\"}}'\r\n>>> df = pd.DataFrame({'a': [str(11)]})\r\n>>> df.memory_usage(index=True, deep=True)\r\nIndex    80\r\na        59\r\ndtype: int64\r\n>>> df.to_json()\r\n'{\"a\":{\"0\":\"11\"}}'\r\n>>> df.memory_usage(index=True, deep=True)\r\nIndex    80\r\na        71\r\ndtype: int64\r\n>>> df = pd.DataFrame({'a': [str(111)]})\r\n>>> df.memory_usage(index=True, deep=True)\r\nIndex    80\r\na        60\r\ndtype: int64\r\n>>> df.to_json()\r\n'{\"a\":{\"0\":\"111\"}}'\r\n>>> df.memory_usage(index=True, deep=True)\r\nIndex    80\r\na        76\r\ndtype: int64\r\n>>> \r\n\r\n```"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "commented",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 24,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/io/tests/json/test_pandas.py",
      "pandas/src/ujson/python/objToJSON.c"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15347,
    "reporter": "jreback",
    "created_at": "2017-02-08T13:50:10+00:00",
    "closed_at": "2017-02-27T20:48:07+00:00",
    "resolver": "jreback",
    "resolved_in": "e15de4d484dbff8f941c9d5cc31869d503d9c020",
    "resolver_commit_num": 4251,
    "title": "MIGRATE: pandas-gbq",
    "body": "I created the [pandas-gbq](-gbq) package to migrate the guts of pandas/io/gbq.py. Once this is release and on PyPi and conda-forge. Then we can switch the internal interface to use this code. I think this will allow this package to grow.\r\n\r\n",
    "labels": [
      "IO Google",
      "Note To Selves"
    ],
    "comments": [
      "I have add all pandas to the repo as well as @parthea . If you would like to be added, please ping.",
      "CC @clifton",
      "Big +1 to have this as a separate package.\r\n\r\n> Then we can switch the internal interface to use this code\r\n\r\n@jreback What is your idea on how to approach this?\r\n\r\nLet `read_gqb`/`to_gbq` call the external package and pass everything on?\r\n\r\nRoughly like\r\n\r\n```\r\ndef read_gbq(*args, **kwargs):\r\n    from pandas_gbq import read_gbq\r\n    return read_gbq(*args, **kwargs)\r\n\r\nclass DataFrame():\r\n    ...\r\n    def to_gbq(self, *args, **kwargs):\r\n        from pandas_gbq import to_gbq\r\n        to_gbq(self, *args, **kwargs)\r\n```\r\n\r\n(with of course some checking of the import etc)",
      "exactly the internal pandas implementation becomes trivial\r\n\r\n\r\n",
      "ok moved all of the docs, so will change that as well: http://pandas-gbq.readthedocs.io/en/latest/index.html#"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "commented",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 11,
    "additions": 116,
    "deletions": 2677,
    "changed_files_list": [
      "ci/requirements-2.7.pip",
      "ci/requirements-3.4.pip",
      "ci/requirements-3.4_SLOW.pip",
      "ci/requirements-3.5.pip",
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/frame.py",
      "pandas/io/gbq.py",
      "pandas/tests/io/test_gbq.py",
      "pandas/util/decorators.py",
      "pandas/util/print_versions.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15363,
    "reporter": "jreback",
    "created_at": "2017-02-10T18:16:34+00:00",
    "closed_at": "2017-02-10T19:17:21+00:00",
    "resolver": "jreback",
    "resolved_in": "dcb4e47a0b6620f1efbe5e02ed493e6513fc8763",
    "resolver_commit_num": 4214,
    "title": "COMPAT: numpy 1.12",
    "body": "-ci.org/pandas-dev/pandas/jobs/200390418\r\n\r\nthese are test failures from an API change: \r\ncc @shoyer \r\n\r\nI think we don't have ``numexpr`` installed on the dev build so these tests are skipped :<\r\n\r\n",
    "labels": [
      "Compat",
      "Testing"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "mentioned",
      "subscribed",
      "milestoned",
      "referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 19,
    "deletions": 3,
    "changed_files_list": [
      ".gitignore",
      "ci/requirements-3.5.build",
      "ci/requirements-3.5.run",
      "pandas/tests/test_expressions.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15370,
    "reporter": "bmcfee",
    "created_at": "2017-02-11T22:02:48+00:00",
    "closed_at": "2017-02-20T19:24:11+00:00",
    "resolver": "bmcfee",
    "resolved_in": "0b4fdf988e3125f7c55aaf6e08a2dfa7d9e2e8a0",
    "resolver_commit_num": 0,
    "title": "deepcopy failure on empty dataframes with non-empty column set (numpy 1.12 compatibility)",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nThis only occurs with numpy 1.12 (and, presumably above): when deepcopying an empty dataframe with a non-empty column set, it fails with the following:\r\n\r\n\r\n\r\nIf the column set is also empty, everything works as expected.\r\n\r\nOn older numpy versions (1.11), it also works as expected.\r\n\r\n#### Expected Output\r\n\r\nNot failing.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n# Paste the output here pd.show_versions() here\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.0.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-62-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 32.3.1.post20170108\r\nCython: 0.25.2\r\nnumpy: 1.12.0\r\nscipy: 0.18.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 2.0.0\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "Compat",
      "Difficulty Novice",
      "Effort Low",
      "Reshaping"
    ],
    "comments": [
      "not shocked this doesn't work, see #8571 \r\n\r\nwe are not using an override (which we should be), to simply call our custom methods.\r\n\r\ne.g. basically\r\n\r\n```\r\ndef __copy__(self):\r\n    return self.copy()\r\n```\r\n\r\nif you'd like to take a crack at this issue (and add this as a test) would be appreciated!",
      "> if you'd like to take a crack at this issue (and add this as a test) would be appreciated!\r\n\r\nNot sure I know how best to fix it, as I haven't mucked around the internals of dataframes much.  It's not obvious to me that this should be fixed in dataframe, ndframe, or somewhere else?",
      "@bmcfee simply need to define ``__copy__`` and ``__deepcopy__``, can be done in ``pandas/core/generic/NDFrame``. These are *already* defined for ``Index`` (in ``pandas/indexes/base.py`` (it may simply be possible to move those to ``pandas/core/base`` though as we have a mixin that goes thru the entire hierarchy ``PandasObject``."
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 41,
    "deletions": 7,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/generic.py",
      "pandas/indexes/base.py",
      "pandas/tests/test_generic.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15376,
    "reporter": "stephenrauch",
    "created_at": "2017-02-12T15:42:48+00:00",
    "closed_at": "2017-02-27T14:45:44+00:00",
    "resolver": "stephenrauch",
    "resolved_in": "fb7dc7dcbde1d81dea28b1b83e1c3bd171a7e73d",
    "resolver_commit_num": 1,
    "title": "BUG: Parse two date columns broken in read_csv with multiple headers",
    "body": "#### Problem description\r\n\r\nAs can be seen [here](-read-csv-how-do-i-parse-two-columns-as-datetimes-in-a-hierarchically-ind/42185111#42185111), a `KeyError` is being thrown when when trying to merge two date columns from a `csv` with multiple header lines.\r\n\r\nSo this `csv` file:\r\n\r\n\r\n\r\nCauses this line:\r\n\r\n\r\n\r\nto raise a `KeyError`\r\n\r\nNOTE: I will be submitting a PR shortly.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 32\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel\r\nbyteorder: little\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 8.1.2\r\nsetuptools: 18.2\r\nCython: None\r\nnumpy: 1.12.0\r\ndateutil: 2.6.0\r\npytz: 2016.6.1\r\n</details>",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "Effort Low",
      "IO CSV"
    ],
    "comments": [
      "so your csv is invalid as far as multi-line parsing goes. It 'works' but is not very useful.\r\n\r\nA multi-line csv header needs non-sparsity (this is in fact how '.to_csv' writes it).\r\n```\r\nIn [16]: data\r\nOut[16]: 'X,X,Y,Y,Z,Z\\nDate,Time,A,B,A,B\\n2017-01-21,01:57:49.390,0,1,2,3\\n2017-01-21,01:57:50.400,4,5,7,9\\n2017-01-21,01:57:51.410,3,2,4,1\\n'\r\n\r\nIn [17]: pandas.read_csv(StringIO(data), header=[0,1], parse_dates={'datetime' : [('X', 'Date'), ('X', 'Time')]})\r\nOut[17]: \r\n                 datetime  (Y, A)  (Y, B)  (Z, A)  (Z, B)\r\n0 2017-01-21 01:57:49.390       0       1       2       3\r\n1 2017-01-21 01:57:50.400       4       5       7       9\r\n2 2017-01-21 01:57:51.410       3       2       4       1\r\n```\r\n\r\nI think we have a an issue to parse it with the sparsity but not sure.\r\n\r\nSo it parses correctly when the names are given (as tuples). I suppose its a bug that ``parse_dates`` doesn't handle the column numbers.\r\n\r\nAnd so your original example parses when the columns are fully declared. (though again not very useful).\r\n```\r\nIn [22]: pandas.read_csv(StringIO(data), header=[0,1], parse_dates={'datetime' : [('X', 'Date'), ('Unnamed: 1_level_0', 'Time')]})\r\nOut[22]: \r\n                 datetime  (Y, A)  (Unnamed: 3_level_0, B)  (Z, A)  (Unnamed: 5_level_0, B)\r\n0 2017-01-21 01:57:49.390       0                        1       2                        3\r\n1 2017-01-21 01:57:50.400       4                        5       7                        9\r\n2 2017-01-21 01:57:51.410       3                        2       4                        1\r\n```\r\n\r\nFurther make a single level is just not useful in a multi-level frame.\r\n\r\nI would prob do this:\r\n```\r\nIn [25]: pandas.read_csv(StringIO(data), header=0, skiprows=1, parse_dates={'datetime':[0,1]})\r\nOut[25]: \r\n                 datetime  A  B  A.1  B.1\r\n0 2017-01-21 01:57:49.390  0  1    2    3\r\n1 2017-01-21 01:57:50.400  4  5    7    9\r\n2 2017-01-21 01:57:51.410  3  2    4    1\r\n```\r\n\r\nSo in summary I'll mark this as a bug, but you have other issues.\r\ncc @gfyoung ",
      "Wow, thanks for nice response.  Do you mind if I cross post this response on Stack Overflow?\r\n\r\nAlso, the problem exists with/without the sparsity.  It is the tuple causing the problem.  A proposed fix is running in the CI now, I will submit a PR shortly.\r\n\r\nThanks again for your quick and very detailed attention.",
      "sure. Its not the tuple, rather the positional indexers which look up the tuple."
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 21,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/io/parsers.py",
      "pandas/tests/io/parser/parse_dates.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15420,
    "reporter": "dfd",
    "created_at": "2017-02-16T04:01:20+00:00",
    "closed_at": "2017-02-24T19:57:32+00:00",
    "resolver": "jeet63",
    "resolved_in": "3fe85afef47e9e079a0fa24f826bb6faaa2341d5",
    "resolver_commit_num": 0,
    "title": "rank incorrectly orders ordered categories",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nrank seems to be ignoring the order of ordered categories.\r\n\r\n#### Expected Output\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Categorical",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "@dfd Thanks for the report! That is indeed clearly a bug. \r\nFor example in `sort_values`, it takes the correct order into account, but `rank` was apparently missed.\r\n\r\n```\r\nIn [6]: a.A.sort_values()\r\nOut[6]: \r\n0     first\r\n1    second\r\n2     third\r\n3    fourth\r\n4     fifth\r\n5     sixth\r\nName: A, dtype: category\r\nCategories (6, object): [first < second < third < fourth < fifth < sixth]\r\n```\r\n\r\nI think this should be a rather easy fix (in the `pd.core.algorithms.rank`, we should need to check for categorical, and then pass the underlying integer codes). If you would be interested in trying to do a pull request with a fix, always welcome!"
    ],
    "events": [
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 105,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/algorithms.py",
      "pandas/core/categorical.py",
      "pandas/tests/series/test_analytics.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15421,
    "reporter": "yegulalp",
    "created_at": "2017-02-16T12:33:52+00:00",
    "closed_at": "2017-02-16T13:24:50+00:00",
    "resolver": "gwpdt",
    "resolved_in": "37e5f78b4e9ff03cbff4dea928445cc3b1f707c8",
    "resolver_commit_num": 0,
    "title": "Unexpected string->float conversion in DataFrame.groupby().apply()",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n\r\n\r\n#### Problem description\r\ngroupby.apply() does an unexpected conversion from string to float for column 'B' in the example above.  The bug is triggered only when both of the following happen:\r\n1.  A column ('B' in the example above) has string values, some of which are parseable as numbers and some which are not.\r\n2. Another column ('T' in the example above) in the dataframe has timestamps.\r\n\r\n\r\n#### Expected Output\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n# Paste the output here pd.show_versions() here\r\n\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.13.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.2\r\nnumpy: 1.11.3\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.5.1\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.2.2\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.4.0\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.6\r\nlxml: 3.7.2\r\nbs4: 4.5.3\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: 1.1.4\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8.1\r\nboto: 2.45.0\r\npandas_datareader: 0.2.1\r\n</details>\r\n",
    "labels": [],
    "comments": [
      "@yegulalp Thanks for the report!\r\n\r\nThis is a duplicate of https://github.com/pandas-dev/pandas/issues/14849, and also related to https://github.com/pandas-dev/pandas/issues/14423\r\n\r\nContributions to try to fix this are always welcome!"
    ],
    "events": [
      "commented",
      "mentioned"
    ],
    "changed_files": 3,
    "additions": 54,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/groupby.py",
      "pandas/tests/groupby/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15424,
    "reporter": "toobaz",
    "created_at": "2017-02-16T15:39:14+00:00",
    "closed_at": "2017-02-20T14:44:01+00:00",
    "resolver": "toobaz",
    "resolved_in": "821be3991cca866a5cc9cf3407cd9f68c66c0306",
    "resolver_commit_num": 18,
    "title": "Indexing MultiIndex with NDFrame indexer fails if index of indexer does not contain 0",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nThe index of the indexer should not matter at all... and indeed there is no bug when indexing a (non-``Multi``)``Index``:\r\n\r\n#### Expected Output\r\n\r\nThe same as ``d.set_index(['a', 'b']).loc[pd.Series([1, 2])]``.\r\n\r\nA PR is on its way.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.7.0-1-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.utf8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.19.0+464.g0b28e5a\r\npytest: None\r\npip: 8.1.2\r\nsetuptools: 28.0.0\r\nCython: 0.23.4\r\nnumpy: 1.12.0\r\nscipy: 0.18.1\r\nxarray: None\r\nIPython: 5.1.0.dev\r\nsphinx: 1.4.8\r\npatsy: 0.3.0-dev\r\ndateutil: 2.5.3\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.2.2\r\nnumexpr: 2.6.0\r\nfeather: None\r\nmatplotlib: 2.0.0rc2\r\nopenpyxl: None\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.3\r\nlxml: None\r\nbs4: 4.5.1\r\nhtml5lib: 0.999\r\n 0.9.1\r\napiclient: 1.5.2\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\ns3fs: None\r\npandas_datareader: 0.2.1\r\n\r\n\r\n</details>\r\n",
    "labels": [
      "Indexing",
      "MultiIndex",
      "Bug"
    ],
    "comments": [
      "ok this looks like a bug. IIRC we had a similar issue (maybe was yours)?",
      "There was #14730 (this bug might actually be a side effect of its fix, but I didn't check).",
      "@toobaz ok, can you add those examples as confirmation tests (they look similar). if they pass, then add that issue as well in the whatsnew note.",
      "I'm lost... have you seen the PR? (#15425)",
      "of course that is where they should be added ",
      "OK, I asked because that issue [_is already_ in the whatsnew note](https://github.com/pandas-dev/pandas/pull/15425/files#diff-52364fb643114f3349390ad6bcf24d8fL502)\r\n\r\nSo: you suggest I add to the tests I already wrote the ones in _this_ bug report, right? (except for the data content, they are exactly the same...)",
      "ahh ok then"
    ],
    "events": [
      "cross-referenced",
      "labeled",
      "labeled",
      "commented",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented"
    ],
    "changed_files": 3,
    "additions": 55,
    "deletions": 7,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/indexing.py",
      "pandas/tests/indexing/test_multiindex.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15426,
    "reporter": "munierSalem",
    "created_at": "2017-02-16T15:48:12+00:00",
    "closed_at": "2017-02-27T15:46:31+00:00",
    "resolver": "stephenrauch",
    "resolved_in": "6c17f67aafd7de8af96032aa415fc798fa3b73ca",
    "resolver_commit_num": 2,
    "title": "BUG: timezone lost in groupby-agg with cython functions",
    "body": "xref #10668 (for more examples)\r\n\r\nHello!\r\n\r\nI'm running into some odd behavior trying to group rows of a pandas dataframe by ID and then selecting out max/min datetimes (w/ timezones). This is with python 2.7, pandas 0.18.1 and numpy 1.11.1 (I saw in earlier posts a similar problem was apparently fixed w/ pandas 0.15).\r\n\r\nSpecifically, if I try:\r\n`print orders.groupby('OrderID')['start_time'].agg(np.min).iloc[:5]`\r\n\r\nI get:\r\n\r\n\r\nWhere the raw data had times closer to 8 am (US/Eastern). In other words, it reverted back to UTC times, even though it says it's eastern times, and has UTC-4 offset.\r\n\r\nBut if I instead try:\r\n`print orders.groupby('OrderID')['start_time'].agg(lambda x: np.min(x)).iloc[:5]`\r\n\r\nI now get:\r\n\r\n\r\nWhich is the behavior I intended. This second method is vastly slower, and I would have assumed the two approaches would yield identical results ... \r\n",
    "labels": [
      "Bug",
      "Groupby",
      "Timezones",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "Could you try it out on a more recent version of pandas, or add a copy-pastable example so that someone else can check? Might have been fixed already.",
      "further ``np.min`` almost always does the wrong thing, it doesn't respect timezones.\r\n\r\nuse ``.min()``",
      "Actually, here's a repro:\r\n\r\n```python\r\nIn [63]: ts = pd.Series(pd.date_range('2016', periods=12, freq='H').tz_localize(\"UTC\").tz_convert(\"US/Eastern\"))\r\n\r\nIn [64]: ts\r\nOut[64]:\r\n0    2015-12-31 19:00:00-05:00\r\n1    2015-12-31 20:00:00-05:00\r\n2    2015-12-31 21:00:00-05:00\r\n3    2015-12-31 22:00:00-05:00\r\n4    2015-12-31 23:00:00-05:00\r\n                ...\r\n7    2016-01-01 02:00:00-05:00\r\n8    2016-01-01 03:00:00-05:00\r\n9    2016-01-01 04:00:00-05:00\r\n10   2016-01-01 05:00:00-05:00\r\n11   2016-01-01 06:00:00-05:00\r\ndtype: datetime64[ns, US/Eastern]\r\n\r\nIn [65]: ts.groupby(level=0).agg(np.min)\r\nOut[65]:\r\n0    2016-01-01 00:00:00-05:00\r\n1    2016-01-01 01:00:00-05:00\r\n2    2016-01-01 02:00:00-05:00\r\n3    2016-01-01 03:00:00-05:00\r\n4    2016-01-01 04:00:00-05:00\r\n                ...\r\n7    2016-01-01 07:00:00-05:00\r\n8    2016-01-01 08:00:00-05:00\r\n9    2016-01-01 09:00:00-05:00\r\n10   2016-01-01 10:00:00-05:00\r\n11   2016-01-01 11:00:00-05:00\r\ndtype: datetime64[ns, US/Eastern]\r\n\r\nIn [66]: ts.groupby(level=0).min()\r\nOut[66]:\r\n0    2016-01-01 00:00:00-05:00\r\n1    2016-01-01 01:00:00-05:00\r\n2    2016-01-01 02:00:00-05:00\r\n3    2016-01-01 03:00:00-05:00\r\n4    2016-01-01 04:00:00-05:00\r\n                ...\r\n7    2016-01-01 07:00:00-05:00\r\n8    2016-01-01 08:00:00-05:00\r\n9    2016-01-01 09:00:00-05:00\r\n10   2016-01-01 10:00:00-05:00\r\n11   2016-01-01 11:00:00-05:00\r\ndtype: datetime64[ns, US/Eastern]\r\n\r\n```\r\n\r\n> further np.min almost always does the wrong thing, it doesn't respect timezones.\r\n\r\nmy thought too, but `.min` also shows the issue.",
      "I think the expected output there is identical to the input (since the index is already unique).",
      "```\r\nIn [19]: data = \"\"\"O161101XVS100000044   2016-11-01 12:03:12.920000-04:00\r\n    ...: O161101XVS100000047   2016-11-01 12:03:36.693000-04:00\r\n    ...: O161101XVS100000098   2016-11-01 12:09:08.330000-04:00\r\n    ...: O161101XVS100000122   2016-11-01 12:09:59.950000-04:00\r\n    ...: O161101XVS100000152   2016-11-01 12:11:29.790000-04:00\r\n    ...: \"\"\"\r\n\r\nIn [20]: df = pd.read_csv(StringIO(data),header=None,sep='\\s+')\r\n\r\nIn [21]: df.columns=['value','date','time']\r\n\r\nIn [22]: df['datetime'] = pd.to_datetime(df.date + ' ' + df.time).dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\r\n\r\nIn [23]: df\r\nOut[23]: \r\n                 value        date                   time                         datetime\r\n0  O161101XVS100000044  2016-11-01  12:03:12.920000-04:00 2016-11-01 12:03:12.920000-04:00\r\n1  O161101XVS100000047  2016-11-01  12:03:36.693000-04:00 2016-11-01 12:03:36.693000-04:00\r\n2  O161101XVS100000098  2016-11-01  12:09:08.330000-04:00 2016-11-01 12:09:08.330000-04:00\r\n3  O161101XVS100000122  2016-11-01  12:09:59.950000-04:00 2016-11-01 12:09:59.950000-04:00\r\n4  O161101XVS100000152  2016-11-01  12:11:29.790000-04:00 2016-11-01 12:11:29.790000-04:00\r\n\r\nIn [24]: df.dtypes\r\nOut[24]: \r\nvalue                           object\r\ndate                            object\r\ntime                            object\r\ndatetime    datetime64[ns, US/Eastern]\r\ndtype: object\r\n\r\nIn [25]: df.groupby('value').datetime.min()\r\nOut[25]: \r\nvalue\r\nO161101XVS100000044   2016-11-01 16:03:12.920000-04:00\r\nO161101XVS100000047   2016-11-01 16:03:36.693000-04:00\r\nO161101XVS100000098   2016-11-01 16:09:08.330000-04:00\r\nO161101XVS100000122   2016-11-01 16:09:59.950000-04:00\r\nO161101XVS100000152   2016-11-01 16:11:29.790000-04:00\r\nName: datetime, dtype: datetime64[ns, US/Eastern]\r\n```",
      "dupe of this: https://github.com/pandas-dev/pandas/issues/10668\r\n\r\nthough I like this example.",
      "actually, let's leave this one open instead.",
      "@munierSalem if you'd like to debug would be great!\r\n\r\nThe groupby tz support is a bit buggy. Basically since these are converted to i8 undert the hood to actually do the operations, need to:\r\n\r\n- if a datetime64tz (by-definition this will be a single tz, if its multiple this would be an ``object`` column)\r\n  - convert to UTC, keep track of tz\r\n  - convert to i8 (already there)\r\n  - perform operation, comes back as i8\r\n  - localize to UTC, convert to original tz\r\n\r\nroughtly here:\r\nhttps://github.com/pandas-dev/pandas/blob/master/pandas/core/groupby.py#L1896",
      "@jreback I can fix in my local repo, but I'll need to wait to do so from home to push back ... working behind a draconian corporate firewall :(",
      "sure np "
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "closed",
      "labeled",
      "milestoned",
      "commented",
      "reopened",
      "unlabeled",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "renamed",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 4,
    "additions": 44,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/tests/groupby/test_aggregate.py",
      "pandas/tests/types/test_cast.py",
      "pandas/types/cast.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15428,
    "reporter": "luca-s",
    "created_at": "2017-02-16T21:53:44+00:00",
    "closed_at": "2017-03-08T13:43:24+00:00",
    "resolver": "luca-s",
    "resolved_in": "d32acaa7fbe95a96a7118a32324beea1e2e8ae32",
    "resolver_commit_num": 1,
    "title": "BUG: pd.cut with bins=1 and input all 0s",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n\r\n#### Problem description\r\n\r\nThe special case of an input containing all 0s raises a ValueError. I had a look at the [code](-dev/pandas/blob/master/pandas/tools/tile.py#L107) and it is straightforward to fix. I can provide a PR if requested.\r\n\r\n#### Expected Output\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.8.0-36-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 8.1.1\r\nsetuptools: 32.3.1\r\nCython: 0.23.4\r\nnumpy: 1.12.0\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.5.1\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999\r\n 0.9.1\r\napiclient: None\r\nsqlalchemy: 1.1.4\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8.1\r\nboto: None\r\npandas_datareader: 0.2.1\r\n\r\n</details>\r\n",
    "labels": [
      "Algos",
      "Bug",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "Sure a fix would be great."
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "labeled",
      "labeled",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 83,
    "deletions": 8,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/tests/tools/test_tile.py",
      "pandas/tools/tile.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15431,
    "reporter": "luca-s",
    "created_at": "2017-02-16T22:24:42+00:00",
    "closed_at": "2017-02-17T14:45:26+00:00",
    "resolver": "luca-s",
    "resolved_in": "d32acaa7fbe95a96a7118a32324beea1e2e8ae32",
    "resolver_commit_num": 1,
    "title": "BUG: pd.qcut with q=1 and input with identical values",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nIf the input contains identical values pd.qcut fails even with q=1. I had a look at the [code](-dev/pandas/blob/master/pandas/tools/tile.py#L209) and I can try to relax the check to allow this specific case to pass and avoid the exception, but then I have to make sure the code can handle a single edge.  \r\n\r\n#### Expected Output\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.8.0-36-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 8.1.1\r\nsetuptools: 32.3.1\r\nCython: 0.23.4\r\nnumpy: 1.12.0\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.5.1\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999\r\n 0.9.1\r\napiclient: None\r\nsqlalchemy: 1.1.4\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8.1\r\nboto: None\r\npandas_datareader: 0.2.1\r\n\r\n</details>\r\n",
    "labels": [],
    "comments": [
      "this is the same as #15428 so ideally let's solve in the same PR., just list it as a separate case there."
    ],
    "events": [
      "renamed"
    ],
    "changed_files": 3,
    "additions": 83,
    "deletions": 8,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/tests/tools/test_tile.py",
      "pandas/tools/tile.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15434,
    "reporter": "toobaz",
    "created_at": "2017-02-17T00:28:39+00:00",
    "closed_at": "2017-02-20T14:44:01+00:00",
    "resolver": "toobaz",
    "resolved_in": "821be3991cca866a5cc9cf3407cd9f68c66c0306",
    "resolver_commit_num": 18,
    "title": "MultiIndex can't be indexed with an np.array",
    "body": "From #15425 \r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nThe ``array`` should be accepted as it is accepted for the (non-``Multi``)``Index``\r\n\r\n#### Expected Output\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.7.0-1-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.utf8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.19.0+466.g5a8883b\r\npytest: 3.0.6\r\npip: 8.1.2\r\nsetuptools: 28.0.0\r\nCython: 0.23.4\r\nnumpy: 1.12.0\r\nscipy: 0.18.1\r\nxarray: None\r\nIPython: 5.1.0.dev\r\nsphinx: 1.4.8\r\npatsy: 0.3.0-dev\r\ndateutil: 2.5.3\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.2.2\r\nnumexpr: 2.6.0\r\nfeather: None\r\nmatplotlib: 2.0.0rc2\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.3\r\nlxml: 3.6.4\r\nbs4: 4.5.1\r\nhtml5lib: 0.999\r\n 0.9.1\r\napiclient: 1.5.2\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\ns3fs: None\r\npandas_datareader: 0.2.1\r\n\r\n\r\n</details>\r\n",
    "labels": [
      "Indexing",
      "MultiIndex",
      "Compat"
    ],
    "comments": [
      "why don't you add the fix for this in #15425 as well (as its quite similar). Be sure that the ndarray is 1d though (if its NOT 1d it should raise), and pls test this."
    ],
    "events": [
      "cross-referenced",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "unlabeled",
      "milestoned",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 55,
    "deletions": 7,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/indexing.py",
      "pandas/tests/indexing/test_multiindex.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15447,
    "reporter": "toobaz",
    "created_at": "2017-02-17T21:55:02+00:00",
    "closed_at": "2017-03-07T21:17:20+00:00",
    "resolver": "toobaz",
    "resolved_in": "c52ff68a536fafc0204c5afea57abb943a6c37ce",
    "resolver_commit_num": 19,
    "title": "Wrong result of pandas.sparse.series.SparseSeries.loc with indexer of length 1",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nIt should return as below\r\n\r\n#### Expected Output\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: f65a6415f15d438432cc6954ead61b052c5d4d60\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.7.0-1-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.utf8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.19.0+473.gf65a641\r\npytest: 3.0.6\r\npip: 8.1.2\r\nsetuptools: 28.0.0\r\nCython: 0.23.4\r\nnumpy: 1.12.0\r\nscipy: 0.18.1\r\nxarray: None\r\nIPython: 5.1.0.dev\r\nsphinx: 1.4.8\r\npatsy: 0.3.0-dev\r\ndateutil: 2.5.3\r\npytz: 2015.7\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.2.2\r\nnumexpr: 2.6.0\r\nfeather: None\r\nmatplotlib: 2.0.0rc2\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.3\r\nlxml: 3.6.4\r\nbs4: 4.5.1\r\nhtml5lib: 0.999\r\n 0.9.1\r\napiclient: 1.5.2\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\ns3fs: None\r\npandas_datareader: 0.2.1\r\n\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Indexing",
      "MultiIndex",
      "Sparse",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "The bug actually comes from ``SparseSeries.reindex()``\r\n\r\n``` python\r\nIn [8]: sparse.reindex(['A'], level=0)\r\nOut[8]: \r\nA   NaN\r\ndtype: float64\r\nBlockIndex\r\nBlock locations: array([], dtype=int32)\r\nBlock lengths: array([], dtype=int32)\r\n```\r\n\r\nPR on its way."
    ],
    "events": [
      "renamed",
      "referenced",
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "cross-referenced",
      "referenced",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 61,
    "deletions": 20,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/sparse/series.py",
      "pandas/tests/sparse/test_indexing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15463,
    "reporter": "kernc",
    "created_at": "2017-02-20T23:27:52+00:00",
    "closed_at": "2017-02-23T13:24:28+00:00",
    "resolver": "csizsek",
    "resolved_in": "b94186d4c58ee055656a84f55618be537db0095a",
    "resolver_commit_num": 0,
    "title": "Series.rolling(3).quantile(10) segmentation fault",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nThe value 10 is over the fraction of 1 which .quantile() works with, but I guess it still shouldn't crash my interpreter.\r\n\r\n#### Expected Output\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\npandas 0.19.0+479.g4842bc7",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "Numeric",
      "Reshaping",
      "Effort Low"
    ],
    "comments": [
      "yep, not very friendly...PR's appreciated!",
      "Hi @kernc and @jreback , I'm thinking about fixing this issue. Any idea where to start? (I have read the contribution documentation, set up the environment and was able to reproduce.)",
      "you can add a validation right here ``0 <= quantile <= 1``: https://github.com/pandas-dev/pandas/blob/master/pandas/window.pyx#L1288, and raise a ``ValueError`` if not in range. please add some tests as well.\r\n\r\nthanks!"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 19,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/tests/test_window.py",
      "pandas/window.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15487,
    "reporter": "abast",
    "created_at": "2017-02-23T17:35:55+00:00",
    "closed_at": "2017-02-24T20:41:46+00:00",
    "resolver": "abast",
    "resolved_in": "d80275dfaa6a8ad50bc49dbaef9eacd5509008dc",
    "resolver_commit_num": 0,
    "title": "support CategoricalIndex for read_msgpack",
    "body": "#### The following code fails:\r\n\r\n\r\n\r\n\r\n#### Problem description\r\n`read_msgpack` apparently does not seem to support a CategoricalIndex, however, it is possible to save a dataframe with a CategoricalIndex using `to_msgpack`. \r\n\r\nBackground: I am currently using the to_msgpack method to save a dask dataframe, where the index is (something like) a time stamp. It is not unique. I am overall very satisfied with the performance of `to_msgpack`, however when it comes to space efficency, having a categorical index would probably provide a significant improvement.\r\n\r\nOr maybe it works, but I am using it wrong?\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.16.60-0.42.5-smp\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 8.1.2\r\nsetuptools: 27.2.0\r\nCython: 0.24.1\r\nnumpy: 1.11.1\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.2.2\r\nsphinx: 1.4.6\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.6.1\r\nblosc: 1.4.4\r\nbottleneck: 1.1.0\r\ntables: 3.2.3.1\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.3\r\nopenpyxl: 2.3.2\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.3\r\nlxml: 3.6.4\r\nbs4: 4.5.1\r\nhtml5lib: None\r\n 0.9.2\r\napiclient: None\r\nsqlalchemy: 1.0.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.42.0\r\npandas_datareader: None\r\n\r\n\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Categorical",
      "Difficulty Novice",
      "Effort Low",
      "Msgpack"
    ],
    "comments": [
      "this is almost trivial to add, just add the import (or maybe should just look things up as ``getattr(pd, ....)`` rather than ``globals()[...]``.\r\n\r\nof course some tests would be good!"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 11,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/io/packers.py",
      "pandas/tests/io/test_packers.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15488,
    "reporter": "kernc",
    "created_at": "2017-02-23T18:36:33+00:00",
    "closed_at": "2017-03-07T13:28:16+00:00",
    "resolver": "kernc",
    "resolved_in": "38a34be9108fc76b68e57860506f428d8d67e002",
    "resolver_commit_num": 3,
    "title": "Repr-ing SparseDataFrame after setting a value",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nAfter some value is set on a copy of a SparseSeries, the enclosing SparseDataFrame can no longer be printed (raises AttributeError).\r\n\r\n#### Expected Output\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n0.19.0+479.git",
    "labels": [
      "Indexing",
      "Sparse",
      "Bug"
    ],
    "comments": [
      "These are not implemented, so not sure that \r\n\r\n```\r\nIn [3]: sdf.loc[0,0] = 1\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\n```\r\n\r\nthis should even work"
    ],
    "events": [
      "cross-referenced",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "labeled"
    ],
    "changed_files": 3,
    "additions": 13,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/formats/format.py",
      "pandas/tests/sparse/test_format.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15492,
    "reporter": "chris-b1",
    "created_at": "2017-02-23T20:27:33+00:00",
    "closed_at": "2017-03-03T15:07:47+00:00",
    "resolver": "chris-b1",
    "resolved_in": "04e116851337cd852b4255f8221d9be44829e0e1",
    "resolver_commit_num": 63,
    "title": "ERR: better message for HDFStore date query with non date columns",
    "body": "When querying a `HDFStore` with a timestamp, on a column that (in my case, inadvertently) isn't a datetime, the error message isn't helpful; I think we can trap and show something better here.\r\n\r\n\r\n\r\npandas 0.19.2\r\n",
    "labels": [
      "Error Reporting",
      "IO HDF5",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "what you don't like ``SyntaxError`` :<? (hahha)",
      "your change didn't fail any existing tests, interesting. ok thanks for the fix. merging."
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "commented"
    ],
    "changed_files": 3,
    "additions": 82,
    "deletions": 14,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/computation/pytables.py",
      "pandas/tests/io/test_pytables.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15498,
    "reporter": "jreback",
    "created_at": "2017-02-24T13:38:39+00:00",
    "closed_at": "2017-03-01T21:56:17+00:00",
    "resolver": "jeet63",
    "resolved_in": "1c106c8427513775c59e1e93a20829fc67a0a983",
    "resolver_commit_num": 1,
    "title": "PERF: categorical rank",
    "body": "xref -dev/pandas/pull/15422#discussion_r102941126\r\n\r\neasy enough after #15422 to rank the categories themselves rather than using expanded values; prob most relevant for ``object`` dtypes.\r\n\r\n",
    "labels": [
      "Categorical",
      "Difficulty Novice",
      "Effort Low",
      "Performance"
    ],
    "comments": [
      "@jreback @jorisvandenbossche :  in the _values_for_rank method in Categorical, re-organizing and moving the typecast to float outside the if condition, like below, has the advantage that i can set a single rank function, for categoricals, in the _get_data_algo function in pandas/core/algorithms.py. Which imo is cleaner. Should i move it out or do you think otherwise?  \r\n\r\n````\r\n    def _values_for_rank(self):\r\n        from pandas import Series\r\n        if self.ordered:\r\n            values = self.codes\r\n            mask = values == -1\r\n            values = values.astype('float64')\r\n            if mask.any():\r\n                values[mask] = np.nan\r\n        else:\r\n            values = np.array(\r\n                self.rename_categories(Series(self.categories).rank())\r\n            )\r\n        return values\r\n\r\n\r\n    def _get_data_algo(values, func_map):\r\n\r\n        f = None\r\n\r\n        if is_float_dtype(values):\r\n            f = func_map['float64']\r\n            values = _ensure_float64(values)\r\n        ...\r\n        elif is_categorical_dtype(values):\r\n            f = func_map['float64']\r\n            values = values._values_for_rank()\r\n        ...\r\n\r\n````",
      "yes going to need some reorg \r\nmainly have to pass in the actual rank args themselves (easy enough just pass them thur as kwargs)",
      "@ikilledthecat You can open a PR with the above change, that will be the easiest to discuss (in any case, the above certainly looks reasonable. A reason to keep the astype in the if condition is to avoid a conversion of the data from int to float when not needed, which will give a (small) performance penalty.)\r\n\r\n@jreback why is it needed to pass rank args? The above (or similar) seems OK to me without additional args",
      "Another thing we could do for performance is for the unordered categorical to first check whether the categories are sorted before doing the renaming (from a quick test this checking is much less expensive than the actual renaming). Although that may not be worth the complexity.",
      "so when rank is called on the categories it's fine\r\nbut needs na--position in order to order any na\r\n\r\nthough that will be handled when the categories are re expanded so maybe not needed\r\n\r\nyeah makes for sense that way "
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 5,
    "additions": 69,
    "deletions": 9,
    "changed_files_list": [
      "asv_bench/benchmarks/categoricals.py",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/algorithms.py",
      "pandas/core/categorical.py",
      "pandas/tests/series/test_analytics.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15503,
    "reporter": "adbull",
    "created_at": "2017-02-25T12:52:38+00:00",
    "closed_at": "2017-03-05T22:11:28+00:00",
    "resolver": "jreback",
    "resolved_in": "09360d80da730008a6a89f38f3780bb1d55f9e25",
    "resolver_commit_num": 4263,
    "title": "PERF/API: fast paths for product MultiIndex?",
    "body": "#### Feature Proposal\r\n\r\nAt the moment, we have a few different methods for storing indexed higher-dimensional arrays:\r\n- DataFrame/Series with a product MultiIndex (**PMI**), which can be slow\r\n- Panel, which is fast, but less flexible, and may be deprecated soon (#13563)\r\n- xarray, which is designed for this, but has a smaller API\r\n\r\nFor some datasets, I've found the PMI to be the best option, together with occasional workarounds for performance bottlenecks. Operations which are slow for a general MultiIndex, like `unstack()` or `swaplevel().sortlevel()`, can be sped up for PMIs (see below).\r\n\r\nIt would be great if we could do something like this more generally, with fast paths for PMIs. We could maybe have `MultiIndex.from_product()` return a PMI object, which would upcast to MultiIndex when necessary. We could also have `stack()` and `unstack()` create PMI objects where possible, and perhaps add an argument to `concat()` and `set_index()` to create PMIs. Slow MultiIndex operations could then have a fast path for PMI objects.\r\n\r\n#### Code Sample\r\n\r\n",
    "labels": [
      "Performance",
      "Reshaping"
    ],
    "comments": [
      "So this is quite a lot of work to actually create a separate MI type of index to do this. The bottleneck is not indexing anyhow. Its the reshaping.\r\n\r\n```\r\ndiff --git a/pandas/core/reshape.py b/pandas/core/reshape.py\r\nindex 87cb088..64d89bb 100644\r\n--- a/pandas/core/reshape.py\r\n+++ b/pandas/core/reshape.py\r\n@@ -175,6 +175,7 @@ class _Unstacker(object):\r\n         return DataFrame(values, index=index, columns=columns)\r\n \r\n     def get_new_values(self):\r\n+\r\n         values = self.values\r\n \r\n         # place the values\r\n@@ -184,23 +185,26 @@ class _Unstacker(object):\r\n         result_shape = (length, result_width)\r\n \r\n         # if our mask is all True, then we can use our existing dtype\r\n-        if self.mask.all():\r\n-            dtype = values.dtype\r\n-            new_values = np.empty(result_shape, dtype=dtype)\r\n+        if self.mask.all() and len(values):\r\n+            new_values = self.sorted_values.reshape(result_shape)\r\n+            new_mask = np.ones(result_shape, dtype=bool)\r\n         else:\r\n-            dtype, fill_value = _maybe_promote(values.dtype, self.fill_value)\r\n-            new_values = np.empty(result_shape, dtype=dtype)\r\n-            new_values.fill(fill_value)\r\n-\r\n-        new_mask = np.zeros(result_shape, dtype=bool)\r\n+            new_mask = np.zeros(result_shape, dtype=bool)\r\n+            if self.mask.all():\r\n+                dtype = values.dtype\r\n+                new_values = np.empty(result_shape, dtype=dtype)\r\n+            else:\r\n+                dtype, fill_value = _maybe_promote(values.dtype, self.fill_value)\r\n+                new_values = np.empty(result_shape, dtype=dtype)\r\n+                new_values.fill(fill_value)\r\n \r\n-        # is there a simpler / faster way of doing this?\r\n-        for i in range(values.shape[1]):\r\n-            chunk = new_values[:, i * width:(i + 1) * width]\r\n-            mask_chunk = new_mask[:, i * width:(i + 1) * width]\r\n+            # is there a simpler / faster way of doing this?\r\n+            for i in range(values.shape[1]):\r\n+                chunk = new_values[:, i * width:(i + 1) * width]\r\n+                mask_chunk = new_mask[:, i * width:(i + 1) * width]\r\n \r\n-            chunk.flat[self.mask] = self.sorted_values[:, i]\r\n-            mask_chunk.flat[self.mask] = True\r\n+                chunk.flat[self.mask] = self.sorted_values[:, i]\r\n+                mask_chunk.flat[self.mask] = True\r\n \r\n         return new_values, new_mask\r\n \r\n```\r\n\r\nmakes this about 10x faster, BUT there are several cases that are failing. You are welcome to have a look at seeing if this can pass the test suite.",
      "```\r\nIn [1]: m = 100\r\n   ...: n = 1000\r\n   ...: \r\n   ...: levels = np.arange(m)\r\n   ...: index = pd.MultiIndex.from_product([levels]*2)\r\n   ...: columns = np.arange(n)\r\n   ...: values = np.arange(m*m*n).reshape(m*m, n)\r\n   ...: df = pd.DataFrame(values, index, columns)\r\n   ...: \r\n\r\nIn [2]: %timeit df.unstack()\r\n1 loop, best of 3: 289 ms per loop\r\n\r\n# with change\r\nIn [2]: %timeit df.unstack()\r\n10 loops, best of 3: 33.8 ms per loop\r\n```\r\n",
      "@adbull ok done in #15510 \r\n\r\n(note that my final version is about 2x slower than before), because I have to do multiple reshapings to get things in the correct order. But still about 4x faster.\r\n\r\nyou generally cannot simply do a reshaping, and esp directly on ``.values`` (well you can but you have to do it by dtype; which we already handle internally), and the ordering is a bit tricky.",
      "That's awesome, thanks! I was thinking we'd need a separate type to avoid checking the index each time, but I guess that's not an issue.\r\n\r\nWith `df` as above, would it be possible to do the same for `df.T.stack()`? `df.swaplevel().sortlevel()`?",
      ".stack and .sorting are separate issues \r\n\r\nwhy don't u profile a bit and see where the hotspots are"
    ],
    "events": [
      "commented",
      "commented",
      "labeled",
      "labeled",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "referenced",
      "commented",
      "commented",
      "referenced",
      "referenced"
    ],
    "changed_files": 7,
    "additions": 196,
    "deletions": 15,
    "changed_files_list": [
      "asv_bench/benchmarks/reshape.py",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/reshape.py",
      "pandas/src/reshape.pyx",
      "pandas/src/reshape_helper.pxi.in",
      "pandas/tests/frame/test_reshape.py",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15519,
    "reporter": "jreback",
    "created_at": "2017-02-27T14:49:06+00:00",
    "closed_at": "2017-03-18T02:00:51+00:00",
    "resolver": "jaehoonhwang",
    "resolved_in": "ee19222c98175a99ec47b1359973141bb9f1dc50",
    "resolver_commit_num": 1,
    "title": "TST: move pandas/tests/io/test_date_converters.py to pandas/tests/io/parsers/parse_dates.py",
    "body": "see -dev/pandas/pull/15378#issuecomment-282740398\r\n\r\nwe have centralized ``parse_dates`` testsing with all parsers. Need to move these tests appropriately.\r\n\r\n",
    "labels": [
      "IO CSV",
      "Testing",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "Can I take this? @jreback ",
      "sure!"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 147,
    "deletions": 152,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/tests/io/parser/parse_dates.py",
      "pandas/tests/io/test_date_converters.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15520,
    "reporter": "jreback",
    "created_at": "2017-02-27T16:16:01+00:00",
    "closed_at": "2017-04-26T13:32:58+00:00",
    "resolver": "analyticalmonk",
    "resolved_in": "61ca02274b898cc2dd15f305a0b885e2f6348dd8",
    "resolver_commit_num": 0,
    "title": "BUG: invalid dtypes should raise",
    "body": "xref \r\n\r\nwhen an explict dtype is provided, we should raise if its not a valid ``numpy/pandas`` dtype.\r\n\r\n",
    "labels": [
      "Difficulty Novice",
      "Dtypes",
      "Effort Low",
      "Error Reporting"
    ],
    "comments": [
      "Hi, I am looking into contributing to open source projects and this looks like a good first-time contibution to start with. I'd like to give it a try.",
      "great\r\n\r\ncontributing docs are http://pandas.pydata.org/pandas-docs/stable/contributing.html",
      "Put in a pull request for this issue. This is my first contribution attempt, so please let me know if I'm way off base here. ",
      "Should be addressing this at the source of the problem (raise anytime a function passes a non-valid dtype), or should we raise only when this happens in a Series function call?",
      "Here is my suggestion for this issue: 607ae9c38e7d8f85d3a7f0b161545d5437afd285"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files": 8,
    "additions": 57,
    "deletions": 18,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/dtypes/cast.py",
      "pandas/core/dtypes/common.py",
      "pandas/core/generic.py",
      "pandas/core/series.py",
      "pandas/tests/dtypes/test_common.py",
      "pandas/tests/series/test_constructors.py",
      "pandas/tests/test_strings.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15578,
    "reporter": "jreback",
    "created_at": "2017-03-05T16:25:14+00:00",
    "closed_at": "2017-03-07T13:26:22+00:00",
    "resolver": "mroeschke",
    "resolved_in": "fdee92214dedf87f351f1ae0613d9f25061359b0",
    "resolver_commit_num": 35,
    "title": "Timestamp .round precision for ns",
    "body": "xref -dev/pandas/pull/15568/files#r104304074\r\n\r\n\r\n\r\nThis is the unfortunate problem of converting from int64->float64->int64. I am not sure there is an easy way to fix this. We could:\r\n\r\n- ignore (maybe doc it)\r\n- raise/warn\r\n\r\n",
    "labels": [
      "Error Reporting",
      "Timeseries"
    ],
    "comments": [
      "so actually this is 'easy'; I think for values of round < 1us (e.g. ns values), you can simply do something like this.\r\n\r\nsimulating ``.round('10ns')``\r\n```\r\nIn [48]: i\r\nOut[48]: DatetimeIndex(['2016-10-17 12:00:00.001501031'], dtype='datetime64[ns]', freq=None)\r\n\r\nIn [49]: DatetimeIndex(1000000 * (i.asi8 // 1000000) + 10*(np.round((i.asi8 % 1000000)/float(10))).astype('i8'))\r\nOut[49]: DatetimeIndex(['2016-10-17 12:00:00.001501030'], dtype='datetime64[ns]', freq=None)\r\n\r\n```\r\nIOW, we avoid the float precision problem by only working with the last 6 digits, then adding back the original  portion.\r\n\r\ncc @mroeschke "
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 51,
    "deletions": 7,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/tests/indexes/datetimes/test_ops.py",
      "pandas/tests/scalar/test_timestamp.py",
      "pandas/tseries/base.py",
      "pandas/tslib.pyx"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15605,
    "reporter": "memilanuk",
    "created_at": "2017-03-07T18:45:59+00:00",
    "closed_at": "2017-03-17T15:42:26+00:00",
    "resolver": "lorenzocestaro",
    "resolved_in": "0ad89761df376d52eaee90b52b9b15eb0f06af54",
    "resolver_commit_num": 0,
    "title": "Broken link in cookbook for read_csv",
    "body": "The second link in the section -docs/stable/cookbook.html#csv\r\nis broken.\r\n\r\nI.e. the link [read_csv in action](?p=635) is supposed to go to Wes McKinney's blog, but returns a 404 error.  \r\n\r\nA cursory look at said blog didn't result in anything jumping out at me as far as what *should* be the correct entry to link to.\r\n",
    "labels": [
      "Docs",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "@wesm did this change? ``http://wesmckinney.com/blog/?p=635``",
      "Yeah, i migrated to pelican. think tha tlink may be http://wesmckinney.com/blog/update-on-upcoming-pandas-v0-10-new-file-parser-other-performance-wins/",
      "Hi guys, this is my first attempt to contribute to an open source project. I have a branch ready with the fix: I changed the broken link with the one provided by @wesm.\r\n[This](https://github.com/LorenzoCestaro/pandas/commit/006eefa210fed693030a7fa50374441ef7ee90bf) is the commit with the change on my fork.\r\n\r\nI'd like to know if it is ok if I go ahead and submit a PR, also let me know if there is something else I should do before opening the PR (tests, builds, ...).\r\nThanks!",
      "nope, go ahead an make a PR"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "commented",
      "labeled",
      "labeled",
      "referenced",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files": 1,
    "additions": 1,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/cookbook.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15622,
    "reporter": "mwiebusch78",
    "created_at": "2017-03-08T19:13:50+00:00",
    "closed_at": "2017-04-07T15:18:24+00:00",
    "resolver": "jreback",
    "resolved_in": "f478e4f4b0a353fa48ddb19e70cb9abe5b36e1b5",
    "resolver_commit_num": 4359,
    "title": "sort_index doesn't work after concat",
    "body": "The sort_index method does not seem to work properly if the dataframe was created with concat. See this example:\r\n\r\n\r\n\r\nThe 0.5 tuples should come before the 0.8 ones. Everything works fine if I create the multi-index from a product:\r\n\r\n\r\n\r\nI'm on pandas version 0.18.1.",
    "labels": [
      "API Design",
      "MultiIndex",
      "Difficulty Advanced",
      "Effort Low"
    ],
    "comments": [
      "Thanks for the report.  xref #14015\r\n\r\nThere are two issues here.  The first, which is the linked issue above is that `sort_index` sorts by the ordering of the `levels`, not necessarily lexicographically.  In other words:\r\n\r\n```python\r\ndf = pd.DataFrame(index=pd.MultiIndex(levels=[[0.8, 0.5], ['a', 'b']], labels=[[0, 1], [0, 1]]))\r\n\r\ndf.index\r\nOut[105]: \r\nMultiIndex(levels=[[0.8, 0.5], ['a', 'b']],\r\n           labels=[[0, 1], [0, 1]])\r\n\r\n\r\ndf.sort_index()\r\nOut[106]: \r\nEmpty DataFrame\r\nColumns: []\r\nIndex: [(0.8, a), (0.5, b)]\r\n```\r\n\r\nThe specific problem here (which could be changed) - is that `concat` doesn't lexographically sort levels which constructing the resulting `MultiIndex`.  Most other methods of a constructing a mi do sort the levels, so I think that change would be consistent.\r\n\r\n\r\n\r\n",
      "Is there another method which re-orders a level of a multi-index? I've tried `sortlevel` but that doesn't have an effect either.\r\n\r\n```python\r\n>>> df = pd.DataFrame(index=pd.MultiIndex(levels=[[0.8, 0.5], ['a', 'b']], labels=[[0, 1], [0, 1]]))\r\n>>> df.sortlevel(0).index\r\nMultiIndex(levels=[[0.8, 0.5], ['a', 'b']],\r\n           labels=[[0, 1], [0, 1]])\r\n```",
      "You can call `.sort_values` on the `MultiIndex` itself and pass to `reindex`.\r\n\r\n```python\r\ndf.reindex(index=df.index.sort_values())\r\nOut[156]: \r\nEmpty DataFrame\r\nColumns: []\r\nIndex: [(0.5, b), (0.8, a)]\r\n```"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "cross-referenced",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 15,
    "additions": 593,
    "deletions": 57,
    "changed_files_list": [
      "asv_bench/benchmarks/timeseries.py",
      "doc/source/advanced.rst",
      "doc/source/api.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/core/sorting.py",
      "pandas/indexes/multi.py",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tests/series/test_analytics.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tests/tools/test_hashing.py",
      "pandas/tests/tools/test_pivot.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15635,
    "reporter": "adbull",
    "created_at": "2017-03-09T16:15:03+00:00",
    "closed_at": "2017-03-10T11:40:10+00:00",
    "resolver": "mroeschke",
    "resolved_in": "4ce9c0c9b9ef0c6665a0d9ead1afbfb05a864252",
    "resolver_commit_num": 36,
    "title": "BUG: cython groupby cummin/cummax assumes groups ordered",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nThe current github version of pandas has cython implementations of `groupby.cummin` and `groupby.cummax` which give the wrong answer when the groups are unordered. (See #15048, 0fe491d.)\r\n\r\n#### Expected Output\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.8-100.fc24.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: C\r\nLANG: C\r\nLOCALE: None.None\r\n\r\npandas: 0.19.0+575.g5667a3a\r\npytest: 3.0.5\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.2\r\nnumpy: 1.11.3\r\nscipy: 0.18.1\r\nxarray: 0.9.1\r\nIPython: 4.2.0\r\nsphinx: 1.5.1\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.3.0\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.4.1\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 0.9.6\r\nlxml: 3.7.2\r\nbs4: 4.5.3\r\nhtml5lib: 0.999\r\nsqlalchemy: 1.1.5\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.4\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Difficulty Intermediate",
      "Effort Low",
      "Groupby"
    ],
    "comments": [
      "looks like a bug in how the maxval is updated \r\n\r\nit's not about the group ordering or not but a straight up bug\r\n\r\npr to fix is welcome",
      "cc @mroeschke ",
      "Ah, agreed. I think it happens to get the right answer on ordered groups, which is why the tests passed.\r\n\r\nSuspect it just requires the following, plus tests of course?\r\n\r\n```diff\r\ndiff --git a/pandas/_libs/algos_groupby_helper.pxi.in b/pandas/_libs/algos_groupby_helper.pxi.in\r\nindex 9552b42..c86a4b4 100644\r\n--- a/pandas/_libs/algos_groupby_helper.pxi.in\r\n+++ b/pandas/_libs/algos_groupby_helper.pxi.in\r\n@@ -603,7 +603,7 @@ def group_cummin_{{name}}(ndarray[{{dest_type2}}, ndim=2] out,\r\n     \"\"\"\r\n     cdef:\r\n         Py_ssize_t i, j, N, K, size\r\n-        {{dest_type2}} val, min_val = 0\r\n+        {{dest_type2}} val\r\n         ndarray[{{dest_type2}}, ndim=2] accum\r\n         int64_t lab\r\n \r\n@@ -629,8 +629,7 @@ def group_cummin_{{name}}(ndarray[{{dest_type2}}, ndim=2] out,\r\n                 if val == val:\r\n                 {{endif}}\r\n                     if val < accum[lab, j]:\r\n-                        min_val = val\r\n-                    accum[lab, j] = min_val\r\n+                        accum[lab, j] = val\r\n                     out[i, j] = accum[lab, j]\r\n \r\n \r\n@@ -645,7 +644,7 @@ def group_cummax_{{name}}(ndarray[{{dest_type2}}, ndim=2] out,\r\n     \"\"\"\r\n     cdef:\r\n         Py_ssize_t i, j, N, K, size\r\n-        {{dest_type2}} val, max_val = 0\r\n+        {{dest_type2}} val\r\n         ndarray[{{dest_type2}}, ndim=2] accum\r\n         int64_t lab\r\n \r\n@@ -670,8 +669,7 @@ def group_cummax_{{name}}(ndarray[{{dest_type2}}, ndim=2] out,\r\n                 if val == val:\r\n                 {{endif}}\r\n                     if val > accum[lab, j]:\r\n-                        max_val = val\r\n-                    accum[lab, j] = max_val\r\n+                        accum[lab, j] = val\r\n                     out[i, j] = accum[lab, j]\r\n \r\n {{endfor}}\r\n```",
      "yep that might do it. "
    ],
    "events": [
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 22,
    "deletions": 11,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/_libs/algos_groupby_helper.pxi.in",
      "pandas/tests/groupby/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15640,
    "reporter": "jreback",
    "created_at": "2017-03-10T02:39:56+00:00",
    "closed_at": "2017-03-12T15:16:30+00:00",
    "resolver": "rouzazari",
    "resolved_in": "e0b37f9bb40e2d27629c573bb985d75360282cd4",
    "resolver_commit_num": 4,
    "title": "CLN: cleanup rank tests",
    "body": "we have rank tests in:\r\n\r\nCreate a new ``pandas/tests/series/test_rank``\r\n- ``pandas/tests/series/test_analytics``\r\n\r\nCreate a new ``pandas/tests/frame/test_rank``\r\n- ``pandas/tests/frame/test_analytics``\r\n\r\n- ``pandas/tests/test_algos``\r\n\r\nLet's remove this and split among the other three\r\n- ``pandas/tests/test_stats`` \r\n\r\n",
    "labels": [
      "Clean",
      "Difficulty Novice",
      "Effort Low",
      "Testing"
    ],
    "comments": [
      "@jreback, just so I understand: the end result will have two files (`series/test_rank` and `frame/test_rank`), correct? \r\n\r\nThe `rank`-related tests from all four source files (`series/test_analytics`, `frame/test_analytics`, `test_algos`, and `test_stats`) should end up in the relevant `test_rank` files. All unrelated (i.e. non-`rank`) tests should remain in the existing files. ",
      "yes though you might have some common tests in test_algos (if that makes sense)",
      "Great. I submitted a PR. I believe the tests that remain in `test_algos` should remain there because those tests are not specifically related to `Series` or `DataFrame`; they seem to be related to testing `rank` in general."
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "commented"
    ],
    "changed_files": 5,
    "additions": 592,
    "deletions": 554,
    "changed_files_list": [
      "pandas/tests/frame/test_analytics.py",
      "pandas/tests/frame/test_rank.py",
      "pandas/tests/series/test_analytics.py",
      "pandas/tests/series/test_rank.py",
      "pandas/tests/test_stats.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15662,
    "reporter": "zym1010",
    "created_at": "2017-03-12T15:31:28+00:00",
    "closed_at": "2017-03-15T13:30:35+00:00",
    "resolver": "zym1010",
    "resolved_in": "76e5185a5ad07672688b096acc94ad5a8a2ec18d",
    "resolver_commit_num": 0,
    "title": "Pandas test fails with Scipy 0.19",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nthere are some errors w.r.t. interpolation related functions. Seems that this is related to the change of behavior for some Scipy functions in 0.19. But which is correct?\r\n\r\n~~~\r\n======================================================================\r\nFAIL: test_interp_various (pandas.tests.frame.test_missing.TestDataFrameInterpolate)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/Users/yimengzh/miniconda2/envs/default35/lib/python3.5/site-packages/pandas/tests/frame/test_missing.py\", line 513, in test_interp_various\r\n    assert_frame_equal(result, expected)\r\n  File \"/Users/yimengzh/miniconda2/envs/default35/lib/python3.5/site-packages/pandas/util/testing.py\", line 1313, in assert_frame_equal\r\n    obj='DataFrame.iloc[:, {0}]'.format(i))\r\n  File \"/Users/yimengzh/miniconda2/envs/default35/lib/python3.5/site-packages/pandas/util/testing.py\", line 1181, in assert_series_equal\r\n    obj='{0}'.format(obj))\r\n  File \"pandas/src/testing.pyx\", line 59, in pandas._testing.assert_almost_equal (pandas/src/testing.c:4156)\r\n  File \"pandas/src/testing.pyx\", line 173, in pandas._testing.assert_almost_equal (pandas/src/testing.c:3274)\r\n  File \"/Users/yimengzh/miniconda2/envs/default35/lib/python3.5/site-packages/pandas/util/testing.py\", line 1018, in raise_assert_detail\r\n    raise AssertionError(msg)\r\nAssertionError: DataFrame.iloc[:, 0] are different\r\n\r\nDataFrame.iloc[:, 0] values are different (28.57143 %)\r\n[left]:  [1.0, 2.0, 2.81547781415, 4.0, 5.0, 5.52964175305, 7.0]\r\n[right]: [1.0, 2.0, 2.81621174, 4.0, 5.0, 5.64146581, 7.0]\r\n\r\n======================================================================\r\nFAIL: test_interp_scipy_basic (pandas.tests.series.test_missing.TestSeriesInterpolateData)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/Users/yimengzh/miniconda2/envs/default35/lib/python3.5/site-packages/pandas/tests/series/test_missing.py\", line 711, in test_interp_scipy_basic\r\n    assert_series_equal(result, expected)\r\n  File \"/Users/yimengzh/miniconda2/envs/default35/lib/python3.5/site-packages/pandas/util/testing.py\", line 1181, in assert_series_equal\r\n    obj='{0}'.format(obj))\r\n  File \"pandas/src/testing.pyx\", line 59, in pandas._testing.assert_almost_equal (pandas/src/testing.c:4156)\r\n  File \"pandas/src/testing.pyx\", line 173, in pandas._testing.assert_almost_equal (pandas/src/testing.c:3274)\r\n  File \"/Users/yimengzh/miniconda2/envs/default35/lib/python3.5/site-packages/pandas/util/testing.py\", line 1018, in raise_assert_detail\r\n    raise AssertionError(msg)\r\nAssertionError: Series are different\r\n\r\nSeries values are different (33.33333 %)\r\n[left]:  [1.0, 3.0, 6.82352941176, 12.0, 18.0588235294, 25.0]\r\n[right]: [1.0, 3.0, 6.769231, 12.0, 18.230769, 25.0]\r\n\r\n======================================================================\r\nFAIL: test_cmov_window_regular (pandas.tests.test_window.TestMoments)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/Users/yimengzh/miniconda2/envs/default35/lib/python3.5/site-packages/pandas/tests/test_window.py\", line 878, in test_cmov_window_regular\r\n    tm.assert_series_equal(xp, rs)\r\n  File \"/Users/yimengzh/miniconda2/envs/default35/lib/python3.5/site-packages/pandas/util/testing.py\", line 1181, in assert_series_equal\r\n    obj='{0}'.format(obj))\r\n  File \"pandas/src/testing.pyx\", line 59, in pandas._testing.assert_almost_equal (pandas/src/testing.c:4156)\r\n  File \"pandas/src/testing.pyx\", line 173, in pandas._testing.assert_almost_equal (pandas/src/testing.c:3274)\r\n  File \"/Users/yimengzh/miniconda2/envs/default35/lib/python3.5/site-packages/pandas/util/testing.py\", line 1018, in raise_assert_detail\r\n    raise AssertionError(msg)\r\nAssertionError: Series are different\r\n\r\nSeries values are different (60.0 %)\r\n[left]:  [nan, nan, 9.28667, 10.34667, 12.00556, 13.33889, 13.38, 12.33667, nan, nan]\r\n[right]: [nan, nan, 9.60058823529, 10.8523529412, 12.86, 13.52, 12.7629411765, 12.2070588235, nan, nan]\r\n\r\n======================================================================\r\nFAIL: test_cmov_window_regular_linear_range (pandas.tests.test_window.TestMoments)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/Users/yimengzh/miniconda2/envs/default35/lib/python3.5/site-packages/pandas/tests/test_window.py\", line 895, in test_cmov_window_regular_linear_range\r\n    tm.assert_series_equal(xp, rs)\r\n  File \"/Users/yimengzh/miniconda2/envs/default35/lib/python3.5/site-packages/pandas/util/testing.py\", line 1181, in assert_series_equal\r\n    obj='{0}'.format(obj))\r\n  File \"pandas/src/testing.pyx\", line 59, in pandas._testing.assert_almost_equal (pandas/src/testing.c:4156)\r\n  File \"pandas/src/testing.pyx\", line 173, in pandas._testing.assert_almost_equal (pandas/src/testing.c:3274)\r\n  File \"/Users/yimengzh/miniconda2/envs/default35/lib/python3.5/site-packages/pandas/util/testing.py\", line 1018, in raise_assert_detail\r\n    raise AssertionError(msg)\r\nAssertionError: Series are different\r\n\r\nSeries values are different (60.0 %)\r\n[left]:  [nan, nan, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, nan, nan]\r\n[right]: [nan, nan, 2.35294117647, 3.35294117647, 4.35294117647, 5.35294117647, 6.35294117647, 7.35294117647, nan, nan]\r\n\r\n======================================================================\r\nFAIL: test_cmov_window_regular_missing_data (pandas.tests.test_window.TestMoments)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/Users/yimengzh/miniconda2/envs/default35/lib/python3.5/site-packages/pandas/tests/test_window.py\", line 928, in test_cmov_window_regular_missing_data\r\n    tm.assert_series_equal(xp, rs)\r\n  File \"/Users/yimengzh/miniconda2/envs/default35/lib/python3.5/site-packages/pandas/util/testing.py\", line 1181, in assert_series_equal\r\n    obj='{0}'.format(obj))\r\n  File \"pandas/src/testing.pyx\", line 59, in pandas._testing.assert_almost_equal (pandas/src/testing.c:4156)\r\n  File \"pandas/src/testing.pyx\", line 173, in pandas._testing.assert_almost_equal (pandas/src/testing.c:3274)\r\n  File \"/Users/yimengzh/miniconda2/envs/default35/lib/python3.5/site-packages/pandas/util/testing.py\", line 1018, in raise_assert_detail\r\n    raise AssertionError(msg)\r\nAssertionError: Series are different\r\n\r\nSeries values are different (80.0 %)\r\n[left]:  [nan, nan, 9.33167, 9.76125, 9.28667, 10.34667, 12.00556, 13.82125, 14.49429, 13.765]\r\n[right]: [nan, nan, 9.61230769231, 9.24125, 9.60058823529, 10.8523529412, 12.86, 14.3857142857, 14.1308333333, 13.3433333333]\r\n\r\n======================================================================\r\nFAIL: test_cmov_window_special (pandas.tests.test_window.TestMoments)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/Users/yimengzh/miniconda2/envs/default35/lib/python3.5/site-packages/pandas/tests/test_window.py\", line 955, in test_cmov_window_special\r\n    tm.assert_series_equal(xp, rs)\r\n  File \"/Users/yimengzh/miniconda2/envs/default35/lib/python3.5/site-packages/pandas/util/testing.py\", line 1181, in assert_series_equal\r\n    obj='{0}'.format(obj))\r\n  File \"pandas/src/testing.pyx\", line 59, in pandas._testing.assert_almost_equal (pandas/src/testing.c:4156)\r\n  File \"pandas/src/testing.pyx\", line 173, in pandas._testing.assert_almost_equal (pandas/src/testing.c:3274)\r\n  File \"/Users/yimengzh/miniconda2/envs/default35/lib/python3.5/site-packages/pandas/util/testing.py\", line 1018, in raise_assert_detail\r\n    raise AssertionError(msg)\r\nAssertionError: Series are different\r\n\r\nSeries values are different (60.0 %)\r\n[left]:  [nan, nan, 9.86851, 11.02969, 11.65161, 12.75129, 12.90702, 12.83757, nan, nan]\r\n[right]: [nan, nan, 9.95592026795, 11.1636020642, 11.8220886145, 12.6904154014, 12.797644924, 12.8491264878, nan, nan]\r\n\r\n======================================================================\r\nFAIL: test_cmov_window_special_linear_range (pandas.tests.test_window.TestMoments)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/Users/yimengzh/miniconda2/envs/default35/lib/python3.5/site-packages/pandas/tests/test_window.py\", line 973, in test_cmov_window_special_linear_range\r\n    tm.assert_series_equal(xp, rs)\r\n  File \"/Users/yimengzh/miniconda2/envs/default35/lib/python3.5/site-packages/pandas/util/testing.py\", line 1181, in assert_series_equal\r\n    obj='{0}'.format(obj))\r\n  File \"pandas/src/testing.pyx\", line 59, in pandas._testing.assert_almost_equal (pandas/src/testing.c:4156)\r\n  File \"pandas/src/testing.pyx\", line 173, in pandas._testing.assert_almost_equal (pandas/src/testing.c:3274)\r\n  File \"/Users/yimengzh/miniconda2/envs/default35/lib/python3.5/site-packages/pandas/util/testing.py\", line 1018, in raise_assert_detail\r\n    raise AssertionError(msg)\r\nAssertionError: Series are different\r\n\r\nSeries values are different (60.0 %)\r\n[left]:  [nan, nan, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, nan, nan]\r\n[right]: [nan, nan, 2.07210900201, 3.07210900201, 4.07210900201, 5.07210900201, 6.07210900201, 7.07210900201, nan, nan]\r\n\r\n----------------------------------------------------------------------\r\nRan 10252 tests in 287.945s\r\n\r\nFAILED (SKIP=627, failures=7)\r\n~~~\r\n\r\n#### Expected Output\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 8.1.2\r\nsetuptools: 26.1.1.post20160901\r\nCython: None\r\nnumpy: 1.11.3\r\nscipy: 0.19.0\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.6.1\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "Numeric",
      "Compat",
      "Testing"
    ],
    "comments": [
      "well, since scipy 0.19.0 was just released and something I imagine changed I am not surprised. Would you have a look and see what changed and/or provide a PR to fix? (keeping in mind that we are back-compat thru scipy 0.13)",
      "further scipy is not on the default conda channels as of yet, so this doesn't hit our tests yet.",
      "@jreback I will have a look at it later this week.",
      "@jreback after some digging, I think those `cmov_window` errors should be due to <https://github.com/scipy/scipy/pull/6483>, and those `interp` errors should be due to change of implementation for quadratic and cubic interpolation.\r\n\r\nTo fix the first one, it should do to pass an additional argument `False` to <https://github.com/pandas-dev/pandas/blob/648ae4f03622d8eafe1ca3b833bd6a99f56bece4/pandas/core/window.py#L547>. For the second, no idea.",
      "@zym1010 for 1st one: so looks like they added an argument. Were we just ignoring this before? (so its now required). did they give *any* warning? is it back-compat to prior versions? (if so, let's just add the arg, no big deal).\r\n\r\nFor the 2nd just add in a conditional in the tests:\r\n\r\n```\r\nif scipy.__version__ >= LooseVersion('0.19.0'):\r\n    # this results\r\nelse:\r\n    # old results\r\n```\r\nWe do this in a couple of other places as well for scipy results.\r\n\r\ncan you do a PR? (note that we are not actually testing with 0.19.0 yet as we don't have a fully conda-forge build for a couple of reasons), but I think I can change one of ours to do that.\r\n\r\n",
      "@jreback for 1st one: the argument is always there, and it's nullified if the window length is odd in previous versions. Now it's not the case. So in some sense I would say this is a bug on pandas' side. But yeah, they gave no warning on this nullifying behavior, and I don't think this change is in their release notes.\r\n\r\nfor second one. Alright. I will take your approach. BTW, the change is due to their spline generator (previously they used `splmake`, now not). See <https://github.com/scipy/scipy/issues/6710>.\r\n\r\n> can you do a PR?\r\n\r\nNo problem. Should be done by midnight today when I'm more or less done with other work.",
      "@zym1010 awesome! I will put in place a PR to have testing on conda-forge (so we use 0.19.0 for scipy soon). It will fail master, but hopefully your PR should fix!.",
      "ok, merged #15668 \r\n\r\nchange the skipping on scipy 0.19.0 when you push.",
      "@jreback can you have a look at #15689? I'm not sure if I \"change the skipping on scipy 0.19.0\" correctly."
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 5,
    "additions": 46,
    "deletions": 18,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/window.py",
      "pandas/tests/frame/test_missing.py",
      "pandas/tests/series/test_missing.py",
      "pandas/tests/test_window.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15670,
    "reporter": "gwpdt",
    "created_at": "2017-03-13T14:55:36+00:00",
    "closed_at": "2017-03-13T15:02:29+00:00",
    "resolver": "gwpdt",
    "resolved_in": "37e5f78b4e9ff03cbff4dea928445cc3b1f707c8",
    "resolver_commit_num": 0,
    "title": "Date Type Corrupting Other Types in Group-by/Apply",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n\r\n#### Problem description\r\n\r\nWhen I change the type of the Date column to a Pandas datetime, it causes other columns' types to change in unexpected ways when doing a group-by/apply.  Notice the contents of the \"Str\" column changes to a numeric type in the final group-by/apply (a contributing factor is probably that one of the elements is the string \"inf\").  The \"inf\" value has become inf, and the \"foo\" value has become NaN.\r\n\r\n#### Expected Output\r\n\r\nI expect the Str column to remain a string type, and contain the original strings.  I.e.:\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.11.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.10.0-327.10.1.el7.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: 1.3.7\r\npip: None\r\nsetuptools: 0.6\r\nCython: 0.24.1\r\nnumpy: 1.11.1\r\nscipy: 0.18.0\r\nstatsmodels: 0.6.1\r\nxarray: 0.7.0\r\nIPython: 5.0.0\r\nsphinx: 1.3.5\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.6.1\r\nblosc: None\r\nbottleneck: 1.1.0\r\ntables: 3.2.3.1\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.3.2\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.2\r\nlxml: 3.6.1\r\nbs4: 4.4.1\r\nhtml5lib: 0.999\r\n None\r\napiclient: None\r\nsqlalchemy: 1.0.13\r\npymysql: 0.6.7.None\r\npsycopg2: 2.5.4 (dt dec pq3 ext)\r\njinja2: 2.8\r\nboto: 2.40.0\r\npandas_datareader: None\r\n\r\n</details>\r\n",
    "labels": [],
    "comments": [
      "this is a duplicate of this: https://github.com/pandas-dev/pandas/issues/14423\r\n\r\nsoln is pretty easy if you'd like to do a PR"
    ],
    "events": [],
    "changed_files": 3,
    "additions": 54,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/groupby.py",
      "pandas/tests/groupby/test_groupby.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15687,
    "reporter": "8one6",
    "created_at": "2017-03-14T18:04:56+00:00",
    "closed_at": "2017-04-07T15:18:24+00:00",
    "resolver": "jreback",
    "resolved_in": "f478e4f4b0a353fa48ddb19e70cb9abe5b36e1b5",
    "resolver_commit_num": 4359,
    "title": "sort_index fails on MutiIndex'ed DataFrame resulting from groupby.apply",
    "body": "I feel like this is part of a suite of bugs that come from a failure to notice when a `MultiIndex` that was once `lexsort`ed loses its `lexsort`edness.  I submitted one example of this a couple years ago (see #8017), but this related bug persists.\r\n\r\nOn Pandas 0.19.0:\r\n\r\n\r\n\r\nSo before the `apply` command, `df` was properly sorted on the row index.  However, as you can see, `res` is not properly sorted, even though its creation ends with a `sort_index` command.\r\n\r\nThis is a bug, right?  I would think we want people to be able to assume that anytime they call `sort_index` that the result comes out lexicographically sorted, no?\r\n",
    "labels": [
      "Groupby",
      "MultiIndex",
      "Reshaping"
    ],
    "comments": [
      "this is a dupe of #15622 \r\n\r\nyes the bug here is that it is constructed in a different order when concat is used (though still lexsorted). ``.concat`` is used inside ``groupby.apply`` here",
      "actually this is not a dupe of #15622, rather this is correct.\r\n\r\n@8one6 see below. lexsortedness has to do with the label orderings w.r.t. to the levels (which may or may not be sorted). see if the below makes sense to you\r\n\r\n```\r\n# the result from above\r\nIn [7]: res\r\nOut[7]: \r\n                       near       far\r\nletter size                          \r\na      big   newz  0.978738  2.240893\r\n             newa  1.764052  0.400157\r\n       small newz  0.950088 -0.151357\r\n             newa  1.867558 -0.977278\r\nb      big   newz  0.144044  1.454274\r\n             newa -0.103219  0.410599\r\n       small newz  0.443863  0.333674\r\n             newa  0.761038  0.121675\r\n\r\n# just setting names and a count\r\nIn [8]: res.index.names=['letter','size','other']\r\n\r\nIn [9]: res['count'] = np.arange(len(res))\r\n\r\nIn [10]: res\r\nOut[10]: \r\n                        near       far  count\r\nletter size  other                           \r\na      big   newz   0.978738  2.240893      0\r\n             newa   1.764052  0.400157      1\r\n       small newz   0.950088 -0.151357      2\r\n             newa   1.867558 -0.977278      3\r\nb      big   newz   0.144044  1.454274      4\r\n             newa  -0.103219  0.410599      5\r\n       small newz   0.443863  0.333674      6\r\n             newa   0.761038  0.121675      7\r\n```\r\n\r\nThis is a resort based on the 3 columns, we are reconstructing things here (IOW starting new), NOT from the existing levels\r\n```\r\nIn [11]: res.reset_index().sort_values(['letter', 'size', 'other'])\r\nOut[11]: \r\n  letter   size other      near       far  count\r\n1      a    big  newa  1.764052  0.400157      1\r\n0      a    big  newz  0.978738  2.240893      0\r\n3      a  small  newa  1.867558 -0.977278      3\r\n2      a  small  newz  0.950088 -0.151357      2\r\n5      b    big  newa -0.103219  0.410599      5\r\n4      b    big  newz  0.144044  1.454274      4\r\n7      b  small  newa  0.761038  0.121675      7\r\n6      b  small  newz  0.443863  0.333674      6\r\n\r\n# if we set it again, its sorted (we set it this way)\r\nIn [12]: res.reset_index().sort_values(['letter', 'size', 'other']).set_index(['letter', 'size', 'other'])\r\nOut[12]: \r\n                        near       far  count\r\nletter size  other                           \r\na      big   newa   1.764052  0.400157      1\r\n             newz   0.978738  2.240893      0\r\n       small newa   1.867558 -0.977278      3\r\n             newz   0.950088 -0.151357      2\r\nb      big   newa  -0.103219  0.410599      5\r\n             newz   0.144044  1.454274      4\r\n       small newa   0.761038  0.121675      7\r\n             newz   0.443863  0.333674      6\r\n\r\n# this IS lexsorted\r\nIn [13]: res.reset_index().sort_values(['letter', 'size', 'other']).set_index(['letter', 'size', 'other']).index.is_lexsorted()\r\nOut[13]: True\r\n\r\nIn [14]: res.reset_index().sort_values(['letter', 'size', 'other']).set_index(['letter', 'size', 'other']).index\r\nOut[14]: \r\nMultiIndex(levels=[['a', 'b'], ['big', 'small'], ['newa', 'newz']],\r\n           labels=[[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 0, 0, 1, 1], [0, 1, 0, 1, 0, 1, 0, 1]],\r\n           names=['letter', 'size', 'other'])\r\n\r\n# as is the original (but notice that the levels themselves are ordered differently).\r\nIn [15]: res.index.is_lexsorted()\r\nOut[15]: True\r\n\r\nIn [16]: res.index\r\nOut[16]: \r\nMultiIndex(levels=[['a', 'b'], ['big', 'small'], ['newz', 'newa']],\r\n           labels=[[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 0, 0, 1, 1], [0, 1, 0, 1, 0, 1, 0, 1]],\r\n           names=['letter', 'size', 'other'])\r\n```\r\n\r\nI agree this is a tricky concept and we don't reorder level values when sorting.",
      "this is also related to this: https://github.com/pandas-dev/pandas/issues/14672 (or maybe that will make more sense)",
      "First off, thanks for taking the timer to answer this.\r\n\r\nBut can you help me get my head around this: Are you saying this behavior is correct as above (i.e. that the goal of `sort_index` is to sort things in the order specified in the internals of the `MultiIndex`), or are you agreeing that the example above demonstrates a bug (and that `sort_index` should always return something that is sorted in the traditional use of the term...that is alphabetically)?\r\n\r\nIf the former, let me raise my hand to say \"isn't this going to confuse a ton of people\"?  And if the former, is there another method which would achieve the simple end of \"sorting the index\" in the usual way?  And if the former, can we try to put this in the docs somewhere?  If we're going this way, then I think the intended behavior of `sort_index` is likely to get users into a lot of trouble with unexpected behaviors.\r\n\r\nIf the latter, what, if anything, can be done to fix the current behavior?\r\n\r\nThanks!",
      "well its currently the former. and yes its confusing. I believe its this ways because this can have a detrimental effect on performance (though its only if you are actually sorting that this matters).\r\n\r\nI will reopen this because I think I can fix this and actually make it sort w/o regard to the lexsortedness.",
      "I think I can (if I twist my head around funny) understand a context where a person might want to take advantage of the existing behavior.  For example it could be a bit like an ordered categorical, where I just decide that for me the fruits `['grape', 'apple', 'cantaloupe']` should be considered in that order within their level (maybe i'm thinking about ordering fruits based on size).  And then maybe, just maybe, when I sort a `MultiIndex` that contains those as a level, that I want them to come out in this very not alphabetical ordering.\r\n\r\nBut I'd stress that a) I can't imagine that's the more common use case and b) as of now I think the behavior is totally undocumented.  I don't fully understand how the guts work here...but if I can help by proofreading new documentation or testing new functions, I'd love to.\r\n\r\nComing at this from my use cases, I would think that it is incredibly rare to see people deliberately creating ordered levels on a `MultiIndex` in any context other than an explicit call to the constructor.  So for me, I'd love to see `sort_index` always do a \"natural\", \"alphabetical\" sort (leaving it to others to decide on how to compare across `type`s).  And perhaps there could be a `sort_index_bespoke` or something, or a kwarg for `sort_index` itself that would preserve the current behavior for those (uncommon?) cases where it is really desired.\r\n\r\nI've found myself using the hack of \r\n```\r\ndf.loc[sorted(df.index), :]\r\n```\r\nbut that's just ugly, right?"
    ],
    "events": [
      "commented",
      "closed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "reopened",
      "milestoned",
      "demilestoned",
      "unlabeled",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 15,
    "additions": 593,
    "deletions": 57,
    "changed_files_list": [
      "asv_bench/benchmarks/timeseries.py",
      "doc/source/advanced.rst",
      "doc/source/api.rst",
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/frame.py",
      "pandas/core/groupby.py",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/core/sorting.py",
      "pandas/indexes/multi.py",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tests/series/test_analytics.py",
      "pandas/tests/test_multilevel.py",
      "pandas/tests/tools/test_hashing.py",
      "pandas/tests/tools/test_pivot.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15716,
    "reporter": "amirshavit",
    "created_at": "2017-03-17T11:18:26+00:00",
    "closed_at": "2017-04-03T12:43:26+00:00",
    "resolver": "funnycrab",
    "resolved_in": "7059d898511a62710d6bd6487c8b40d7f535c1a1",
    "resolver_commit_num": 1,
    "title": "to_json float precision bug",
    "body": "xref #15864 for more tests\r\n\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\n...\r\n\r\n#### Expected Output\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-66-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 34.3.2\r\nCython: 0.24.1\r\nnumpy: 1.12.0\r\nscipy: 0.18.1\r\nstatsmodelsdouble_precision: 0.6.1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.4.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: 1.1.0\r\ntables: 3.2.3.1\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.3\r\nopenpyxl: 2.3.2\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.3\r\nlxml: 3.6.4\r\nbs4: 4.5.1\r\nhtml5lib: None\r\n 0.10.3\r\napiclient: 1.5.3\r\nsqlalchemy: 1.1.2\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.7.3\r\nboto: 2.42.0\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "Difficulty Intermediate",
      "Effort Medium",
      "IO JSON"
    ],
    "comments": [
      "I think this is expected. You have 17 decimal places (`len('99999999999999944')`), and the max is 15.",
      "I think 1.0 is expected, not 0.1",
      "Ha, indeed. Sorry I missed that. Interested in taking a look at what's going on? Probably somewhere around [here](https://github.com/pandas-dev/pandas/blob/61f6f6333fb7bb2dedf82736aee6c9878382a06f/pandas/_libs/src/ujson/python/objToJSON.c#L2368). That's all implemented in C, so maybe a bit tricky."
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 75,
    "deletions": 5,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/_libs/src/ujson/lib/ultrajsonenc.c",
      "pandas/tests/io/json/test_pandas.py",
      "pandas/tests/io/json/test_ujson.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15723,
    "reporter": "ozak",
    "created_at": "2017-03-17T17:48:10+00:00",
    "closed_at": "2017-03-21T21:54:22+00:00",
    "resolver": "bashtage",
    "resolved_in": "1c9d46a3bb8737c877b0a15aaea15dfb0172ac1c",
    "resolver_commit_num": 30,
    "title": "ERR: validate encoding on to_stata",
    "body": "It seems ``pandas`` in ``python3.5`` causes issues due to encoding. For example the following generates a corrupt output file\r\n\r\n\r\n\r\nwhile \r\n\r\n\r\n\r\ngenerates a correct file. I imagine this may be due to use of encoding and the difference in the treatment between ``python 2`` and ``python 3``, which breaks compatibility of scripts across ``python`` versions. I guess it would be nice if it does not take this option into account on ``python 3``, unless the error is caused by something else.\r\n",
    "labels": [
      "Difficulty Intermediate",
      "Effort Medium",
      "Error Reporting",
      "IO Stata",
      "Unicode"
    ],
    "comments": [
      "```\r\nIn [8]: df1 = pd.DataFrame(np.array([1,2,3,4]), columns=['var1'])\r\n   ...: df1.to_stata('corrupt.dta', write_index=False, encoding='latin1')\r\n   ...: \r\n   ...: \r\n   ...: \r\n\r\nIn [9]: pd.read_stata('corrupt.dta')\r\nOut[9]: \r\n   var1\r\n0     1\r\n1     2\r\n2     3\r\n3     4\r\n\r\n```\r\n\r\ndoc-string\r\n```\r\nSignature: df1.to_stata(fname, convert_dates=None, write_index=True, encoding='latin-1', byteorder=None, time_stamp=None, data_label=None, variable_labels=None)\r\nDocstring:\r\nA class for writing Stata binary dta files from array-like objects\r\n\r\nParameters\r\n----------\r\nfname : str or buffer\r\n    String path of file-like object\r\nconvert_dates : dict\r\n    Dictionary mapping columns containing datetime types to stata\r\n    internal format to use when wirting the dates. Options are 'tc',\r\n    'td', 'tm', 'tw', 'th', 'tq', 'ty'. Column can be either an integer\r\n    or a name. Datetime columns that do not have a conversion type\r\n    specified will be converted to 'tc'. Raises NotImplementedError if\r\n    a datetime column has timezone information\r\nwrite_index : bool\r\n    Write the index to Stata dataset.\r\nencoding : str\r\n    Default is latin-1. Unicode is not supported\r\nbyteorder : str\r\n    Can be \">\", \"<\", \"little\", or \"big\". default is `sys.byteorder`\r\ntime_stamp : datetime\r\n    A datetime to use as file creation date.  Default is the current\r\n    time.\r\ndataset_label : str\r\n    A label for the data set.  Must be 80 characters or smaller.\r\nvariable_labels : dict\r\n    Dictionary containing columns as keys and variable labels as\r\n    values. Each label must be 80 characters or smaller.\r\n\r\n```\r\n\r\nI would say this is technically correct, passing a unicode encoding is invalid. But I think we should simply reject these, rather than actually write an invalid format. want to do a PR to do this? (now I am not sure which encoding stata can actually support, any idea?)",
      "I am a it confused now. From the docs it should fail in both ``python 2`` and ``3``, but in ``python 2``  \r\n\r\n    df1.to_stata('corrupt.dta', write_index=False, encoding='utf8')\r\n\r\ngenerates the correct file. So, I would think it may be better to just ignore the option in ``python 3``, but keep it in ``python 2``.\r\n\r\nI think ``stata 14`` now supports ``UTF8`` as a default, but it actually may be more general, not 100% sure ([see here](http://www.stata.com/manuals14/dunicodeencoding.pdf)). I'll try to find some time to write a PR. I'll see what the code actually does. I'll let you know.",
      "cc @bashtage any ideas here?",
      "There is no UTF8 in Stata.  Only ASCII And the simple 8 bit encoding Latin-1. ",
      "In Stata I mean in `to_stata`.  Adding UTF8 is a major effort since the current format is all fixed width and I don't see much of a case for making the effort. ",
      "@bashtage so we should then validate ``encoding='ascii'|'latin1'|None`` as the only allowed encodings at all.",
      "So why doesn't it generate a corrupt file in ``python 2``, but does in ``python 3``? I have the same ``pandas`` version in both, so it may not be ``pandas`` specific?",
      "because its actually encoding it in PY3 with the passed in encoding (utf8), rather than the default of latin1.",
      "So in PY2 it is not encoded as UTF8 even when giving the option? How is it saved then? What in ``pandas`` is affecting the IO to Stata that corrupts the file? Given that Stata14 uses UTF8 as the default it should not have an issue opening UTF8 encoded files.",
      "Just noticed one more thing\r\n\r\n```python\r\nimport pandas as pd\r\ndf1 = pd.DataFrame(np.array([u'\u00e1',u'\u00d6']), columns=['var1'])\r\ndf1.to_stata('not-corrupt.dta', write_index=False, encoding='utf8')\r\n\r\ndf = pd.read_stata('corrupt3.dta')\r\ndf == df1\r\n```\r\n\r\ngenerates a usable file in both PY2 and PY3. Still, the data is wrong as seen in the example.",
      "> So in PY2 it is not encoded as UTF8 even when giving the option? \r\n\r\nnot really sure it is *actually* encoding as utf8 internally. lots of things in py2 are wonky. it probably happens to work.",
      "in any event. seems you have a bunch of tests cases! it seems easy enough to simply validate the encoding that is passed and raise if its not valid.",
      "Indeed. \r\n\r\nI guess my example shows that PY2 is not encoding at all."
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 32,
    "deletions": 7,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/io/stata.py",
      "pandas/tests/io/test_stata.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15755,
    "reporter": "toobaz",
    "created_at": "2017-03-21T00:25:11+00:00",
    "closed_at": "2017-03-21T18:06:01+00:00",
    "resolver": "toobaz",
    "resolved_in": "163d18ed0d46eeb375f8170f1044808ff40b2a65",
    "resolver_commit_num": 20,
    "title": "nrows incompatible with chunksize in read_csv",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n\r\n(as introduced by #6774 )\r\n\r\n#### Problem description\r\n\r\nSupporting them together is not complicated - PR on its way.",
    "labels": [
      "API Design",
      "Enhancement",
      "IO CSV"
    ],
    "comments": [],
    "events": [
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files": 4,
    "additions": 37,
    "deletions": 24,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/io/parsers.py",
      "pandas/tests/io/parser/common.py",
      "pandas/tests/io/parser/test_unsupported.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15764,
    "reporter": "jreback",
    "created_at": "2017-03-21T12:50:34+00:00",
    "closed_at": "2017-03-22T11:57:12+00:00",
    "resolver": "StanczakDominik",
    "resolved_in": "2a3b05a3a7167c7b384375e9442c350f740e9629",
    "resolver_commit_num": 0,
    "title": "CLN/INT: rename *possibly* -> *maybe* functions",
    "body": "we have a mixed bag on naming this. Let's change ``possibly`` -> ``maybe``. Keep everything else the same. (if you do the same search for ``maybe`` you will fine it to be far more common). Note that the *usage* of these functions will have to be renamed as well :>\r\n\r\n",
    "labels": [
      "Clean",
      "Difficulty Novice",
      "Effort Medium",
      "Internals"
    ],
    "comments": [
      "cc @gfyoung ",
      "Seem reasonable.",
      "also (separately), we should rename all of the functions in ``pandas.types.cast`` to remove the leading ``_``. Just for some readability (they were like this because they were explicity private when I ported from ``pandas.core.common``). ``pandas.types`` is private so this is not big deal now.",
      "Figured I would take a stab at this one."
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "commented",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 22,
    "additions": 228,
    "deletions": 231,
    "changed_files_list": [
      "pandas/computation/expr.py",
      "pandas/core/algorithms.py",
      "pandas/core/categorical.py",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/groupby.py",
      "pandas/core/internals.py",
      "pandas/core/nanops.py",
      "pandas/core/ops.py",
      "pandas/core/panel.py",
      "pandas/core/reshape.py",
      "pandas/core/series.py",
      "pandas/indexes/base.py",
      "pandas/indexes/frozen.py",
      "pandas/io/parsers.py",
      "pandas/sparse/array.py",
      "pandas/sparse/frame.py",
      "pandas/tests/types/test_cast.py",
      "pandas/tools/util.py",
      "pandas/tseries/index.py",
      "pandas/tseries/tdi.py",
      "pandas/types/cast.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15777,
    "reporter": "mroeschke",
    "created_at": "2017-03-22T05:44:04+00:00",
    "closed_at": "2017-04-08T21:59:13+00:00",
    "resolver": "mroeschke",
    "resolved_in": "f35209e2279154ed7060dd0e17d41da96f9c0186",
    "resolver_commit_num": 39,
    "title": "Bug: Timestamp removes timezone localization",
    "body": "\r\n\r\n#### Problem description\r\n\r\nLocalization of the timezone should be maintained when creating a Timestamp. I also believe this is the root issue of #13238 since `resample()` calls `groupby()` which (I think) reconstructs the index with Timestamps.\r\n\r\n\r\n\r\n#### Expected Output\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 92239f5dcfb02f97b5b1eed651895fe70dfd7eb1\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-45-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.0+644.g92239f5.dirty\r\npytest: 3.0.6\r\npip: 8.1.2\r\nsetuptools: 27.2.0\r\nCython: 0.24.1\r\nnumpy: 1.11.2\r\nscipy: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.4.8\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 1.5.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n",
    "labels": [
      "Timeseries",
      "Timezones",
      "Bug",
      "Difficulty Intermediate",
      "Effort Medium",
      "Prio-high"
    ],
    "comments": [
      "xref https://github.com/pandas-dev/pandas/issues/7825\r\nxref https://github.com/pandas-dev/pandas/issues/11481\r\n\r\nthat lists a bunch of relatives issues and cases all stemming from this\r\nthe constructor is off when doing this - it should be changed to work like DTI\r\n\r\nfixing this would be great! the ",
      "@mroeschke some additional DST crossing examples in #15823 ",
      "Thanks @jreback.\r\n\r\nI believe I found the issue here: https://github.com/pandas-dev/pandas/blob/master/pandas/_libs/tslib.pyx#L220. In `tz_convert_single` there's a check (`_is_tzlocal`) and codepath for localize timezones. The check seems to be specifically for `dateutil` objects while `pytz` objects can pass through here as well. Including a check for localized pytz objects (i.e. `tz._tzname == 'LMT'`, might be a better check than this) seemed to fix this issue. Will investigate further tonight. ",
      "yeah the treatment is prob a bit off. see also ``_localize_tso`` which *does* handle this correctly.\r\n\r\nnote that ``_is_fixed_offset`` picks this up correctly. so maybe this is only a problem when ``_is_tzlocal`` is called and NOT ``_is_fixed_offset``).\r\n\r\nhappy to have this restructued btw. odd that more things are not failing.",
      "Unfortunately my initial suggestion did not fix the issue after playing around with it.\r\n\r\nOn a related note, it seems like localizing a naive `Timestamp` (which should be the same as constructing a Timestamp with a tz) operates the same as pytz.\r\n\r\n```\r\nIn [3]: d = datetime(2017, 1, 1)\r\n\r\nIn [4]: tz = pytz.timezone('US/Pacific')\r\n\r\nIn [5]: tz\r\nOut[5]: <DstTzInfo 'US/Pacific' LMT-1 day, 16:07:00 STD>\r\n\r\n#localize with pytz\r\nIn [6]: tz.localize(d).tzinfo\r\nOut[6]: <DstTzInfo 'US/Pacific' PST-1 day, 16:00:00 STD>\r\n\r\n#localize with pd.Timestamp (should be same as Timestamp(d, tz=tz))\r\nIn [8]: pd.Timestamp(d).tz_localize(tz).tz\r\nOut[8]: <DstTzInfo 'US/Pacific' PST-1 day, 16:00:00 STD>\r\n\r\n#localize with pd.DatetimeIndex\r\nIn [9]: pd.DatetimeIndex([d]).tz_localize(tz).tz\r\nOut[9]: <DstTzInfo 'US/Pacific' LMT-1 day, 16:07:00 STD>\r\n```\r\n\r\n`'US/Pacific' LMT` and `'US/Pacific' PST` have different UTC offsets (~7.8 hours vs. ~8 hours respectively). I am not a timezone expert so I am not sure if these three examples should fundamentally agree or if Pandas has a different strategy for storing time zones? It would be nice if these all fundamentally agreed though.",
      "these conceptually HAVE to be different. The timezone on a single localized timestamp is defined exactly. However, the string tz on a DatetimeIndex is something like 'US/Pacific', it *cannot* itself be localized because it doesn't have a reference date. It actually has *many* reference dates (e.g. each point in the index). So which one shall you pick?\r\n\r\nso the tzinfo on a DTI is just today's I think.\r\n\r\nNote in practice this doesn't actually make any difference, its just a display thing.",
      "Ah okay that makes sense that a DatetimeIndex has multiple reference dates while a Timestamp has a definitive reference date, and that this is a display thing."
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "milestoned",
      "demilestoned",
      "labeled",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 91,
    "deletions": 7,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/_libs/tslib.pyx",
      "pandas/tests/series/test_indexing.py",
      "pandas/tests/tseries/test_timezones.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15787,
    "reporter": "seth-a",
    "created_at": "2017-03-23T15:51:42+00:00",
    "closed_at": "2017-04-10T12:10:24+00:00",
    "resolver": "funnycrab",
    "resolved_in": "9cb2c2db0dd763bb9e6586d3103a564875ed25d5",
    "resolver_commit_num": 7,
    "title": "BUG: concat of MultiIndex with names passed",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\nIn python, datatypes generally don't matter.  A dataframe is a dataframe, but as shown in the example code concat'ing dataframes with an index does not have the same behavior as  dataframes without an index.  The label for a level of the index is dropped.  This is a small bug. Run it several times (10-12 seems to do it) and you will  see a much more worrisome issue: on occasion, the label is not dropped.  Yes, the output of concat is random.  \r\n#### Expected Output\r\n\r\n#### Output of ``pd.show_versions()``\r\n`",
    "labels": [
      "Reshaping",
      "Difficulty Intermediate",
      "Effort Low",
      "Bug",
      "MultiIndex"
    ],
    "comments": [
      "> In python, datatypes generally don't matter. A dataframe is a dataframe, but as shown in the example \r\n\r\nnot sure what you mean",
      "can you update your example to show exactly what is the problem. you seems to be doing lots of stuff, but its not clear what the issue is (if any).\r\n\r\nand show a constructed frame that matches your expectations. IOW if this *is* a bug, then ultimately we need an example that we can do something like\r\n\r\n```\r\nfrom pandas.util import testing as tm\r\nresult = .....\r\nexpected = DataFrame(....)\r\ntm.assert_frame_equal(result, expected)\r\n```\r\n",
      "The label of the second column of the index is being changed to None.  Run the example, uncomment the line that says \"Uncomment for example of correct behavior\" to see what is expected.  Yes, the example is doing lots of \"stuff\", but that's what it takes to reproduce the bug.",
      "@seth-a can you post an expected result and then run your example and post. the more information you show and the more obvious the quicker this will get looked at.",
      "I've updated the formatting in the expected results section, are you looking for something beyond what is there?",
      "slightly modified.\r\n\r\n```\r\nIn [1]: import numpy as np\r\n   ...: import pandas as pd\r\n   ...: \r\n   ...: res = []\r\n   ...: for _ in range(2):\r\n   ...:     res1 = []\r\n   ...:     # Only occurs when dataframe is used with measure\r\n   ...:     data = np.zeros((30, 21))\r\n   ...:     idx = np.random.randint(0, 5, 30)\r\n   ...:     df = pd.DataFrame(data, index=idx).loc[3]\r\n   ...:     #df = pd.DataFrame(data[::5, :])  # Uncomment for example of correct behavior\r\n   ...: \r\n   ...:     df2 = pd.DataFrame(sum(data.dot(df.T)))\r\n   ...:     df2.index.name = 'level2'\r\n   ...:     res1.append(df2)\r\n   ...:     tmp = pd.concat(res1, keys=[1], names=['level1'])\r\n   ...: \r\n   ...:     res.append(tmp)\r\n   ...: final = pd.concat(res, keys=[i for i in range(2)])\r\n   ...: \r\n\r\nIn [2]: final\r\nOut[2]: \r\n                   0\r\n  level1 level2     \r\n0 1      0       0.0\r\n         1       0.0\r\n         2       0.0\r\n         3       0.0\r\n         4       0.0\r\n1 1      0       0.0\r\n         1       0.0\r\n         2       0.0\r\n```\r\n\r\nThe the issue is that you are concatting 2 MultiIndexes with names of ``['level1', None]``\r\nyour original ``res`` is this:\r\n\r\n```\r\nIn [2]: res\r\nOut[2]: \r\n[            0\r\n level1       \r\n 1      0  0.0\r\n        1  0.0\r\n        2  0.0\r\n        3  0.0,             0\r\n level1       \r\n 1      0  0.0\r\n        1  0.0\r\n        2  0.0\r\n        3  0.0\r\n        4  0.0\r\n        5  0.0\r\n        6  0.0\r\n        7  0.0]\r\n```\r\n\r\nso you are expecting the passed in named to 'automatically' figure out that the *new* names are the existing first level name plus the passed in names?\r\n\r\nhow does that make sense?\r\n\r\nI suppose that its actually an error to pass in *any* names if you have ``MultiIndex`` that you are concatting in the first place. So would take a PR for that.",
      "FYI [here](https://github.com/pandas-dev/pandas/blob/master/pandas/tools/concat.py#L517) is the code for creating the concatted index. It is slightly non-trivial!",
      "Hmm, clearly you've missed the point. In fact pandas does automatically figure out that the new names should be the existing index plus the passed in names, as can be shown by un-commenting the line in my example saying \"Uncomment for example of correct behavior\".    Here's an example that may make it a little easier to see,\r\n\r\n```python\r\n    def concat_multiple(bug=False):\r\n        \"\"\"\r\n        Make a mess with DataFrames, if bug is True change behavior so that bug shows\r\n        \"\"\"\r\n        res = []\r\n        for _ in range(2):\r\n            res1 = []\r\n            # Only occurs when dataframe is used with measure\r\n            data = np.zeros((30, 21))\r\n            idx = np.random.randint(0, 5, 30)\r\n\r\n            df = pd.DataFrame(data, index=idx)\r\n            # Bug is right here.  Same dataframe, just different method of indexing\r\n            # This is the only thing that changes\r\n            if bug:\r\n                df = df.loc[3]\r\n\r\n            res1.append(pd.DataFrame(sum(data.dot(df.T))))\r\n            tmp = pd.concat(res1, keys=[1], names=['level1'])\r\n\r\n            res.append(tmp)\r\n        return pd.concat(res, keys=[i for i in range(2)], names=['level2'])\r\n\r\n    res_bad = concat_multiple(bug=True)   # Results when bug is present\r\n    res_good = concat_multiple(bug=False) # Results when bug is not present\r\n\r\n    print(\"These indexes should be the same\")\r\n    print (res_bad.index.names)\r\n    print (res_good.index.names)\r\n    if (res_bad.index.names != res_good.index.names):\r\n        print(\"But they aren't\")\r\n\r\n    # It gets worse, let's run it several times and see how often the bug is present\r\n    good = [concat_multiple(bug=True).index.names ==res_good.index.names for _ in range(20)]\r\n    print(\"pandas is broken  {}% of the time\".format((1-sum(good)/len(good))*100 ))\r\n```\r\nAnd here's the output:\r\n```\r\nThese indexes should be the same\r\n['level2', None, None]\r\n['level2', 'level1', None]\r\nBut they aren't\r\npandas is broken  95.0% of the time\r\n```\r\nAnd when a bug like this exists you don't need to link to the source code to convince me it's a mess.",
      "@seth-a \r\n\r\nthis is really really odd thing to do: of course this doesn't work\r\n``idx = np.random.randint(0, 5, 30)``\r\n\r\n",
      "you are creating a non-unique index. sure it might be a bug, but you are by-definition making this non-deterministic. ``df.loc[3]`` depends on the uniqueness of the result\r\n\r\n```\r\nIn [18]: df = DataFrame({'A': [1,2,3]},index=[1,2,3])\r\n\r\nIn [19]: df.loc[3]\r\nOut[19]: \r\nA    3\r\nName: 3, dtype: int64\r\n\r\nIn [20]: df = DataFrame({'A': [1,2,3]},index=[1,3,3])\r\n\r\nIn [21]: df.loc[3]\r\nOut[21]: \r\n   A\r\n3  2\r\n3  3\r\n\r\n```\r\n\r\nthese have different shapes so different things happen.",
      "It *may* be a bug, likely around concatting non-unique MI's. But please produce an example that is deterministic within these parameters. (or at the very least use a random seed).",
      "What do you mean it doesn't work?  Is it throwing an error?",
      "please read my responses.",
      "Ah, so you mean the code works fine, it just isn't a typical use case.  Ok, here's the code using a seed so the behavior is 100% deterministic, aside from whatever pandas is doing:\r\n\r\n```\r\n    def concat_multiple(bug=False):\r\n        \"\"\"\r\n        Make a mess with DataFrames, if bug is True change behavior so that bug shows\r\n        \"\"\"\r\n        np.random.seed(5)\r\n        res = []\r\n        for _ in range(2):\r\n            res1 = []\r\n            # Only occurs when dataframe is used with measure\r\n            data = np.zeros((30, 21))\r\n            idx = np.random.randint(0, 5, 30)\r\n\r\n            df = pd.DataFrame(data, index=idx)\r\n            # Bug is right here.  Same dataframe, just different method of indexing\r\n            if bug:\r\n                df = df.loc[3]\r\n\r\n            res1.append(pd.DataFrame(sum(data.dot(df.T))))\r\n            tmp = pd.concat(res1, keys=[1], names=['level1'])\r\n\r\n            res.append(tmp)\r\n        return pd.concat(res, keys=[i for i in range(2)], names=['level2'])\r\n\r\n    res_bad = concat_multiple(bug=True)   # Results when bug is present\r\n    res_good = concat_multiple(bug=False) # Results when bug is not present\r\n\r\n    print(\"These indexes should be the same\")\r\n    print (res_bad.index.names)\r\n    print (res_good.index.names)\r\n    if (res_bad.index.names != res_good.index.names):\r\n        print(\"But they aren't\")\r\n\r\n    # It gets worse, let's run it several times and see how often the bug is present\r\n    good = [concat_multiple(bug=True).index.names ==res_good.index.names for _ in range(20)]\r\n    print(\"pandas is broken  {}% of the time\".format((1-sum(good)/len(good))*100 ))\r\n```\r\nOutput:\r\n```\r\nThese indexes should be the same\r\n['level2', None, None]\r\n['level2', 'level1', None]\r\nBut they aren't\r\npandas is broken  100.0% of the time\r\n```",
      "Much easier reproducible example:\r\n\r\n```\r\nIn [32]: df = pd.DataFrame({'col': range(5)}, index=pd.MultiIndex.from_product([[1], range(5)], names=['level1', None]\r\n    ...: ))\r\n\r\nIn [33]: df\r\nOut[33]:\r\n          col\r\nlevel1\r\n1      0    0\r\n       1    1\r\n       2    2\r\n       3    3\r\n       4    4\r\n\r\nIn [34]: pd.concat([df, df], keys=[1, 2], names=['level2'])\r\nOut[34]:\r\n                 col\r\nlevel2 level1\r\n1      1      0    0\r\n              1    1\r\n              2    2\r\n              3    3\r\n              4    4\r\n2      1      0    0\r\n              1    1\r\n              2    2\r\n              3    3\r\n              4    4\r\n\r\nIn [35]: pd.concat([df, df[:3]], keys=[1, 2], names=['level2'])\r\nOut[35]:\r\n            col\r\nlevel2\r\n1      1 0    0\r\n         1    1\r\n         2    2\r\n         3    3\r\n         4    4\r\n2      1 0    0\r\n         1    1\r\n         2    2      \r\n```\r\n\r\n@seth-a your example is really complex, which makes it much harder to see what is exactly going on, or what you think is wrong. \r\nBut the above in any case clearly shows a bug.",
      "thanks @jorisvandenbossche \r\n\r\nyeah the name handling is prob not very robust.",
      " @jorisvandenbossche, It's a much-simplified version of a script I was working on.  I'm glad you found an easier case where it appears.",
      "Additional observation: it only occurs when the name of the other level is None, otherwise it works fine:\r\n\r\n```\r\nIn [40]: df = pd.DataFrame({'col': range(5)}, index=pd.MultiIndex.from_product([[1], range(5)], names=['level1', 'leve\r\n    ...: l3']))\r\n\r\nIn [41]: pd.concat([df, df[:3]], keys=[1, 2], names=['level2'])\r\nOut[41]:\r\n                      col\r\nlevel2 level1 level3\r\n1      1      0         0\r\n              1         1\r\n              2         2\r\n              3         3\r\n              4         4\r\n2      1      0         0\r\n              1         1\r\n              2         2\r\n```\r\n\r\nand (as already see in the example above) only when the index values are not exactly the same (different length in the example above, or different values of same length also triggers it)"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "renamed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "unlabeled",
      "labeled",
      "renamed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 26,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/indexes/api.py",
      "pandas/tests/tools/test_concat.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15817,
    "reporter": "mchwalisz",
    "created_at": "2017-03-27T11:10:55+00:00",
    "closed_at": "2017-04-08T21:54:05+00:00",
    "resolver": "mchwalisz",
    "resolved_in": "860d555a9d27958b151412527034fddffb446b31",
    "resolver_commit_num": 0,
    "title": "to_datetime() ns precision inconsistencies between s and ns",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\nOutput\r\n\r\n\r\n\r\n#### Problem description\r\nI would expect that all of the following methods would give the same result. In the [documentation](-docs/stable/timeseries.html#epoch-timestamps) that the epoch times will be rounded to the nearest nanosecond. I don't understand why conversion from float using seconds `s` unit gives different rounding than nano seconds `ns`. Not to mention strange float parsing coming from csv.\r\n\r\nI expect the whole issue to be connected to #7307.\r\n\r\n#### Expected Output\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.10-040910-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 34.3.2\r\nCython: 0.25.2\r\nnumpy: 1.10.4\r\nscipy: 0.19.0\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.3.0\r\nsphinx: 1.4.8\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.3.0\r\nnumexpr: 2.6.2\r\nmatplotlib: 2.0.0\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\n 0.9.1\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.5\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "Usage Question",
      "Docs",
      "Timeseries"
    ],
    "comments": [
      "This is not really related to #7307, but more with the (possible) precision of floats. \r\n\r\nThe float \"1490195805.433502912\" is too large / has too high precision to be represented faithfully as a floating point. Therefore, when converting this float to a datetime64 (which representation is based on integers) you get a conversion error.\r\n\r\n```\r\nIn [11]: 1490195805.433502912\r\nOut[11]: 1490195805.433503\r\n\r\nIn [12]: 1490195805433502912\r\nOut[12]: 1490195805433502912\r\n\r\nIn [14]: pd.to_datetime(1490195805.433502912, unit='s') \r\nOut[14]: Timestamp('2017-03-22 15:16:45.433503') \r\n\r\nIn [15]: pd.to_datetime(1490195805433502912, unit='ns')  \r\nOut[15]: Timestamp('2017-03-22 15:16:45.433502912')  \r\n```",
      "How do you explain the following then?\r\n\r\n```\r\ndf['from float^9 to ns'] = pd.to_datetime(df['full'] * 10**9, unit='ns')\r\n\r\nfrom float^9 to ns     2017-03-22 15:16:45.433502720\r\n```\r\n\r\n",
      "A string needs to be parsed to an actual floating point, and also here differences can occur based on which implementation you use:\r\n\r\n```\r\nIn [25]: 1490195805.433502912\r\nOut[25]: 1490195805.433503\r\n\r\nIn [26]: float('1490195805.433502912')\r\nOut[26]: 1490195805.433503\r\n\r\nIn [27]: pd.read_csv(StringIO(\"a\\n1490195805.433502912\")).iloc[0,0]\r\nOut[27]: 1490195805.4335027\r\n\r\nIn [28]: pd.read_csv(StringIO(\"a\\n1490195805.433502912\"), float_precision='high').iloc[0,0]\r\nOut[28]: 1490195805.4335029\r\n```\r\n\r\nAnd additionally, also floating point arithmetic will give small differences, a float * 10^9 does not necessarily result in the same number as combining both separate parts as integers",
      "@mchwalisz \r\n\r\nfurthermore, python floats are unbounded precision, while this is casted to a ``float64`` which has high (though limited precision), about 15 significant digits. rounding is unavoidable. The only way to have exact precision is to use a fixed-width exact type (e.g. an int64). This is why we store the datetimes as int64. You can round-trip them exactly even via text this way.\r\n\r\n```\r\nIn [6]: pd.to_datetime(1490195805.4335027 * 10**9, unit='ns').value\r\nOut[6]: 1490195805433502720\r\n\r\nIn [7]: pd.to_datetime(1490195805.4335029 * 10**9, unit='ns').value\r\nOut[7]: 1490195805433502976\r\n\r\nIn [8]: 1490195805.4335027 * 10**9\r\nOut[8]: 1.4901958054335027e+18\r\n\r\nIn [9]: 1490195805.4335029 * 10**9\r\nOut[9]: 1.490195805433503e+18\r\n```\r\n",
      "@mchwalisz happy to have a short section in ``timeseries.rst`` about this if you want / possibly in ``.to_datetime`` doc-string as well. not sure where else to put any docs about this. ",
      "A string needs to be parsed to an actual floating point, and also here differences can occur based on which implementation you use:\r\n\r\n```\r\nIn [25]: 1490195805.433502912\r\nOut[25]: 1490195805.433503\r\n\r\nIn [26]: float('1490195805.433502912')\r\nOut[26]: 1490195805.433503\r\n\r\nIn [27]: pd.read_csv(StringIO(\"a\\n1490195805.433502912\")).iloc[0,0]\r\nOut[27]: 1490195805.4335027\r\n\r\nIn [28]: pd.read_csv(StringIO(\"a\\n1490195805.433502912\"), float_precision='high').iloc[0,0]\r\nOut[28]: 1490195805.4335029\r\n```\r\n\r\nAnd additionally, also floating point arithmetic will give small differences, a float * 10^9 does not necessarily result in the same number as combining both separate parts as integers.\r\n\r\nThis is one of the reasons the timestamps are stored as integers.\r\n\r\nIf you want to read more information about this issue, you can see http://floating-point-gui.de/"
    ],
    "events": [
      "commented",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "labeled",
      "closed",
      "reopened",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 2,
    "additions": 32,
    "deletions": 6,
    "changed_files_list": [
      "doc/source/timeseries.rst",
      "pandas/tseries/tools.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15822,
    "reporter": "AndsonYe",
    "created_at": "2017-03-28T07:02:08+00:00",
    "closed_at": "2017-03-28T16:49:16+00:00",
    "resolver": "jreback",
    "resolved_in": "a9406057b5f48d579d9a9136a183a594c4b1f758",
    "resolver_commit_num": 4333,
    "title": "get localized Timestamp using \".at\" cannot get the time zone info",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n\r\n#### Problem description\r\n\r\nThe sample code is as above.\r\nI have a column in DataFrame that contains the datetime values which are Timestamp object in pandas.\r\nAfter I localise the timestamp, while using .at getter and .loc getter, I get different results.\r\nOne is without time zone info, the other is with time zone info.\r\n\r\nIs this a bug? How can I get the timestamp with time zone info using \".at\" getter?\r\n\r\n#### Expected Output\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n# Paste the output here pd.show_versions() here\r\n\r\nIn [20]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.19.0-66-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_HK.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 25.1.1\r\nCython: 0.24.1\r\nnumpy: 1.12.1\r\nscipy: 0.18.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.0.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999\r\n 0.8\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: None\r\npandas_datareader: None\r\n\r\n\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Indexing",
      "Regression",
      "Timezones",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "I can confirm this bug on 0.19.2. \r\nAnd actually, there is also a regression on master, as now both `at` and `loc` return the timestamp without timezone information:\r\n\r\n```\r\nIn [2]: test_df.dtypes\r\nOut[2]: \r\ndate    datetime64[ns, Asia/Shanghai]\r\nname                           object\r\ndtype: object\r\n\r\nIn [3]: test_df.at[0,'date']\r\nOut[3]: Timestamp('2017-03-13 05:32:56')\r\n\r\nIn [4]: test_df.loc[0,'date']\r\nOut[4]: Timestamp('2017-03-13 05:32:56')\r\n\r\nIn [5]: pd.__version__\r\nOut[5]: '0.19.0+686.g71f621f'\r\n```\r\n\r\n@AndsonYe As a temporary workaround, you can do the indexing in two steps: first access the column, then the value from that column:\r\n\r\n```\r\nIn [6]: test_df['date'][0]\r\nOut[6]: Timestamp('2017-03-13 13:32:56+0800', tz='Asia/Shanghai')\r\n```",
      "this is a duplicate: https://github.com/pandas-dev/pandas/issues/12938"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "labeled",
      "labeled",
      "cross-referenced",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 26,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/frame.py",
      "pandas/tests/indexing/test_scalar.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15835,
    "reporter": "fortooon",
    "created_at": "2017-03-29T13:41:23+00:00",
    "closed_at": "2017-04-03T20:47:30+00:00",
    "resolver": "gfyoung",
    "resolved_in": "ff652a5abafee88cbd858c12cc06dd60e73a6647",
    "resolver_commit_num": 158,
    "title": "read_excel failed with specific configuration",
    "body": "#### Code Sample to reproduce bug\r\n\r\n\r\nuse attached file : [test12.xlsx](-dev/pandas/files/879107/test12.xlsx)\r\n\r\n\r\n\r\n#### Problem description\r\n\r\nI can't read row as header with empty value cell with number formatting.\r\nParser just failed.\r\n\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.7.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-66-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.17.1\r\nnose: None\r\npip: 7.0.3\r\nsetuptools: 1.1.6\r\nCython: None\r\nnumpy: 1.7.1\r\nscipy: 0.14.0\r\nstatsmodels: None\r\nIPython: 0.13.2\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.3\r\npytz: 2014.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: 2.3.0\r\nxlrd: 0.9.4\r\nxlwt: 1.0.0\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\nJinja2: None\r\n\r\n</details>\r\n",
    "labels": [
      "Difficulty Novice",
      "IO Excel",
      "Effort Low",
      "Bug",
      "IO CSV"
    ],
    "comments": [
      "`keep_default_na` for some reason doesn't seem to work with `read_excel`, suppose it should.  Passing something to `na_values` is a workaround, PR to fix welcome.\r\n\r\n```python\r\npre_kwargs = {\r\n  \"sheetname\" : \"Sheet1\",\r\n  \"parse_cols\" : [0,3],\r\n  \"keep_default_na\" : False,\r\n  \"na_values\" : set(), # addition\r\n  \"header\" : 0\r\n}\r\n\r\nf = 'https://github.com/pandas-dev/pandas/files/879107/test12.xlsx'\r\n\r\npd.read_excel(f, **pre_kwargs)\r\nOut[87]: \r\n   1\r\n0  1\r\n\r\n```",
      "@fortooon : Nice catch!  You just happened to hit a corner case in parsing:\r\n~~~python\r\n>>> from pandas import read_csv\r\n>>> from pandas.compat import StringIO\r\n>>>\r\n>>> data = \"a,1\\b,2\"\r\n>>> read_csv(StringIO(data), keep_default_na=False, index_col=0)\r\n...\r\nTypeError: unsupported operand type(s) for |: 'list' and 'set'\r\n~~~\r\nNote that this is using the C engine (also breaks with the Python engine).  PR coming soon to patch this.\r\n\r\n@jreback : This is not `compat` issue, just a plain bug.\r\n\r\n@chris-b1 : This is not a `read_excel` bug.  In fact, propagation is just fine."
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "labeled",
      "unlabeled",
      "labeled",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 12,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/io/parsers.py",
      "pandas/tests/io/parser/na_values.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15864,
    "reporter": "funnycrab",
    "created_at": "2017-04-02T05:00:51+00:00",
    "closed_at": "2017-04-02T14:18:12+00:00",
    "resolver": "funnycrab",
    "resolved_in": "7059d898511a62710d6bd6487c8b40d7f535c1a1",
    "resolver_commit_num": 1,
    "title": "another to_json float precision bug",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nThis issue is very similar to [#15716](-dev/pandas/issues/15716). \r\n\r\nJust this one  reveals a more generalized pattern. \r\n\r\n#### Expected Output\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.19.0-42-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_HK.UTF-8\r\nLOCALE: en_HK.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 8.1.2\r\nsetuptools: 27.2.0\r\nCython: 0.24.1\r\nnumpy: 1.11.3\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.4.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.6.1\r\nblosc: None\r\nbottleneck: 1.1.0\r\ntables: 3.2.3.1\r\nnumexpr: 2.6.2\r\nmatplotlib: 1.5.3\r\nopenpyxl: 2.3.2\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.3\r\nlxml: 3.6.4\r\nbs4: 4.5.1\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: 1.0.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.42.0\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [],
    "comments": [
      "After a quick check, I believe this issue may not be related to [here](https://github.com/pandas-dev/pandas/blob/61f6f6333fb7bb2dedf82736aee6c9878382a06f/pandas/_libs/src/ujson/python/objToJSON.c#L2368) as pointed out in the [#15716](https://github.com/pandas-dev/pandas/issues/15716).\r\n\r\nIt seems to have something to do with rollover handling implemented [here](https://github.com/pandas-dev/pandas/blob/master/pandas/_libs/src/ujson/lib/ultrajsonenc.c#L826).\r\n\r\nA [pull request](https://github.com/pandas-dev/pandas/pull/15865) has been submitted for fixing this issue. Please be so kind as to take a look.",
      "these seem almost the same, simply roll them together."
    ],
    "events": [
      "referenced",
      "cross-referenced",
      "commented"
    ],
    "changed_files": 4,
    "additions": 75,
    "deletions": 5,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/_libs/src/ujson/lib/ultrajsonenc.c",
      "pandas/tests/io/json/test_pandas.py",
      "pandas/tests/io/json/test_ujson.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15869,
    "reporter": "richard-bibb",
    "created_at": "2017-04-02T18:25:24+00:00",
    "closed_at": "2017-04-04T22:15:17+00:00",
    "resolver": "jreback",
    "resolved_in": "e0b60c07295a92eb760c38870c5f8c40e412f7dc",
    "resolver_commit_num": 4354,
    "title": "\"IndexError: tuple index out of range\" error with numpy array contain datetimes",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nThe above code works in an old version of pandas (0.7.3) but fails in the current version (0.19.2)\r\n\r\n#### Expected Output\r\nNo error and dataframe containing the numpy date array\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n>>> pandas.show_versions()\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 26 Stepping 5, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 20.10.1\r\nCython: None\r\nnumpy: 1.12.1\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: None\r\npandas_datareader: None\r\n>>>\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Difficulty Intermediate",
      "Effort Low",
      "Reshaping"
    ],
    "comments": [
      "Interestingly, this works:\r\n\r\n```python\r\npd.DataFrame(np.array([None, None, dt.now(), None]))\r\n```\r\n\r\nBut this does not:\r\n\r\n```python\r\npd.DataFrame(np.array([None, None, None, dt.now(), None]))\r\n```\r\n\r\nNumpy doesn't seem to handle the input arrays any differently, but `create_block_manager_from_blocks()` definitely shows different `blocks`. The first is \r\n\r\n```python\r\n[array([['NaT', 'NaT', '2017-04-04T13:54:03.236544000', 'NaT']], dtype='datetime64[ns]')]\r\n```\r\n\r\nwhile the second is \r\n\r\n```python\r\n[array([None, None, None, datetime.datetime(2017, 4, 4, 13, 54, 10, 804563),\r\n       None], dtype=object)]\r\n```\r\n\r\nI'm going to dig some more into this.",
      "@chrisaycock here's why.\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/master/pandas/types/cast.py#L810\r\n\r\nWhen we have a 1-d array that is ``object`` we want to see whether its datetimelike or not. So I did this old thing where I would sample the first 3 elements. This is bogus now as can simply do this (also need to tests for timedelta as well).\r\n\r\n```\r\nIn [1]: from pandas._libs.lib import is_datetime_array\r\n\r\nIn [2]: is_datetime_array(np.array([None, None, datetime.datetime.now()]))\r\nOut[2]: True\r\n\r\nIn [3]: is_datetime_array(np.array([None, None, None, datetime.datetime.now()]))\r\nOut[3]: True\r\n```\r\n\r\nIt *might* be better to simply modify ``is_possible_datetimelike_array`` as an alternative (it is called below as well).\r\n\r\nThese routines are robust to null values, check for the requested (datetime or timedelta-like), and don't read the entire routine if is just strings (which is the point here, we are trying to see w/o doing too much work if we can coerce this). This is called for *every* ``object`` array on construction, so this needs to be cheap.\r\n\r\nThis may seem like overly paranoid / extreme. But remember we can actually have mixed dtypes that get coerced, e.g.:\r\n\r\n```\r\nIn [4]: Series(['NaT', None, pd.Timestamp('20130101'), datetime.datetime.now(), '20150101'])\r\nOut[4]: \r\n0                          NaT\r\n1                          NaT\r\n2   2013-01-01 00:00:00.000000\r\n3   2017-04-04 14:07:52.743198\r\n4   2015-01-01 00:00:00.000000\r\ndtype: datetime64[ns]\r\n```\r\n\r\nbut we need it be robust to non-matching types.\r\n```\r\nIn [6]: Series(['NaT', None, pd.Timestamp('20130101'), datetime.datetime.now(), '20150101', np.nan, pd.Timedelta('1 day')])\r\nOut[6]: \r\n0                           NaT\r\n1                          None\r\n2           2013-01-01 00:00:00\r\n3    2017-04-04 14:08:34.334113\r\n4                      20150101\r\n5                           NaN\r\n6               1 days 00:00:00\r\ndtype: object\r\n```",
      "@jreback Agreed that the culprit is the \"quick inference\" on the first three elements.\r\n\r\nIs your proposal to modify `maybe_infer_to_datetimelike()` to use `is_possible_datetimelike_array()` instead of the logic that begins with the \"sample\"? Are there other gotchas given all of the conditionals below it?\r\n\r\nThe alternative is to `infer_dtype()` the entire array instead of the first three elements. Would the performance impact be just too much then?",
      "i think maybe best way is to trash all of the logic  for the sample and below\r\n\r\nthen modify: is_possible_datetimelike_array to return 3 states (string is prob fine)\r\n\r\n- datetime\r\n- timedelta\r\n- mixed\r\n\r\nthe u know if u should attempt to_datetime or to_timedelta or are done\r\n\r\nit short circuits so should be performant \r\nthough maybe inside that routine you can take the first 3 valid points (non null) and actually try to convert them - if that fails then you are done \r\n\r\nthe issue is to try to not iterate (and convert) all strings ",
      "@chrisaycock this was a can of worms, see #15892 "
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "referenced"
    ],
    "changed_files": 6,
    "additions": 149,
    "deletions": 62,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/_libs/src/inference.pyx",
      "pandas/tests/frame/test_constructors.py",
      "pandas/tests/frame/test_misc_api.py",
      "pandas/tests/series/test_constructors.py",
      "pandas/types/cast.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15908,
    "reporter": "stangirala",
    "created_at": "2017-04-05T18:45:33+00:00",
    "closed_at": "2017-04-14T13:30:24+00:00",
    "resolver": "stangirala",
    "resolved_in": "3fde134617822773b23cf484310820298d9f88ac",
    "resolver_commit_num": 0,
    "title": "Class label colours with parallel coordinates",
    "body": "#### Problem description\r\nThe parallel coordinates function seems to currently assume that the class labels don't have an inherent ordering. I'd like to provide a sort option or a class-to-colour mapping to capture this. My specific use case is situations where the label are quantized buckets and a smooth colour change between buckets make the final plot easy to visualize.\r\n\r\nThe current function zips the available classes and the colour_values without any sorting.\r\n\r\nI've identified where in the current '_parallel_coordinates' function a change should be made and would like to work on this.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.2.45-0.6.wd.865.49.315.metal1.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 34.3.2\r\nCython: None\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.3.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 2.0.0\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999999999\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.5\r\nboto: None\r\npandas_datareader: None\r\n\r\n</details>",
    "labels": [
      "Difficulty Novice",
      "Effort Low",
      "Visualization"
    ],
    "comments": [],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 31,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/tests/plotting/test_misc.py",
      "pandas/tools/plotting.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15912,
    "reporter": "mp-v2",
    "created_at": "2017-04-05T21:54:48+00:00",
    "closed_at": "2017-04-06T07:35:45+00:00",
    "resolver": "nrebena",
    "resolved_in": "3f02e6331fd4b17f2d12d3cd546e0d555ea40cd4",
    "resolver_commit_num": 0,
    "title": "Pandas dataframe + groupby = failed zooming for x-axis ticks",
    "body": "This was posted onto stackoverflow but it seems more like a bug than anything:\r\n\r\n-dataframe-groupby-failed-zooming-for-x-axis-ticks\r\n\r\n\r\n",
    "labels": [],
    "comments": [
      "Smaller reproducible example:\r\n\r\n```\r\nIn [43]: df = pd.DataFrame(np.random.randn(4, 2), columns=['A', 'B'], index=pd.MultiIndex.from_product([[2012, 2013], [1,2]]))\r\n\r\nIn [44]: df\r\nOut[44]: \r\n               A         B\r\n2012 1  0.144222  0.895911\r\n     2  0.885869  0.075482\r\n2013 1 -0.312316  0.705149\r\n     2 -0.974501  1.443551\r\n\r\nIn [45]: df.plot()\r\nOut[45]: <matplotlib.axes._subplots.AxesSubplot at 0x7f3e6a262ef0>\r\n```\r\n\r\nThe xtick label formatter cannot really handle a MultiIndex, as once you zoom in, the labels are put at the wrong place.\r\n\r\nBut the same is true when eg having a string index. So just a non-numerical / non-datetime index has this issue.",
      "This is a duplicate of https://github.com/pandas-dev/pandas/issues/7612"
    ],
    "events": [
      "commented"
    ],
    "changed_files": 3,
    "additions": 45,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.25.0.rst",
      "pandas/plotting/_core.py",
      "pandas/tests/plotting/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15944,
    "reporter": "winklerand",
    "created_at": "2017-04-07T19:59:52+00:00",
    "closed_at": "2017-04-07T20:04:14+00:00",
    "resolver": "winklerand",
    "resolved_in": "7d4a260cbe6d5c1825541adcd0d5310f32a3ba42",
    "resolver_commit_num": 2,
    "title": "BUG: Resampling PeriodIndex-ed to multiple of frequencies not working as expected",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n\r\nTo compare with, results for resampling to base frequency (no multiples) returning PeriodIndex-ed and covering the full original time span:\r\n<details>\r\n\r\n\r\n</details>\r\n\r\n#### Problem description\r\n\r\n* I'd expect resampling PeriodIndex-ed series/dataframes would return a PeriodIndex-ed result by default, even more when given.\r\n* Moreover, I'd expect the original full time span to be covered by the resampling result (upsampling A->2Q would return 2 periods per year, downsampling M->2Q would return 2 periods per 12 months).\r\n\r\nAs indicated by the warning message, resampling to multiple of frequencies falls back to timestamp-based resampling. Both work fine when resampling to a \"base\" frequency without any multiple.\r\n\r\n#### Expected Output\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: de_DE.UTF-8\r\nLOCALE: de_DE.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 33.1.1.post20170320\r\nCython: None\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\nstatsmodels: 0.8.0\r\nxarray: None\r\nIPython: 5.3.0\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: None\r\nnumexpr: 2.6.2\r\nmatplotlib: 2.0.0\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.5.3\r\nhtml5lib: 0.999\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.5\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [],
    "comments": [
      "xref https://github.com/pandas-dev/pandas/issues/12884\r\n\r\nthere are quite a few issues w.r.t. PeriodIndex and resampling. just need someone motivated to digin an fix.\r\n\r\nthis is basically a dupe as things are returned as a DTI. But i'll link this for tests and such."
    ],
    "events": [],
    "changed_files": 3,
    "additions": 340,
    "deletions": 172,
    "changed_files_list": [
      "doc/source/whatsnew/v0.21.0.txt",
      "pandas/core/resample.py",
      "pandas/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15996,
    "reporter": "rgbkrk",
    "created_at": "2017-04-14T15:52:26+00:00",
    "closed_at": "2017-04-25T23:55:20+00:00",
    "resolver": "jreback",
    "resolved_in": "5b07d02bbad8fb13388e12e7797e4b022c02b2b5",
    "resolver_commit_num": 4401,
    "title": "Table Schema bombs with MultiIndex",
    "body": "Using the new (ok, yet to be released) table schemaized with the generated MultiIndex as mentioned in #15379, I noticed that it creates a traceback (and falls back on HTML)\r\n\r\n#### Code Sample\r\n\r\n\r\n\r\n#### Problem description\r\n\r\n<details>\r\n<summary>Full Traceback</summary>\r\n\r\n\r\n\r\n</details>\r\n\r\nThe key line in there is `expected list, got FrozenList` from `arrays = list(lib.to_object_array_tuples(tuples).T)`\r\n",
    "labels": [
      "Bug",
      "MultiIndex",
      "Output-Formatting"
    ],
    "comments": [
      "cc @TomAugspurger ",
      "Right... I think I knew about this and forgot to handle it. I think the issue is that [the spec](http://specs.frictionlessdata.io/table-schema/) wants the field names to be a string. We would probably represent this as a tuple.\r\n\r\nSo we can do this, it just means we won't be compliant. Thoughts? I guess we could have a strict mode that would raise in cases like this? But for use-cases like sending data to nteract, we don't really want to raise an exception, so we'd be laxer.",
      "Ah right, I remember a little bit of chatter on this. I think it would be ok to only put out an HTML table in cases like these, I'd prefer compliance over niceties for multiindex. We can move specs forward over time.\r\n\r\n/cc @pwalsh",
      "Oh, we also generate invalid JSON with a MultiIndex in the columns \ud83d\ude2c  https://github.com/pandas-dev/pandas/issues/15273. Don't think I'll have time to get to that before the release.\r\n\r\nIf you think falling back in these cases is appropriate, I'll put that logic in the publish part.\r\n\r\nAnother option is to serialize all the level names down to a string like `<level1>-<level2>...` and add some additional fields with the information needed to deserialze it (number of levels, separator...)\r\n",
      "There were indeed still a bunch of cases where the json schema generation errors, see the list in the PR: https://github.com/pandas-dev/pandas/pull/14904#issuecomment-278288199 (and multi-index columns is one of them). We should probably open an issue for those (or use this one as the general follow-up issue).",
      "we could add a ``NotImplementedError`` for now?",
      "@rgbkrk what's the ideal behavior here, as far as front-ends are concerned? Do you want us to just not hava a `\"application/vnd.dataresource+json\"` key (`text/html` should still be there)? Is there another channel we should publish information on? I'm looking through the jupyter client docs now to see if there are any recommendations.",
      "Yeah, just don't have the `application/vnd.dataresource+json` if it's not supported and only provide `text/html`.",
      "As for another channel, are you asking about where to send errors to?",
      "> As for another channel, are you asking about where to send errors to?\r\n\r\nYeah, my memory is a bit hazy, but I thought there were other channels to publish errors. It looks like I was misremembering though. Is there some way to notify you that vnd.dataresource+json version failed?",
      "Beyond a traceback? There's probably a way to push out other errors. Emitting a warning is fine too.\r\n\r\nWhat do you think @minrk and @carreau?",
      "Warning on stderr will be printed in red in classic notebook. (I believe it should be yellow because red is scary). An error will stop the execution while just printing on stderr is considered as just \"text\" and the notebook should keep on processing as planned. Does that answer your question ? ",
      "> Does that answer your question ?\r\n\r\nI think so, thanks. I'll emit a warning when we fail to serialize the object, and publish just the text and html reprs."
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "closed",
      "reopened",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 2,
    "additions": 22,
    "deletions": 2,
    "changed_files_list": [
      "pandas/io/json/json.py",
      "pandas/tests/io/formats/test_printing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 15999,
    "reporter": "dhimmel",
    "created_at": "2017-04-14T18:11:49+00:00",
    "closed_at": "2017-04-15T13:46:44+00:00",
    "resolver": "dhimmel",
    "resolved_in": "61d84dbf161f72081165d170a36978d2d942e19d",
    "resolver_commit_num": 4,
    "title": "Series.map should return default dictionary values rather than NaN",
    "body": "`collections.Counter` and `collections.defaultdict` both have default values. However, [`pandas.Series.map`](-dev/pandas/blob/614a48e3e6640b5694fd1ed6cbe56a760e89dd50/pandas/core/series.py#L2042-L2145) does not respect these defaults and instead returns missing values.\r\n\r\nThe issue is illustrated below:\r\n\r\n\r\n\r\nHere's the output:\r\n\r\n\r\n\r\nThe workaround is rather easy (`lambda x: dictionary[x]`) and shouldn't be to hard to implement. Are people on board with the change? Is there a performance concern with looking up each key independently?",
    "labels": [
      "Enhancement",
      "Missing-data"
    ],
    "comments": [
      "why would you do this?",
      "I've ran into this issue several times with `collections.Counter`. Most recently see [cell 6 of this notebook](https://github.com/greenelab/crossref/blob/dc4df4abcaf550ccc5a9a2757895238b22b46734/2.types-to-dataframe.ipynb). With counters, if you haven't observed a key, it defaults to zero (since they're used for counting occurrences).\r\n\r\nBy using a defaultdict or Counter, the user has chosen that they would like default values. If they don't want defaults, they should just convert or use dict.",
      "``.map`` does not accept a ``Counter``, sure its dictlike but not sure why you would actually do this anyhow.",
      "looks like you just should do ``.groupby(...).value_counts()`` anyhow.",
      "> not sure why you would actually do this anyhow\r\n\r\nBecause I have a counter of occurrences that I want to add as a column to a dataframe. In many cases the counter cannot be created in pandas using `.value_counts()`. For example:\r\n\r\n+ the counter is created by iteratively reading a file that won't fit in memory\r\n+ code must deal with a counter that is returned by another function\r\n\r\nNow you could always use `series.map(counter).fillna(0).astype(int)` but this forces the user to deal with the conversion of ints to float when there's missing data (which is one of the must frustrating aspects of pandas and should be avoided when possible).\r\n\r\n> .map does not accept a Counter\r\n\r\nMap does accept a Counter, since it's a subclass of dict, and provides no warning.",
      "Right now we take a fastpath, building an `Index` out of the dict keys.\r\nhttps://github.com/pandas-dev/pandas/blob/614a48e3e6640b5694fd1ed6cbe56a760e89dd50/pandas/core/series.py#L2137\r\n\r\nSo probably either should add a slowpath that respects full semantics if passed a dict subclass, or just raise.",
      "> So probably either should add a slowpath that respects full semantics if passed a dict subclass, or just raise.\r\n\r\nWhat about adding the following to the head of the function?\r\n\r\n```python\r\nif isinstance(arg, (collections.Counter, collections.defaultdict)):\r\n    dictionary = arg\r\n    arg = lambda x: dictionary[x]\r\n```\r\n\r\nNote there are other ways of simplifying the function's code I would also explore.\r\n\r\nI'm happy to submit a PR and add tests if this is an enhancement that would be accepted."
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "labeled",
      "labeled",
      "milestoned",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 75,
    "deletions": 12,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/series.py",
      "pandas/tests/series/test_apply.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 16012,
    "reporter": "toobaz",
    "created_at": "2017-04-16T00:31:35+00:00",
    "closed_at": "2017-07-18T23:33:51+00:00",
    "resolver": "Morgan243",
    "resolved_in": "a1dfb037de79e6982a0e7ccf883e5af11e9cc843",
    "resolver_commit_num": 1,
    "title": ".isin() raises TypeError for Series with > 1 million elements if argument has mixed dtypes",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nAlthough the length of ``alt`` also matters in some way, even with a completely different dataset the problem still started at 1M elements in the ``Series`` (or even ``Index``) being searched. By the way, triggering the error takes much more time than the successful operation.\r\n\r\nMight be related to #13432 , although that one is unrelated to the length of the ``Series``.\r\n\r\n#### Expected Output\r\n\r\nLike ``In [4]``, just with one more element.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.7.0-1-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.utf8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.19.0+783.gcd35d22a0\r\npytest: 3.0.6\r\npip: 9.0.1\r\nsetuptools: 33.1.1\r\nCython: 0.25.2\r\nnumpy: 1.12.0\r\nscipy: 0.18.1\r\nxarray: 0.9.1\r\nIPython: 5.1.0.dev\r\nsphinx: 1.4.9\r\npatsy: 0.3.0-dev\r\ndateutil: 2.5.3\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.6\r\nlxml: 3.7.1\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\n\r\n\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "Dtypes",
      "Effort Low",
      "Numeric",
      "Performance"
    ],
    "comments": [
      "no this is a numpy bug. Its not too friendly to object dtypes.\r\n\r\nWe should put a try/except around this and fallback if it fails (then convert to object and try again with the pandas isin).\r\n\r\nThough maybe just use the pandas impl for ``object`` dtype anyhow. The reason we use ``in1d`` is its faster for really large N (> 1M).",
      "Working on this",
      "Something I've found - if the two arrays for the operation pass this condition in numpy's in1d:\r\n```python\r\nif len(ar2) < 10 * len(ar1) ** 0.145:\r\n    ...\r\n```\r\nThen a masking technique is used for performance reasons (https://github.com/numpy/numpy/pull/81) . If this masking technique is used, then the bug doesn't appear to happen.\r\n\r\nSo the bug is triggered by first having the 1e6 array to trigger pandas use of np.in1d, but then the comparison array must be sufficiently large in order to not pass numpy's special-speedup condition.\r\n\r\nI've got a fix that ensures the array type isn't an object before using np.in1d, otherwise defaulting to the 'ismember_object' method that `f` is set to by default. I'm not sure this is a bug for the numpy folks, but I'll do some reading and maybe open an issue to see what they think."
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 19,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.21.0.txt",
      "pandas/core/algorithms.py",
      "pandas/tests/series/test_analytics.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 16033,
    "reporter": "jreback",
    "created_at": "2017-04-17T13:30:41+00:00",
    "closed_at": "2018-03-05T15:27:24+00:00",
    "resolver": "jreback",
    "resolved_in": "c728a227840a550f4dfe1b9a43a901e384fc23fb",
    "resolver_commit_num": 4400,
    "title": "DOC: styler warnings in doc-build",
    "body": "-ci.org/pandas-dev/pandas/jobs/222779268\r\n\r\n\r\n\r\ncc @TomAugspurger @jorisvandenbossche \r\n\r\nI just pushed a change to fix the path of the imports (after ``pandas.formats`` change), but I think it still needs something.",
    "labels": [
      "Docs"
    ],
    "comments": [
      "Yeah saw that. I'll clean  up the warnings in the doc build today.",
      "For this specific one, it seems like it tries to auto-doc all of these\r\n\r\n```python\r\n    Attributes\r\n    ----------\r\n    env : Jinja2 Environment\r\n    template: Jinja2 Template\r\n    loader : Jinja2 Loader\r\n\r\n```\r\n\r\nbut `template` isn't a python module. I'll see if there's an easy way to skip just one.",
      "For reference, there's the `:exclude-members: template` directive that might have worked. It's supposed to work with autodoc, but I haven't found anything about whether it works with autosummary. https://github.com/numpy/numpydoc/issues/69 might be involved\r\n\r\nI've turned those attributes into properties with their own docstings, and the warning is fixed.\r\n\r\nI'll bush a batch of doc fixes a bit later today.",
      "FYI @TomAugspurger this was just an incorrect usage of ``pytest.warn`` rather than ``tm.assert_produces_warning``",
      "sorry @TomAugspurger wrong issue.",
      "I don't see those anymore in the doc build output on travis: https://travis-ci.org/pandas-dev/pandas/jobs/349024108, so closing"
    ],
    "events": [
      "labeled",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "closed",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "reopened",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files": 1,
    "additions": 13,
    "deletions": 9,
    "changed_files_list": [
      "pandas/tests/io/formats/test_css.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 16112,
    "reporter": "kernc",
    "created_at": "2017-04-24T17:20:04+00:00",
    "closed_at": "2017-07-22T19:00:29+00:00",
    "resolver": "kernc",
    "resolved_in": "ee6412aee8bbf350aea89bbafbfdfb0f8d7620ed",
    "resolver_commit_num": 8,
    "title": "SparseDataFrame.fillna() doesn't fill all NaNs",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\nThis is fine, since I want the implied zeros\u00b9 of scipy matrices to remain zeros and have explicit missing data remain missing.\r\n\r\n\u00b9 `spm.mean(0) == matrix([[      nan,  0.3333,  0.3333]])`\r\n\r\nHowever, if I don't specify a fill value:\r\n\r\n#### Problem description\r\n\r\nOn `sdf.fillna(-1)`, I'd expect _all_ NaNs to be filled. Don't know whether it's a bug or a feature, but it certainly is strange.\r\n\r\nxref: -dev/pandas/issues/15533\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\npandas v0.20.0rc1 844013b\r\n\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Sparse",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "So this is only an issue coming from a `scipy.sparse` matrix, correct?\r\n\r\n```python\r\nIn [34]: pd.SparseDataFrame({\"A\": [1, np.nan, np.nan], \"B\": [0, 1, 0.], \"C\": [0, 0, 1.]}).fillna(-1)\r\nOut[34]:\r\n     A    B    C\r\n0  1.0  0.0  0.0\r\n1 -1.0  1.0  0.0\r\n2 -1.0  0.0  1.0\r\n\r\nIn [35]: pd.SparseDataFrame(spm).fillna(-1)\r\nOut[35]:\r\n     0    1    2\r\n0  1.0 -1.0 -1.0\r\n1  NaN  1.0 -1.0\r\n2  NaN -1.0  1.0\r\n```\r\n\r\nThese two should have the same output `Out[34]`?\r\n\r\ncc @sinhrks",
      "> These two should have the same output Out[34]?\r\n\r\nThe second case, I don't think so. I would expect all NaN to be filled, but not some with zeros (as in `Out[34]`).\r\n```py\r\n>>> pd.SparseDataFrame(spm)\r\n     0    1    2\r\n0  1.0  NaN  NaN\r\n1  NaN  1.0  NaN\r\n2  NaN  NaN  1.0\r\n\r\n>>> pd.SparseDataFrame(spm).fillna(-1)\r\n     0    1    2\r\n0  1.0 -1.0 -1.0\r\n1 -1.0  1.0 -1.0\r\n2 -1.0 -1.0  1.0\r\n```"
    ],
    "events": [
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "labeled",
      "labeled",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced"
    ],
    "changed_files": 3,
    "additions": 43,
    "deletions": 9,
    "changed_files_list": [
      "doc/source/whatsnew/v0.21.0.txt",
      "pandas/core/sparse/array.py",
      "pandas/tests/sparse/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 16164,
    "reporter": "toobaz",
    "created_at": "2017-04-28T00:12:39+00:00",
    "closed_at": "2017-04-28T21:31:24+00:00",
    "resolver": "toobaz",
    "resolved_in": "a7a0574a0bcb8a4c65ed7f5006972e24f90ae3df",
    "resolver_commit_num": 24,
    "title": "df.reset_index() fails if df.index.name is a tuple",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nIn general, one expects ``df.set_index([df.columns[0]]).reset_index()`` to return (a copy of) ``df``. Instead, this does not happen if ``df.columns`` is a ``MultiIndex``.\r\n\r\n@jreback I know you were [not really convinced](-dev/pandas/issues/16120#issuecomment-296844552) this should work, but I'm uploading a PR in a couple of minutes, and as you will see the fix is extremely simple.\r\n\r\n#### Expected Output\r\n\r\n``df``\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.7.0-1-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.utf8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.20.0rc1+29.g075eca1fa\r\npytest: 3.0.6\r\npip: 9.0.1\r\nsetuptools: 33.1.1\r\nCython: 0.25.2\r\nnumpy: 1.12.0\r\nscipy: 0.18.1\r\nxarray: 0.9.1\r\nIPython: 5.1.0.dev\r\nsphinx: 1.4.9\r\npatsy: 0.3.0-dev\r\ndateutil: 2.5.3\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.6\r\nlxml: 3.7.1\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\n\r\n\r\n</details>\r\n",
    "labels": [
      "Bug",
      "MultiIndex"
    ],
    "comments": [],
    "events": [
      "mentioned",
      "subscribed",
      "cross-referenced",
      "labeled",
      "labeled",
      "milestoned"
    ],
    "changed_files": 3,
    "additions": 72,
    "deletions": 41,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.0.txt",
      "pandas/core/frame.py",
      "pandas/tests/test_multilevel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 16284,
    "reporter": "WBare",
    "created_at": "2017-05-08T13:56:34+00:00",
    "closed_at": "2018-02-01T13:44:37+00:00",
    "resolver": "WBare",
    "resolved_in": "35812eaaecebeeee0ddf07dee4b583c4eea07785",
    "resolver_commit_num": 1,
    "title": "Enhancement Request: control extrapolation on .interpolate",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n\r\n#### Problem description\r\nIt would be very nice to have a limit_direction='inside' that would make interpolate only fill values that are surrounded (both in front and behind) with valid values.\r\n\r\nThis would allow an interpolate to only fill missing values in a series and **not extend** the series beyond its original limits.  The key here is that it is sometimes important to maintain the original range of a series, but still fill in the gaps.\r\n\r\nThe example shows a simple DataFrame with an 'inside' interpolation.\r\n\r\n#### Expected Output\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.0.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-75-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 34.4.1\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.5.1\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.4.1\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 0.9.6\r\nlxml: 3.7.2\r\nbs4: 4.5.3\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: 1.1.5\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.4\r\nboto: 2.45.0\r\npandas_datareader: 0.2.1\r\nNone\r\n\r\n\r\n</details>\r\n",
    "labels": [
      "API Design",
      "Enhancement",
      "Numeric"
    ],
    "comments": [
      "So, this kind of already works when you use the `scipy` methods, since that's the default for scipy when you extraploate\r\n\r\n```python\r\nIn [31]: dfMain.interpolate(method='slinear')\r\nOut[31]:\r\n     a    b    c\r\n0  0.0  NaN  0.0\r\n1  1.0  NaN  1.0\r\n2  2.0  NaN  2.0\r\n3  3.0  3.0  3.0\r\n4  4.0  4.0  NaN\r\n```\r\n\r\nThis is an implementation detail that the user shouldn't need to worry about... But I'm not sure that we can make this consistent across methods in a backwards-compatible way.",
      "Thank you very much, @TomAugspurger !  I did not know that, and that solves the problem for me, but I agree completely that it would be nice to somehow make this more visible to the user.\r\n\r\nI don't know, but I'm guessing method='slinear' will not have an option to respect the limit= on number of NaNs to fill in, otherwise the code could just intercept something like limit_direction='inside' and save a bunch of work.\r\n\r\nThanks again!!",
      "This needs to run through the existing interpolate function so that it will respect the limit=n parameter correctly.",
      "Continuing from https://github.com/pandas-dev/pandas/pull/16307#issuecomment-303125751\r\n\r\nHmm I hadn't considered the interaction of `limit` and extrapolation...  I think that's covered by our current limit handling though.\r\n\r\nMy initial (very rough) thoughts are something like\r\n\r\n```python\r\ndef interpolate(self, method='linear', axis=0, limit=None, inplace=False,\r\n                limit_direction='forward', downcast=None, extrapolate=None, **kwargs)\r\n    \"\"\"\r\n    ...\r\n    extrapolate : array-like, two-tuple, \"extrapolate\", or None, optional\r\n\r\n        This is similar to scipy.interpolate.interp1d's `fill_value` keyword,\r\n        with special handling for pandas interpolation methods. By default,\r\n        pandas interpolation methods (...) will extrapolate forward only by\r\n        repeating the last valid observation, while scipy methods will not\r\n        interpolate (following the default for scipy). To disable extrapolation\r\n        for pandas methods, use `extrapolate=np.nan`.\r\n\r\n```\r\n\r\nThe difference between pandas and scipy methods is unfortunate, but I don't think it's worth deprecating one or the other (willing to change my mind on this).\r\n\r\nI don't think an `interpolate=True` argument is necessary. @WBare d",
      "> I don't think an interpolate=True argument is necessary. \r\n\r\n@TomAugspurger I agree in that I personally do not have a case where I would only want to extrapolate only, BUT, I'm concerned we are arbitrarily eliminating a use case that should get completed while we are in the code.\r\n\r\nMore generally, as a complete set of options, if we have the ability to interpolate only, why not the ability to extrapolate only?  I can imagine a user that would like to extend a series, but not disturb existing NaNs inside the series.\r\n\r\nI have not needed that myself so I'm basing this more on a logically complete set options than personal experience.\r\n\r\nWith that said, if we do choose to have an option for both, I'm changing my mind on the parameter.  I like your initial idea better (something like limit_type='interpolate' | 'extrapolate') as opposed to extrapolate=False because the logical flags allow for a combination that would never make sense (i.e. both interpolate=False and extrapolate=False). The single parameter more clearly conveys the idea of limiting to one or the other.\r\n\r\n\r\n",
      "> BUT, I'm concerned we are arbitrarily eliminating a use case that should get completed while we are in the code.\r\n\r\nYeah, that makes sense. You'll just have to balance ease of implementation with not breaking existing code :)",
      "OK, cool.  Any ideas on the actual parameter?\r\n\r\nIs everyone cool with\r\n\r\nlimit_type= ('interpolate' | 'extrapolate' | None=default to both which is current_behavior)\r\n\r\nThis seems consistent with existing parameters limit=n and limit_direction\r\n\r\n",
      "hey guys, if you let me jump in. I have the feeling the `limit` kwarg does not behave as you would expect it to when working with time series. To cite @rhkarls in the issue #1892 :\r\n\r\n> Say limit=2, if there is a NaN gap of 2 it would be completely filled with interpolated values. If there is a NaN gap of 4 nothing is filled, which is different from the fillna limit where the two first entries would be filled when using forward filling. This is very applicable for time series where it is often valid to interpolate between small gaps, while larger gaps should not be filled.\r\n\r\nSo lemme write an example:\r\n\r\n```python\r\nimport pandas as pd\r\n\r\n\r\ndf = pd.DataFrame(\r\n    index=pd.date_range(\r\n        start='02-01-2017 06:00:00',\r\n        end='02-07-2017 06:00:00'),\r\n    data={'A': range(7)})\r\ndf = df.drop(pd.to_datetime('2017-02-02 06:00:00'), axis=0)\r\n\r\ndf.head()\r\n\r\n                     A\r\n2017-02-01 06:00:00  0\r\n2017-02-03 06:00:00  2\r\n2017-02-04 06:00:00  3\r\n2017-02-05 06:00:00  4\r\n2017-02-06 06:00:00  5\r\n```\r\n\r\nNow what I want is to resample and interpolate the time series every 12 hours, but only for the consecutive days, so as not to make too big assumptions on the behavior of the time series for larger time deltas. That is not immediately possible currently, because of how `limit` works. See below, where putting `limit` of 2 (i.e. limit of a day) means that if two consecutive values are NaN, please do not fill in:\r\n\r\n```python\r\ndf.resample(rule='12H',base=6).interpolate('time', limit=2)\r\n\r\n                       A\r\n2017-02-01 06:00:00  0.0\r\n2017-02-01 18:00:00  0.5  # I would expect this to be NaN\r\n2017-02-02 06:00:00  1.0  # I would expect this to be NaN\r\n2017-02-02 18:00:00  NaN\r\n2017-02-03 06:00:00  2.0\r\n2017-02-03 18:00:00  2.5\r\n2017-02-04 06:00:00  3.0\r\n2017-02-04 18:00:00  3.5\r\n2017-02-05 06:00:00  4.0\r\n2017-02-05 18:00:00  4.5\r\n2017-02-06 06:00:00  5.0\r\n2017-02-06 18:00:00  5.5\r\n2017-02-07 06:00:00  6.0\r\nIn [ ]:\r\n\r\n```\r\n\r\nTo achieve what I want now, I have to use these functions I made:\r\n\r\n```python\r\ndef interpolate_consecutive(df, frequency):\r\n    \"\"\"\r\n    Only interpolates value at the frequency asked if the\r\n    values where separated by a day.\r\n    \r\n    Paramteres\r\n    ----------\r\n    df : pd.DataFrame\r\n        Dataframe with Time series index\r\n    frequency : basestring\r\n        Frequency to use to resample then interpolate.\r\n        Only expects 'H' or 'T' based rules, but that's\r\n        because I only need to support these in my case.\r\n    \r\n    Returns\r\n    -------\r\n    df : pd.DataFrame\r\n        Resampled and interpolated dataframe.\r\n\r\n    \"\"\"\r\n    base = 6 if 'H' in frequency else 0\r\n    start_indices, end_indices = get_non_consecutive(\r\n        df, pd.Timedelta(days=1))\r\n    df = df.resample(rule=frequency, base=base).interpolate('time')\r\n\r\n    indices_to_drop = []\r\n    for start_date, end_date in zip(start_indices, end_indices):\r\n        indices_to_drop.extend(list(df.index[\r\n            np.logical_and(start_date < df.index,\r\n                           df.index < end_date)]))\r\n    df.drop(indices_to_drop, axis=0, inplace=True)\r\n    return df\r\n\r\n\r\ndef get_non_consecutive(df, timedelta):\r\n    \"\"\"\r\n    Get the tuple start_indices, end_indices of all\r\n    non consecutive period in the dataframe index.\r\n    Two timestamps separated with more than timedelta\r\n    are considered non consecutive.\r\n    \r\n    Parameters\r\n    ----------\r\n    df : pandas.DataFrame\r\n        Dataframe with Time series index\r\n    timedelta : pd.Timedelta\r\n        Time delta.\r\n    \r\n    Returns\r\n    -------\r\n    start_dates : array-like\r\n        List of start dates of non consecutive periods\r\n    end_dates : array-like\r\n        List of end dates of non consecutive periods\r\n\r\n    \"\"\"\r\n    where = np.where(\r\n        df.index[1:] - df.index[:-1] > timedelta)[0]\r\n    return df.index[where], df.index[where + 1]\r\n```\r\n\r\nusing these function I now get my desired output:\r\n\r\n```python\r\ninterpolate_consecutive(df, '12H')\r\n\r\n                       A\r\n2017-02-01 06:00:00  0.0\r\n2017-02-03 06:00:00  2.0\r\n2017-02-03 18:00:00  2.5\r\n2017-02-04 06:00:00  3.0\r\n2017-02-04 18:00:00  3.5\r\n2017-02-05 06:00:00  4.0\r\n2017-02-05 18:00:00  4.5\r\n2017-02-06 06:00:00  5.0\r\n2017-02-06 18:00:00  5.5\r\n2017-02-07 06:00:00  6.0\r\n```\r\n\r\ntldr, `limit` should actually not always do forward filling, but check the length of the NaN gap and not fill in anything if this gap is longer than the limit.\r\n\r\nThank you for taking the time to read this, hope I made myself clear.\r\n\r\n",
      "@naifrec thanks for the detailed example, I think I understand the behavior you're looking for.\r\n\r\n`limit` currently has the clearly defined behavior of \"fill at most this many NaNs in a row\", which is useful so we can't change that. We'll have to add another keyword to interpolate.\r\n\r\nI think we should add an additional option to `limit_direction` like `consecutive` (there's probably a better word. Something that describes \"all or nothing\").\r\n\r\nCould you open up a new issue for this (you can just copy your last message). This issue is focusing on extrapolation (which would be orthogonal to this issue).",
      "Perhaps max_gap meaning it will only interpolate over gaps up to a given size?",
      "I'm going to get started on this.  \r\n\r\nI think we need to move the naifrec idea of limiting \"gap size\" or \"all or none\" to another issue.\r\n\r\nI did not get any comments on the suggested parameter, so I will use if everyone is cool with that.\r\n\r\nlimit_type= ('interpolate' | 'extrapolate' | None=default to both which is current_behavior)\r\n",
      "@TomAugspurger , I've got this change ready to go but in writing the docs, I realized I may create a little confusion.\r\n\r\nTechnically we are limiting the the .interpolate method to either doing an interpolation or an extrapolation, but since the name of the method is interpolate, it seems weird, from a documentation perspective, to say we can limit the interpolate method only extrapolate and not interpolate.\r\n\r\nIt is easy to describe these values as 'inside' (i.e. NaNs surrounded by valid values - interpolated), or 'outside' (beyond any existing valid value.  How about if we call it this:\r\n\r\nlimit_range= ('inside' | 'outside' | None=default to both which is current_behavior)",
      "Or, since range has meaning, limit_area =('inside' | 'outside') may be even better.  That sort of fits with limit_direction since you move in a direction and move in an area."
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "renamed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "demilestoned",
      "milestoned",
      "cross-referenced",
      "demilestoned",
      "milestoned"
    ],
    "changed_files": 7,
    "additions": 208,
    "deletions": 73,
    "changed_files_list": [
      "doc/source/missing_data.rst",
      "doc/source/whatsnew/v0.23.0.txt",
      "pandas/core/generic.py",
      "pandas/core/internals.py",
      "pandas/core/missing.py",
      "pandas/core/resample.py",
      "pandas/tests/series/test_missing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 16353,
    "reporter": "gsmafra",
    "created_at": "2017-05-14T19:59:22+00:00",
    "closed_at": "2017-05-15T10:10:14+00:00",
    "resolver": "jreback",
    "resolved_in": "6b0c7e72b141831b7a9a5651f9e19eef53ec9e76",
    "resolver_commit_num": 4622,
    "title": "DataFrame apply results in a DataFrame when lambda returns a list of length equal to the number of DataFrame features",
    "body": "Hello guys, I'm using the apply method with axis=1 argument and returning a list for each row. The weird thing is when the length of this list is equal to the number of features in the original dataframe, this apply returns another dataframe, and a series if anything else. I don't know if this is intended, but it was very unexpected.\r\n\r\n",
    "labels": [],
    "comments": [
      "see https://github.com/pandas-dev/pandas/issues/15628 and linked issues.\r\n\r\n``.apply`` infers the output type based on the input shape. Returning an arbitrary shape breaks this guarantee and the results have to be coerced to the existing structure."
    ],
    "events": [],
    "changed_files": 9,
    "additions": 885,
    "deletions": 192,
    "changed_files_list": [
      "doc/source/basics.rst",
      "doc/source/whatsnew/v0.23.0.txt",
      "pandas/core/apply.py",
      "pandas/core/frame.py",
      "pandas/core/sparse/frame.py",
      "pandas/io/formats/style.py",
      "pandas/tests/frame/test_apply.py",
      "pandas/tests/sparse/frame/test_apply.py",
      "pandas/tests/sparse/frame/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 16416,
    "reporter": "jorisvandenbossche",
    "created_at": "2017-05-22T17:17:41+00:00",
    "closed_at": "2017-07-15T23:45:39+00:00",
    "resolver": "aernlund",
    "resolved_in": "1212fe034b7302f40bf253aedd9e3989514eeb52",
    "resolver_commit_num": 0,
    "title": "DOC: add examples to docstrings",
    "body": "This often used functions lacks some illustrating examples\r\n\r\n- [x] ``sort_values`` (#16437)\r\n- [x] ``reset_values`` (#16967)\r\n- [x] ``set_index`` (#16467)\r\n- [x] ``fillna`` (#16437)\r\n- [x] ``drop`` (#16437)\r\n- [x] ``pop`` (#16520)",
    "labels": [
      "Difficulty Novice",
      "Docs",
      "Effort Low"
    ],
    "comments": [
      "Other docstrings that are in the top most viewed and lacking examples: `reset_index`, `set_index`, `fillna`, `drop`",
      "@jorisvandenbossche I'd like to take this one. ",
      "@geoninja any chance you're at Pycon doing sprints right now?",
      "I can add some examples for `fillna` and `drop`.",
      "@jorisvandenbossche Both Vincent and I are at PyCon so we will split the work on this one",
      "I'm going to work on reset_values so we can close this issue.",
      "if anyone at sprints wants to add to ``.reset_index()`` we can close this one\r\n\r\n@TomAugspurger ",
      "I'm working on .reset_index() example."
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "renamed",
      "milestoned",
      "cross-referenced",
      "commented",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced"
    ],
    "changed_files": 2,
    "additions": 63,
    "deletions": 0,
    "changed_files_list": [
      "pandas/core/frame.py",
      "pandas/core/series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 16459,
    "reporter": "ProsperousHeart",
    "created_at": "2017-05-23T17:37:05+00:00",
    "closed_at": "2017-06-01T10:57:28+00:00",
    "resolver": "economy",
    "resolved_in": "d419be4333dcb8cf643bdd04c7c4e990feae49f9",
    "resolver_commit_num": 1,
    "title": "BUG: cut does not respect order of passed labels",
    "body": "#### \r\nThis issue was found when working on #16432\r\n\r\n\r\n#### Problem description\r\n\r\nWhen running `pytest -x --doctest-modules core/reshape/tile.py` against the work inside of the \"reshape-3439\" branch, we see the following:\r\n\r\n\r\n\r\nThis is backwards.\r\n\r\n#### Expected Output\r\n\r\nShould clear this - this is not an issue.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nUnable to run the pd.show_versions()\r\n\r\n</details>\r\n",
    "labels": [
      "Categorical",
      "Regression"
    ],
    "comments": [
      "```\r\nIn [3]: pd.__version__\r\nOut[3]: '0.19.2'\r\n\r\nIn [4]: pd.cut(np.array([.2, 1.4, 2.5, 6.2, 9.7, 2.1]), 3, labels=[\"good\",\"medium\",\"bad\"])\r\nOut[4]: \r\n[good, good, good, medium, bad, good]\r\nCategories (3, object): [good < medium < bad]\r\n```\r\n\r\n```\r\nIn [12]: pd.__version__\r\nOut[12]: '0.21.0.dev+68.gcc734ba'\r\n\r\nIn [13]: pd.cut(np.array([.2, 1.4, 2.5, 6.2, 9.7, 2.1]), 3, labels=[\"good\",\"medium\",\"bad\"])\r\nOut[13]: \r\n[good, good, good, medium, bad, good]\r\nCategories (3, object): [bad < good < medium]\r\n```",
      "I'm looking into this. \r\n\r\n`core/reshape/tile.py` line 255: looks like `pd.Categorical(ordered=True)` orders the labels alphabetically regardless of how they're passed in.\r\n\r\n```\r\nIn [14]: labels = ['okay', 'good', 'bad']\r\nIn [15]: pd.Categorical(labels, ordered=True)\r\n\r\nOut [15]: [okay, good, bad]\r\nCategories (3, object): [bad < good < okay]\r\n``` \r\n",
      "If I have a categorical, and I want it to be ordered, and I pass a list of strings as labels, it doesn't seem like I'd ever want the _sorted_ version of those labels to be my rank ordering. Is there a case where I'd actually want to sort those strings alphabetically?",
      "@economy `pd.Categorical(labels, categories=labels, ordered=True)`. I thought it respected the observed order with `ordered=True` when `categories=None`, but I guess not.",
      "So, if I'm reading this correctly, updating the test to explicitly set `categories` fixes the issue and follows the appropriate way to use `pd.Categorical`",
      "I think we need a new test in `test_tile.py` You can just copy the example https://github.com/pandas-dev/pandas/issues/16459#issuecomment-303477917 and assert that it matches the output from 0.19.2 (with the order \"good\" < \"medium\" < \"bad\".\r\n\r\nIdeally you would \r\n\r\n1. checkout a new branch\r\n2. write the test\r\n3. verify that the test fails\r\n4. write the fix (by passing `categories=labels`\r\n5. document the change",
      "@TomAugspurger, I can create a new test for this, but `pd.cut()` still doesn't act as expected. \r\n\r\nIf I update `core/reshape/tile.py` line 255 to call `pd.Categorical` using the `categories` attribute correctly, the original test passes and output for `pd.cut()` is as described (expected) above. ",
      "> the original test passes and output for pd.cut() is as described (expected) above.\r\n\r\nDo you mean the one reported by @ProsperousHeart? That is a doctest, which aren't setup to run automatically (yet, @ProsperousHeart is getting them fixed up so that we can do that). In the meantime we need a regular unit test to verify that the fix works.\r\n\r\n"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 10,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.2.txt",
      "pandas/core/reshape/tile.py",
      "pandas/tests/reshape/test_tile.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 16493,
    "reporter": "CRP",
    "created_at": "2017-05-25T10:12:28+00:00",
    "closed_at": "2017-06-01T10:51:09+00:00",
    "resolver": "CRP",
    "resolved_in": "e3ee1869ce955df5d3daa59d59e08749471e5be5",
    "resolver_commit_num": 0,
    "title": "to_html(index_names=False) still renders a row with index names",
    "body": "The index_names parameter is apparently ignored.\r\nI tried to debug the code, and it looks like line 1291 of pandas/io/formats/format.py should contain \"and self.fmt.show_index_names\" in the if clause.\r\n\r\nhere is sample code:\r\n\r\n\r\nwhich produces the following output:\r\n<table border=\"1\" class=\"dataframe\">\r\n  <thead>\r\n    <tr style=\"text-align: right;\">\r\n      <th></th>\r\n      <th>a</th>\r\n      <th>b</th>\r\n      <th>c</th>\r\n      <th>d</th>\r\n    </tr>\r\n    <tr>\r\n      <th>pippo</th>\r\n      <th></th>\r\n      <th></th>\r\n      <th></th>\r\n      <th></th>\r\n    </tr>\r\n  </thead>\r\n  <tbody>\r\n    <tr>\r\n      <th>0</th>\r\n      <td>1.715511</td>\r\n      <td>-1.582624</td>\r\n      <td>0.027399</td>\r\n      <td>-0.276980</td>\r\n    </tr>\r\n    <tr>\r\n      <th>1</th>\r\n      <td>-0.003603</td>\r\n      <td>-1.227605</td>\r\n      <td>0.434763</td>\r\n      <td>0.039167</td>\r\n    </tr>\r\n    <tr>\r\n      <th>2</th>\r\n      <td>0.246343</td>\r\n      <td>-0.062897</td>\r\n      <td>-0.724532</td>\r\n      <td>-0.352672</td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n\r\nthe second row of the thead should not be there...\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.20.1\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 35.0.2\r\nCython: None\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\nxarray: 0.9.5\r\nIPython: 6.0.0\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.4.2\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: 2.4.7\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 0.9.6\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.1.9\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "Bug",
      "IO HTML",
      "Output-Formatting",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments": [
      "May be as simple as also checking `self.fmt.index_names` in https://github.com/pandas-dev/pandas/blob/e41fe7f52a7ae6be962e683f40500624b2ba2cf6/pandas/io/formats/format.py#L1295 (and tests).\r\n\r\n@CRP interested in submitting a fix?",
      "just created pull request, not sure if I did everything right..."
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 11,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v0.20.2.txt",
      "pandas/io/formats/format.py",
      "pandas/tests/io/formats/test_to_html.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 16734,
    "reporter": "dr-leo",
    "created_at": "2017-06-20T08:34:40+00:00",
    "closed_at": "2017-06-21T10:53:10+00:00",
    "resolver": "toobaz",
    "resolved_in": "8a98f5ed541c87a9bf101c9331bd6cfa8f007cc9",
    "resolver_commit_num": 28,
    "title": "Enable label-based indexing with .loc even when DataFrame is not lexsorted",
    "body": "As shown in the following code example, .loc is more restrictive than the deprecated .ix. Label-based indexing without slicing should be possible with .loc even when the dataframe is not lexsorted. The docs could be clearer in this respect. In addition, performance warnings should ideally be suppressible through options.\r\n\r\n\r\nIn [1]: from pandasdmx import *\r\n\r\nIn [2]: estat = Request('estat')\r\n\r\nIn [3]: dsd_resp = estat.datastructure('DSD_une_rt_a')\r\n\r\nIn [4]: df = dsd_resp.write().codelist\r\n\r\nIn [5]: df.ix[['AGE', 'UNIT']]\r\nC:\\Users\\stefan\\Anaconda3\\envs\\py35\\Scripts\\ipython-script.py:1:\r\nDeprecationWarn\r\ning:\r\n.ix is deprecated. Please use\r\n.loc for label based indexing or\r\n.iloc for positional indexing\r\n\r\nSee the documentation here:\r\n-docs/stable/indexing.html#deprecate_ix\r\n\r\n  if __name__ == '__main__':\r\nOut[5]:\r\n             dim_or_attr                             name\r\nAGE  AGE               D                              AGE\r\n     TOTAL             D                            Total\r\n     Y25-74            D              From 25 to 74 years\r\n     Y_LT25            D               Less than 25 years\r\nUNIT UNIT              D                             UNIT\r\n     PC_ACT            D  Percentage of active population\r\n     PC_POP            D   Percentage of total population\r\n     THS_PER           D                 Thousand persons\r\n\r\nIn [6]: df.loc[['AGE', 'UNIT']]\r\n---------------------------------------------------------------------------\r\nUnsortedIndexError                        Traceback (most recent call last)\r\n<ipython-input-6-7994d8369b48> in <module>()\r\n----> 1 df.loc[['AGE', 'UNIT']]\r\n\r\nC:\\Users\\stefan\\Anaconda3\\envs\\py35\\lib\\site-packages\\pandas\\core\\indexing.py\r\nin\r\n __getitem__(self, key)\r\n   1326         else:\r\n   1327             key = com._apply_if_callable(key, self.obj)\r\n-> 1328             return self._getitem_axis(key, axis=0)\r\n   1329\r\n   1330     def _is_scalar_access(self, key):\r\n\r\nC:\\Users\\stefan\\Anaconda3\\envs\\py35\\lib\\site-packages\\pandas\\core\\indexing.py\r\nin\r\n _getitem_axis(self, key, axis)\r\n   1543             # nested tuple slicing\r\n   1544             if is_nested_tuple(key, labels):\r\n-> 1545                 locs = labels.get_locs(key)\r\n   1546                 indexer = [slice(None)] * self.ndim\r\n   1547                 indexer[axis] = locs\r\n\r\nC:\\Users\\stefan\\Anaconda3\\envs\\py35\\lib\\site-packages\\pandas\\core\\indexes\\multi.\r\npy in get_locs(self, tup)\r\n   2267                                      'to be fully lexsorted\r\ntuple len ({\r\n0}), '\r\n   2268                                      'lexsort depth ({1})'\r\n-> 2269                                      .format(len(tup),\r\nself.lexsort_dept\r\nh))\r\n   2270\r\n   2271         # indexer\r\n\r\nUnsortedIndexError: 'MultiIndex Slicing requires the index to be fully\r\nlexsorted\r\n tuple len (1), lexsort depth (0)'\r\n\r\n\r\nI'd like to see .loc do the job as .ix did, maybe with a performance\r\nwarning. Ideally the latter should be suppressible by setting an option\r\nrather than wrapping everything in a context manager from the warnings\r\nstdlib module.\r\n\r\n\r\n\r\n",
    "labels": [
      "Bug",
      "Difficulty Intermediate",
      "Effort Low",
      "Enhancement",
      "Indexing",
      "MultiIndex"
    ],
    "comments": [
      "> Label-based indexing without slicing should be possible with .loc even when the dataframe is not lexsorted\r\n\r\nNotice this is specific to indexing with lists (and booleans, but not single labels). Anyway, I'll push a PR in minutes.",
      "Great!\n\nTo add one perhaps self-evident corner case: in a multi-indexed df, ':'\nshould be permissible as well.\n\ndf.loc[['a', 'c'], :, ['two', 'one']]\n\nThis is not a tested code example. But the idea should be clear: If the\nslice captures the entire row/col, sorting should not be required either.\n\nAm 20.06.2017 um 11:03 schrieb Pietro Battiston:\n>\n>     Label-based indexing without slicing should be possible with .loc\n>     even when the dataframe is not lexsorted\n>\n> Notice this is specific to indexing with lists (and booleans, but not\n> single labels). Anyway, I'll push a PR in minutes.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pandas-dev/pandas/issues/16734#issuecomment-309691379>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADA58tH0g-2wYxOZpP_7dFMyrQ35Nzbxks5sF4r4gaJpZM4N_QR5>.\n>\n\n",
      "> If the slice captures the entire row/col, sorting should not be required either.\r\n\r\nYes, the fix contemplates this use case.",
      "well @toobaz pushed a fix, but ideally you would have a copy-pastable example in the top section. IOW is just code and not relying on external library.",
      "For future reference:\r\n\r\n``` python\r\nIn [2]: _mklbl = lambda prefix, n: [\"%s%s\" % (prefix, i) for i in range(n)]\r\n\r\nIn [3]: index = MultiIndex.from_product([_mklbl('A', 4), _mklbl('B', 2),\r\n   ...:                                  _mklbl('C', 4), _mklbl('D', 2)])\r\n\r\nIn [4]: columns = MultiIndex.from_tuples([('a', 'foo'), ('a', 'bar'),\r\n   ...:                                   ('b', 'foo'), ('b', 'bah')],\r\n   ...:                                  names=['lvl0', 'lvl1'])\r\n\r\nIn [5]: df = DataFrame(-1, index=index, columns=columns)\r\n\r\nIn [6]: df.loc['A1', ('a', slice(None))]\r\n---------------------------------------------------------------------------\r\nUnsortedIndexError                        Traceback (most recent call last)\r\n<ipython-input-6-0160dbc496d0> in <module>()\r\n----> 1 df.loc['A1', ('a', slice(None))]\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in __getitem__(self, key)\r\n   1327             except (KeyError, IndexError):\r\n   1328                 pass\r\n-> 1329             return self._getitem_tuple(key)\r\n   1330         else:\r\n   1331             key = com._apply_if_callable(key, self.obj)\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_tuple(self, tup)\r\n    834     def _getitem_tuple(self, tup):\r\n    835         try:\r\n--> 836             return self._getitem_lowerdim(tup)\r\n    837         except IndexingError:\r\n    838             pass\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_lowerdim(self, tup)\r\n    946         # we may have a nested tuples indexer here\r\n    947         if self._is_nested_tuple_indexer(tup):\r\n--> 948             return self._getitem_nested_tuple(tup)\r\n    949 \r\n    950         # we maybe be using a tuple to represent multiple dimensions here\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_nested_tuple(self, tup)\r\n   1025 \r\n   1026             current_ndim = obj.ndim\r\n-> 1027             obj = getattr(obj, self.name)._getitem_axis(key, axis=axis)\r\n   1028             axis += 1\r\n   1029 \r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_axis(self, key, axis)\r\n   1547             # nested tuple slicing\r\n   1548             if is_nested_tuple(key, labels):\r\n-> 1549                 locs = labels.get_locs(key)\r\n   1550                 indexer = [slice(None)] * self.ndim\r\n   1551                 indexer[axis] = locs\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexes/multi.py in get_locs(self, tup)\r\n   2267                                      'to be fully lexsorted tuple len ({0}), '\r\n   2268                                      'lexsort depth ({1})'\r\n-> 2269                                      .format(len(tup), self.lexsort_depth))\r\n   2270 \r\n   2271         # indexer\r\n\r\nUnsortedIndexError: 'MultiIndex Slicing requires the index to be fully lexsorted tuple len (2), lexsort depth (1)'\r\n\r\nIn [7]: df.loc['A1', ('a', ['foo'])]\r\n---------------------------------------------------------------------------\r\nUnsortedIndexError                        Traceback (most recent call last)\r\n<ipython-input-7-84c90c00aa28> in <module>()\r\n----> 1 df.loc['A1', ('a', ['foo'])]\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in __getitem__(self, key)\r\n   1327             except (KeyError, IndexError):\r\n   1328                 pass\r\n-> 1329             return self._getitem_tuple(key)\r\n   1330         else:\r\n   1331             key = com._apply_if_callable(key, self.obj)\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_tuple(self, tup)\r\n    834     def _getitem_tuple(self, tup):\r\n    835         try:\r\n--> 836             return self._getitem_lowerdim(tup)\r\n    837         except IndexingError:\r\n    838             pass\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_lowerdim(self, tup)\r\n    946         # we may have a nested tuples indexer here\r\n    947         if self._is_nested_tuple_indexer(tup):\r\n--> 948             return self._getitem_nested_tuple(tup)\r\n    949 \r\n    950         # we maybe be using a tuple to represent multiple dimensions here\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_nested_tuple(self, tup)\r\n   1025 \r\n   1026             current_ndim = obj.ndim\r\n-> 1027             obj = getattr(obj, self.name)._getitem_axis(key, axis=axis)\r\n   1028             axis += 1\r\n   1029 \r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_axis(self, key, axis)\r\n   1547             # nested tuple slicing\r\n   1548             if is_nested_tuple(key, labels):\r\n-> 1549                 locs = labels.get_locs(key)\r\n   1550                 indexer = [slice(None)] * self.ndim\r\n   1551                 indexer[axis] = locs\r\n\r\n/home/nobackup/repo/pandas/pandas/core/indexes/multi.py in get_locs(self, tup)\r\n   2267                                      'to be fully lexsorted tuple len ({0}), '\r\n   2268                                      'lexsort depth ({1})'\r\n-> 2269                                      .format(len(tup), self.lexsort_depth))\r\n   2270 \r\n   2271         # indexer\r\n\r\nUnsortedIndexError: 'MultiIndex Slicing requires the index to be fully lexsorted tuple len (2), lexsort depth (1)'\r\n\r\n```"
    ],
    "events": [
      "commented",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "commented",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "referenced",
      "referenced"
    ],
    "changed_files": 5,
    "additions": 35,
    "deletions": 18,
    "changed_files_list": [
      "doc/source/whatsnew/v0.21.0.txt",
      "pandas/core/common.py",
      "pandas/core/indexes/multi.py",
      "pandas/tests/indexes/test_multi.py",
      "pandas/tests/indexing/test_multiindex.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 16764,
    "reporter": "chris-b1",
    "created_at": "2017-06-23T14:46:16+00:00",
    "closed_at": "2017-10-02T13:32:53+00:00",
    "resolver": "TomAugspurger",
    "resolved_in": "2310faa109bdfd9ff3ef4fc19a163d790d60c645",
    "resolver_commit_num": 112,
    "title": "PERF: pandas' import time",
    "body": "I wouldn't normally be concerned about this, as of it course it only happens once, but our import time has gotten quite long, to the point I notice it hanging my `ipython` startup.  \r\n\r\nI don't have a good sense of what would be required to improve this, probably deferring more imports to be just in time?\r\n\r\non `0.20.2` - each import in a separate process\r\n\r\n\r\nBelow is a single process, importing deps first\r\n\r\n",
    "labels": [
      "Performance",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "do these in independent processes \r\n",
      "Oh, right - `matplotlib` one already was, updated the top for numpy.",
      "or better yet import numpy and matplotlib first\r\nthen run it",
      "We do attempt to import matplotlib at import time. We could delay that with something like\r\n\r\n```diff\r\ndiff --git a/pandas/plotting/__init__.py b/pandas/plotting/__init__.py\r\nindex c3cbedb0f..8f98e297e 100644\r\n--- a/pandas/plotting/__init__.py\r\n+++ b/pandas/plotting/__init__.py\r\n@@ -4,12 +4,6 @@ Plotting api\r\n \r\n # flake8: noqa\r\n \r\n-try:  # mpl optional\r\n-    from pandas.plotting import _converter\r\n-    _converter.register()  # needs to override so set_xlim works with str/number\r\n-except ImportError:\r\n-    pass\r\n-\r\n from pandas.plotting._misc import (scatter_matrix, radviz,\r\n                                    andrews_curves, bootstrap_plot,\r\n                                    parallel_coordinates, lag_plot,\r\ndiff --git a/pandas/plotting/_core.py b/pandas/plotting/_core.py\r\nindex 391fa377f..9821c89c4 100644\r\n--- a/pandas/plotting/_core.py\r\n+++ b/pandas/plotting/_core.py\r\n@@ -37,12 +37,7 @@ from pandas.plotting._tools import (_subplots, _flatten, table,\r\n                                     _get_xlim, _set_ticks_props,\r\n                                     format_date_labels)\r\n \r\n-\r\n-if _mpl_ge_1_5_0():\r\n-    # Compat with mp 1.5, which uses cycler.\r\n-    import cycler\r\n-    colors = mpl_stylesheet.pop('axes.color_cycle')\r\n-    mpl_stylesheet['axes.prop_cycle'] = cycler.cycler('color', colors)\r\n+_registered = False\r\n \r\n \r\n def _get_standard_kind(kind):\r\n@@ -92,6 +87,7 @@ class MPLPlot(object):\r\n                  secondary_y=False, colormap=None,\r\n                  table=False, layout=None, **kwds):\r\n \r\n+        self._setup()\r\n         self.data = data\r\n         self.by = by\r\n \r\n@@ -175,6 +171,20 @@ class MPLPlot(object):\r\n \r\n         self._validate_color_args()\r\n \r\n+    def _setup(self):\r\n+        global _registered\r\n+        if not _registered:\r\n+            from pandas.plotting import _converter\r\n+            _converter.register()\r\n+\r\n+            if _mpl_ge_1_5_0():\r\n+                # Compat with mp 1.5, which uses cycler.\r\n+                import cycler\r\n+                colors = mpl_stylesheet.pop('axes.color_cycle')\r\n+                mpl_stylesheet['axes.prop_cycle'] = cycler.cycler('color', colors)\r\n+\r\n+            _registered = True\r\n+\r\n     def _validate_color_args(self):\r\n         if 'color' not in self.kwds and 'colors' in self.kwds:\r\n             warnings.warn((\"'colors' is being deprecated. Please use 'color'\"\r\n```\r\n\r\nThat covers all the `.plot` methods. Would need a decorator or something to cover the plotting methods not attached to NDFrame.",
      "Looks like `get_versions` takes up about 25% of the import time for `pandas.__init__.py`; That could easily be delayed.",
      "Oh, sorry, I was thinking of `show_versions`, not `get_versions`. `get_versions` would be a bit harder to fix... I did try out https://github.com/pypa/setuptools_scm instead of versioneer, and it worked well. May be worth looking into.",
      "I did a profile with https://github.com/cournape/import-profiler\r\n\r\nFrom a very quick skim:\r\n- `s3fs` (boto3) also takes a lot of time (140.4 of 786.7 ms). This can maybe be delayed?\r\n- do we need to import `pytest` ? (43 ms) (this is in pandas.util._tester, I think we can easily move the pytest import inside the `test` function?)\r\n- `xlsxwriter` (22 ms) probably doesn't need to be imported on pandas import (but didn't look into it, it is imported in the config files)\r\n\r\nNot huge, but those two would already remove ca 20% of the import time.\r\n\r\nHowever I don't see `get_versions` somewhere in there, so not sure how reliable the results are.\r\n\r\nFull output: <details>\r\n\r\n```\r\nIn [1]: from import_profiler import profile_import\r\n   ...: \r\n   ...: with profile_import() as context:\r\n   ...:     # Anything expensive in here\r\n   ...:     import pandas\r\n   ...: \r\n\r\nIn [2]: context.print_info()\r\n  cumtime (ms)    intime (ms)  name\r\n         786.7           48.4  pandas\r\n         196.9            3.3  +numpy\r\n           1.8            1.8  ++_globals\r\n           1.5            1.5  ++numpy.__config__\r\n           1.9            1.9  ++version\r\n           1.4            1.3  ++_import_tools\r\n         156.9            0    ++\r\n         156.9            3.4  +++numpy.add_newdocs\r\n         150.9            0.8  ++++numpy.lib\r\n         101.6            0.7  +++++type_check\r\n          96.6            2.9  ++++++numpy.core.numeric\r\n          93.7            1.2  +++++++numpy.core\r\n          19              0.1  ++++++++\r\n          18.9           18.8  +++++++++numpy.core.multiarray\r\n           2.4            0.1  ++++++++\r\n           2.3            2.2  +++++++++numpy.core.umath\r\n          17.5            0    ++++++++\r\n          17.5            2.7  +++++++++numpy.core._internal\r\n           2.5            0.6  ++++++++++numpy.compat\r\n           1              0    +++++++++++\r\n           1              0.9  ++++++++++++numpy.compat._inspect\r\n           6.2            3.4  ++++++++++ctypes\r\n           1.5            1.5  +++++++++++_ctypes\r\n           6              3.4  ++++++++++numerictypes\r\n           2.3            2.2  +++++++++++numbers\r\n           9              0    ++++++++\r\n           8.9            2.2  +++++++++numpy.core.numeric\r\n           6.3            1.5  ++++++++++arrayprint\r\n           4.7            1    +++++++++++fromnumeric\r\n           3.5            0    ++++++++++++\r\n           3.4            3.2  +++++++++++++numpy.core._methods\r\n           2.1            0    ++++++++\r\n           2.1            1.7  +++++++++numpy.core.defchararray\r\n           1.3            0    ++++++++\r\n           1.2            1.1  +++++++++numpy.core.records\r\n          36.4            0.1  ++++++++numpy.testing.nosetester\r\n          36.3            0.5  +++++++++numpy.testing\r\n          22.6            0.7  ++++++++++unittest\r\n           3.2            0.8  +++++++++++result\r\n           2.3            0    ++++++++++++\r\n           2.2            2.1  +++++++++++++unittest.util\r\n           3.7            3.4  +++++++++++case\r\n           4.9            4.8  +++++++++++suite\r\n           6.4            6.2  +++++++++++loader\r\n           3.7            1.1  +++++++++++main\r\n           2.5            0    ++++++++++++\r\n           2.4            1.4  +++++++++++++unittest.runner\r\n          13.1            0    ++++++++++\r\n          13.1            2.1  +++++++++++numpy.testing.decorators\r\n          10.9            2.2  ++++++++++++utils\r\n           5              4.9  +++++++++++++nosetester\r\n           3.5            3.3  +++++++++++++numpy.lib.utils\r\n           4.2            4.1  ++++++ufunclike\r\n          27.3            2.5  +++++index_tricks\r\n          15.7            0    ++++++\r\n          15.6           11.2  +++++++numpy.lib.function_base\r\n           3.9            3.8  ++++++++numpy.lib.twodim_base\r\n           6.8            2.7  ++++++numpy.matrixlib\r\n           3.9            3.8  +++++++defmatrix\r\n           2.2            2.2  ++++++numpy.lib.stride_tricks\r\n           1.4            1.3  +++++nanfunctions\r\n           7.1            1.4  +++++polynomial\r\n           1              1    ++++++numpy.lib.twodim_base\r\n           4.5            0.6  ++++++numpy.linalg\r\n           3.4            1.1  +++++++linalg\r\n           2.1            0    ++++++++numpy.linalg\r\n           1.2            1.1  +++++++++numpy.linalg._umath_linalg\r\n           4.6            1.5  +++++npyio\r\n           1.1            1    ++++++_iotools\r\n           2.6            2.5  +++++financial\r\n           3.5            0    ++\r\n           3.4            0.5  +++numpy.fft\r\n           1.6            0.6  ++++fftpack\r\n          10              0    ++\r\n           9.9            0.6  +++numpy.polynomial\r\n           3.4            1.4  ++++polynomial\r\n           1.1            1    ++++chebyshev\r\n           1              0.9  ++++legendre\r\n           1.1            1    ++++hermite\r\n           1.3            1.1  ++++hermite_e\r\n           1.3            1.1  ++++laguerre\r\n           7              0    ++\r\n           7              1.2  +++numpy.random\r\n           4.7            4.6  ++++mtrand\r\n           2.2            0    ++\r\n           2.2            2.1  +++numpy.ctypeslib\r\n           6.9            0    ++\r\n           6.8            0.6  +++numpy.ma\r\n           4.7            0    ++++\r\n           4.6            4.4  +++++numpy.ma.core\r\n           1.5            0    ++++\r\n           1.5            1.3  +++++numpy.ma.extras\r\n           5.6            1.9  +pytz\r\n           1.4            0.7  ++pytz.lazy\r\n          25.5            0.8  +pandas.compat.numpy\r\n          24.7            1.1  ++pandas.compat\r\n           1.9            1.4  +++distutils.version\r\n          14.6            2    +++http.client\r\n           2              2    ++++http\r\n          10.5            4.3  ++++ssl\r\n           4              4    +++++ipaddress\r\n           1.7            1.7  +++++_ssl\r\n           5.3            0    +++dateutil\r\n           5.2            1.6  ++++dateutil.parser\r\n           2.9            0    +++++\r\n           2.9            0.4  ++++++dateutil.tz\r\n           2.5            1.5  +++++++tz\r\n          26.2            0.4  +pandas._libs\r\n          11.8           10.6  ++tslib\r\n          14              1.9  ++pandas._libs.hashtable\r\n          11.3            6.8  +++pandas._libs.lib\r\n           2.7            2.6  ++++_decimal\r\n          30.6            1.8  +pandas.core.config_init\r\n           4.6            2.3  ++pandas.core.config\r\n           2.1            0.5  +++pandas.io.formats.printing\r\n          22.6            0.4  ++xlsxwriter\r\n          22.2            1    +++workbook\r\n           2.5            0.3  ++++compatibility\r\n           1.7            1.7  +++++fractions\r\n           9.3            5    ++++xlsxwriter.worksheet\r\n           3.1            0.6  +++++drawing\r\n           1.8            1.8  ++++++shape\r\n           4              0.4  ++++xlsxwriter.packager\r\n           1.6            0.3  ++++xlsxwriter.chart_area\r\n           1.3            0    +++++\r\n           1.3            1.2  ++++++xlsxwriter.chart\r\n         369.9            0.4  +pandas.core.api\r\n           9.9            1    ++pandas.core.algorithms\r\n           6              0.6  +++pandas.core.dtypes.cast\r\n           5              0.6  ++++common\r\n           2.2            0    +++++pandas._libs\r\n           2.2            2    ++++++pandas._libs.algos\r\n           1.3            1.3  +++++dtypes\r\n           2.7            0    +++pandas.core\r\n           2.7            0.8  ++++pandas.core.common\r\n           1.4            0.2  +++++pandas.api\r\n           1.2            0.4  ++++++pandas.api.types\r\n          15.4            1.1  ++pandas.core.categorical\r\n          13.6            1.4  +++pandas.core.base\r\n           2.9            0.3  ++++pandas.util._validators\r\n           2.6            0.3  +++++pandas.util\r\n           1.7            0.5  ++++++pandas.core.util.hashing\r\n           7.6            0.8  ++++pandas.core.nanops\r\n           6.7            0.4  +++++bottleneck\r\n           1.7            0    ++++++\r\n           1.7            0.4  +++++++bottleneck.slow\r\n           1              1    ++++++reduce\r\n           1              0.5  ++++++bottleneck.benchmark.bench\r\n           1.7            0    ++++pandas.compat.numpy\r\n           1.7            1.6  +++++pandas.compat.numpy.function\r\n         330.5            6.6  ++pandas.core.groupby\r\n          81.5            0.3  +++pandas.core.index\r\n          81.2            0.6  ++++pandas.core.indexes.api\r\n          27.1            2.8  +++++pandas.core.indexes.base\r\n           4.9            0    ++++++pandas._libs\r\n           2.3            1.8  +++++++pandas._libs.index\r\n           2.6            1.9  +++++++pandas._libs.join\r\n          16.2            0.9  ++++++pandas.core.ops\r\n          15.1            0.6  +++++++pandas.core.computation.expressions\r\n          14.5            0.3  ++++++++pandas.core.computation\r\n          14.1            0.6  +++++++++numexpr\r\n           4.6            4.6  ++++++++++cpuinfo\r\n           4.7            1.2  ++++++++++numexpr.expressions\r\n           3.5            0    +++++++++++numexpr\r\n           3.4            3.3  ++++++++++++numexpr.interpreter\r\n           1.4            1    ++++++++++numexpr.necompiler\r\n           2.1            0.2  ++++++++++numexpr.tests\r\n           1.8            1.7  +++++++++++numexpr.tests.test_numexpr\r\n           2.4            2.2  ++++++pandas.core.strings\r\n           2.2            2    +++++pandas.core.indexes.category\r\n          32.4           32.2  +++++pandas.core.indexes.multi\r\n           1.4            1.3  +++++pandas.core.indexes.interval\r\n           1.6            1.4  +++++pandas.core.indexes.numeric\r\n           1              0.9  +++++pandas.core.indexes.range\r\n          11.5            1    +++++pandas.core.indexes.timedeltas\r\n           6.3            1.8  ++++++pandas.tseries.frequencies\r\n           4.1            2.6  +++++++pandas.tseries.offsets\r\n           1.1            0.7  ++++++++pandas.core.tools.datetimes\r\n           3.5            1    ++++++pandas.core.indexes.datetimelike\r\n           2.2            1.6  +++++++pandas._libs.period\r\n           3              1.2  +++++pandas.core.indexes.period\r\n           1.6            1.4  ++++++pandas.core.indexes.datetimes\r\n         235.6            7.2  +++pandas.core.frame\r\n         161.6            3.7  ++++pandas.core.generic\r\n           1.3            1.1  +++++pandas.core.indexing\r\n           7.1            3.4  +++++pandas.core.internals\r\n           3.4            1.1  ++++++pandas.core.sparse.array\r\n           1.9            1.7  +++++++pandas._libs.sparse\r\n         149.3            1.5  +++++pandas.io.formats.format\r\n         147.2            0.7  ++++++pandas.io.common\r\n           1.3            0.6  +++++++csv\r\n         140.4            0.4  +++++++s3fs\r\n         139.5            0.9  ++++++++core\r\n         128              0.5  +++++++++boto3\r\n         127.5            0.4  ++++++++++boto3.session\r\n         116.9            0.7  +++++++++++botocore.session\r\n          57.3            0.3  ++++++++++++botocore.configloader\r\n           3.1            0    +++++++++++++six.moves\r\n           3.1            3    ++++++++++++++configparser\r\n          53.8            1.7  +++++++++++++botocore.exceptions\r\n          52.1            0    ++++++++++++++botocore.vendored.requests.exceptions\r\n          52              0.6  +++++++++++++++botocore.vendored.requests\r\n          25.6            0.7  ++++++++++++++++packages.urllib3.contrib\r\n          22.7            0.1  +++++++++++++++++botocore.vendored.requests.packages.urllib3\r\n          22.6            0.4  ++++++++++++++++++botocore.vendored.requests.packages\r\n          22.2            0    +++++++++++++++++++\r\n          22.2            0.7  ++++++++++++++++++++botocore.vendored.requests.packages.urllib3\r\n          20.2            0.7  +++++++++++++++++++++connectionpool\r\n           1.1            1.1  ++++++++++++++++++++++exceptions\r\n           3.8            0.4  ++++++++++++++++++++++connection\r\n           3.3            0    +++++++++++++++++++++++util.ssl_\r\n           3.3            0.2  ++++++++++++++++++++++++botocore.vendored.requests.packages.urllib3.util\r\n           1.1            1    +++++++++++++++++++++++++url\r\n          11.1            0.3  ++++++++++++++++++++++request\r\n          10.8            0.3  +++++++++++++++++++++++filepost\r\n           9.8            9.4  ++++++++++++++++++++++++uuid\r\n           1.6            0.8  ++++++++++++++++++++++response\r\n           1.1            0.9  +++++++++++++++++++++poolmanager\r\n           2.2            1.5  +++++++++++++++++botocore.vendored.requests.packages.urllib3.contrib.pyopenssl\r\n          20.8            0    ++++++++++++++++\r\n          20.8            0.7  +++++++++++++++++botocore.vendored.requests.utils\r\n           3.3            0.8  ++++++++++++++++++cgi\r\n           2.5            0.7  +++++++++++++++++++html\r\n           1.7            1.7  ++++++++++++++++++++html.entities\r\n          13.6            0.4  ++++++++++++++++++compat\r\n           4              3.2  +++++++++++++++++++urllib.request\r\n           5.2            0    +++++++++++++++++++http\r\n           5.2            5.1  ++++++++++++++++++++http.cookiejar\r\n           3.3            3.3  +++++++++++++++++++http.cookies\r\n           1.2            1.1  ++++++++++++++++++cookies\r\n           2.3            0.8  ++++++++++++++++models\r\n           1.1            0.5  +++++++++++++++++auth\r\n           2.4            0.4  ++++++++++++++++api\r\n           2.1            0    +++++++++++++++++\r\n           2              1.1  ++++++++++++++++++botocore.vendored.requests.sessions\r\n          12.4            2.2  ++++++++++++botocore.credentials\r\n           8.8            0.8  +++++++++++++botocore.compat\r\n           3.3            0    ++++++++++++++botocore.vendored\r\n           3.2            3.1  +++++++++++++++botocore.vendored.six\r\n           4.2            0.6  ++++++++++++++xml.etree.cElementTree\r\n           3              1.1  +++++++++++++++xml.etree.ElementTree\r\n           1.2            1.1  +++++++++++++botocore.utils\r\n          41.8            1    ++++++++++++botocore.client\r\n          29.6            0    +++++++++++++botocore\r\n          29.6            1    ++++++++++++++botocore.waiter\r\n           9.4            0.8  +++++++++++++++jmespath\r\n           8.6            0.1  ++++++++++++++++jmespath\r\n           8.6            2.1  +++++++++++++++++jmespath.parser\r\n           3.4            0.1  ++++++++++++++++++jmespath\r\n           3.3            1.1  +++++++++++++++++++jmespath.lexer\r\n           2.2            1.3  ++++++++++++++++++++jmespath.exceptions\r\n           2.3            0    ++++++++++++++++++jmespath\r\n           2.3            0.8  +++++++++++++++++++jmespath.visitor\r\n           1.4            0    ++++++++++++++++++++jmespath\r\n           1.4            1.3  +++++++++++++++++++++jmespath.functions\r\n          19.2            0.5  +++++++++++++++botocore.docs.docstring\r\n          18.6            0.4  ++++++++++++++++botocore.docs\r\n          18.1            0.7  +++++++++++++++++botocore.docs.service\r\n           1.9            1.8  ++++++++++++++++++botocore.docs.utils\r\n           3.3            0.5  ++++++++++++++++++botocore.docs.client\r\n           2.1            0.5  +++++++++++++++++++botocore.docs.method\r\n          10.9            0.8  ++++++++++++++++++botocore.docs.bcdoc.restdoc\r\n           7.3            0.8  +++++++++++++++++++botocore.docs.bcdoc.docstringparser\r\n           6.5            4.9  ++++++++++++++++++++html.parser\r\n           1.6            1.5  +++++++++++++++++++++_markupbase\r\n           2.3            2.2  +++++++++++++++++++botocore.docs.bcdoc.style\r\n           1.4            0.9  +++++++++++++botocore.auth\r\n           1.2            1    +++++++++++++botocore.awsrequest\r\n           1.4            1.3  +++++++++++++botocore.hooks\r\n           5.6            0.3  +++++++++++++botocore.args\r\n           1.3            0.6  ++++++++++++++botocore.serialize\r\n           3.3            0.3  ++++++++++++++botocore.config\r\n           3              0.8  +++++++++++++++botocore.endpoint\r\n           1.3            0.3  ++++++++++++++++botocore.response\r\n           2.6            0    ++++++++++++botocore\r\n           2.5            1.4  +++++++++++++botocore.handlers\r\n           1              1    +++++++++++boto3.utils\r\n           8.5            0.7  +++++++++++resources.factory\r\n           6.6            0.4  ++++++++++++action\r\n           4.7            0.4  +++++++++++++boto3.docs.docstring\r\n           4.2            0.2  ++++++++++++++boto3.docs\r\n           4              0.5  +++++++++++++++boto3.docs.service\r\n           3.1            0.5  ++++++++++++++++boto3.docs.resource\r\n           1.1            0.3  +++++++++++++++++boto3.docs.action\r\n           9.9            1.1  +++++++++boto3.s3.transfer\r\n           8.4            0.3  ++++++++++concurrent\r\n           8.1            0.3  +++++++++++concurrent.futures\r\n           1.4            1.3  ++++++++++++concurrent.futures._base\r\n           6              0.7  ++++++++++++concurrent.futures.process\r\n           2.3            0.4  +++++++++++++multiprocessing\r\n           1.9            0    ++++++++++++++\r\n           1.8            0.8  +++++++++++++++multiprocessing.context\r\n           2.9            1    +++++++++++++multiprocessing.connection\r\n           1.3            0    +++++++py.path\r\n           1.3            0.8  ++++++++py\r\n           3              1.9  +++++++py._path.local\r\n          58.5            5.2  ++++pandas.core.series\r\n           5.8            0    +++++pandas.core\r\n           5.8            3.6  ++++++pandas.core.window\r\n           2              1.9  +++++++pandas._libs.window\r\n          46.3            0    +++++pandas.plotting._core\r\n          46.2            0.6  ++++++pandas.plotting\r\n          41              0    +++++++pandas.plotting\r\n          40.9            1.3  ++++++++pandas.plotting._converter\r\n          23.7            0.5  +++++++++matplotlib.units\r\n          23.2            9    ++++++++++matplotlib\r\n           2.5            1.5  +++++++++++distutils.sysconfig\r\n           1              1    ++++++++++++errors\r\n           2.9            2    +++++++++++matplotlib.cbook\r\n           7.1            1.2  +++++++++++matplotlib.rcsetup\r\n           2.3            2.3  ++++++++++++matplotlib.fontconfig_pattern\r\n           2.9            1.5  ++++++++++++matplotlib.colors\r\n           1.3            1.3  +++++++++++++_color_data\r\n          15.3            1.3  +++++++++matplotlib.dates\r\n           1.1            0.9  ++++++++++dateutil.rrule\r\n          12.7            1.8  ++++++++++matplotlib.ticker\r\n          10.9            0    +++++++++++matplotlib\r\n          10.8            8.6  ++++++++++++matplotlib.transforms\r\n           1.1            1    +++++++++++++path\r\n           1.6            0.5  +++++++pandas.plotting._misc\r\n           3              2.8  +++++++pandas.plotting._core\r\n           7.8            0.4  ++++pandas.core.computation.eval\r\n           6.4            3.9  +++++pandas.core.computation.expr\r\n           1.6            0.7  ++++++pandas.core.computation.ops\r\n           4.7            4.4  +++pandas.core.panel\r\n           1.3            0    +++pandas._libs\r\n           1.3            1.2  ++++pandas._libs.groupby\r\n           2.1            1.8  ++pandas.core.panel4d\r\n           8.9            0.9  ++pandas.core.reshape.reshape\r\n           7              0.3  +++pandas.core.sparse.api\r\n           3.2            2.2  ++++pandas.core.sparse.series\r\n           3              2.9  ++++pandas.core.sparse.frame\r\n           1.8            1.6  ++pandas.core.resample\r\n           1.6            0.4  +pandas.stats.api\r\n           1              1    ++pandas.stats.moments\r\n           2.8            0.2  +pandas.core.reshape.api\r\n           1.1            0.8  ++pandas.core.reshape.merge\r\n          29              0.3  +pandas.io.api\r\n           5.9            2.7  ++pandas.io.parsers\r\n           2.6            2    +++pandas._libs.parsers\r\n           3              1.5  ++pandas.io.excel\r\n           1.1            1    +++pandas._libs.json\r\n           5.8            3.5  ++pandas.io.pytables\r\n           2              1.9  +++pandas.core.computation.pytables\r\n           2.4            0.5  ++pandas.io.json\r\n           1.8            1    +++json\r\n           2              1.8  ++pandas.io.stata\r\n           5.3            0.7  ++pandas.io.packers\r\n           3.8            1.1  +++pandas.io.msgpack\r\n          43.9            0.2  +pandas.util._tester\r\n          43.7            6.1  ++pytest\r\n           8.8            1.3  +++_pytest.config\r\n           2.9            0.3  ++++_pytest._code\r\n           2              1.2  +++++code\r\n           2              0.4  ++++_pytest.hookspec\r\n           1.5            0.2  +++++_pytest._pluggy\r\n           1.3            1.1  ++++++_pytest.vendored_packages.pluggy\r\n           2.3            0.5  ++++_pytest.assertion\r\n           1.1            0    +++++_pytest.assertion\r\n           1.1            1    ++++++_pytest.assertion.rewrite\r\n           1.9            0.9  +++_pytest.main\r\n           5.7            1.3  +++_pytest.python\r\n           4.3            0    ++++_pytest\r\n           4.3            1.8  +++++_pytest.fixtures\r\n           1.8            1.1  ++++++py._code.code\r\n           1.2            0.5  +++_pytest.unittest\r\n           2.5            0.9  +++_pytest.capture\r\n           1.4            0.7  ++++py._io.capture\r\n           1              0.4  +++_pytest.tmpdir\r\n          10.1            9.1  +++_pytest.junitxml\r\n           3.3            0.3  +pandas.testing\r\n           3              1.7  ++pandas.util.testing\r\n           1.1            0    +++pandas._libs\r\n           1.1            0.9  ++++pandas._libs.testing\r\n```\r\n</details>",
      "Also see #7282, but seems like already more attention here.",
      "> Also see #7282, but seems like already more attention here.\r\n\r\nIt's a bit different issue, this is in general about reducing import time, the other issue is about a specific case where the import takes many seconds (but also numpy takes seconds to import, so IMO it's not pandas specific issue)"
    ],
    "events": [
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files": 21,
    "additions": 183,
    "deletions": 181,
    "changed_files_list": [
      ".travis.yml",
      "ci/check_imports.py",
      "doc/source/whatsnew/v0.21.0.txt",
      "pandas/core/computation/__init__.py",
      "pandas/core/computation/check.py",
      "pandas/core/computation/eval.py",
      "pandas/core/computation/expressions.py",
      "pandas/core/config_init.py",
      "pandas/core/frame.py",
      "pandas/core/internals.py",
      "pandas/core/ops.py",
      "pandas/core/panel.py",
      "pandas/io/common.py",
      "pandas/io/excel.py",
      "pandas/plotting/__init__.py",
      "pandas/plotting/_core.py",
      "pandas/plotting/_style.py",
      "pandas/tests/computation/test_compat.py",
      "pandas/tests/frame/test_query_eval.py",
      "pandas/util/_tester.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 16770,
    "reporter": "adbull",
    "created_at": "2017-06-26T08:33:18+00:00",
    "closed_at": "2017-07-18T23:50:42+00:00",
    "resolver": "ri938",
    "resolved_in": "01d7be51132b31771ff5b5c7a9c333557a902e8e",
    "resolver_commit_num": 2,
    "title": "BUG: error thrown reindexing empty CategoricalIndex",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nReindexing a series or frame with an empty CategoricalIndex throws an error. This behaviour is inconsistent with other indexes, which can reindex an empty series or frame without error. While it is of course possible to work around this issue, it requires unnecessarily complex code when the size of the input is not known in advance.\r\n\r\n#### Expected Output\r\n\r\nNo errors thrown, objects reindexed.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.1.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.10.10-100.fc24.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: C\r\nLANG: C\r\nLOCALE: None.None\r\n\r\npandas: 0.20.2\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 33.1.1.post20170320\r\nCython: 0.25.2\r\nnumpy: 1.13.0\r\nscipy: 0.19.0\r\nxarray: None\r\nIPython: 4.2.1\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.5\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Categorical",
      "Difficulty Intermediate",
      "Effort Low",
      "Indexing"
    ],
    "comments": [
      "yeah this is an annoying numpy thing. need to test ``if len(values)`` here\r\n\r\n```\r\n> /Users/jreback/pandas/pandas/core/indexes/base.py(1806)_assert_take_fillable()\r\n   1804                 taken[mask] = na_value\r\n   1805         else:\r\n-> 1806             taken = values.take(indices)\r\n   1807         return taken\r\n   1808 \r\n\r\nipdb> l\r\n   1801             taken = values.take(indices)\r\n   1802             mask = indices == -1\r\n   1803             if mask.any():\r\n   1804                 taken[mask] = na_value\r\n   1805         else:\r\n-> 1806             taken = values.take(indices)\r\n   1807         return taken\r\n   1808 \r\n```\r\n\r\nPR's welcome."
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 14,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.21.0.txt",
      "pandas/core/indexes/category.py",
      "pandas/tests/indexes/test_category.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 16798,
    "reporter": "jeffknupp",
    "created_at": "2017-06-29T16:27:47+00:00",
    "closed_at": "2017-07-23T16:28:19+00:00",
    "resolver": "jeffknupp",
    "resolved_in": "8d7d3fb545b4273cf9d1a61bf7ea3bfdde8a1199",
    "resolver_commit_num": 0,
    "title": "Pandas core dumps when reading large CSV file using read_csv(..., low_memory=False)",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nFrom the stacktrace in the core file, pandas seems to be throwing an exception complaining \"out of memory\" (which it is not, the machine has 64 G of RAM and the interpreter was using maybe 5 G) but, during the cleanup of that exception, attempts to double free the `self->error_msg` pointer (according to gcc). Results in a `SIGSEGV`.\r\n\r\n#### Expected Output\r\n\r\nPandas successfully converts the CSV into a dataframe\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-81-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 36.0.1\r\nCython: 0.25.2\r\nnumpy: 1.13.0\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: 1.1.4\r\npymysql: None\r\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\r\njinja2: 2.9.6\r\nboto: 2.47.0\r\npandas_datareader: None\r\n\r\n</details>\r\n\r\nI can provide the source CSV if necessary (though it happens reliably with \"large\" CSVs, for a definition of \"large\" I haven't nailed down but in the multi-GB range). Below is the stack trace:\r\n\r\n<details>\r\n#0  0x00007f278e970532 in __GI___libc_free (mem=0x7f2757218b0e) at malloc.c:2967\r\n#1  0x00007f275720fd2a in free_if_not_null (ptr=0x3cf5ee8) at pandas/src/parser/tokenizer.c:94\r\n#2  parser_cleanup (self=self@entry=0x3cf5df0) at pandas/src/parser/tokenizer.c:189\r\n#3  0x00007f275720ff09 in parser_free (self=0x3cf5df0) at pandas/src/parser/tokenizer.c:285\r\n#4  0x00007f27571b0562 in __pyx_pf_6pandas_6parser_10TextReader_4__dealloc__ (__pyx_v_self=0x7f2757166bc8) at pandas/parser.c:6330\r\n#5  __pyx_pw_6pandas_6parser_10TextReader_5__dealloc__ (__pyx_v_self=<pandas.parser.TextReader at remote 0x7f2757166bc8>) at pandas/parser.c:6313\r\n#6  __pyx_tp_dealloc_6pandas_6parser_TextReader (o=<pandas.parser.TextReader at remote 0x7f2757166bc8>) at pandas/parser.c:45130\r\n#7  0x000000000055dbea in dict_dealloc.lto_priv.164 (mp=0x7f2753a0bbc8) at ../Objects/dictobject.c:1594\r\n#8  subtype_dealloc () at ../Objects/typeobject.c:1193\r\n#9  0x000000000055dbea in dict_dealloc.lto_priv.164 (mp=0x7f2753c765c8) at ../Objects/dictobject.c:1594\r\n#10 subtype_dealloc () at ../Objects/typeobject.c:1193\r\n#11 0x00000000004e9137 in frame_dealloc.lto_priv () at ../Objects/frameobject.c:431\r\n#12 0x0000000000541457 in tb_dealloc.lto_priv.286 (tb=0x7f27537b8048) at ../Python/traceback.c:55\r\n#13 0x000000000054146d in tb_dealloc.lto_priv.286 (tb=0x7f27537b8088) at ../Python/traceback.c:54\r\n#14 0x000000000054146d in tb_dealloc.lto_priv.286 (tb=0x7f27537b80c8) at ../Python/traceback.c:54\r\n#15 0x000000000054146d in tb_dealloc.lto_priv.286 (tb=0x7f27537b8108) at ../Python/traceback.c:54\r\n#16 0x0000000000526b99 in PyEval_EvalFrameEx () at ../Python/ceval.c:2132\r\n#17 0x0000000000528814 in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7ffdc3b87590, func=<optimized out>) at ../Python/ceval.c:4803\r\n#18 call_function (oparg=<optimized out>, pp_stack=0x7ffdc3b87590) at ../Python/ceval.c:4730\r\n#19 PyEval_EvalFrameEx () at ../Python/ceval.c:3236\r\n#20 0x0000000000528814 in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7ffdc3b876c0, func=<optimized out>) at ../Python/ceval.c:4803\r\n#21 call_function (oparg=<optimized out>, pp_stack=0x7ffdc3b876c0) at ../Python/ceval.c:4730\r\n#22 PyEval_EvalFrameEx () at ../Python/ceval.c:3236\r\n#23 0x000000000052d2e3 in _PyEval_EvalCodeWithName () at ../Python/ceval.c:4018\r\n#24 0x000000000052dfdf in PyEval_EvalCodeEx () at ../Python/ceval.c:4039\r\n#25 PyEval_EvalCode (co=<optimized out>, globals=<optimized out>, locals=<optimized out>) at ../Python/ceval.c:777\r\n#26 0x00000000005fd2c2 in run_mod () at ../Python/pythonrun.c:976\r\n#27 0x00000000005ff76a in PyRun_FileExFlags () at ../Python/pythonrun.c:929\r\n#28 0x00000000005ff95c in PyRun_SimpleFileExFlags () at ../Python/pythonrun.c:396\r\n#29 0x000000000063e7d6 in run_file (p_cf=0x7ffdc3b87930, filename=0x1e96260 L\"/home/jknupp/lion/components/api/scripts/parquet_export.py\", fp=0x1fa1310) at ../Modules/main.c:318\r\n#30 Py_Main () at ../Modules/main.c:768\r\n#31 0x00000000004cfe41 in main () at ../Programs/python.c:65\r\n#32 0x00007f278e90c830 in __libc_start_main (main=0x4cfd60 <main>, argc=2, argv=0x7ffdc3b87b48, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7ffdc3b87b38)\r\n    at ../csu/libc-start.c:291\r\n#33 0x00000000005d5f29 in _start ()\r\n</details>",
    "labels": [
      "Bug",
      "Difficulty Intermediate",
      "Effort Medium",
      "IO CSV"
    ],
    "comments": [
      "this is fixed in 20.2 give a try ",
      "Great! @jreback Could you point me to the issue that fixes it (just for my records)?",
      "well i would suggest u try it",
      "This doesn't fix the issue. Can you point me to the issue you thought fixed it? It might point me in the right direction.",
      "just search in the whatsnew there were a few\r\n\r\nw/o a repro this is impossible to debug",
      "I can post a link to the CSV in question. It's quite large, but cores almost immediately.",
      "then progressively make it smaller until it doesn't core (and show the 1 before)\r\nthese types of errors are generally repo on small sets",
      "Smallest amount I could get it to core reliably on is 500,000 lines (1 GB uncompressed). Here is a link to a `bzip2` compressed version: https://www.dropbox.com/s/hguhamz0gdv2uzv/test_data.csv.bz2?dl=0\r\n\r\nMD5 of uncompressed file is listed below:\r\n\r\n> MD5 (test_data.csv) = 9a66139195677008b4fcb56468e19234\r\n",
      "If it's helpful, I can create a Dockerfile that reliably reproduces this crash.",
      "#14696 would fix this - give a try\r\nif yes then completing the PR would be great",
      "So that patch fixes the segfault, but the original issue still remains: pandas thinks it is OOM (or is trying to allocate a ridiculously large buffer, I can't tell which yet) when the process is using a reasonable (< 3GB) amount of memory and the machine has 64 GB available. So the issue still remains, it just no longer segfaults while cleaning up the issue (which is all that #14696 fixed).\r\n\r\nI can provide you the logs when run in debug mode and compiled with -DVERBOSE if you'd like. I can also provide a Dockerfile to create an env which reproduces this reliably.",
      "you can post the traceback. someone would need to debug this and figure out what is causing the problem, then make a smaller repro",
      "The traceback won't be useful, since without the patch it's just showing the segfault the patch addresses, while with the patch there is no crash (an Exception is raised in the tokenizer).\r\n\r\nI'm not sure how/why you would expect someone to debug this and figure out the problem and _then_ make a \"smaller repro\" (assuming you're talking about test data size). Once they've figured out the problem (ostensibly using the existing bz2 archive, which is only ~130 MB), couldn't they then just fix it? Why would they need to also reduce the size of the data to reproduce it? \r\n\r\nRegardless, I'm willing to work with anyone who's willing to help solve this issue. I'll try to resolve it myself if I have time, but I was hoping it might be something easy.",
      "> I'm not sure how/why you would expect someone to debug this and figure out the problem and then make a \"smaller repro\" (assuming you're talking about test data size). Once they've figured out the problem (ostensibly using the existing bz2 archive, which is only ~130 MB), \r\n\r\nwell in order to have it part of the test suite we need a repro example that is smalllish, doesn't rely open a remote link, and has minimal deps (iow doesn't use docker)\r\n\r\nvirtually any non trivial change has a test - sure on occasion we don't explicitly add one for something hard to repro - but we actually go pretty far on testing",
      "Ah, OK, you were referring to testing. As this is somewhat related to an issue in Apache Arrow,\r\n [ARROW-1167](https://issues.apache.org/jira/browse/ARROW-1167) (this CSV fails in Pandas as described, but I can get around that by specifying `dtype=<column-types>` and removing `low_memory=False`), which @wesm has been looking at, I'm guessing it's going to turn out to be a similar underlying issue. It may just requiring trying to load a \"CSV\" (really a `StringIO` buffer or a generator that yields random values of a given type and implements the `file` interface or something) that is sufficiently large enough to trigger this issue.\r\n\r\nWes saw that one of the 40+ columns accounted for 2+GB of the 3GB file. I'm guessing there is buffer (re)allocation going on somewhere that eventually tries to allocate a ridiculous sized buffer, or something to that effect.\r\n\r\nI'll take some time now to look at it and see if I can't nail down where the issue is.",
      "If this is a problem that only occurs with large datasets then coming up with a minimal reproduction could be tricky. If you need some help debugging this let me know and I can try to help",
      "There are two bugs involved. The first is rather trivial: the `error_msg` pointer in `tokenizer.c` begins uninitialized and must be `malloc()`d any time you want to fill in an error message. It looks like someone caught this problem a while ago but requiring individual allocations at the point of use rather than just pre-allocating a fixed size buffer seems brittle. Of course, if someone forgets to pre-allocate, you'll get an illegal free when the tokenizer is being cleaned up, which is exactly what happened.\r\n\r\nTwo simple fixes for this: either just add the missing allocations, or pre-allocate a fixed size buffer that will hold an `error_msg`. The latter of course leaves open the possibility that the error message being set is larger than the buffer, but in practice these are all set via `snprintf` so it shouldn't actually be an issue.\r\n\r\nI'll describe the \"real\" issue in the next comment but would like to know how you'd like me to solve this first problem.",
      "The issue that triggers the bug above occurs when reading in a large CSV with `from_csv(..., low_memory=False)`. The underlying buffer used during tokenization is resized based on the size of the stream, but if the stream is large enough this will cause an overflow in the integer keeping track of the capacity, causing it to go negative and trigger its out-of-memory error (the issue above manifests itself because one of the `error_msg`s missing an allocation was \"out of memory\").\r\n\r\nDoes it make sense to change this to a 64-bit integer? I think this will only occur more and more frequently given how cheap RAM is, and ideally, the maximum size of the buffer matches the underlying machine's architecture, where I'm guessing it's far more common to be running x86_64 rather than i386. The other possibliity is to guard against such an overflow and just error out if it _would_ happen, but that limits the amount of RAM one can use to parse very large CSVs\r\n\r\n@wesm @jreback Any input here would be appreciated. I'm happy to fix the issues and write the tests. Just want to know which approach (or another one altogether) you'd like to take.",
      "(And, just to clarify, I propose switching everything to `size_t` rather than `int` if that wasn't clear, so not strictly a 64-bit integer necessarily)",
      "I recommend we use `int64_t` since `size_t` is a platform-specific type",
      "this is also noted in https://github.com/pandas-dev/pandas/issues/14131 where we came to the same conclusion. so @jeffknupp I would\r\n\r\n- fix the error message issue by pre-allocation\r\n- ``int`` -> ``int64_t``\r\n\r\ncan close both issue by this.",
      "I have a patch for this that I'll be sending a PR for in momentarily. @wesm: Re `int64_t` vs `size_t`, isn't the platform-specificity a positive property? If I'm on a 32-bit machine, I wouldn't want pandas assuming it can create byte arrays with more elements than I can address (in that case the \"out of memory\" message would be triggered as expected and, after this patch, just not core dump)? ",
      "> Re int64_t vs size_t, isn't the platform-specificity a positive property?\r\n\r\nIn my experience it's better to stick with platform independent types exclusively if you can -- see Google's C++ guidelines for some discussion of this https://google.github.io/styleguide/cppguide.html#Integer_Types \r\n\r\n> For integers we know can be \"big\" [Wes: on some platform], use int64_t\r\n\r\nI find it reduces the amount of platform specific code that leaks into your application. If you need to convert from `int64_t` to `int32_t`, then you should static cast, but it may be unclear when you need to insert an explicit cast when you convert from a platform specific type to a platform agnostic type or vice versa.",
      "@wesm Yes, but from the Google style guide (see last clause): \r\n\r\n> <stdint.h> defines types like int16_t, uint32_t, int64_t, etc. You should always use those in preference to short, unsigned long long and the like, when you need a guarantee on the size of an integer. Of the C integer types, only int should be used. When appropriate, you are welcome to use standard types like size_t and ptrdiff_t.\r\n\r\nI would say this is one case where the use of a standard type whose sole purpose is to accurately reflect the size of a container seems reasonable but will defer to you if you disagree.",
      "Created PR #17040. I stuck with `size_t` based on the comment above and didn't want to delay pushing the PR before getting a final verdict (can be easily changed if necessary)."
    ],
    "events": [
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 7,
    "additions": 187,
    "deletions": 139,
    "changed_files_list": [
      "doc/source/whatsnew/v0.21.0.txt",
      "pandas/_libs/parsers.pyx",
      "pandas/_libs/src/parser/tokenizer.c",
      "pandas/_libs/src/parser/tokenizer.h",
      "pandas/conftest.py",
      "pandas/tests/io/parser/test_parsers.py",
      "setup.cfg"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 16842,
    "reporter": "mcakes",
    "created_at": "2017-07-06T19:27:11+00:00",
    "closed_at": "2017-08-24T12:59:15+00:00",
    "resolver": "step4me",
    "resolved_in": "96f92eb1c696723b6465fdc273dc8406201c606a",
    "resolver_commit_num": 0,
    "title": "Cannot use tz-aware origin in to_datetime()",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\nAttempting to use a timezone aware origin argument in `pandas.to_datetime()` causes an error due to the line\r\n`offset = tslib.Timestamp(origin) - tslib.Timestamp(0)`\r\n\r\nShould this be allowed? ",
    "labels": [
      "Difficulty Novice",
      "Effort Low",
      "Error Reporting",
      "Timezones"
    ],
    "comments": [
      "``to_datetime`` doesn't explicity handle timezones, rather it gives you back naive datetimes (ex the ``utc=`` kwarg).\r\n\r\n Origin is *only* useful when specifying ``unit=``, so you can't pass anything but ints/floats. I would just localize after.\r\n\r\nHowever a more explicit error message would be good. IOW we will require that the origin (if its a Timestamp is naive).",
      "a PR would be welcome!",
      "I would like to take this up if no one else is working on it!"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 15,
    "deletions": 3,
    "changed_files_list": [
      "doc/source/whatsnew/v0.21.0.txt",
      "pandas/core/tools/datetimes.py",
      "pandas/tests/indexes/datetimes/test_tools.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 16900,
    "reporter": "dwillmer",
    "created_at": "2017-07-12T23:50:17+00:00",
    "closed_at": "2017-07-21T10:51:46+00:00",
    "resolver": "dwillmer",
    "resolved_in": "8f309db542c893b46e7cc6cff72638f68f5a855b",
    "resolver_commit_num": 1,
    "title": "BUG: pd.merge throws TypeError with categoricals",
    "body": "#### Code Sample\r\n\r\n\r\n#### Problem description\r\n\r\nIf you run the example above, you will get the following output:\r\n\r\n\r\n\r\nThis occurs when all of the following are true:\r\n\r\n- Both columns to merge on are categorical dates\r\n- The categoricals have the same dtype, but the values are different\r\n- The merge is 'outer'\r\n\r\nIf you change the merge to 'inner', or change the date values to be the same, then the code works as expected.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\nIn [3]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB.UTF-8\r\nLOCALE: en_GB.UTF-8\r\n\r\npandas: 0.20.2\r\npytest: 3.1.2\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\nxarray: None\r\nIPython: 6.0.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Categorical",
      "Difficulty Intermediate",
      "Effort Medium",
      "Reshaping",
      "Timeseries"
    ],
    "comments": [
      "looks like a bit of a bug in there. This is doing very odd things on the coercion. welcome to have you take a look.",
      "This is also incorrect with inner. Incorrect coercion on the datetimes.\r\n\r\n```\r\nIn [12]: df = pd.DataFrame(\r\n    ...:     [[date(2001, 1, 1), 1.1],\r\n    ...:      [date(2001, 1, 2), 1.3]],\r\n    ...:     columns=['date', 'num2']\r\n    ...: )\r\n    ...: df['date'] = df['date'].astype('category')\r\n    ...: \r\n    ...: df2 = pd.DataFrame(\r\n    ...:     [[date(2001, 1, 1), 1.3],\r\n    ...:      [date(2001, 1, 3), 1.4]],\r\n    ...:     columns=['date', 'num4']\r\n    ...: )\r\n    ...: df2['date'] = df2['date'].astype('category')\r\n    ...: \r\n    ...: result = pd.merge(\r\n    ...:     df, df2, how='inner', on=['date']\r\n    ...: )\r\n    ...: \r\n    ...: print(result)\r\n    ...: \r\n                 date  num2  num4\r\n0  978307200000000000   1.1   1.3\r\n```"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 3,
    "additions": 55,
    "deletions": 9,
    "changed_files_list": [
      "doc/source/whatsnew/v0.21.0.txt",
      "pandas/core/reshape/merge.py",
      "pandas/tests/reshape/test_merge.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 17013,
    "reporter": "toobaz",
    "created_at": "2017-07-18T17:27:29+00:00",
    "closed_at": "2017-07-26T23:50:47+00:00",
    "resolver": "toobaz",
    "resolved_in": "e3b784068a654d13ede6dd4062c8a2b6c9b945c5",
    "resolver_commit_num": 32,
    "title": "uninteded cast to float in pivot_table with margins=True",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nThere is no reason why adding the totals row and column should cast to float.\r\n\r\n#### Expected Output\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 9e7666dae3b3b10d987ce154a51c78bcee6e0728\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-3-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.UTF-8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.21.0.dev+265.g9e7666dae\r\npytest: 3.0.6\r\npip: 9.0.1\r\nsetuptools: None\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\nxarray: None\r\nIPython: 5.1.0.dev\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.2\r\nopenpyxl: None\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.6\r\nlxml: None\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Reshaping"
    ],
    "comments": [
      "@toobaz : Yep, that's a bug alright.  PR is welcome!"
    ],
    "events": [
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "milestoned",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 59,
    "deletions": 14,
    "changed_files_list": [
      "doc/source/whatsnew/v0.21.0.txt",
      "pandas/core/reshape/pivot.py",
      "pandas/tests/reshape/test_pivot.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 17228,
    "reporter": "mattip",
    "created_at": "2017-08-11T12:53:41+00:00",
    "closed_at": "2017-08-15T10:32:33+00:00",
    "resolver": "mattip",
    "resolved_in": "924b43359b2450cd1e6e364c468c30ee7694f0c1",
    "resolver_commit_num": 5,
    "title": "COMPAT: avoid getsizeof on PyPy",
    "body": "Pandas uses ``sys.getsizeof()`` to implement ``memory_usage()``. PyPy does not implement this functionality by design, since memory size is not fixed, depending on the JIT and other optimizations the memory usage of an object can change.\r\n\r\nI would like to suggest one possible solution: using the default in sys.getsizeof(obj, default). If this is unacceptable, I could suggest that using ``deep=True`` would raise a ``TypeErrpr`` instead, but that would be more complicated. Any opinions? (proof-of-concept pull request forthcoming)",
    "labels": [
      "Compat"
    ],
    "comments": [],
    "events": [
      "cross-referenced",
      "labeled",
      "milestoned"
    ],
    "changed_files": 9,
    "additions": 76,
    "deletions": 33,
    "changed_files_list": [
      "doc/source/whatsnew/v0.21.0.txt",
      "pandas/compat/__init__.py",
      "pandas/core/base.py",
      "pandas/core/indexes/multi.py",
      "pandas/core/indexes/range.py",
      "pandas/tests/frame/test_repr_info.py",
      "pandas/tests/test_base.py",
      "pandas/tests/test_categorical.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 17276,
    "reporter": "jreback",
    "created_at": "2017-08-18T00:38:07+00:00",
    "closed_at": "2017-08-21T23:53:51+00:00",
    "resolver": "mgasvoda",
    "resolved_in": "910207ffe518413e84cfa95d772cb66d57a0d08e",
    "resolver_commit_num": 0,
    "title": "BUG: clip should handle null values",
    "body": "We need treat a ``np.nan`` as a clip arg as ``None`` (which means don't clip on that side).\r\nThis makes tihngs more user friendly with no loss of generality.\r\n\r\n\r\n\r\nthis is easily fixed by inside ``.clip()`` by:\r\n\r\n",
    "labels": [
      "Bug",
      "Difficulty Novice",
      "Effort Low",
      "Missing-data"
    ],
    "comments": [
      "cc @cpcloud ",
      "I would like to help out with this as a first-time contributor if that's alright",
      "@mgasvoda Alright and welcomed!  You can read the documentation here:\r\n\r\nhttps://pandas.pydata.org/pandas-docs/stable/contributing.html\r\n\r\nregarding contributing to `pandas`.  Let us know if you have any questions!",
      "Great, thanks for the help!"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files": 4,
    "additions": 30,
    "deletions": 21,
    "changed_files_list": [
      "doc/source/whatsnew/v0.21.0.txt",
      "pandas/core/generic.py",
      "pandas/tests/frame/test_analytics.py",
      "pandas/tests/series/test_analytics.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 17602,
    "reporter": "xvwei1989",
    "created_at": "2017-09-20T06:15:00+00:00",
    "closed_at": "2017-09-20T10:09:56+00:00",
    "resolver": "jreback",
    "resolved_in": "6b0c7e72b141831b7a9a5651f9e19eef53ec9e76",
    "resolver_commit_num": 4622,
    "title": "BUG: different apply function behavior when columns with type Timestamp present",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\nwhen the return type of apply function is dict, if a new column with type Timestamp is added to the dataframe, the result will be unexpected even if the apply function is unchanged\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\ncommit: None\r\npython: 2.7.13.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 14.5.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: zh_CN.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.20.3\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 36.0.1\r\nCython: 0.25.2\r\nnumpy: 1.13.1\r\nscipy: 0.19.1\r\nxarray: None\r\nIPython: 5.4.1\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: None\r\nxlrd: 1.0.0\r\nxlwt: None\r\nxlsxwriter: 0.9.6\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.1.14\r\npymysql: 0.7.11.None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [],
    "comments": [
      "duplicate of #16353 and #15628 \r\n\r\n``.apply`` infers the output dimension based on what you are returning, which looks exactly like a Series. This is not idiomatic pandas, not to mention non-performant.\r\n"
    ],
    "events": [],
    "changed_files": 9,
    "additions": 885,
    "deletions": 192,
    "changed_files_list": [
      "doc/source/basics.rst",
      "doc/source/whatsnew/v0.23.0.txt",
      "pandas/core/apply.py",
      "pandas/core/frame.py",
      "pandas/core/sparse/frame.py",
      "pandas/io/formats/style.py",
      "pandas/tests/frame/test_apply.py",
      "pandas/tests/sparse/frame/test_apply.py",
      "pandas/tests/sparse/frame/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 17610,
    "reporter": "gloryfromca",
    "created_at": "2017-09-21T04:59:20+00:00",
    "closed_at": "2017-12-21T15:28:51+00:00",
    "resolver": "gloryfromca",
    "resolved_in": "7a1b0eead6c0314875aa5d3a9f6d1976165a635a",
    "resolver_commit_num": 0,
    "title": "BUG: duplicate indexing with embedded non-orderables",
    "body": "#### code below is scene that issue happened. \r\n\r\n#### Problem description\r\n#### when I wanted to get a set from a Series , it happended.\r\n#### it has worked well for a long time,but suddenly it broke out. raise Traceback like this: \r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\r\n    self.run()\r\n  File \"/home/datascience/zh/suijiesuihuan/datascience/my_threads.py\", line 87, in run\r\n    ret_data, result = make_decision(int(self.layerId), df_input)\r\n  File \"/home/datascience/zh/suijiesuihuan/datascience/layered_decision.py\", line 52, in make_decision\r\n    data_df, result_df = load_features_group_loader(features_group_loader_id, mocking)(data_df)\r\n  File \"/home/datascience/zh/suijiesuihuan/datascience/dynamic_content_loader.py\", line 166, in inner\r\n    return load_and_make_decision(data_df, config_new)\r\n  File \"/home/datascience/zh/suijiesuihuan/datascience/dynamic_content_loader.py\", line 111, in load_and_make_decision\r\n    new_feature_group_df, error_df = _load_data(pending_df, features_loader_id, features_loader, mocking)\r\n  File \"/home/datascience/zh/suijiesuihuan/datascience/dynamic_content_loader.py\", line 76, in _load_data\r\n    data_df, result_df = features_loader(data_df)\r\n  File \"/home/datascience/zh/suijiesuihuan/dynamic_contents/feature_group_loader/feature_group_loader_0003/feature_group_loader.py\", line 55, in load\r\n    call_number_list = row.get(Names.call.call_number_list)\r\n  File \"/home/datascience/zh/venv/lib/python3.5/site-packages/pandas/core/generic.py\", line 1633, in get\r\n    return self[key]\r\n  File \"/home/datascience/zh/venv/lib/python3.5/site-packages/pandas/core/series.py\", line 611, in __getitem__\r\n    dtype=self.dtype).__finalize__(self)\r\n  File \"/home/datascience/zh/venv/lib/python3.5/site-packages/pandas/core/series.py\", line 227, in __init__\r\n    \"\".format(data.__class__.__name__))\r\nTypeError: 'set' type is unordered\r\n \r\n#### someone knows what happened?\r\n",
    "labels": [
      "Bug",
      "Difficulty Intermediate",
      "Effort Medium",
      "Indexing"
    ],
    "comments": [
      "First, it is not clear at all that this is a bug.  A bug report typically contains a short, runnable example of the problem, and a description of why the observed behavior is wrong.\r\n\r\nI tried to reproduce this with pandas 0.19 but could not.\r\n```\r\nIn [1]: import pandas\r\n\r\nIn [2]: s = pandas.Series({'a': 0, 'b': 1})\r\n\r\nIn [3]: s.get(['b', 'a'])\r\nOut[3]:\r\nb    1\r\na    0\r\ndtype: int64\r\n\r\nIn [4]: s.get({'b', 'a'})  # note set\r\nOut[4]:\r\na    0\r\nb    1\r\ndtype: int64\r\n```\r\n\r\nCould you try calling `get` with a list argument, as in this?\r\n```\r\n        call_number_list = row.get(list(Names.call.call_number_list))\r\n```\r\n",
      "@gloryfromca : Also, could you provide version information (`pandas.show_versions`)?",
      "you can try codes below to reproduce the problem:\n\n>>> from pandas import Series\n>>> Series_contain_duplicates = Series({'1':333,'s':set([1,2,3])})\n>>> Series_contain_duplicates =\nSeries_contain_duplicates.append(Series({'1':2}))\n>>> Series_contain_duplicates['s']\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File\n\"/home/zhanghui/py35/lib/python3.5/site-packages/pandas/core/series.py\",\nline 611, in __getitem__\n    dtype=self.dtype).__finalize__(self)\n  File\n\"/home/zhanghui/py35/lib/python3.5/site-packages/pandas/core/series.py\",\nline 227, in __init__\n    \"\".format(data.__class__.__name__))\nTypeError: 'set' type is unordered\n\nIt's not normal behaviour, right? I'm very confused at this TypeError at\nfirst sight.\n\nmy colleague may find how the problems produce:\ncode in row608:if not self.index.is_unique:\n\nMaybe index.is_index is a wrong determination conditions ?\n\nThank you!\n\n\n\n\n2017-09-21 22:13 GMT+08:00 Nicholas Musolino <notifications@github.com>:\n\n> First, it is not clear at all that this is a bug. A bug report typically\n> contains a short, runnable example of the problem, and a description of why\n> the observed behavior is wrong.\n>\n> I tried to reproduce this with pandas 0.19 but could not.\n>\n> In [1]: import pandas\n>\n> In [2]: s = pandas.Series({'a': 0, 'b': 1})\n>\n> In [3]: s.get(['b', 'a'])\n> Out[3]:\n> b    1\n> a    0\n> dtype: int64\n>\n> In [4]: s.get({'b', 'a'})  # note set\n> Out[4]:\n> a    0\n> b    1\n> dtype: int64\n>\n> Could you try calling get with a list argument, as in this?\n>\n>         call_number_list = row.get(list(Names.call.call_number_list))\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pandas-dev/pandas/issues/17610#issuecomment-331168932>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWW152DsbehNPwYLz-khP8MPuCd3QXQlks5skm7rgaJpZM4PezI0>\n> .\n>\n",
      "Both 0.19 and 0.20 has this problems.\n\n2017-09-22 2:23 GMT+08:00 gfyoung <notifications@github.com>:\n\n> @gloryfromca <https://github.com/gloryfromca> : Also, could you provide\n> version information?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pandas-dev/pandas/issues/17610#issuecomment-331240875>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWW151q9mN8LjbWT8kHSnP4-w8NA1xqEks5skqmvgaJpZM4PezI0>\n> .\n>\n",
      "I guess this is a bug, though I am not sure why you would ever do this. embedding non-scalars (e.g. a ``set``) is non-idiomatic. Using duplicate indices requires care as well.\r\n\r\nI'll mark it, but it would require a community pull request to fix.\r\n\r\n```\r\nIn [2]: s = Series({'1':333,'s':set([1,2,3])})\r\n\r\nIn [3]: s\r\nOut[3]: \r\n1          333\r\ns    {1, 2, 3}\r\ndtype: object\r\n\r\nIn [13]: s2 = s.append(Series({'1':2}))\r\n\r\nIn [14]: s2\r\nOut[14]: \r\n1          333\r\ns    {1, 2, 3}\r\n1            2\r\ndtype: object\r\n\r\nIn [15]: s2[1]\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-15-c2e7db717c6c> in <module>()\r\n----> 1 s2[1]\r\n\r\n~/pandas/pandas/core/series.py in __getitem__(self, key)\r\n    629                         result = self._constructor(\r\n    630                             result, index=[key] * len(result),\r\n--> 631                             dtype=self.dtype).__finalize__(self)\r\n    632 \r\n    633             return result\r\n\r\n~/pandas/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath)\r\n    239             elif isinstance(data, (set, frozenset)):\r\n    240                 raise TypeError(\"{0!r} type is unordered\"\r\n--> 241                                 \"\".format(data.__class__.__name__))\r\n    242             else:\r\n    243 \r\n\r\nTypeError: 'set' type is unordered\r\n```",
      "This problem is mixture of coincidence and bad practice. I am beginner of\nGitHub, so What does a community pull request means? Will you fix it or\nShould I create a pull request for it ?\n\n2017-10-10 7:17 GMT+08:00 Jeff Reback <notifications@github.com>:\n\n> I guess this is a bug, though I am not sure why you would ever do this.\n> embedding non-scalars (e.g. a set) is non-idiomatic. Using duplicate\n> indices requires care as well.\n>\n> I'll mark it, but it would require a community pull request to fix.\n>\n> In [2]: s = Series({'1':333,'s':set([1,2,3])})\n>\n> In [3]: s\n> Out[3]:\n> 1          333\n> s    {1, 2, 3}\n> dtype: object\n>\n> In [13]: s2 = s.append(Series({'1':2}))\n>\n> In [14]: s2\n> Out[14]:\n> 1          333\n> s    {1, 2, 3}\n> 1            2\n> dtype: object\n>\n> In [15]: s2[1]\n> ---------------------------------------------------------------------------\n> TypeError                                 Traceback (most recent call last)\n> <ipython-input-15-c2e7db717c6c> in <module>()\n> ----> 1 s2[1]\n>\n> ~/pandas/pandas/core/series.py in __getitem__(self, key)\n>     629                         result = self._constructor(\n>     630                             result, index=[key] * len(result),\n> --> 631                             dtype=self.dtype).__finalize__(self)\n>     632\n>     633             return result\n>\n> ~/pandas/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath)\n>     239             elif isinstance(data, (set, frozenset)):\n>     240                 raise TypeError(\"{0!r} type is unordered\"\n> --> 241                                 \"\".format(data.__class__.__name__))\n>     242             else:\n>     243\n>\n> TypeError: 'set' type is unordered\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pandas-dev/pandas/issues/17610#issuecomment-335314967>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWW154tj1jfoiGHbNe-dK2mGMRCoIF46ks5sqqlvgaJpZM4PezI0>\n> .\n>\n",
      "> Should I create a pull request for it ?\r\n\r\nsure, docs are: http://pandas.pydata.org/pandas-docs/stable/contributing.html",
      "I fixed the bug, and ran asv benchmarks.Then It raised information as below:\nSOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.\nIs it OK? or the way I fixed the bug is unproper?\nPS:attachment is photo of benchmarks detail information.\n\n2017-10-10 11:19 GMT+08:00 Jeff Reback <notifications@github.com>:\n\n> Should I create a pull request for it ?\n>\n> sure, docs are: http://pandas.pydata.org/pandas-docs/stable/\n> contributing.html\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pandas-dev/pandas/issues/17610#issuecomment-335348548>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWW1562eKtoronGip1xvvx3hfa-HXacfks5squIzgaJpZM4PezI0>\n> .\n>\n"
    ],
    "events": [
      "commented",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "unlabeled",
      "milestoned",
      "renamed",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "demilestoned",
      "milestoned",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 30,
    "deletions": 6,
    "changed_files_list": [
      "doc/source/whatsnew/v0.22.0.txt",
      "pandas/core/series.py",
      "pandas/tests/series/test_indexing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 17813,
    "reporter": "tdpetrou",
    "created_at": "2017-10-07T15:31:07+00:00",
    "closed_at": "2018-02-05T11:13:45+00:00",
    "resolver": "discort",
    "resolved_in": "5b58a20504aeb3efe8858164377edc0e4f02ae02",
    "resolver_commit_num": 4,
    "title": "BUG: groupby with resample using on parameter errors when selecting column to apply function",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nUsing a `groupby` followed by `resample` with the datetime not in the index forces the use of the `on` parameter. Unfortunately, this produces a key error whenever selecting a column to apply a function to.\r\n\r\n#### Expected Output\r\nSame as second one from above.\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.20.3\r\npytest: 3.0.7\r\npip: 9.0.1\r\nsetuptools: 35.0.2\r\nCython: 0.25.2\r\nnumpy: 1.13.1\r\nscipy: 0.19.0\r\nxarray: None\r\nIPython: 6.0.0\r\nsphinx: 1.5.5\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.4.2\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: 2.4.7\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 0.9.6\r\nlxml: 3.7.3\r\nbs4: 4.6.0\r\nhtml5lib: 0.9999999\r\nsqlalchemy: 1.1.9\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: 0.3.0.post\r\n\r\n</details>\r\n",
    "labels": [
      "Difficulty Intermediate",
      "Effort Medium",
      "Groupby",
      "MultiIndex",
      "Resample",
      "Bug"
    ],
    "comments": [
      "similar to https://github.com/pandas-dev/pandas/issues/15072 & #16766 .\r\n\r\na pull request is welcome to fix."
    ],
    "events": [
      "renamed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "labeled",
      "renamed",
      "cross-referenced",
      "demilestoned",
      "milestoned"
    ],
    "changed_files": 3,
    "additions": 30,
    "deletions": 5,
    "changed_files_list": [
      "doc/source/whatsnew/v0.23.0.txt",
      "pandas/core/groupby.py",
      "pandas/tests/test_resample.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 17892,
    "reporter": "ghost",
    "created_at": "2017-10-16T10:48:57+00:00",
    "closed_at": "2017-10-16T11:25:01+00:00",
    "resolver": "jreback",
    "resolved_in": "6b0c7e72b141831b7a9a5651f9e19eef53ec9e76",
    "resolver_commit_num": 4622,
    "title": "ValueError when applying a function that returns a list or tuple to a DataFrame that contains a Timestamp",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\nExecuting\r\n\r\nraises an exception\r\n\r\n\r\n#### Problem description\r\n\r\n* I see the same problem when ``fun`` returns a list (e.g. ``[1,2]``) rather than tuple.\r\n* The error does not occur when apply is called with ``axis=0``.\r\n* The error does not occur when I replace the Timestamp column with a column of integers.\r\n\r\n#### Expected Output\r\n\r\nA pandas Series containing tuples:\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.7.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB.UTF-8\r\nLOCALE: en_GB.UTF-8\r\n\r\npandas: 0.20.3\r\npytest: 3.0.7\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.2\r\nnumpy: 1.13.3\r\nscipy: 0.19.0\r\nxarray: None\r\nIPython: 5.3.0\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.3.0\r\nnumexpr: 2.6.2\r\nfeather: 0.4.0\r\nmatplotlib: 2.0.2\r\nopenpyxl: 2.4.7\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 0.9.6\r\nlxml: 3.7.3\r\nbs4: 4.6.0\r\nhtml5lib: 0.9999999\r\nsqlalchemy: 1.1.9\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n",
    "labels": [],
    "comments": [
      "duplicate of #16353 and #15628\r\n\r\n.apply infers the output dimension based on what you are returning, which looks exactly like a Series. This is not idiomatic pandas, not to mention non-performant."
    ],
    "events": [
      "commented"
    ],
    "changed_files": 9,
    "additions": 885,
    "deletions": 192,
    "changed_files_list": [
      "doc/source/basics.rst",
      "doc/source/whatsnew/v0.23.0.txt",
      "pandas/core/apply.py",
      "pandas/core/frame.py",
      "pandas/core/sparse/frame.py",
      "pandas/io/formats/style.py",
      "pandas/tests/frame/test_apply.py",
      "pandas/tests/sparse/frame/test_apply.py",
      "pandas/tests/sparse/frame/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 17970,
    "reporter": "tdpetrou",
    "created_at": "2017-10-25T01:59:09+00:00",
    "closed_at": "2017-11-10T20:12:41+00:00",
    "resolver": "jreback",
    "resolved_in": "6b0c7e72b141831b7a9a5651f9e19eef53ec9e76",
    "resolver_commit_num": 4622,
    "title": "DataFrame.apply with axis=1 returning (also erroring) different results when returning a list",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\nThere are three possible outcomes. When the length of the returned list is equal to the number of columns then a DataFrame is returned and each column gets the corresponding value in the list.\r\n\r\nIf the length of the returned list is not equal to the number of columns, then a Series of lists is returned.\r\n\r\nIf the length of the returned list equals the number of columns for the first row but has at least one row where the list has a different number of elements than number of columns a ValueError is raised.\r\n\r\n\r\n#### Expected Output\r\nNeed consistency. Probably should default to a Series of lists for all examples.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.21.0rc1\r\npytest: 3.0.7\r\npip: 9.0.1\r\nsetuptools: 35.0.2\r\nCython: 0.25.2\r\nnumpy: 1.13.3\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.0.0\r\nsphinx: 1.5.5\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.4.2\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.1.0\r\nopenpyxl: 2.4.7\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 0.9.6\r\nlxml: 3.7.3\r\nbs4: 4.6.0\r\nhtml5lib: 0.9999999\r\nsqlalchemy: 1.1.9\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: 0.5.0\r\n\r\n</details>\r\n",
    "labels": [],
    "comments": [
      "The problem is wider. I am running the same bug when running the following \r\n```\r\n>>> df = DataFrame({\"a\": [1, 2, 3]})\r\n>>> df.apply(lambda row: np.ones(1), axis=1)\r\n     a\r\n0  1.0\r\n1  1.0\r\n2  1.0\r\n>>> df.apply(lambda row: np.ones(2), axis=1)\r\nValueError: Shape of passed values is (3, 2), indices imply (3, 1)\r\n```\r\n\r\nRelated to https://github.com/pandas-dev/pandas/issues/17437 (where there are some comments from @jreback )",
      "this is a duplicate of #17437 & #15628."
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 9,
    "additions": 885,
    "deletions": 192,
    "changed_files_list": [
      "doc/source/basics.rst",
      "doc/source/whatsnew/v0.23.0.txt",
      "pandas/core/apply.py",
      "pandas/core/frame.py",
      "pandas/core/sparse/frame.py",
      "pandas/io/formats/style.py",
      "pandas/tests/frame/test_apply.py",
      "pandas/tests/sparse/frame/test_apply.py",
      "pandas/tests/sparse/frame/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 17979,
    "reporter": "toobaz",
    "created_at": "2017-10-25T13:06:10+00:00",
    "closed_at": "2017-11-01T11:26:21+00:00",
    "resolver": "GuessWhoSamFoo",
    "resolved_in": "1719437dbf0a24939e0ce7e46e0a5aa742bff773",
    "resolver_commit_num": 8,
    "title": "DataFrame.groupby() interprets tuple as list of keys",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\n``('b', 1)`` is a valid key and should be interpreted as such: instead, it is interpreted as ``['b', 1]``.\r\n\r\nThis is related to #17977 , but the fix should be pretty easy.\r\n\r\n#### Expected Output\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: b539298ca88512fd91cd9522dbd7766619dded55\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-3-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.UTF-8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.21.0rc1+30.gb539298ca\r\npytest: 3.0.6\r\npip: 9.0.1\r\nsetuptools: None\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 5.1.0.dev\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: None\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.6\r\nlxml: None\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\n\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Groupby",
      "MultiIndex"
    ],
    "comments": [],
    "events": [
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "milestoned"
    ],
    "changed_files": 3,
    "additions": 24,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.22.0.txt",
      "pandas/core/groupby.py",
      "pandas/tests/groupby/test_grouping.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 18113,
    "reporter": "jreback",
    "created_at": "2017-11-04T19:52:07+00:00",
    "closed_at": "2017-12-03T18:04:25+00:00",
    "resolver": "gkonefal-reef",
    "resolved_in": "a9e47312697a933e6af495a8682ce73717cf0ff8",
    "resolver_commit_num": 0,
    "title": "BLD: since we already use setuptools, let's remove the optional logic in setup.py",
    "body": "no need to have the branching in setup.py we always have setuptools available.",
    "labels": [
      "Build",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments": [
      "Besides cleaning up `setup.py` does something else should be done? Are there any caveats regarding `pandas` build process?",
      "no i think this all that is needed; we do sufficient testing on building in the CI",
      "@jreback Do you think we could have `Build` subsection in `whatsnew` docs for `BLD` related entries? Or do they fall under one of existing subsections?",
      "the try/except of `import setuptools` also includes `import pkg_resources`.  Is it also always the case that we always have pkg_resources?"
    ],
    "events": [
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 2,
    "additions": 15,
    "deletions": 42,
    "changed_files_list": [
      "doc/source/whatsnew/v0.22.0.txt",
      "setup.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 18447,
    "reporter": "kcajf",
    "created_at": "2017-11-23T11:18:19+00:00",
    "closed_at": "2018-05-13T13:17:50+00:00",
    "resolver": "paul-mannino",
    "resolved_in": "1dcddba2200b89cffe97ae7a32a34cdec3a7c8fb",
    "resolver_commit_num": 2,
    "title": "BUG: joining empty series with dtype: datetime64[ns, UTC]",
    "body": "#### Code Sample\r\n\r\n#### Problem description\r\n\r\nWhen trying to concatenate multiple series (or dataframes) along axis 1, if one of them is empty and has a **UTC** datetime column, the concatenation will fail with IndexError. This applies to joins as well. If you set the datetime column to be non-utc (i.e. tz-naive), it works as expected. If you concatenate 2 empty objects, one of which has a UTC datetime column, it works as expected.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.12.9-300.fc26.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB.utf8\r\nLOCALE: en_GB.UTF-8\r\n\r\npandas: 0.21.0\r\npytest: 3.0.7\r\npip: 9.0.1\r\nsetuptools: 36.6.0\r\nCython: None\r\nnumpy: 1.13.3\r\nscipy: 0.19.1\r\npyarrow: 0.7.1\r\nxarray: 0.9.6\r\nIPython: 6.2.1\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.4.2\r\nnumexpr: 2.6.4\r\nfeather: 0.4.0\r\nmatplotlib: 2.0.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 1.0b10\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: 0.4.0\r\n\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Difficulty Intermediate",
      "Effort Medium",
      "Reshaping",
      "Timezones"
    ],
    "comments": [
      "this is relateed to #12396\r\n\r\nshould be straightforward to fix, if you can do a PR!"
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "demilestoned",
      "milestoned",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files": 4,
    "additions": 167,
    "deletions": 6,
    "changed_files_list": [
      "doc/source/whatsnew/v0.23.0.txt",
      "pandas/core/indexes/base.py",
      "pandas/core/internals.py",
      "pandas/tests/reshape/test_concat.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 18478,
    "reporter": "nmartensen",
    "created_at": "2017-11-25T00:19:50+00:00",
    "closed_at": "2017-12-21T15:31:57+00:00",
    "resolver": "nmartensen",
    "resolved_in": "75b97a70110b0d924a557bd8e7929bcf1f4509a2",
    "resolver_commit_num": 1,
    "title": "TimeFormatter broken for sub-second resolution",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nThe TimeFormatter code has several issues. The first is here:\r\n\r\n-dev/pandas/blob/4fce7846be56e12999fe8758abb2ea2f2794259d/pandas/plotting/_converter.py#L113\r\n\r\n`ms` needs to be multiplied by 1e3, otherwise we'd be subtracting milliseconds from microseconds.\r\n\r\nThe next problem is here:\r\n\r\n-dev/pandas/blob/4fce7846be56e12999fe8758abb2ea2f2794259d/pandas/plotting/_converter.py#L118\r\n\r\nThe strftime format does not support the `%6f` notation. This needs to be `%f`, otherwise the result would be either a literal '%6f' in the output or a ValueError (invalid format string).\r\n\r\n-dev/pandas/blob/4fce7846be56e12999fe8758abb2ea2f2794259d/pandas/plotting/_converter.py#L120\r\n\r\nSame problems, plus: '%f' means microseconds for strftime(), `%3f` cannot turn this into milliseconds.\r\n\r\nAnd the last one:\r\n\r\n-dev/pandas/blob/4fce7846be56e12999fe8758abb2ea2f2794259d/pandas/plotting/_converter.py#L122\r\n\r\nThis timestamp is missing the milliseconds entirely. (Assuming the fix to the microsecond calculation mentioned above.)\r\n\r\n#### Expected Output\r\n\r\nCorrectly formatted time stamp strings as tick labels.\r\n\r\n#### Solution Approaches\r\n\r\n1. Drop the TimeFormatter entirely (after checking if MPL already copes with pydt.time())\r\n2. Drop any sub-second resolution support from the TimeFormatter, as it was broken for a long time and nobody complained.\r\n3. Make it work correctly in the way it was most likely intended to work.\r\n\r\nWhat do you think? I can set up a pull request for option 3 if that's the preferred solution.",
    "labels": [
      "Visualization"
    ],
    "comments": [
      "@jorisvandenbossche can comment, but ``.time()`` is not a first class object in pandas at all, you almost certainly want to use a ``Timedelta``\r\n\r\n```\r\ndf = pandas.DataFrame({'A': 1}, index=[pd.Timedelta(\"01:01:01.0001\"), pd.Timedelta(\"01:01:01.001\")])\r\ndf.plot()\r\n```\r\n\r\nworks just fine."
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "milestoned"
    ],
    "changed_files": 4,
    "additions": 77,
    "deletions": 20,
    "changed_files_list": [
      "doc/source/whatsnew/v0.22.0.txt",
      "pandas/plotting/_converter.py",
      "pandas/tests/plotting/test_converter.py",
      "pandas/tests/plotting/test_datetimelike.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 18573,
    "reporter": "meditativeape",
    "created_at": "2017-11-30T02:00:26+00:00",
    "closed_at": "2017-11-30T11:15:04+00:00",
    "resolver": "jreback",
    "resolved_in": "6b0c7e72b141831b7a9a5651f9e19eef53ec9e76",
    "resolver_commit_num": 4622,
    "title": "Cannot apply lambda function to DataFrame with columns of type numpy.datetime64",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nApplying lambda function to a DataFrame with columns of type numpy.datetime64 throws an unexpected error: `ValueError: Shape of passed values is (2, 2), indices imply (2, 3)`. The error message is not helpful, and I believe this error is thrown due to a bug. Dropping columns of type numpy.datetime64 fixes the issue.\r\n\r\n#### Expected Output\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.14.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.7.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.20.3\r\npytest: 3.2.1\r\npip: 9.0.1\r\nsetuptools: 36.5.0.post20170921\r\nCython: 0.26.1\r\nnumpy: 1.13.3\r\nscipy: 0.19.1\r\nxarray: None\r\nIPython: 5.4.1\r\nsphinx: 1.6.3\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.4.2\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.1.0\r\nopenpyxl: 2.4.8\r\nxlrd: 1.1.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 1.0.2\r\nlxml: 4.1.0\r\nbs4: 4.6.0\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.1.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n",
    "labels": [
      "Apply",
      "Duplicate"
    ],
    "comments": [
      "duplicate of #15628 \r\n\r\n``.apply`` has to guess the return shape from your opaque passed function. This is not always possible.\r\n\r\nidiomatically what you are doing is quite odd, simply do this.\r\n\r\n```\r\nIn [43]: df[['number', 'string']]\r\nOut[43]: \r\n   number string\r\n0     1.0   haha\r\n1     2.0   wawa\r\n```"
    ],
    "events": [
      "labeled",
      "labeled",
      "milestoned"
    ],
    "changed_files": 9,
    "additions": 885,
    "deletions": 192,
    "changed_files_list": [
      "doc/source/basics.rst",
      "doc/source/whatsnew/v0.23.0.txt",
      "pandas/core/apply.py",
      "pandas/core/frame.py",
      "pandas/core/sparse/frame.py",
      "pandas/io/formats/style.py",
      "pandas/tests/frame/test_apply.py",
      "pandas/tests/sparse/frame/test_apply.py",
      "pandas/tests/sparse/frame/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 18870,
    "reporter": "jreback",
    "created_at": "2017-12-20T13:37:36+00:00",
    "closed_at": "2017-12-29T19:03:00+00:00",
    "resolver": "jreback",
    "resolved_in": "ff865b464d6d0edf3dad6fc8a01c0e8f0a6528ca",
    "resolver_commit_num": 4590,
    "title": "TST: broken tests as data not installed",
    "body": "skipping these tests: -dev/pandas/pull/18873\r\n\r\nconda install is finally working. surfacing some failing tests because of data files not being installed.\r\n-ci.org/pandas-dev/pandas/jobs/319150721\r\n",
    "labels": [
      "Testing"
    ],
    "comments": [
      "I noticed that the fixture for the io tests does not make use of ``os.path.abspath`` whereas the ``tm.get_data_path`` function does. Perhaps using abspath on the former here could resolve?\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/f833103a1883672751b45d94d14a3641095df672/pandas/tests/io/conftest.py#L6\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/c753e1e08b01a438aaa252327de046109bf4bcfd/pandas/util/testing.py#L795",
      "thank @WillAyd I made that change. Though this is *still* failing, but only on master. Not really sure what the problem is, but moved this build out of the main matrix for now. Its not very easy to test because it *only* fails on master and not on a fork. very weird.",
      "closed via https://github.com/pandas-dev/pandas/pull/18990\r\n\r\nthis is a travis bug:  # https://github.com/travis-ci/travis-ci/issues/8920#issuecomment-352661024\r\n\r\nfixes curtesy of @matthew-brett \r\n"
    ],
    "events": [
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "closed",
      "reopened",
      "commented",
      "referenced",
      "cross-referenced",
      "closed",
      "commented",
      "mentioned",
      "subscribed",
      "reopened",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files": 3,
    "additions": 13,
    "deletions": 5,
    "changed_files_list": [
      ".travis.yml",
      "ci/install_travis.sh",
      "pandas/tests/io/test_common.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 18901,
    "reporter": "Casyfill",
    "created_at": "2017-12-21T22:00:06+00:00",
    "closed_at": "2017-12-22T15:50:47+00:00",
    "resolver": "jreback",
    "resolved_in": "6b0c7e72b141831b7a9a5651f9e19eef53ec9e76",
    "resolver_commit_num": 4622,
    "title": "df.apply ignores reduce=True, if function returns list",
    "body": " \r\n ( python 3.6, pandas 0.21.0 )\r\n\r\nThe behavior I want to achieve is to get a single column of lists.\r\nHowever, despite `reduce=True` this `apply` tries to map returned list as a series (?), raising the following:\r\n\r\n`ValueError: Shape of passed values is (10, 8), indices imply (10, 23)`\r\n(here, 10 is the size of the sample, 23 is the number of columns in the sample, 8 is the number of elements in the list that was returned for the first column?\r\n\r\nWhen I change `get_candidates_3` to return set instead, I get a full new dataframe of the same shape (10,23), filled with exactly the same sets row-wise, instead.\r\n\r\nI am pretty sure I was using the same pattern successfully with a previous version...\r\n\r\n",
    "labels": [],
    "comments": [
      "this is related to the other 10 issues duplicated and closed by #18577 "
    ],
    "events": [
      "cross-referenced"
    ],
    "changed_files": 9,
    "additions": 885,
    "deletions": 192,
    "changed_files_list": [
      "doc/source/basics.rst",
      "doc/source/whatsnew/v0.23.0.txt",
      "pandas/core/apply.py",
      "pandas/core/frame.py",
      "pandas/core/sparse/frame.py",
      "pandas/io/formats/style.py",
      "pandas/tests/frame/test_apply.py",
      "pandas/tests/sparse/frame/test_apply.py",
      "pandas/tests/sparse/frame/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 18919,
    "reporter": "gwerbin",
    "created_at": "2017-12-23T04:20:05+00:00",
    "closed_at": "2017-12-23T19:59:22+00:00",
    "resolver": "jreback",
    "resolved_in": "6b0c7e72b141831b7a9a5651f9e19eef53ec9e76",
    "resolver_commit_num": 4622,
    "title": "Unexpected interaction in DataFrame.apply(f) when `f` returns a list",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nWhen a `DataFrame` has a `MultiIndex`, and the function passed to `DataFrame.apply` returns all `list`s, weird stuff happens and an unintelligible error occurs.\r\n\r\nWhat ends up happening is that the result somehow gets coerced to a list of arrays (not sure where or why the list->array conversion happens), and then submitted to `DataFrame.__init__`, which tries to massage it that into a DataFrame, and fails.\r\n\r\nResulting error: `ValueError: Empty data passed with indices specified.`, emitted from deep within the bowels of `pandas/core/internals.py`, specifically  `create_block_manager_from_arrays`.\r\n\r\nThis happens regardless of what the `reduce=` argument is set to.\r\n\r\n#### Expected Output\r\n\r\nDon't try to manipulate the output. Return a `Series` of `list`s.\r\n\r\nIn the example above, that'd be:\r\n\r\n\r\n\r\n#### Output of `pd.show_versions()`\r\n\r\n\r\n",
    "labels": [],
    "comments": [
      "duplicate of many issues, se #18557 which will close."
    ],
    "events": [],
    "changed_files": 9,
    "additions": 885,
    "deletions": 192,
    "changed_files_list": [
      "doc/source/basics.rst",
      "doc/source/whatsnew/v0.23.0.txt",
      "pandas/core/apply.py",
      "pandas/core/frame.py",
      "pandas/core/sparse/frame.py",
      "pandas/io/formats/style.py",
      "pandas/tests/frame/test_apply.py",
      "pandas/tests/sparse/frame/test_apply.py",
      "pandas/tests/sparse/frame/test_frame.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 19368,
    "reporter": "quale1",
    "created_at": "2018-01-24T05:59:39+00:00",
    "closed_at": "2018-02-06T01:24:54+00:00",
    "resolver": "hexgnu",
    "resolved_in": "a01f74cf27314817acff6289f36b6eba9c49fb6c",
    "resolver_commit_num": 3,
    "title": "Python crashes when executing memory_usage(deep=True) on a sparse series",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nExecuting the `memory_usage(deep=True)` method on a sparse series crashes Python. (With `deep=False` the method works as expected.)\r\n#### Expected Output\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.3.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.22.0\r\npytest: 3.2.1\r\npip: 9.0.1\r\nsetuptools: 36.5.0.post20170921\r\nCython: 0.26.1\r\nnumpy: 1.13.3\r\nscipy: 0.19.1\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.1.0\r\nsphinx: 1.6.3\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.1\r\ntables: 3.4.2\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.1.0\r\nopenpyxl: 2.4.8\r\nxlrd: 1.1.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 1.0.2\r\nlxml: 4.1.0\r\nbs4: 4.6.0\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.1.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n</details>\r\n",
    "labels": [
      "Bug",
      "Sparse",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "sparse is not fully test covered. a pull request to fix is welcome!",
      "I hooked up gdb and tracked down the issue to inside of the lib.pyx which assumes that series's are of length > 0. I made a PR that _should_ fix it, though we'll see how the tests chooch."
    ],
    "events": [
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "demilestoned",
      "milestoned"
    ],
    "changed_files": 4,
    "additions": 29,
    "deletions": 4,
    "changed_files_list": [
      "doc/source/whatsnew/v0.23.0.txt",
      "pandas/core/base.py",
      "pandas/core/sparse/array.py",
      "pandas/tests/sparse/series/test_series.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 19526,
    "reporter": "jbandlow",
    "created_at": "2018-02-03T18:13:25+00:00",
    "closed_at": "2018-02-06T23:49:35+00:00",
    "resolver": "jbandlow",
    "resolved_in": "983d71fa8477439aaa227367d7f2f14952e4e235",
    "resolver_commit_num": 0,
    "title": "Rounding errors with Timestamps and `.last()`",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\nThe value of the Timestamp in the output is not equal to the value in the input. \r\n\r\n#### Expected Output\r\n\r\nThe issue is a bit subtle. Here are three related examples, all of which **do** work as expected:\r\n\r\nI haven't been able to repro with `.nth(-1)` in place of `last()`:\r\n\r\n\r\n\r\nI haven't been able to repro if `NaT` is not present:\r\n\r\n\r\nI haven't been able to repro for times which are integer seconds:\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n\r\n</details>\r\n",
    "labels": [
      "Groupby",
      "Dtypes",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [
      "Can you reproduce without the groupby step?",
      "I can't repro without groupby.  I did a little more experimentation, and it appears that the set of aggregation functions that have the issue are exactly [these](https://github.com/pandas-dev/pandas/blob/v0.22.0/pandas/core/groupby.py#L1290-L1295) (`max`, `min`, `first`, `last`).",
      "this should not be going thru a float round trip which is the symptom you are seeing\r\nif you want to have a more detailed look and see where the issue is",
      "Thanks for the hint @jreback . The offending block seems to be [here](https://github.com/pandas-dev/pandas/blob/v0.22.0/pandas/core/groupby.py#L2279-L2283) where the integer result is getting cast as float.  The coercion back to a datetime happens [in here](https://github.com/pandas-dev/pandas/blob/v0.22.0/pandas/core/groupby.py#L3674), and from what I can tell, that \"does the right thing\" with `iNaT`.\r\n\r\nThis is my first look at pandas internals, but if the fix is likely to be just ripping out those lines and adding a test, I can probably manage to put a PR together.",
      "it\u2019s not likely to be \u2018ripping it out\u2019 but pls have a look"
    ],
    "events": [
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 20,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v0.23.0.txt",
      "pandas/core/groupby.py",
      "pandas/tests/groupby/aggregate/test_cython.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 19646,
    "reporter": "toobaz",
    "created_at": "2018-02-11T16:23:55+00:00",
    "closed_at": "2018-04-01T17:47:39+00:00",
    "resolver": "toobaz",
    "resolved_in": "4efb39f01f5880122fa38d91e12d217ef70fad9e",
    "resolver_commit_num": 62,
    "title": "Actual dtypes of pd.DataFrame(None, [...], dtype=int)",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\ncompare to\r\n\r\n#### Problem description\r\n\r\nSee -dev/pandas/pull/18600#r167405218\r\n\r\nWrong test in -dev/pandas/blob/324379ce75269aa6bced90ecf3edb692539a2742/pandas/tests/frame/test_constructors.py#L738\r\n \r\nFixed by #18600\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 50528421ae79b27a26b32ba715b17271c8dfda7e\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-5-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: it_IT.UTF-8\r\nLOCALE: it_IT.UTF-8\r\n\r\npandas: 0.23.0.dev0+248.g50528421a\r\npytest: 3.2.3\r\npip: 9.0.1\r\nsetuptools: 36.7.0\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\npyarrow: None\r\nxarray: None\r\nIPython: 6.2.1\r\nsphinx: 1.5.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.1\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: 1.2.0dev\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nfeather: 0.3.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.3.0\r\nxlrd: 1.0.0\r\nxlwt: 1.3.0\r\nxlsxwriter: 0.9.6\r\nlxml: 4.1.1\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.0.15\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: 0.2.1\r\n\r\n</details>\r\n",
    "labels": [
      "Reshaping",
      "Dtypes",
      "Compat",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments": [],
    "events": [
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "unlabeled",
      "milestoned",
      "renamed",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 9,
    "additions": 92,
    "deletions": 56,
    "changed_files_list": [
      "asv_bench/benchmarks/frame_ctor.py",
      "doc/source/whatsnew/v0.23.0.txt",
      "pandas/core/frame.py",
      "pandas/core/generic.py",
      "pandas/core/internals.py",
      "pandas/core/series.py",
      "pandas/tests/frame/test_constructors.py",
      "pandas/tests/frame/test_reshape.py",
      "pandas/tests/io/test_excel.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 20071,
    "reporter": "jorisvandenbossche",
    "created_at": "2018-03-09T09:04:34+00:00",
    "closed_at": "2018-04-14T14:33:56+00:00",
    "resolver": "jreback",
    "resolved_in": "3885cedb884a8b22e0875d9ffbc8d28123d82b48",
    "resolver_commit_num": 4653,
    "title": "Revert temporary github PR template for sprint",
    "body": "Revert -dev/pandas/pull/20055 after the sprint",
    "labels": [
      "Admin"
    ],
    "comments": [
      "prob would do this (though might be worth leaving some text about showing a doc-string in the template if that is what the PR is about)"
    ],
    "events": [
      "milestoned",
      "commented",
      "labeled"
    ],
    "changed_files": 1,
    "additions": 0,
    "deletions": 24,
    "changed_files_list": [
      ".github/PULL_REQUEST_TEMPLATE.md"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 20747,
    "reporter": "jorisvandenbossche",
    "created_at": "2018-04-19T14:51:33+00:00",
    "closed_at": "2018-11-11T14:48:37+00:00",
    "resolver": "jreback",
    "resolved_in": "8fd8d0dc136e0720ce9b2d05f5f8ea8f62833c47",
    "resolver_commit_num": 4681,
    "title": "ExtensionArray construction with given dtype (sort of shallow_copy?)",
    "body": "Many of the extension arrays tests are skipped for `Categorical` because the reconstruction of the expected result does not preserve the categoricals (so kind of the \"metadata\" of the dtype).\r\n\r\nFor example:\r\n\r\n-dev/pandas/blob/78fee04e95e3c53c83c938285580c39e7761ddc8/pandas/tests/extension/category/test_categorical.py#L70-L72\r\n\r\nbecause in the actual test, the expected result is constructed from a list:\r\n\r\n-dev/pandas/blob/78fee04e95e3c53c83c938285580c39e7761ddc8/pandas/tests/extension/base/reshaping.py#L47-L50\r\n\r\n(still with `type(data)(..)`, but replacing that with `data._constructor_from_sequence(..)` in -dev/pandas/pull/20746).\r\n\r\nThis is kind of a recurrent pattern, so that might indicate we should find a solution for this?\r\n\r\nSo do we want a canonical way in the extension array interface to construct an ExtensionArray that has a certain dtype?\r\n\r\nPossible solution is to add a `dtype` keyword to `_constructor_from_sequence`",
    "labels": [
      "ExtensionArray"
    ],
    "comments": [
      "> This is kind of a recurrent pattern, so that might indicate we should find a solution for this?\r\n\r\nAgreed. This is somewhat similar to `_from_factorized` right, where we pass `original` rather than the `dtype`? \r\n\r\nNo thoughts on a solution yet, but a good test may be to parametrize `DecimalDtype` with a `decimal.Context` context. Then we could ensure that the correct `Context` is passed through on these kinds of operations, via the `dtype`.",
      "The tests that I mentioned in the original post are still skipped, so we need to check if this can now be resolved with the added `dtype` argument."
    ],
    "events": [
      "labeled",
      "milestoned",
      "commented",
      "demilestoned",
      "milestoned",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "closed",
      "referenced",
      "reopened",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced"
    ],
    "changed_files": 30,
    "additions": 1634,
    "deletions": 119,
    "changed_files_list": [
      "doc/source/whatsnew/v0.24.0.txt",
      "pandas/core/arrays/__init__.py",
      "pandas/core/arrays/base.py",
      "pandas/core/arrays/categorical.py",
      "pandas/core/arrays/integer.py",
      "pandas/core/arrays/interval.py",
      "pandas/core/dtypes/cast.py",
      "pandas/core/dtypes/common.py",
      "pandas/core/dtypes/concat.py",
      "pandas/core/indexes/base.py",
      "pandas/core/internals.py",
      "pandas/core/missing.py",
      "pandas/core/ops.py",
      "pandas/core/series.py",
      "pandas/tests/extension/base/__init__.py",
      "pandas/tests/extension/base/dtype.py",
      "pandas/tests/extension/base/getitem.py",
      "pandas/tests/extension/base/ops.py",
      "pandas/tests/extension/base/reshaping.py",
      "pandas/tests/extension/category/test_categorical.py",
      "pandas/tests/extension/decimal/array.py",
      "pandas/tests/extension/decimal/test_decimal.py",
      "pandas/tests/extension/integer/__init__.py",
      "pandas/tests/extension/integer/test_integer.py",
      "pandas/tests/extension/json/array.py",
      "pandas/tests/extension/json/test_json.py",
      "pandas/tests/extension/test_common.py",
      "pandas/tests/indexes/interval/test_astype.py",
      "pandas/tests/indexes/interval/test_construction.py",
      "pandas/tests/series/test_constructors.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 21639,
    "reporter": "h-vetinari",
    "created_at": "2018-06-26T13:04:49+00:00",
    "closed_at": "2018-11-01T12:03:49+00:00",
    "resolver": "ingwinlu",
    "resolved_in": "6b9318c132f53fd4e2ca588c67c16249b75c3b16",
    "resolver_commit_num": 0,
    "title": "DEPS/DEPR: Allow import of feather through pyarrow",
    "body": "Currently, `DataFrame.to_feather()` tries to `import feather`, which only works if `feather-format` is installed. The same capabilities could also be imported as follows: `import pyarrow.feather as feather`.\r\n\r\nThe issue is that `pyarrow` is more easily available (e.g. `conda install feather-format` doesn't work without specifying a specific channel; but `pyarrow` works), and is under more active development.\r\n\r\nI asked if there is a preference between the two imports in wesm/feather#341, and Wes answered [my bold]:\r\n> You can use either method; there is no explicit recommendation right now. **`import feather` is provided for backwards compatibility**\r\n\r\nI think the import should at least try `import pyarrow.feather as feather` as well, if `import feather` fails.",
    "labels": [
      "Deprecate"
    ],
    "comments": [
      "would take a PR to use pyarrow.feather\r\ni suppose should continue to accept feather-format but deprecate it as a dep"
    ],
    "events": [
      "commented",
      "renamed",
      "labeled",
      "cross-referenced",
      "milestoned"
    ],
    "changed_files": 12,
    "additions": 63,
    "deletions": 68,
    "changed_files_list": [
      "ci/azure-windows-36.yaml",
      "ci/requirements-optional-conda.txt",
      "ci/requirements-optional-pip.txt",
      "ci/travis-27.yaml",
      "ci/travis-36-doc.yaml",
      "ci/travis-36.yaml",
      "ci/travis-37.yaml",
      "doc/source/install.rst",
      "doc/source/whatsnew/v0.24.0.txt",
      "pandas/io/feather_format.py",
      "pandas/tests/io/test_common.py",
      "pandas/tests/io/test_feather.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 21749,
    "reporter": "alphaCTzo7G",
    "created_at": "2018-07-05T18:34:00+00:00",
    "closed_at": "2022-02-06T22:30:19+00:00",
    "resolver": "salomondush",
    "resolved_in": "73a69fd2aff3108fc377339d1f56cb3101d856b8",
    "resolver_commit_num": 0,
    "title": "DOCS: Update import statements in docstrings",
    "body": "xref -dev/pandas/pull/21731#pullrequestreview-134460126\r\n\r\n- [ ] ./pandas/plotting/_converter.py|134 col 17| \">>> from pandas.plotting import register_matplotlib_converters\"\r\n- [ ] pandas/plotting/_misc.py|501 col 5| >>> from pandas import read_csv\r\n- [ ] pandas/plotting/_misc.py|502 col 5| >>> from pandas.tools.plotting import parallel_coordinates\r\n- [x] ./pandas/util/testing.py|602 col 5| >>> from pandas.util.testing import capture_stdout -dev/pandas/pull/42979\r\n- [x] ./pandas/util/testing.py|650 col 5| >>> from pandas.util.testing import capture_stderr -dev/pandas/pull/42979\r\n- [x] ./pandas/util/testing.py|2213 col 7| >>> from pandas.util.testing import network -dev/pandas/pull/42979\r\n- [x] ./pandas/util/testing.py|2214 col 7| >>> from pandas.io.common import urlopen -dev/pandas/pull/42979\r\n- [x] ./pandas/tseries/holiday.py|145 col 9| >>> from pandas.tseries.holiday import Holiday, nearest_workday -dev/pandas/pull/45485\r\n- [ ] ./pandas/tseries/holiday.py|146 col 9| >>> from pandas import DateOffset\r\n- [ ] ./pandas/core/sparse/series.py|739 col 9| >>> from numpy import nan\r\n- [ ] pandas/core/frame.py|2844 col 9| >>> from numpy.random import randn\r\n- [ ] pandas/core/frame.py|2845 col 9| >>> from pandas import DataFrame\r\n- [ ] ./pandas/core/algorithms.py|1519 col 5| >>> from pandas.api.extensions import take\r\n- [x] ./pandas/core/dtypes/concat.py|285 col 5| >>> from pandas.api.types import union_categoricals -dev/pandas/pull/45346\r\n- [ ] ./pandas/io/pytables.py|457 col 5| >>> from pandas import DataFrame\r\n- [ ] ./pandas/io/pytables.py|458 col 5| >>> from numpy.random import randn\r\n- [ ] ./pandas/io/json/normalize.py|138 col 5| >>> from pandas.io.json import json_normalize\r\n- [x] pandas/io/stata.py|2713 col 5| >>> from pandas.io.stata import StataWriter117 -dev/pandas/pull/45346",
    "labels": [
      "Docs",
      "good first issue"
    ],
    "comments": [
      "What's this for?",
      "This is related to this.. https://github.com/pandas-dev/pandas/pull/21731#discussion_r200216595",
      "@TomAugspurger : I've updated the issue to clarify the purpose.",
      "I modified the import statements in the following files from the list in the OP and some others:\r\n\r\n```\r\nMODIFIED\r\n    ./pandas/io/pytables.py|457 col 5| >>> from pandas import DataFrame\r\n    ./pandas/io/pytables.py|458 col 5| >>> from numpy.random import randn\r\n    ./pandas/core/sparse/series.py|739 col 9| >>> from numpy import nan\r\n    ./pandas/tseries/holiday.py|146 col 9| >>> from pandas import DateOffset\r\n```\r\n\r\nbut kept these intact..\r\n\r\n```\r\n./pandas/plotting/_converter.py|134 col 17| \">>> from pandas.plotting import register_matplotlib_converters\"\r\n\r\n    pandas/plotting/_misc.py|502 col 5| >>> from pandas.tools.plotting import parallel_coordinates\r\n    ./pandas/util/testing.py|602 col 5| >>> from pandas.util.testing import capture_stdout\r\n    ./pandas/util/testing.py|650 col 5| >>> from pandas.util.testing import capture_stderr\r\n    ./pandas/util/testing.py|2213 col 7| >>> from pandas.util.testing import network\r\n    ./pandas/util/testing.py|2214 col 7| >>> from pandas.io.common import urlopen\r\n    ./pandas/tseries/holiday.py|145 col 9| >>> from pandas.tseries.holiday import Holiday, nearest_workday\r\n    ./pandas/core/algorithms.py|1519 col 5| >>> from pandas.api.extensions import take\r\n    ./pandas/core/dtypes/concat.py|285 col 5| >>> from pandas.api.types import union_categoricals\r\n    ./pandas/io/json/normalize.py|138 col 5| >>> from pandas.io.json import json_normalize\r\n    pandas/io/stata.py|2713 col 5| >>> from pandas.io.stata import StataWriter117\r\n```\r\n\r\nDo you think we should \r\nchange these as well to the form `import pandas as pd; pd...`. If we do, it wil\r\nresult in long function names as in `module.class.function(args)`\r\n\r\n\r\nThese were modified in a different PR\r\n\r\n```\r\n    pandas/core/frame.py|2844 col 9| >>> from numpy.random import randn\r\n    pandas/core/frame.py|2845 col 9| >>> from pandas import Dataframe\r\n```",
      "Do we have an idea which of these are still relevant?",
      "Take",
      "Hello, I was wondering if it's okay to take on this issue because I noticed that some statements still use the old form",
      "reopening as I don't think the linked PR closes all the items",
      "@salomondush Are you still working on this, as I wanted to work on this issue?",
      "I worked on some parts of it, but you can take the remaining check points",
      "@salomondush Thank you so much this is my first time contributing to a big project. I might need your guidance.",
      "take\r\n",
      "@salomondush  Hey, Can you tell me how did you find the exact location of docstrings that require fix? I followed the file path but didn't found those import statements that require fixing.",
      "Most of them have been fixed, but if you use VS code, you should be able to search specific lines of code throughout all files. ",
      "I will modify the documents in \r\npandas/io/stata.py and pandas/core/dtypes/concat.py.",
      "I'm addressing the following import update in the PR above:\r\n\r\n./pandas/core/algorithms.py|1519 col 5| >>> from pandas.api.extensions import take\r\n\r\nThat's the only outdated import statement I could find. Does this mean all others have been addressed? Thanks!",
      "Appears like https://github.com/pandas-dev/pandas/pull/45841 closed the last of the above checklist.\r\n\r\nClosing, but if there are other leftover, individual issues can be opened instead"
    ],
    "events": [
      "commented",
      "renamed",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "milestoned",
      "demilestoned",
      "milestoned",
      "commented",
      "unsubscribed",
      "labeled",
      "commented",
      "assigned",
      "commented",
      "referenced",
      "cross-referenced",
      "closed",
      "commented",
      "reopened",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "commented",
      "unassigned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "assigned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "unassigned",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "commented"
    ],
    "changed_files": 5,
    "additions": 17,
    "deletions": 21,
    "changed_files_list": [
      "pandas/_testing/_io.py",
      "pandas/_testing/asserters.py",
      "pandas/core/dtypes/base.py",
      "pandas/io/formats/latex.py",
      "pandas/tseries/holiday.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 22835,
    "reporter": "lukasz0004",
    "created_at": "2018-09-26T11:08:36+00:00",
    "closed_at": "2018-10-13T11:37:13+00:00",
    "resolver": "TomAugspurger",
    "resolved_in": "56d8e789155231b04131c6bf4674971ec3f38f95",
    "resolver_commit_num": 295,
    "title": "Negate pandas SparseSeries - AssertionError",
    "body": "#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n\r\n#### Problem description\r\n\r\nWhen I want to do loc selection pandas dataframe with negated pandas SparseSeries it just doesn't work. I know how to solve this problem in another way, but I think there might be something wrong in the code.\r\nThis is an error message:\r\n\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-153-e6f8589a6478> in <module>()\r\n      3 criteria = (sparse_df['a'] > 2)\r\n      4 sparse_df_gt_2 = sparse_df.loc[criteria, :].copy()\r\n----> 5 sparse_df_lt_2 = sparse_df.loc[~criteria, :].copy()\r\n~/Projects/ml_projects/ml_python_venv/lib/python3.6/site-packages/pandas/core/generic.py in __invert__(self)\r\n   1142         try:\r\n   1143             arr = operator.inv(com._values_from_object(self))\r\n-> 1144             return self.__array_wrap__(arr)\r\n   1145         except Exception:\r\n   1146 \r\n~/Projects/ml_projects/ml_python_venv/lib/python3.6/site-packages/pandas/core/sparse/series.py in __array_wrap__(self, result, context)\r\n    281                                  sparse_index=self.sp_index,\r\n    282                                  fill_value=fill_value,\r\n--> 283                                  copy=False).__finalize__(self)\r\n    284 \r\n    285     def __array_finalize__(self, obj):\r\n~/Projects/ml_projects/ml_python_venv/lib/python3.6/site-packages/pandas/core/sparse/series.py in __init__(self, data, index, sparse_index, kind, fill_value, name, dtype, copy, fastpath)\r\n    114                     data, sparse_index, fill_value = res\r\n    115                 else:\r\n--> 116                     assert (len(data) == sparse_index.npoints)\r\n    117 \r\n    118             elif isinstance(data, SingleBlockManager):\r\nAssertionError: \r\n\r\n#### Expected Output\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.6.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.15.0-34-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.23.4\r\npytest: None\r\npip: 18.0\r\nsetuptools: 39.1.0\r\nCython: None\r\nnumpy: 1.14.5\r\nscipy: 1.1.0\r\npyarrow: 0.10.0\r\nxarray: None\r\nIPython: 6.5.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.7.3\r\npytz: 2018.5\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.4.4\r\nnumexpr: 2.6.8\r\nfeather: None\r\nmatplotlib: 2.2.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 1.0.1\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: 2.7.5 (dt dec pq3 ext lo64)\r\njinja2: 2.10\r\ns3fs: 0.1.6\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n",
    "labels": [
      "Sparse"
    ],
    "comments": [
      "Fixing this in https://github.com/pandas-dev/pandas/issues/22835\r\n\r\nThat branch will be merged into master in the next week or two, if you're interested in trying it out."
    ],
    "events": [
      "labeled",
      "renamed",
      "renamed",
      "cross-referenced",
      "commented"
    ],
    "changed_files": 50,
    "additions": 3346,
    "deletions": 1421,
    "changed_files_list": [
      "doc/source/whatsnew/v0.24.0.txt",
      "pandas/_libs/sparse.pyx",
      "pandas/core/arrays/base.py",
      "pandas/core/common.py",
      "pandas/core/dtypes/common.py",
      "pandas/core/dtypes/concat.py",
      "pandas/core/dtypes/missing.py",
      "pandas/core/internals/__init__.py",
      "pandas/core/internals/blocks.py",
      "pandas/core/internals/concat.py",
      "pandas/core/internals/managers.py",
      "pandas/core/ops.py",
      "pandas/core/reshape/reshape.py",
      "pandas/core/series.py",
      "pandas/core/sparse/api.py",
      "pandas/core/sparse/array.py",
      "pandas/core/sparse/dtype.py",
      "pandas/core/sparse/frame.py",
      "pandas/core/sparse/series.py",
      "pandas/tests/api/test_api.py",
      "pandas/tests/dtypes/test_common.py",
      "pandas/tests/dtypes/test_dtypes.py",
      "pandas/tests/extension/arrow/bool.py",
      "pandas/tests/extension/arrow/test_bool.py",
      "pandas/tests/extension/base/interface.py",
      "pandas/tests/extension/base/ops.py",
      "pandas/tests/extension/base/reshaping.py",
      "pandas/tests/extension/test_sparse.py",
      "pandas/tests/frame/test_api.py",
      "pandas/tests/frame/test_indexing.py",
      "pandas/tests/frame/test_subclass.py",
      "pandas/tests/internals/test_internals.py",
      "pandas/tests/reshape/test_reshape.py",
      "pandas/tests/series/test_combine_concat.py",
      "pandas/tests/series/test_missing.py",
      "pandas/tests/series/test_quantile.py",
      "pandas/tests/series/test_subclass.py",
      "pandas/tests/sparse/frame/test_apply.py",
      "pandas/tests/sparse/frame/test_frame.py",
      "pandas/tests/sparse/frame/test_to_from_scipy.py",
      "pandas/tests/sparse/series/test_series.py",
      "pandas/tests/sparse/test_arithmetics.py",
      "pandas/tests/sparse/test_array.py",
      "pandas/tests/sparse/test_combine_concat.py",
      "pandas/tests/sparse/test_dtype.py",
      "pandas/tests/sparse/test_format.py",
      "pandas/tests/sparse/test_groupby.py",
      "pandas/tests/sparse/test_indexing.py",
      "pandas/util/_test_decorators.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 23283,
    "reporter": "ispmarin",
    "created_at": "2018-10-22T18:39:41+00:00",
    "closed_at": "2018-11-10T12:12:32+00:00",
    "resolver": "anjsudh",
    "resolved_in": "8ed92efc65b51a9010bd14e0e666fc722226e50a",
    "resolver_commit_num": 1,
    "title": "Write partitioned Parquet file using to_parquet",
    "body": "Hi, \r\n\r\nI'm trying to write a partitioned Parquet file using the `to_parquet` function:\r\n\r\n\r\n#### Problem description\r\nIt was my understanding that the `to_parquet` method pass the kwargs to Pyarrow and save a partitioned table. \r\n\r\n#### Expected Output\r\nPartitioned Parquet file saved.\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.0-5-amd64\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.23.4\r\npytest: None\r\npip: 18.0\r\nsetuptools: 32.3.1\r\nCython: None\r\nnumpy: 1.15.2\r\nscipy: 1.1.0\r\npyarrow: 0.11.0\r\nxarray: None\r\nIPython: 7.0.1\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.7.3\r\npytz: 2018.5\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 3.0.0\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 1.0.1\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.10\r\ns3fs: None\r\nfastparquet: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n</details>\r\n\r\nThanks!",
    "labels": [
      "IO Parquet"
    ],
    "comments": [
      "pandas uses `pyarrow.parquet.write_table`. It seems like multi-part Datasets are written using `pyarrow.parquet.write_to_dataset`.\r\n\r\nI'm not sure whether it makes sense for us to (optionally) use `write_to_dataset`, or whether pyarrow should support `partition_cols` in `write_table`.\r\n\r\ncc @wesm if you have thoughts here.",
      "In the case of `partition_cols`, one should use `write_to_dataset`. `write_table` is much more simple/low level function. ",
      "So, pandas could look for kwargs like `partition_cols` (any others?) and if that's detected use `write_to_dataset(table, ...)`. That seems fine to me.",
      "Will pick this up"
    ],
    "events": [
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned"
    ],
    "changed_files": 7,
    "additions": 167,
    "deletions": 15,
    "changed_files_list": [
      "doc/source/io.rst",
      "doc/source/whatsnew/v0.24.0.txt",
      "pandas/core/frame.py",
      "pandas/io/parquet.py",
      "pandas/tests/io/test_parquet.py",
      "pandas/tests/util/test_testing.py",
      "pandas/util/testing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 25487,
    "reporter": "WillAyd",
    "created_at": "2019-02-28T22:21:50+00:00",
    "closed_at": "2019-06-30T20:53:06+00:00",
    "resolver": "yehia67",
    "resolved_in": "bd49d2f2af5c44d4f96031757743ba83e6b29408",
    "resolver_commit_num": 0,
    "title": "DOC: Force Removal of pandas From Dependencies",
    "body": "From a discussion on Gitter with @TomAugspurger when following the pandas contributing guide the step:\r\n\r\n\r\n\r\nCauses pandas to be installed in the environment from conda as a result of dependency resolution with third party libraries (ex: statsmodels, pyarrow, seaborn). \r\n\r\nTo prevent undesired conflicts between the local development version and the installed version we should update the documentation to have an explicit removal of the bundled version as such:\r\n\r\n",
    "labels": [
      "Docs",
      "good first issue"
    ],
    "comments": [
      "So you want to Update this section \r\n```\r\n# Create and activate the build environment\r\nconda env create -f environment.yml\r\nconda activate pandas-dev\r\n```\r\n\r\nto be like that?\r\n\r\n```\r\n# Create and activate the build environment\r\nconda uninstall --force pandas\r\nconda env create -f environment.yml\r\nconda activate pandas-dev\r\n```\r\n",
      "Swap the first and second lines but yes\n\nSent from my iPhone\n\n> On Feb 28, 2019, at 2:45 PM, yehia67 <notifications@github.com> wrote:\n> \n> So you want to Update this section\n> \n> # Create and activate the build environment\n> conda env create -f environment.yml\n> conda activate pandas-dev\n> to be like that?\n> \n> # Create and activate the build environment\n> conda uninstall --force pandas\n> conda env create -f environment.yml\n> conda activate pandas-dev\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n",
      "okay",
      "I am not sure this is actually needed to recommend. It's true that conda does not work very nice with `-e` installed dev packages, but by force removing it, you might also run into troubles when updating / installing other packages that depend on pandas (as they don't find pandas, and conda then might install pandas again shadowing the dev version. Not sure this is what happens though).\r\n\r\nThe latest conda release with better pip integration enabled also solves this, by actually recognizing the dev installed package. But I think this is not yet enabled by default I think.",
      "I think the common case is \"setup an env from scratch\", in which case the\nold way would guarantee a double-install of pandas, right?\n\nOn Mon, Mar 4, 2019 at 7:54 AM Joris Van den Bossche <\nnotifications@github.com> wrote:\n\n> I am not sure this is actually needed to recommend. It's true that conda\n> does not work very nice with -e installed dev packages, but by force\n> removing it, you might also run into troubles when updating / installing\n> other packages that depend on pandas (as they don't find pandas, and conda\n> then might install pandas again shadowing the dev version. Not sure this is\n> what happens though).\n>\n> The latest conda release with better pip integration enabled also solves\n> this, by actually recognizing the dev installed package. But I think this\n> is not yet enabled by default I think.\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pandas-dev/pandas/issues/25487#issuecomment-469260527>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABQHIpt4iINt-v63E_T7valTRTxXFKQQks5vTSWLgaJpZM4bX3FA>\n> .\n>\n",
      "Yes, but you typically don't keep this new env as is for the rest of your contributing lifetime no? (or at least, I update my dev environment from time to time in place, not each time re-creating it from scratch). What happens when I eg update seaborn that depends on pandas, if pandas was force-removed like this?",
      "Yes I think so. Will conda overwrite a locally installed pandas? If so,\nthen yes, we should maybe revert this.\n\nOn Mon, Mar 4, 2019 at 8:04 AM Joris Van den Bossche <\nnotifications@github.com> wrote:\n\n> Yes, but you typically don't keep this new env as is for the rest of your\n> contributing lifetime no? (or at least, I update my dev environment from\n> time to time in place, not each time re-creating it from scratch). What\n> happens when I eg update seaborn that depends on pandas, if pandas was\n> force-removed like this?\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pandas-dev/pandas/issues/25487#issuecomment-469263657>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABQHIm1DfO4hkkmt93nyHT0ir0NuRdmmks5vTSfRgaJpZM4bX3FA>\n> .\n>\n",
      "It seems so. Eg doing `conda install seaborn` in the example below would have installed pandas 0.24.1 again. \r\nDoing a `conda config --set pip_interop_enabled True` seems to prevent that however (but with that option, you would probably also not have to force remove pandas in the first place). Maybe we should recommend doing that instead.\r\n\r\n<details>\r\n\r\n```\r\n(base) joris@joris-XPS-13-9350:~/scipy$ conda create -n test-pandas python=3.7 pandas cython\r\nCollecting package metadata: done\r\nSolving environment: done\r\n\r\n## Package Plan ##\r\n\r\n  environment location: /home/joris/miniconda3/envs/test-pandas\r\n\r\n  added / updated specs:\r\n    - cython\r\n    - pandas\r\n    - python=3.7\r\n\r\n\r\nThe following packages will be downloaded:\r\n\r\n    package                    |            build\r\n    ---------------------------|-----------------\r\n    cython-0.29.6              |   py37hf484d3e_0         2.2 MB  conda-forge\r\n    numpy-1.16.2               |py37_blas_openblash1522bff_0         4.3 MB  conda-forge\r\n    openssl-1.1.1b             |       h14c3975_0         4.0 MB  conda-forge\r\n    python-3.7.1               |    h381d211_1002        36.4 MB  conda-forge\r\n    ------------------------------------------------------------\r\n                                           Total:        46.8 MB\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n  blas               conda-forge/linux-64::blas-1.1-openblas\r\n  bzip2              conda-forge/linux-64::bzip2-1.0.6-h14c3975_1002\r\n  ca-certificates    conda-forge/linux-64::ca-certificates-2018.11.29-ha4d7672_0\r\n  certifi            conda-forge/linux-64::certifi-2018.11.29-py37_1000\r\n  cython             conda-forge/linux-64::cython-0.29.6-py37hf484d3e_0\r\n  libffi             conda-forge/linux-64::libffi-3.2.1-hf484d3e_1005\r\n  libgcc-ng          conda-forge/linux-64::libgcc-ng-7.3.0-hdf63c60_0\r\n  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-7.2.0-hdf63c60_3\r\n  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-7.3.0-hdf63c60_0\r\n  ncurses            conda-forge/linux-64::ncurses-6.1-hf484d3e_1002\r\n  numpy              conda-forge/linux-64::numpy-1.16.2-py37_blas_openblash1522bff_0\r\n  openblas           conda-forge/linux-64::openblas-0.3.3-h9ac9557_1001\r\n  openssl            conda-forge/linux-64::openssl-1.1.1b-h14c3975_0\r\n  pandas             conda-forge/linux-64::pandas-0.24.1-py37hf484d3e_0\r\n  pip                conda-forge/linux-64::pip-19.0.3-py37_0\r\n  python             conda-forge/linux-64::python-3.7.1-h381d211_1002\r\n  python-dateutil    conda-forge/noarch::python-dateutil-2.8.0-py_0\r\n  pytz               conda-forge/noarch::pytz-2018.9-py_0\r\n  readline           conda-forge/linux-64::readline-7.0-hf8c457e_1001\r\n  setuptools         conda-forge/linux-64::setuptools-40.8.0-py37_0\r\n  six                conda-forge/linux-64::six-1.12.0-py37_1000\r\n  sqlite             conda-forge/linux-64::sqlite-3.26.0-h67949de_1000\r\n  tk                 conda-forge/linux-64::tk-8.6.9-h84994c4_1000\r\n  wheel              conda-forge/linux-64::wheel-0.33.1-py37_0\r\n  xz                 conda-forge/linux-64::xz-5.2.4-h14c3975_1001\r\n  zlib               conda-forge/linux-64::zlib-1.2.11-h14c3975_1004\r\n\r\n\r\nProceed ([y]/n)? \r\n\r\n\r\nDownloading and Extracting Packages\r\npython-3.7.1         | 36.4 MB   | ##################################################################################################################################### | 100% \r\nnumpy-1.16.2         | 4.3 MB    | ##################################################################################################################################### | 100% \r\nopenssl-1.1.1b       | 4.0 MB    | ##################################################################################################################################### | 100% \r\ncython-0.29.6        | 2.2 MB    | ##################################################################################################################################### | 100% \r\nPreparing transaction: done\r\nVerifying transaction: done\r\nExecuting transaction: done\r\n#\r\n# To activate this environment, use\r\n#\r\n#     $ conda activate test-pandas\r\n#\r\n# To deactivate an active environment, use\r\n#\r\n#     $ conda deactivate\r\n\r\n(base) joris@joris-XPS-13-9350:~/scipy$ act test-pandas\r\n(test-pandas) joris@joris-XPS-13-9350:~/scipy$ conda uninstall --force pandas\r\n\r\n## Package Plan ##\r\n\r\n  environment location: /home/joris/miniconda3/envs/test-pandas\r\n\r\n  removed specs:\r\n    - pandas\r\n\r\n\r\nThe following packages will be REMOVED:\r\n\r\n  pandas-0.24.1-py37hf484d3e_0\r\n\r\n\r\nProceed ([y]/n)? \r\n\r\nPreparing transaction: done\r\nVerifying transaction: done\r\nExecuting transaction: done\r\n\r\n(test-pandas) joris@joris-XPS-13-9350:~/scipy/pandas$ pip install -e .\r\nObtaining file:///home/joris/scipy/pandas\r\nRequirement already satisfied: python-dateutil>=2.5.0 in /home/joris/miniconda3/envs/test-pandas/lib/python3.7/site-packages (from pandas==0.25.0.dev0+192.gbd49d2f2a) (2.8.0)\r\nRequirement already satisfied: pytz>=2011k in /home/joris/miniconda3/envs/test-pandas/lib/python3.7/site-packages (from pandas==0.25.0.dev0+192.gbd49d2f2a) (2018.9)\r\nRequirement already satisfied: numpy>=1.12.0 in /home/joris/miniconda3/envs/test-pandas/lib/python3.7/site-packages (from pandas==0.25.0.dev0+192.gbd49d2f2a) (1.16.2)\r\nRequirement already satisfied: six>=1.5 in /home/joris/miniconda3/envs/test-pandas/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas==0.25.0.dev0+192.gbd49d2f2a) (1.12.0)\r\nInstalling collected packages: pandas\r\n  Running setup.py develop for pandas\r\nSuccessfully installed pandas\r\n(test-pandas) joris@joris-XPS-13-9350:~/scipy/pandas$ conda list\r\n# packages in environment at /home/joris/miniconda3/envs/test-pandas:\r\n#\r\n# Name                    Version                   Build  Channel\r\nblas                      1.1                    openblas    conda-forge\r\nbzip2                     1.0.6             h14c3975_1002    conda-forge\r\nca-certificates           2018.11.29           ha4d7672_0    conda-forge\r\ncertifi                   2018.11.29            py37_1000    conda-forge\r\ncython                    0.29.6           py37hf484d3e_0    conda-forge\r\nlibffi                    3.2.1             hf484d3e_1005    conda-forge\r\nlibgcc-ng                 7.3.0                hdf63c60_0    conda-forge\r\nlibgfortran-ng            7.2.0                hdf63c60_3    conda-forge\r\nlibstdcxx-ng              7.3.0                hdf63c60_0    conda-forge\r\nncurses                   6.1               hf484d3e_1002    conda-forge\r\nnumpy                     1.16.2          py37_blas_openblash1522bff_0  [blas_openblas]  conda-forge\r\nopenblas                  0.3.3             h9ac9557_1001    conda-forge\r\nopenssl                   1.1.1b               h14c3975_0    conda-forge\r\npandas                    0.25.0.dev0+192.gbd49d2f2a           dev_0    <develop>\r\npip                       19.0.3                   py37_0    conda-forge\r\npython                    3.7.1             h381d211_1002    conda-forge\r\npython-dateutil           2.8.0                      py_0    conda-forge\r\npytz                      2018.9                     py_0    conda-forge\r\nreadline                  7.0               hf8c457e_1001    conda-forge\r\nsetuptools                40.8.0                   py37_0    conda-forge\r\nsix                       1.12.0                py37_1000    conda-forge\r\nsqlite                    3.26.0            h67949de_1000    conda-forge\r\ntk                        8.6.9             h84994c4_1000    conda-forge\r\nwheel                     0.33.1                   py37_0    conda-forge\r\nxz                        5.2.4             h14c3975_1001    conda-forge\r\nzlib                      1.2.11            h14c3975_1004    conda-forge\r\n(test-pandas) joris@joris-XPS-13-9350:~/scipy/pandas$ conda install seaborn\r\nCollecting package metadata: done\r\nSolving environment: done\r\n\r\n## Package Plan ##\r\n\r\n  environment location: /home/joris/miniconda3/envs/test-pandas\r\n\r\n  added / updated specs:\r\n    - seaborn\r\n\r\n\r\nThe following packages will be downloaded:\r\n\r\n    package                    |            build\r\n    ---------------------------|-----------------\r\n    matplotlib-3.0.3           |           py37_0           6 KB  conda-forge\r\n    matplotlib-base-3.0.3      |   py37h167e16e_0         6.6 MB  conda-forge\r\n    tornado-6.0.1              |   py37h14c3975_0         635 KB  conda-forge\r\n    ------------------------------------------------------------\r\n                                           Total:         7.2 MB\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n  atk                conda-forge/linux-64::atk-2.25.90-hb9dd440_1002\r\n  cairo              conda-forge/linux-64::cairo-1.16.0-ha4e643d_1000\r\n  cycler             conda-forge/noarch::cycler-0.10.0-py_1\r\n  dbus               conda-forge/linux-64::dbus-1.13.0-h4e0c4b3_1000\r\n  expat              conda-forge/linux-64::expat-2.2.5-hf484d3e_1002\r\n  fontconfig         conda-forge/linux-64::fontconfig-2.13.1-h2176d3f_1000\r\n  freetype           conda-forge/linux-64::freetype-2.9.1-h94bbf69_1005\r\n  gdk-pixbuf         conda-forge/linux-64::gdk-pixbuf-2.36.12-h49783d7_1002\r\n  gettext            conda-forge/linux-64::gettext-0.19.8.1-h9745a5d_1001\r\n  glib               conda-forge/linux-64::glib-2.58.2-hf63aee3_1001\r\n  gobject-introspec~ conda-forge/linux-64::gobject-introspection-1.58.2-py37h2da5eee_1000\r\n  graphite2          conda-forge/linux-64::graphite2-1.3.13-hf484d3e_1000\r\n  gstreamer          conda-forge/linux-64::gstreamer-1.14.4-h66beb1c_1001\r\n  gtk2               conda-forge/linux-64::gtk2-2.24.31-hb68c50a_1001\r\n  harfbuzz           conda-forge/linux-64::harfbuzz-2.3.1-h6824563_0\r\n  icu                conda-forge/linux-64::icu-58.2-hf484d3e_1000\r\n  jpeg               conda-forge/linux-64::jpeg-9c-h14c3975_1001\r\n  kiwisolver         conda-forge/linux-64::kiwisolver-1.0.1-py37h6bb024c_1002\r\n  libiconv           conda-forge/linux-64::libiconv-1.15-h14c3975_1004\r\n  libpng             conda-forge/linux-64::libpng-1.6.36-h84994c4_1000\r\n  libtiff            conda-forge/linux-64::libtiff-4.0.10-h9022e91_1002\r\n  libuuid            conda-forge/linux-64::libuuid-2.32.1-h14c3975_1000\r\n  libxcb             conda-forge/linux-64::libxcb-1.13-h14c3975_1002\r\n  libxml2            conda-forge/linux-64::libxml2-2.9.8-h143f9aa_1005\r\n  matplotlib         conda-forge/linux-64::matplotlib-3.0.3-py37_0\r\n  matplotlib-base    conda-forge/linux-64::matplotlib-base-3.0.3-py37h167e16e_0\r\n  pandas             conda-forge/linux-64::pandas-0.24.1-py37hf484d3e_0\r\n  pango              conda-forge/linux-64::pango-1.40.14-h4ea9474_1004\r\n  patsy              conda-forge/noarch::patsy-0.5.1-py_0\r\n  pcre               conda-forge/linux-64::pcre-8.41-hf484d3e_1003\r\n  pixman             conda-forge/linux-64::pixman-0.34.0-h14c3975_1003\r\n  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h14c3975_1001\r\n  pyparsing          conda-forge/noarch::pyparsing-2.3.1-py_0\r\n  pyqt               conda-forge/linux-64::pyqt-5.6.0-py37h13b7fb3_1008\r\n  qt                 conda-forge/linux-64::qt-5.6.2-hce4f676_1013\r\n  scipy              conda-forge/linux-64::scipy-1.2.1-py37_blas_openblash1522bff_0\r\n  seaborn            conda-forge/noarch::seaborn-0.9.0-py_0\r\n  sip                conda-forge/linux-64::sip-4.18.1-py37hf484d3e_1000\r\n  statsmodels        conda-forge/linux-64::statsmodels-0.9.0-py37h3010b51_1000\r\n  tornado            conda-forge/linux-64::tornado-6.0.1-py37h14c3975_0\r\n  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h14c3975_1002\r\n  xorg-libice        conda-forge/linux-64::xorg-libice-1.0.9-h14c3975_1004\r\n  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.3-h4937e3b_1000\r\n  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.6.7-h14c3975_1000\r\n  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.9-h14c3975_0\r\n  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.2-h14c3975_1007\r\n  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.3-h14c3975_1004\r\n  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.10-h14c3975_1002\r\n  xorg-libxt         conda-forge/linux-64::xorg-libxt-1.1.5-h14c3975_1002\r\n  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h14c3975_1002\r\n  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h14c3975_1002\r\n  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h14c3975_1007\r\n  zstd               conda-forge/linux-64::zstd-1.3.3-1\r\n\r\nThe following packages will be DOWNGRADED:\r\n\r\n  openssl                                 1.1.1b-h14c3975_0 --> 1.0.2r-h14c3975_0\r\n  python                                3.7.1-h381d211_1002 --> 3.7.1-hd21baee_1001\r\n\r\n\r\nProceed ([y]/n)? n\r\n\r\n\r\nExiting.\r\n\r\n```\r\n\r\n\r\n</details>",
      "Thanks for checking. Recommending that conda config seems OK to me. Happy\nto volunteer our contributors a guinea pigs for conda :)\n\nOn Mon, Mar 4, 2019 at 8:59 AM Joris Van den Bossche <\nnotifications@github.com> wrote:\n\n> It seems so. Eg doing conda install seaborn in the example below would\n> have installed pandas 0.24.1 again.\n> Doing a conda config --set pip_interop_enabled True seems to prevent that\n> however (but with that option, you would probably also not have to force\n> remove pandas in the first place). Maybe we should recommend doing that\n> instead.\n>\n> (base) joris@joris-XPS-13-9350:~/scipy$ conda create -n test-pandas python=3.7 pandas cython\n> Collecting package metadata: done\n> Solving environment: done\n>\n> ## Package Plan ##\n>\n>   environment location: /home/joris/miniconda3/envs/test-pandas\n>\n>   added / updated specs:\n>     - cython\n>     - pandas\n>     - python=3.7\n>\n>\n> The following packages will be downloaded:\n>\n>     package                    |            build\n>     ---------------------------|-----------------\n>     cython-0.29.6              |   py37hf484d3e_0         2.2 MB  conda-forge\n>     numpy-1.16.2               |py37_blas_openblash1522bff_0         4.3 MB  conda-forge\n>     openssl-1.1.1b             |       h14c3975_0         4.0 MB  conda-forge\n>     python-3.7.1               |    h381d211_1002        36.4 MB  conda-forge\n>     ------------------------------------------------------------\n>                                            Total:        46.8 MB\n>\n> The following NEW packages will be INSTALLED:\n>\n>   blas               conda-forge/linux-64::blas-1.1-openblas\n>   bzip2              conda-forge/linux-64::bzip2-1.0.6-h14c3975_1002\n>   ca-certificates    conda-forge/linux-64::ca-certificates-2018.11.29-ha4d7672_0\n>   certifi            conda-forge/linux-64::certifi-2018.11.29-py37_1000\n>   cython             conda-forge/linux-64::cython-0.29.6-py37hf484d3e_0\n>   libffi             conda-forge/linux-64::libffi-3.2.1-hf484d3e_1005\n>   libgcc-ng          conda-forge/linux-64::libgcc-ng-7.3.0-hdf63c60_0\n>   libgfortran-ng     conda-forge/linux-64::libgfortran-ng-7.2.0-hdf63c60_3\n>   libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-7.3.0-hdf63c60_0\n>   ncurses            conda-forge/linux-64::ncurses-6.1-hf484d3e_1002\n>   numpy              conda-forge/linux-64::numpy-1.16.2-py37_blas_openblash1522bff_0\n>   openblas           conda-forge/linux-64::openblas-0.3.3-h9ac9557_1001\n>   openssl            conda-forge/linux-64::openssl-1.1.1b-h14c3975_0\n>   pandas             conda-forge/linux-64::pandas-0.24.1-py37hf484d3e_0\n>   pip                conda-forge/linux-64::pip-19.0.3-py37_0\n>   python             conda-forge/linux-64::python-3.7.1-h381d211_1002\n>   python-dateutil    conda-forge/noarch::python-dateutil-2.8.0-py_0\n>   pytz               conda-forge/noarch::pytz-2018.9-py_0\n>   readline           conda-forge/linux-64::readline-7.0-hf8c457e_1001\n>   setuptools         conda-forge/linux-64::setuptools-40.8.0-py37_0\n>   six                conda-forge/linux-64::six-1.12.0-py37_1000\n>   sqlite             conda-forge/linux-64::sqlite-3.26.0-h67949de_1000\n>   tk                 conda-forge/linux-64::tk-8.6.9-h84994c4_1000\n>   wheel              conda-forge/linux-64::wheel-0.33.1-py37_0\n>   xz                 conda-forge/linux-64::xz-5.2.4-h14c3975_1001\n>   zlib               conda-forge/linux-64::zlib-1.2.11-h14c3975_1004\n>\n>\n> Proceed ([y]/n)?\n>\n>\n> Downloading and Extracting Packages\n> python-3.7.1         | 36.4 MB   | ##################################################################################################################################### | 100%\n> numpy-1.16.2         | 4.3 MB    | ##################################################################################################################################### | 100%\n> openssl-1.1.1b       | 4.0 MB    | ##################################################################################################################################### | 100%\n> cython-0.29.6        | 2.2 MB    | ##################################################################################################################################### | 100%\n> Preparing transaction: done\n> Verifying transaction: done\n> Executing transaction: done\n> #\n> # To activate this environment, use\n> #\n> #     $ conda activate test-pandas\n> #\n> # To deactivate an active environment, use\n> #\n> #     $ conda deactivate\n>\n> (base) joris@joris-XPS-13-9350:~/scipy$ act test-pandas\n> (test-pandas) joris@joris-XPS-13-9350:~/scipy$ conda uninstall --force pandas\n>\n> ## Package Plan ##\n>\n>   environment location: /home/joris/miniconda3/envs/test-pandas\n>\n>   removed specs:\n>     - pandas\n>\n>\n> The following packages will be REMOVED:\n>\n>   pandas-0.24.1-py37hf484d3e_0\n>\n>\n> Proceed ([y]/n)?\n>\n> Preparing transaction: done\n> Verifying transaction: done\n> Executing transaction: done\n>\n> (test-pandas) joris@joris-XPS-13-9350:~/scipy/pandas$ pip install -e .\n> Obtaining file:///home/joris/scipy/pandas\n> Requirement already satisfied: python-dateutil>=2.5.0 in /home/joris/miniconda3/envs/test-pandas/lib/python3.7/site-packages (from pandas==0.25.0.dev0+192.gbd49d2f2a) (2.8.0)\n> Requirement already satisfied: pytz>=2011k in /home/joris/miniconda3/envs/test-pandas/lib/python3.7/site-packages (from pandas==0.25.0.dev0+192.gbd49d2f2a) (2018.9)\n> Requirement already satisfied: numpy>=1.12.0 in /home/joris/miniconda3/envs/test-pandas/lib/python3.7/site-packages (from pandas==0.25.0.dev0+192.gbd49d2f2a) (1.16.2)\n> Requirement already satisfied: six>=1.5 in /home/joris/miniconda3/envs/test-pandas/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas==0.25.0.dev0+192.gbd49d2f2a) (1.12.0)\n> Installing collected packages: pandas\n>   Running setup.py develop for pandas\n> Successfully installed pandas\n> (test-pandas) joris@joris-XPS-13-9350:~/scipy/pandas$ conda list\n> # packages in environment at /home/joris/miniconda3/envs/test-pandas:\n> #\n> # Name                    Version                   Build  Channel\n> blas                      1.1                    openblas    conda-forge\n> bzip2                     1.0.6             h14c3975_1002    conda-forge\n> ca-certificates           2018.11.29           ha4d7672_0    conda-forge\n> certifi                   2018.11.29            py37_1000    conda-forge\n> cython                    0.29.6           py37hf484d3e_0    conda-forge\n> libffi                    3.2.1             hf484d3e_1005    conda-forge\n> libgcc-ng                 7.3.0                hdf63c60_0    conda-forge\n> libgfortran-ng            7.2.0                hdf63c60_3    conda-forge\n> libstdcxx-ng              7.3.0                hdf63c60_0    conda-forge\n> ncurses                   6.1               hf484d3e_1002    conda-forge\n> numpy                     1.16.2          py37_blas_openblash1522bff_0  [blas_openblas]  conda-forge\n> openblas                  0.3.3             h9ac9557_1001    conda-forge\n> openssl                   1.1.1b               h14c3975_0    conda-forge\n> pandas                    0.25.0.dev0+192.gbd49d2f2a           dev_0    <develop>\n> pip                       19.0.3                   py37_0    conda-forge\n> python                    3.7.1             h381d211_1002    conda-forge\n> python-dateutil           2.8.0                      py_0    conda-forge\n> pytz                      2018.9                     py_0    conda-forge\n> readline                  7.0               hf8c457e_1001    conda-forge\n> setuptools                40.8.0                   py37_0    conda-forge\n> six                       1.12.0                py37_1000    conda-forge\n> sqlite                    3.26.0            h67949de_1000    conda-forge\n> tk                        8.6.9             h84994c4_1000    conda-forge\n> wheel                     0.33.1                   py37_0    conda-forge\n> xz                        5.2.4             h14c3975_1001    conda-forge\n> zlib                      1.2.11            h14c3975_1004    conda-forge\n> (test-pandas) joris@joris-XPS-13-9350:~/scipy/pandas$ conda install seaborn\n> Collecting package metadata: done\n> Solving environment: done\n>\n> ## Package Plan ##\n>\n>   environment location: /home/joris/miniconda3/envs/test-pandas\n>\n>   added / updated specs:\n>     - seaborn\n>\n>\n> The following packages will be downloaded:\n>\n>     package                    |            build\n>     ---------------------------|-----------------\n>     matplotlib-3.0.3           |           py37_0           6 KB  conda-forge\n>     matplotlib-base-3.0.3      |   py37h167e16e_0         6.6 MB  conda-forge\n>     tornado-6.0.1              |   py37h14c3975_0         635 KB  conda-forge\n>     ------------------------------------------------------------\n>                                            Total:         7.2 MB\n>\n> The following NEW packages will be INSTALLED:\n>\n>   atk                conda-forge/linux-64::atk-2.25.90-hb9dd440_1002\n>   cairo              conda-forge/linux-64::cairo-1.16.0-ha4e643d_1000\n>   cycler             conda-forge/noarch::cycler-0.10.0-py_1\n>   dbus               conda-forge/linux-64::dbus-1.13.0-h4e0c4b3_1000\n>   expat              conda-forge/linux-64::expat-2.2.5-hf484d3e_1002\n>   fontconfig         conda-forge/linux-64::fontconfig-2.13.1-h2176d3f_1000\n>   freetype           conda-forge/linux-64::freetype-2.9.1-h94bbf69_1005\n>   gdk-pixbuf         conda-forge/linux-64::gdk-pixbuf-2.36.12-h49783d7_1002\n>   gettext            conda-forge/linux-64::gettext-0.19.8.1-h9745a5d_1001\n>   glib               conda-forge/linux-64::glib-2.58.2-hf63aee3_1001\n>   gobject-introspec~ conda-forge/linux-64::gobject-introspection-1.58.2-py37h2da5eee_1000\n>   graphite2          conda-forge/linux-64::graphite2-1.3.13-hf484d3e_1000\n>   gstreamer          conda-forge/linux-64::gstreamer-1.14.4-h66beb1c_1001\n>   gtk2               conda-forge/linux-64::gtk2-2.24.31-hb68c50a_1001\n>   harfbuzz           conda-forge/linux-64::harfbuzz-2.3.1-h6824563_0\n>   icu                conda-forge/linux-64::icu-58.2-hf484d3e_1000\n>   jpeg               conda-forge/linux-64::jpeg-9c-h14c3975_1001\n>   kiwisolver         conda-forge/linux-64::kiwisolver-1.0.1-py37h6bb024c_1002\n>   libiconv           conda-forge/linux-64::libiconv-1.15-h14c3975_1004\n>   libpng             conda-forge/linux-64::libpng-1.6.36-h84994c4_1000\n>   libtiff            conda-forge/linux-64::libtiff-4.0.10-h9022e91_1002\n>   libuuid            conda-forge/linux-64::libuuid-2.32.1-h14c3975_1000\n>   libxcb             conda-forge/linux-64::libxcb-1.13-h14c3975_1002\n>   libxml2            conda-forge/linux-64::libxml2-2.9.8-h143f9aa_1005\n>   matplotlib         conda-forge/linux-64::matplotlib-3.0.3-py37_0\n>   matplotlib-base    conda-forge/linux-64::matplotlib-base-3.0.3-py37h167e16e_0\n>   pandas             conda-forge/linux-64::pandas-0.24.1-py37hf484d3e_0\n>   pango              conda-forge/linux-64::pango-1.40.14-h4ea9474_1004\n>   patsy              conda-forge/noarch::patsy-0.5.1-py_0\n>   pcre               conda-forge/linux-64::pcre-8.41-hf484d3e_1003\n>   pixman             conda-forge/linux-64::pixman-0.34.0-h14c3975_1003\n>   pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h14c3975_1001\n>   pyparsing          conda-forge/noarch::pyparsing-2.3.1-py_0\n>   pyqt               conda-forge/linux-64::pyqt-5.6.0-py37h13b7fb3_1008\n>   qt                 conda-forge/linux-64::qt-5.6.2-hce4f676_1013\n>   scipy              conda-forge/linux-64::scipy-1.2.1-py37_blas_openblash1522bff_0\n>   seaborn            conda-forge/noarch::seaborn-0.9.0-py_0\n>   sip                conda-forge/linux-64::sip-4.18.1-py37hf484d3e_1000\n>   statsmodels        conda-forge/linux-64::statsmodels-0.9.0-py37h3010b51_1000\n>   tornado            conda-forge/linux-64::tornado-6.0.1-py37h14c3975_0\n>   xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h14c3975_1002\n>   xorg-libice        conda-forge/linux-64::xorg-libice-1.0.9-h14c3975_1004\n>   xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.3-h4937e3b_1000\n>   xorg-libx11        conda-forge/linux-64::xorg-libx11-1.6.7-h14c3975_1000\n>   xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.9-h14c3975_0\n>   xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.2-h14c3975_1007\n>   xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.3-h14c3975_1004\n>   xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.10-h14c3975_1002\n>   xorg-libxt         conda-forge/linux-64::xorg-libxt-1.1.5-h14c3975_1002\n>   xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h14c3975_1002\n>   xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h14c3975_1002\n>   xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h14c3975_1007\n>   zstd               conda-forge/linux-64::zstd-1.3.3-1\n>\n> The following packages will be DOWNGRADED:\n>\n>   openssl                                 1.1.1b-h14c3975_0 --> 1.0.2r-h14c3975_0\n>   python                                3.7.1-h381d211_1002 --> 3.7.1-hd21baee_1001\n>\n>\n> Proceed ([y]/n)? n\n>\n>\n> Exiting.\n>\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pandas-dev/pandas/issues/25487#issuecomment-469283114>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABQHInqI3nkO9vYfytt2ClfPVfPN128Vks5vTTTmgaJpZM4bX3FA>\n> .\n>\n",
      "Is this issue still open?",
      "@rajat315315 It says it's going to be closed when the commit hksonngan did on March, 12th is merged to master (check the information icon to the right of the commit entry)",
      "It was reopened afterwards, because we are not fully sure it was actually the good solution."
    ],
    "events": [
      "labeled",
      "labeled",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "milestoned",
      "closed",
      "referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "reopened",
      "commented",
      "referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files": 1,
    "additions": 1,
    "deletions": 0,
    "changed_files_list": [
      "doc/source/development/contributing.rst"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 27297,
    "reporter": "jbrockmendel",
    "created_at": "2019-07-08T21:22:34+00:00",
    "closed_at": "2019-07-22T23:28:21+00:00",
    "resolver": "jbrockmendel",
    "resolved_in": "76247c142893c710e970c4cf8a25d73121aa5a2b",
    "resolver_commit_num": 633,
    "title": "BUG: DatetimeArray/Series incorrectly accepts timedelta64('NaT') for __setitem__",
    "body": "\r\n\r\nI haven't checked, but I expect the same thing happens in reverse for Timedelta, possible Period dtypes.",
    "labels": [
      "Bug",
      "Missing-data",
      "Timedelta"
    ],
    "comments": [
      "\"should\" behavior in OP is wrong, it should cast to object dtype, but does does not",
      "I think I incorrectly marked #27311 as closing this, while that only fixes the Series portion of this.  I still need to fix the DTA version.",
      "Woops, #27331 does fix the other half of this."
    ],
    "events": [
      "commented",
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "closed",
      "referenced",
      "commented",
      "reopened"
    ],
    "changed_files": 4,
    "additions": 38,
    "deletions": 2,
    "changed_files_list": [
      "doc/source/whatsnew/v1.0.0.rst",
      "pandas/_libs/index.pyx",
      "pandas/core/internals/blocks.py",
      "pandas/tests/series/indexing/test_indexing.py"
    ]
  },
  {
    "owner": "pandas-dev",
    "name": "pandas",
    "number": 37705,
    "reporter": "simon-spier0",
    "created_at": "2020-11-08T20:47:34+00:00",
    "closed_at": "2020-11-11T02:30:16+00:00",
    "resolver": "inspurwusixuan",
    "resolved_in": "ee1b75c5af60ab4cd7b8c757ce1ca9ef3aef7505",
    "resolver_commit_num": 0,
    "title": "BUG: read_html - file path cannot be pathlib.Path type",
    "body": "\r\n\r\nWhy do `read_excel()`, `read_csv()`, `to_excel()`, `to_csv()`, ... support `pathlib.Path` while `read_html()` doesn't?",
    "labels": [
      "Bug",
      "IO HTML",
      "good first issue"
    ],
    "comments": [
      "thanks @simon-spier0 for the report!\r\n\r\nAn easy fix would be to call `stringify_path` (from pandas/io/common.py) in `read_html`.",
      "> thanks @simon-spier0 for the report!\r\n> \r\n> An easy fix would be to call `stringify_path` (from pandas/io/common.py) in `read_html`.\r\n\r\nHi, I'm new here and want to start my first contribution. Can I take this issue? :)  @twoertwein \r\n\r\nThanks!",
      "@inspurwusixuan  of course!"
    ],
    "events": [
      "labeled",
      "labeled",
      "unlabeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files": 3,
    "additions": 14,
    "deletions": 1,
    "changed_files_list": [
      "doc/source/whatsnew/v1.2.0.rst",
      "pandas/io/html.py",
      "pandas/tests/io/test_html.py"
    ]
  }
]