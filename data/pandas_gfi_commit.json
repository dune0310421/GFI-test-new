[
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":1026,
    "reporter":"echlebek",
    "created_at":1334162543000,
    "closed_at":1336413754000,
    "resolver":"kisielk",
    "resolved_in":"0b5a0078c8774a3563f78a0ede22ff6a2a2d5c07",
    "resolver_commit_num":1,
    "title":"Indexing with namedtuple is broken",
    "body":"Although it is possible to index MultiIndexed DataFrames with multiple index columns, one or more of which have a compound type, it is not possible to index an Indexed DataFrame with a compound type for its column, nor is it possible to index a MultiIndexed Dataframe with a single column that has a compound type.\n\ntl;dr - I can't index a DataFrame with a namedtuple, even though I can create one.\n\nIn the first example, I try to index a dataframe with a namedtuple with a regular Index, which fails.\n\nIn the second example, I index a dataframe with a tuple of namedtuples (MultiIndex), which succeeds.\n\nIn the third example, I try to index a dataframe with a length-1 tuple of namedtuples, again with a MultiIndex, which fails.\n\n\n",
    "labels":[

    ],
    "comments":[
      "The issue looks a bit more complicated now...\n\nFirst of all, we realized the above test is reporting false positive because of #1069\n\nSecondly, an additional problem lies [here](https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/core\/indexing.py#L168). In particular, `_is_list_like` prevents using any iterable object as an `Index` key.\n\nAt this point, it's a question of where you want to go with the indexing interface. I think might be reasonable to limit the types (aside from `Index` itself) used for supplying index sequences to, say, `tuple`, `list` and `numpy.array`. The upside is not having to think about adding more exceptions (currently there's `basestring`, plus, in our case, a tuple subclass); the downside is not supporting arbitrary iterables such as generators. I would personally be in favour of the former because it is the simplest of the two (internal logic and behaviour-wise) in the long run.\n"
    ],
    "events":[
      "subscribed",
      "subscribed",
      "subscribed",
      "closed",
      "subscribed",
      "commented",
      "subscribed",
      "referenced",
      "reopened"
    ],
    "changed_files":2,
    "additions":10,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":826,
    "reporter":"wesm",
    "created_at":1330114725000,
    "closed_at":1339437215000,
    "resolver":"paddymul",
    "resolved_in":"0cbee1253ee406792f0f7cf2529a59701b498377",
    "resolver_commit_num":0,
    "title":"Try to workaround Yahoo! finance 404 errors, better error messages",
    "body":"randomly get this crap\n\n\n",
    "labels":[

    ],
    "comments":[
      "What is the desired behavior for calling code when yahoo returns a 404?  Should this call fail?  Retry?  \n",
      "It should probably retry once or twice and then raise an exception if all 2-3 attempts fail\n"
    ],
    "events":[
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "referenced",
      "cross-referenced"
    ],
    "changed_files":2,
    "additions":46,
    "deletions":11
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":1805,
    "reporter":"twiecki",
    "created_at":1345741284000,
    "closed_at":1347071327000,
    "resolver":"minrk",
    "resolved_in":"7eaf5ca6daec0159c8d7df344579012e12758ee3",
    "resolver_commit_num":0,
    "title":"Include .c files in repo.",
    "body":"`pip install git+`\n\ndoes currently not work because the .pyx files are not being cythonized (not sure why). An easy fix is to include .c files in the git repo which should make it easier for people to deploy.\n\nI used a simple try: import cython in setup.py that cythonizes if cython is installed and uses the .c files otherwise:\n\n-devs\/hddm\/blob\/develop\/setup.py#L4\n",
    "labels":[

    ],
    "comments":[
      "I don't want to include the .c files in the repo because of the diff noise. @minrk do you know what could be wrong?\n",
      "I might.  @twiecki - do you have vanilla setuptools or distribute?\n\nI encountered this or similar in pyzmq, where old-style setuptools will actually explicitly convert all your `.pyx` extensions back to `.c` if _pyrex_ is not importable.  [my workaround](https:\/\/github.com\/zeromq\/pyzmq\/commit\/e9f623981c77f60663656559d38d2fd1427ff18e).  distribute doesn't have this problem.\n\nI can confirm on my system that a virtualenv created with setuptools cannot run the install above, but one with distribute instead can, so it's probably the same issue.\n",
      "@minrk Yes, it's in a virtualenv. Not sure what it was created with (used virtualenvwrapper), it might have both actually.\n",
      "Try this, then:\n\n```\npip install git+https:\/\/github.com\/minrk\/pandas.git@badsetuptools\n```\n",
      "(if it works, I'll do a PR)\n",
      "@minrk: yes, that did the trick\n\ncythoning pandas\/src\/generated.pyx to pandas\/src\/generated.c\n\nThanks!\n\nOn Thu, Aug 23, 2012 at 2:34 PM, Min RK notifications@github.com wrote:\n\n> (if it works, I'll do a PR)\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps:\/\/github.com\/pydata\/pandas\/issues\/1805#issuecomment-7979354.\n",
      "See #1806\n"
    ],
    "events":[
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "commented",
      "cross-referenced"
    ],
    "changed_files":1,
    "additions":8,
    "deletions":0
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":1897,
    "reporter":"erg",
    "created_at":1347391404000,
    "closed_at":1347461620000,
    "resolver":"erg",
    "resolved_in":"15717e7bf9b249efd9e16bad4104c1578fcc642d",
    "resolver_commit_num":0,
    "title":"pandas.rolling_min\/max fail if window size is larger than input array",
    "body":"This is a regression since July 12.\n\n\n",
    "labels":[

    ],
    "comments":[

    ],
    "events":[
      "referenced",
      "cross-referenced"
    ],
    "changed_files":2,
    "additions":54,
    "deletions":27
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":2226,
    "reporter":"quintusdias",
    "created_at":1352662954000,
    "closed_at":1352695058000,
    "resolver":"justincjohnson",
    "resolved_in":"a72d886ed1d9b715844c2742a7b06b47abdf9e21",
    "resolver_commit_num":0,
    "title":"Better error message in pandas.core.index",
    "body":"If sortlevel is called with an out-of-range level, the error message is a bit confusing, particularly if the level is equal to the maximum number of levels.\n\n\n",
    "labels":[

    ],
    "comments":[

    ],
    "events":[

    ],
    "changed_files":1,
    "additions":2,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":4131,
    "reporter":"davidshinn",
    "created_at":1373030224000,
    "closed_at":1373084254000,
    "resolver":"davidshinn",
    "resolved_in":"1c6440e1fa01b0d637ddc69cfbb55a6d85ddb483",
    "resolver_commit_num":1,
    "title":"Excel parser cannot change *keep_default_na* value",
    "body":"The parse method in the ExcelFile class does not pass the argument _keep_default_na_ to the TextParser from pandas.io.parsers.  There is no clean way to override the default na values when parsing excel files with the current code.\n\nMy specific problem involves \"NA\" as a typical value for \"North America\" in excel files.\n",
    "labels":[

    ],
    "comments":[
      "@davidshinn I think #4139 should fix this - just changed `parse` and `_parse_excel` to pass keyword arguments through\n",
      "@jtratner This fixes the problem perfectly and other potential kwds missing from upstream functions. I'm extremely impressed with the quick commits.  On another note, I noticed that because this doesn't specifically add the keyword argument, the feature is not as visible through ipython [tab completion](http:\/\/ipython.org\/ipython-doc\/stable\/interactive\/tutorial.html#tab-completion) and [introspection](http:\/\/ipython.org\/ipython-doc\/stable\/interactive\/tutorial.html#exploring-your-objects).  However from my perspective, the main issue is closed.  Maybe I'll tackle those niceties myself.  Thanks.\n"
    ],
    "events":[
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files":1,
    "additions":10,
    "deletions":0
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":6645,
    "reporter":"jreback",
    "created_at":1394894260000,
    "closed_at":1395748111000,
    "resolver":"jsexauer",
    "resolved_in":"599156fe06ca7f4bc7f87fca982f561f0ec675a9",
    "resolver_commit_num":1,
    "title":"DEPR: deprecate cols \/ change to columns in to_csv \/ to_excel",
    "body":"related is #6581\n",
    "labels":[
      "Deprecate"
    ],
    "comments":[
      "cc @jsexauer\n",
      "I'm trying to convert this issue into a PR using the API:\n\nURI:  https:\/\/api.github.com\/repos\/pydata\/pandas\/pulls\nPOST Payload:  `{\"head\": \"jsexauer:fix6645\", \"base\": \"master\", \"issue\":\"6645\"}`\nResponse: \n\n```\n{\n  \"message\": \"Validation Failed\",\n  \"documentation_url\": \"http:\/\/developer.github.com\/v3\/pulls\/#create-a-pull-request\",\n  \"errors\": [\n    {\n      \"resource\": \"PullRequest\",\n      \"field\": \"issue\",\n      \"code\": \"unauthorized\"\n    }\n  ]\n}\n```\n\nI've done this before successfully but now it says I'm unauthorized.  Any idea why this may be?\n",
      "never tried using the API\nI just go to the branch an pull it on the website\n",
      "But won't that create a new issue number?\n",
      "yep\nyou want to do that\nthe pr will then close this issue \n"
    ],
    "events":[
      "milestoned",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "unlabeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files":11,
    "additions":206,
    "deletions":106
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":7758,
    "reporter":"fulmicoton",
    "created_at":1405426882000,
    "closed_at":1405511833000,
    "resolver":"fulmicoton",
    "resolved_in":"a797b28c87d90a439dfa2c12b4a11e62bf0d6db2",
    "resolver_commit_num":0,
    "title":"astype(unicode) does not work as expected",
    "body":"astype unicode seems to call str, so that the following code throws\n\n\n\nraises :\n\n\n",
    "labels":[
      "Unicode",
      "Dtypes",
      "Enhancement",
      "Bug"
    ],
    "comments":[
      "you can do: `df['somecol'].values.astype('unicode')`\n\nwhat are you doing with this?\n\npandas keeps all string-likes as `object` dtype so this is really only for external usage\n",
      "I have a method that detects whether a column should be considered as a category based on its type and cardinality. Columns that are considered as categories are casted into unicode object.\n\nI know how to workaround this issue, but I thought I should report what I thought was a bug.\n\nLet me know if you need more information.\n",
      "ok, this could be more informative, but its fundamentally an issue. This would return a numpy array (and NOT a series, and that would simply recast, and lose the cast to unicode).\n\nI think that is a bit odd though. What do you think should happen?\n",
      "Ideally, I would have either wanted the cast to work as python unicode() function.\nThat is : returned object are always of the \"unicode\" type.\n- Unicode objects are left unchanged.\n- Numbers are stringified into unicode strings.\n- str object are decoded using the default encoding and a unicode object is returned.\n\nDoes that make sense in Pandas?\n",
      "@fulmicoton Why do you need to convert to unicode? Do you have things that are convertible to unicode but aren't already converted? Can you give a more detailed example that illustrates why you need to do this. I think I'm just missing something. \n",
      "This could all be done I think (may need to allow an `encoding` argument for your 3rd bullet.\nKeep in mind that current pandas does not have a unicode type per-se (str and unicode are stored as `object` dtype), but its really not a big deal, as when a `unicode` dtype is presented it can simply be inferred.\n\nhere's a picture of the internal structure:\n\n```\nIn [16]: df\nOut[16]: \n  somecol\n0      \u9069\u5f53\n\nIn [17]: df._data\nOut[17]: \nBlockManager\nItems: Index([u'somecol'], dtype='object')\nAxis 1: Int64Index([0], dtype='int64')\nObjectBlock: slice(0, 1, 1), 1 x 1, dtype: object\n\nIn [18]: df._data.blocks[0]\nOut[18]: ObjectBlock: slice(0, 1, 1), 1 x 1, dtype: object\n\nIn [19]: df._data.blocks[0].values\nOut[19]: array([[u'\\u9069\\u5f53']], dtype=object)\n\nIn [20]: pd.lib.infer_dtype(df._data.blocks[0].values)\nOut[20]: 'unicode'\n```\n",
      "@fulmicoton interested in doing a pull-request for this?\n",
      "@cpcloud Just having a piece of code trying to coerce a bunch of columns marked as categorical into unicode strings. Some of them are already unicode, some of them have been detected as int but have such a low cardinality I want to handle them as categories.\nThey are getting dummified after... So it's important they all end up as unicode string at one point or another.\n",
      "@jreback I'll take a look at that tonight.\n",
      "@fulmicoton you might wasn to explore this as well (just merged in): http:\/\/pandas-docs.github.io\/pandas-docs-travis\/categorical.html. Prob not a lot of tests for unicode (but it should work)\n",
      "Here is the pull requests. I didn't have to use infer_dtype, so I hope I didn't do anything wrong.\n"
    ],
    "events":[
      "commented",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "commented",
      "referenced"
    ],
    "changed_files":4,
    "additions":44,
    "deletions":3
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":6852,
    "reporter":"jreback",
    "created_at":1397072984000,
    "closed_at":1412476270000,
    "resolver":"asobrien",
    "resolved_in":"ab97e0a3fa542416fc28db3c55cea4f3ce640f19",
    "resolver_commit_num":0,
    "title":"ENH: showing memory used",
    "body":"maybe a function `nbytes()` to return memory usage\n\nmaybe as a tuple : the Series in [6] and then a series of the axis data?\n\n\n",
    "labels":[
      "Ideas",
      "Performance",
      "Low-Memory",
      "Good as first PR"
    ],
    "comments":[
      "my 2c. i think `meminfo` should return your series and a property `nbytes` should return `df.meminfo().sum()`\n",
      "Using the following DataFrame as an example:\n\n``` python\nIn [11]:  df = pd.DataFrame({ 'float' : np.random.randn(10000000), 'int' : np.random.randint(0,5,size=10000000), 'date' : Timestamp('20130101'), 'string' : 'foo', 'smallint' : np.random.randint(0,5,size=10000000).astype('int16') })\n```\n\nI have implemented `meminfo()` as as DataFrame method, resulting in a Series. Note that units can be specified:\n\n``` python\n\nIn [13]: # Default units = B\nIn [14]: df.meminfo()\nOut[14]: \ndate        80000000\nfloat       80000000\nint         80000000\nsmallint    20000000\nstring      80000000\ndtype: float64\n\nIn [15]: df.meminfo(units=\"MB\")\nOut[15]: \ndate        76.293945\nfloat       76.293945\nint         76.293945\nsmallint    19.073486\nstring      76.293945\ndtype: float64\n```\n\nTotal memory usage of the DataFrame has been implemented as part of `df.info()` where the units representation can be specified:\n\n``` python\nIn [16]: df.info()             \n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 10000000 entries, 0 to 9999999\nData columns (total 5 columns):\ndate        datetime64[ns]\nfloat       float64\nint         int64\nsmallint    int16\nstring      object\ndtypes: datetime64[ns](1), float64(1), int16(1), int64(1), object(1)\nmemory_usage: 340000000.00 B\n\nIn [18]: df.info(mem_unit='MB')\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 10000000 entries, 0 to 9999999\nData columns (total 5 columns):\ndate        datetime64[ns]\nfloat       float64\nint         int64\nsmallint    int16\nstring      object\ndtypes: datetime64[ns](1), float64(1), int16(1), int64(1), object(1)\nmemory_usage: 324.25 MB\n```\n\nAny more thoughts?\n",
      "I think that would be useful!\n\nwould need a couple of display options I think:\n- default unit (MB?)\n- whether to display this by default (in say df.info()), default False for now\n",
      "Let's use MB as the default unit and add a `memory_usage` argument to the DataFrame `info()` method to specify unit type (where default is None).\n\nTotal memory usage of the DataFrame (by default in MB) can always be determined by using:\n\n``` python\ndf.meminfo().sum()\n```\n",
      "Seems like it would be easy to infer the unit by taking the log10 of the number of bytes, dividing by 10 to that number and using the bisect module to lookup the corresponding name of the unit in an array. \n",
      "I have units mapped in a dictionary with unit options consisting of (B, KB, MB, GB, TB, PB)\n\n``` python\n        byte_conv = {\n                    \"B\": 2**0,\n                    \"KB\": 2**10,\n                    \"MB\": 2**20,\n                    \"GB\": 2**30,\n                    \"TB\": 2**40,\n                    \"PB\": 2**50}\n```\n\nYet, this requires an explicit indication of desired units (as implemented [here](https:\/\/github.com\/asobrien\/pandas\/blob\/df-mem-info\/pandas\/core\/frame.py#L1489). Are you suggesting to use a method to automatically infer the appropriate unit based on DataFrame size?\n",
      "Yep. Just an intuition, haven't thought it out completely. \n",
      "Inferring units would be nice for `df.info()` but not desirable for the machine readable `df.meminfo()`.\n",
      "@shoyer `df.meminfo()` would have no arguments, so that the series would always return memory usage in columns in terms of bytes. In this manner, usage would be consistent.\n\n@cpcloud a [small function](http:\/\/stackoverflow.com\/a\/1094933) could be used to infer the memory usage in `df.info()` with the appropriate order of magnitude if a`memory_usage` argument is `True`.\n\n``` python\ndef sizeof_fmt(num):\n    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n        if num < 1024.0:\n            return \"%3.1f %s\" % (num, x)\n        num \/= 1024.0\n    return \"%3.1f %s\" % (num, 'PB')\n```\n"
    ],
    "events":[
      "milestoned",
      "labeled",
      "labeled",
      "cross-referenced",
      "labeled",
      "labeled",
      "unlabeled",
      "labeled",
      "unlabeled",
      "commented",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files":7,
    "additions":188,
    "deletions":7
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":9708,
    "reporter":"flying-sheep",
    "created_at":1427112707000,
    "closed_at":1427324218000,
    "resolver":"flying-sheep",
    "resolved_in":"f7c7ee0e92c61870626256eea93f64fda940cb95",
    "resolver_commit_num":0,
    "title":"Allow to select index in drop_duplicates and duplicated",
    "body":"there\u2019s no way to drop rows with duplicated index using `drop_duplicates`.\r\n\r\nwe\u2019d have to add a copy of the index as column, or do this:\r\n\r\n\r\n",
    "labels":[
      "Docs",
      "Groupby"
    ],
    "comments":[
      "Typically I'll use a `df.groupby(level=0).last()` (or more typically `.first()`). It works fine, but a groupby isn't necessarily the first thought for deduplication.\n\nI'm +0 on whether we should have a dedicated method for this.\n",
      "As @TomAugspurger indicates the following are equivalent.\n\nI suppose the `drop_duplicates` section could have this an alterative example. If you would like to pull-request for a doc update would be ok.\n\n```\nIn [6]: df = pd.DataFrame({'A' : range(4), 'B' : list('aabb')})            \n\nIn [7]: df                                                                 \nOut[7]:                                                                    \n   A  B                                                                    \n0  0  a                                                                    \n1  1  a                                                                    \n2  2  b                                                                    \n3  3  b                                                                    \n\nIn [9]: df2 = df.set_index('B')                                            \n\nIn [10]: df2                                                               \nOut[10]:                                                                   \n   A                                                                       \nB                                                                          \na  0                                                                       \na  1                                                                       \nb  2                                                                       \nb  3   \n\n```\n\n```\nIn [13]: df2.groupby(level=0).first()                        \nOut[13]:                                                     \n   A                                                         \nB                                                            \na  0                                                         \nb  2                                                         \n\nIn [16]: df2.reset_index().drop_duplicates(subset='B',take_last=False).set_index('B')                                                      \nOut[16]:                                                                                                                                   \n   A                                                                                                                                       \nB                                                                                                                                          \na  0                                                                                                                                       \nb  2\n```\n",
      "sorry, i don\u2019t get it. you mean i should add the second code block as exemple to the docs?\n",
      "I would add the groupby method as an alternative as its is another common way of performing this task \n",
      "to which file? `indexing.rtf`?\n",
      "http:\/\/pandas.pydata.org\/pandas-docs\/stable\/indexing.html#duplicate-data\n (which is in `indexing.rst`)\n"
    ],
    "events":[
      "commented",
      "closed",
      "reopened",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "referenced",
      "cross-referenced"
    ],
    "changed_files":1,
    "additions":10,
    "deletions":0
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":9807,
    "reporter":"hayd",
    "created_at":1428113110000,
    "closed_at":1429017585000,
    "resolver":"jcrist",
    "resolved_in":"30580e7acd6cafd55616d91bedbb8999d65177f5",
    "resolver_commit_num":1,
    "title":"Groupby mean transform not converting to float",
    "body":"\n\n\n",
    "labels":[
      "Bug",
      "Groupby",
      "Dtypes",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments":[

    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "referenced",
      "cross-referenced"
    ],
    "changed_files":3,
    "additions":12,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":7872,
    "reporter":"hayd",
    "created_at":1406662617000,
    "closed_at":1429021183000,
    "resolver":"hoh",
    "resolved_in":"8fe1cf6d1931071c378fcc3170b82a145a037da0",
    "resolver_commit_num":0,
    "title":"Give all NotImplementedErrors a description",
    "body":"Currently some NotImplementedErrors don't have a description (most do!) .\n\n\n\nI think this would be pretty useful, and makes it less confusing for users, and at least they would be pointed in the right direction to request said feature (and know which bit of specifically is NotImplemented)...\n\nYou can find them all with grep (probably more usefully the same regex in your text editor):\n\n\n",
    "labels":[
      "Good as first PR",
      "Error Reporting",
      "PEP8",
      "Docs",
      "Effort Medium"
    ],
    "comments":[
      "Fix in #7899 .\n\nI just noticed the PEP8 label. \n`pep8 | wc -l` gave me 17687 pep8 errors. \n",
      "no pepping\n",
      "Starting to work on this\n"
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "cross-referenced",
      "commented",
      "commented",
      "milestoned",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "labeled",
      "commented",
      "referenced",
      "cross-referenced"
    ],
    "changed_files":20,
    "additions":108,
    "deletions":72
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":8735,
    "reporter":"onesandzeroes",
    "created_at":1415161471000,
    "closed_at":1440785816000,
    "resolver":"ringw",
    "resolved_in":"59da7811fa567352cbbab5f10a296e543ec9e1fe",
    "resolver_commit_num":1,
    "title":"Strange output from `DataFrame.apply` when applied func creates a dict",
    "body":"Just had something odd come up while trying to come up with something for [this SO question](-pandas-dataframe-rows-to-a-pandas-series).\n\nIf we use `DataFrame.apply()` to try and create dictionaries from the rows of a dataframe, it seems to return the `dict.values()` method rather than returning the dict itself.\n\n\n\nLooks like it's probably something to do with trying to grab the `values` attribute when the output of the applied function is a Series or something similar.\n\nLibrary versions:\n\n\n",
    "labels":[
      "API Design",
      "Bug"
    ],
    "comments":[
      "This inference is done in cython. It is indeed trying to get the values attribute (e.g. `_values_from_object`). I suppose you could fix it.\n\nDo you want something like this?\n\n```\nIn [1]: df = pd.DataFrame({'k': ['a', 'b', 'c'], 'v': [1, 2, 3]})\n\nIn [2]: def f(row):\n   ...:         return pd.Series({row['k']: row['v']})\n   ...: \n\nIn [3]: df.apply(f,axis=1)\nOut[3]: \n    a   b   c\n0   1 NaN NaN\n1 NaN   2 NaN\n2 NaN NaN   3\n```\n",
      "Hi all, I've run into the same issue myself. I actually need to pass a list of dicts for each row as my output, and it would be nice to be able to do `list(df.apply(lambda x: x.to_dict(), 0))`. This would require `df.apply` to return a Series of dtype object, where the elements are dicts.\n\n`apply` returning a Series of dicts would be consistent with the behavior when the passed function returns an object Pandas doesn't understand (apparently, any non-numeric without a `values` attribute). I guess I can add a check to `_values_from_object` to make sure the input is, say, a subclass of PandasObject.\n"
    ],
    "events":[
      "commented",
      "labeled",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "labeled",
      "referenced",
      "referenced"
    ],
    "changed_files":4,
    "additions":29,
    "deletions":6
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":11963,
    "reporter":"Tux1",
    "created_at":1451996457000,
    "closed_at":1452630286000,
    "resolver":"Tux1",
    "resolved_in":"4437e9c9e161b884450e8027daa3622f74c4b9e7",
    "resolver_commit_num":1,
    "title":"Timestamp rounding wrong implementation",
    "body":"Hi,\nI was looking at the recent ENH #4314 and there is a wrong implementation of `round` method for datetime.\n\n\n\nYou are flooring the nano timestamp instead of rounding. You should replace `np.floor` by `np.round`.\nAlso I propose to add `floor` and `ceil` method to Timestamp in order to have the same behavior that `float`. (It is very usefull to get the upper\/lower bound for a specific freq, likely more usefull than round)\n\nDo you agree @jreback ? \n",
    "labels":[
      "Bug",
      "Timeseries",
      "Timedelta"
    ],
    "comments":[
      "hmm, though looks right. Will need some specific tests that catch this.\n\nI would add addtl functions `.ceil` & `.floor` (with the same signature). You can of course re-use the impl completely. (just subst the functions). (maybe move the impl to a private `._round` which takes the rounding function).\n\nThis is used in `DatetimeIndex\/TimedeltaIndex`, `Timestamp`, `Timedelta` (the scalars have slightly diff impl).\n\nJust add the this issue number onto the existing note in whatsnew.\n",
      "Yes, I will try to do it\n",
      "@Tux1 gr8!\n\nalso need to add for `.dt` as well (this is trivial, just add to the accessors list, and test in test_series)\n",
      "yeah, I have looked at your previous commit on that\n"
    ],
    "events":[
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced"
    ],
    "changed_files":9,
    "additions":156,
    "deletions":49
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":11786,
    "reporter":"mrocklin",
    "created_at":1449504754000,
    "closed_at":1453233726000,
    "resolver":"jdeschenes",
    "resolved_in":"567bc5ceb33d2147ca68b6eee9b180e6059ae247",
    "resolver_commit_num":1,
    "title":"read_csv in multiple theads causes segmentation fault",
    "body":"The following script causes a segfault on my machine\n\n\n\n\n\nPython 3.4, Pandas 0.17.1, Ubuntu 14.04\n",
    "labels":[
      "Bug",
      "CSV",
      "Prio-high",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments":[
      "FWIW on OSX that script just hangs at 99% CPU use. pandas 0.17.1, python 3.5.\n",
      "cc @jdeschenes can you have a look\n",
      "@mrocklin thanks for the repro!\n",
      "I think I found the issue, see my pull request. The issue was caused by a misplaced PyGilState_ensure(It was called after a Py_XDECREF being called.\n\nUsing read_csv with threads on such an object might have a big impact on performance.\n"
    ],
    "events":[
      "cross-referenced",
      "renamed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "labeled",
      "cross-referenced",
      "referenced",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files":3,
    "additions":85,
    "deletions":3
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12048,
    "reporter":"Sereger13",
    "created_at":1452870620000,
    "closed_at":1453675208000,
    "resolver":"Sereger13",
    "resolved_in":"ab3291d7ddbe821733f199c6ce414d7a6b9c18e0",
    "resolver_commit_num":0,
    "title":"ERR: read_csv with dtype specified on empty data",
    "body":"File has this content:\n\n\n\nWhen I do `pd.read_csv(filepath)` - all looks fine, however if I specify _skiprows_ and _dtype_ parameters it fails with the following error:\n\n\n\n> AttributeError: type object 'str' has no attribute 'items'\n\n---\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.10.final.0\npython-bits: 64\nOS: Linux\nOS-release: 2.6.18-238.9.1.el5\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US\n\n**pandas: 0.17.1**\n",
    "labels":[
      "Error Reporting",
      "CSV",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "This is equivalent to this. So this doesn't have any data, so the dtype converter should handle this properly.\n\n```\nIn [13]: read_csv(StringIO(data),header=0,skiprows=1)\nOut[13]: \nEmpty DataFrame\nColumns: [3, 3.1]\nIndex: []\n\nIn [14]: data\nOut[14]: 'A,A\\n3,3'\n```\n\nSo an even simpler repro is this.\n\n```\nIn [16]: read_csv(StringIO('A,B'),dtype=str)\nAttributeError: type object 'str' has no attribute 'items'\n```\n\ncare to do a pull-request to fix?\n",
      "Prepared the fix locally but getting this error when trying to send pull-request: \n'It seems you do not have permission to push your changes to this repository'.\n",
      "see contributing docs [here](http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.17.1\/contributing.html)\n\nyou need to fork, then push to your branch and open a PR.\n\nyou are trying to push to master\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "renamed",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":19,
    "deletions":2
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":9746,
    "reporter":"amcpherson",
    "created_at":1427604100000,
    "closed_at":1454182076000,
    "resolver":"amcpherson",
    "resolved_in":"de460567ecde0f2a846116e210541d52c8f682f2",
    "resolver_commit_num":0,
    "title":"fill_value kwarg for unstack",
    "body":"Currently:\n\n\n\nIf I want to fill with -1, i need to `fillna` and then `astype` back to `int`.  Ideally:\n\n\n",
    "labels":[
      "Missing-data",
      "API Design",
      "Usage Question"
    ],
    "comments":[
      "You can do this by specifying the `downcast` keyword. This is NOT automatic as a general operation this _can_ be expensive.\n\n```\nIn [10]: df.set_index(['x','y']).unstack().fillna(-1,downcast='infer')\nOut[10]: \n   z   \ny  j  k\nx      \na  0  1\nb  2 -1\n\nIn [11]: df.set_index(['x','y']).unstack().fillna(-1,downcast='infer').dtypes\nOut[11]: \n   y\nz  j    int64\n   k    int64\ndtype: object\n```\n",
      "There may be some merit to this being allowed directly, even if the functionality can be accomplished with a series of operations.  For instance, when trying to limit memory usage on a big dataset, perhaps it would be preferable to keep the data as `np.int8`.\n\n``` python\nIn [15]: idx = np.array([0, 0, 1], dtype=np.int32)\n\nIn [16]: idx2 = np.array([0, 1, 0], dtype=np.int8)\n\nIn [17]: value = np.array([0, 1, 2], dtype=np.int8)\n\nIn [18]: df = pd.DataFrame({'idx':idx, 'idx2':idx2, 'value':value})\n\nIn [19]: df.dtypes\nOut[19]:\nidx      int32\nidx2      int8\nvalue     int8\ndtype: object\n\nIn [20]: df.set_index(['idx', 'idx2']).unstack().dtypes\nOut[20]:\n       idx2\nvalue  0       float64\n       1       float64\ndtype: object\n```\n\nAfter the unstack my data table is suddenly much larger than necessary.\n\nAlso, from looking at the code this would be fairly trivial to implement, without much impact on existing code.\n",
      "@amcpherson ok, if you can find a reasonable way to do this w\/o affecting perf then would be ok to have a `fill_value` argument.\n"
    ],
    "events":[
      "commented",
      "closed",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "reopened",
      "milestoned"
    ],
    "changed_files":7,
    "additions":200,
    "deletions":13
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12180,
    "reporter":"DGrady",
    "created_at":1454091375000,
    "closed_at":1454357356000,
    "resolver":"DGrady",
    "resolved_in":"34d98391f450fc4e083784f164ad0f426710af9e",
    "resolver_commit_num":0,
    "title":"Series.str.get_dummies fails if one of the categorical variables is called 'name'",
    "body":"This works as expected:\n\n\n\nHowever, if any of the categorical variables is named exactly `'name'`, then there's a problem.\n\n\n\nThis is presumably related to the `pandas\/core\/strings.py` code in the stacktrace.\n\n\n",
    "labels":[
      "Bug",
      "Strings",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "I think this is a bug in `_wrap_result`. The `getattr(result,'Name',None)` only should be checked if 'name' is in `_metadata` IOW, its a `Series`.\n",
      "@DGrady want to take a crack at it?\n",
      "Absolutely \u2014 I'll take a look at it later today.\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":12,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12052,
    "reporter":"TomAugspurger",
    "created_at":1452876084000,
    "closed_at":1454795404000,
    "resolver":"MattRijk",
    "resolved_in":"e8ef4f8b517de8bff8983c52bd6e9d200a795f14",
    "resolver_commit_num":0,
    "title":"DOC: Put deprecation warning in convert_objects docstring",
    "body":"-docs\/version\/0.17.1\/generated\/pandas.DataFrame.convert_objects.html#pandas.DataFrame.convert_objects\n\nProbably other docstrings missing deprecations if anyone wants to track those down.\n",
    "labels":[
      "Docs",
      "Difficulty Novice",
      "Deprecate",
      "Effort Low"
    ],
    "comments":[
      "master list is in #6581 \n"
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced"
    ],
    "changed_files":1,
    "additions":17,
    "deletions":8
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12042,
    "reporter":"jaradc",
    "created_at":1452838282000,
    "closed_at":1454945338000,
    "resolver":"BranYang",
    "resolved_in":"62363d2ff2079f4c7e0cc69b0a9dc4dc579a5bd3",
    "resolver_commit_num":0,
    "title":"Pandas get_dummies() and n-1 Categorical Encoding Option to avoid Collinearity?",
    "body":"When doing linear regression and encoding categorical variables, perfect collinearity can be a problem.  To get around this, the suggested approach is to use n-1 columns.  It would be useful if `pd.get_dummies()` had a boolean parameter that returns n-1 for each categorical column that gets encoded.\n\nExample:\n\n\n\n\n\nInstead, I'd like to have some parameter such as `drop_first=True` in `get_dummies()` and it does something like this:\n\n\n\n**Sources**\n-categorical-data-into-numbers-with-pandas-and-scikit-learn\/\n-to-get-pandas-get-dummies-to-emit-n-1-variables-to-avoid-co-lineraity\n\n",
    "labels":[
      "Stats",
      "Reshaping"
    ],
    "comments":[
      "Sounds good, interested in submitting a pull request?\n",
      ":+1: \n"
    ],
    "events":[
      "labeled",
      "labeled",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned"
    ],
    "changed_files":3,
    "additions":173,
    "deletions":6
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":11555,
    "reporter":"jreback",
    "created_at":1447028682000,
    "closed_at":1455246456000,
    "resolver":"frankcleary",
    "resolved_in":"a424bb21e47ad8550a01bca6da8ed88356f1326b",
    "resolver_commit_num":0,
    "title":"DOC: pd.read_csv doc-string clarification",
    "body":"need to update the docs for `read_csv` to match the doc-string. Let's make sure they have the same text \/ examples as much as possible (and are in the same order and such).\n\nIn particular, the `header` option doesn't have `header='infer'` which is the default in the docs, but it exists in the doc-string.\n\nFurther, should show what this does:\n\n\n\nmeaning if the the `header` kw is not specified, this it is set to the first line if no `names` are specified, else `None`.\n",
    "labels":[
      "Docs",
      "Difficulty Novice",
      "CSV"
    ],
    "comments":[

    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files":4,
    "additions":384,
    "deletions":290
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12171,
    "reporter":"jreback",
    "created_at":1454018525000,
    "closed_at":1455715215000,
    "resolver":"troglotit",
    "resolved_in":"5d1857cf0df7555abf3c173e0053c29a5ccfd392",
    "resolver_commit_num":0,
    "title":"CLN: change getargspec -> signature (using from pandas.compat)",
    "body":"this eliminates some deprecation warnings\n\n\n",
    "labels":[
      "Difficulty Novice",
      "Compat",
      "Effort Low"
    ],
    "comments":[

    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":4,
    "additions":31,
    "deletions":8
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12336,
    "reporter":"toobaz",
    "created_at":1455577726000,
    "closed_at":1455801308000,
    "resolver":"rinoc",
    "resolved_in":"672fb146980c104f7b01b835f3839dac9d8cee1e",
    "resolver_commit_num":1,
    "title":"Initializing category with single value raises AttributeError",
    "body":"\n\n(Not a particularly brilliant use of categories, but they can then be concatenated with others.)\n",
    "labels":[
      "Bug",
      "Categorical",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments":[
      "yeah looks buggy.\n",
      "obviously the expected result\n\n```\nIn [2]: pd.Series(0, index=range(3)).astype(\"category\")\nOut[2]: \n0    0\n1    0\n2    0\ndtype: category\nCategories (1, int64): [0]\n```\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "cross-referenced"
    ],
    "changed_files":3,
    "additions":11,
    "deletions":0
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12409,
    "reporter":"multiloc",
    "created_at":1456080665000,
    "closed_at":1456239261000,
    "resolver":"multiloc",
    "resolved_in":"e45e3b42e42c756d52005988e26203602b992d5e",
    "resolver_commit_num":0,
    "title":"date_range breaks with tz-aware start\/end dates and closed intervals in 0.18.0.rc1",
    "body":"The following works on 0.17.1 but breaks on the release candidate 0.18.0.rc1\n\n\n\nOn 0.18.0.rc1:\n\n\n",
    "labels":[
      "Regression",
      "Timezones"
    ],
    "comments":[
      "The fix looks simple, I'll post a PR in a bit\n",
      "gr8!\n"
    ],
    "events":[
      "commented",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files":3,
    "additions":32,
    "deletions":2
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12278,
    "reporter":"joshlk",
    "created_at":1455100616000,
    "closed_at":1456239436000,
    "resolver":"Dorozhko-Anton",
    "resolved_in":"6b544de628fa56a62c4834922e7bcc5a22ba00bf",
    "resolver_commit_num":0,
    "title":"Performance: .unique \/ .nunique of categorical series slow on large data set",
    "body":"I've noticed that `Series.unique` and `Series.nunique` when used with a categorical series can be slow on large dataset. Presumably its not utilising the shortcuts:\n\n\n\nHeres an example in iPhython:\n\n\n\nIts significantly slower indicating its not using the above shortcut.\n\npd.show_versions()\n\n\n",
    "labels":[
      "Performance",
      "Categorical",
      "Docs",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "pls pd.show_versions()\nand an example \n",
      "Sorry @jreback more details have been added above\n",
      "so, `.unique` on a categorical has a couple of guarantees, namely that it is in the order of appearance, and it only includes values that are actually present, e.g.\n\n```\nIn [3]: s = Series(list('babc')).astype('category',categories=list('abcd'))\n\nIn [4]: s.unique()\nOut[4]: \n[b, a, c]\nCategories (3, object): [b, a, c]\n\nIn [6]: s.cat.categories\nOut[6]: Index([u'a', u'b', u'c', u'd'], dtype='object')\n```\n\nso if you want `.categories`, then just use that. \n",
      "I suppose it is possible to add an option (to all `.unique`), not just this one, to do some sort of `quick` uniqueness like this, but not sure of the utility beyond categoricals.\n",
      "I didn't realise that, I thought they were equivalent. Thanks\n",
      "It would be good to clarify this in the docs I think:\n- [ ] add a note to the docstring of `categories` that this includes all categories and not only those present in the data\n- [ ] add the above example (https:\/\/github.com\/pydata\/pandas\/issues\/12278#issuecomment-182426808) to the tutorial docs `categorical.rst`\n",
      "ok, let's repurpose this then. @joshlk want to do a PR?\n",
      "Can I get this one?\n",
      "sure\n"
    ],
    "events":[
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "closed",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "reopened",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":1,
    "additions":17,
    "deletions":0
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12393,
    "reporter":"phil20686",
    "created_at":1455882825000,
    "closed_at":1456452823000,
    "resolver":"MasonGallo",
    "resolved_in":"e1cec52695f29976538cd0f3d0b28f32b03b53c5",
    "resolver_commit_num":0,
    "title":"Confusing behaviour of df.empty",
    "body":"This is as much a documentation issue as anything else. Basically it seems confusing that df.empty != df.dropna().empty I.e. that a a dataframe consiting entirely of na is not treated as empty. Obviously this is a bit of an edge case, but it caused a bunch of failures for me when used eith pd.read_sql methods, as database tables will often have columns that are not available for partiicular entities, and so can return an entire series of na. \n\nIt seems to me that in all cases df.empty should be the same as df.dropna().empty, but I understand that opinions might differ on this point, but at least the behaviour should be clearly documented.\n",
    "labels":[
      "Docs",
      "Reshaping",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "Can you show an example, the following is works. I agree certainly could update the documentation (with some examples and such)\n\n```\nIn [8]: df = DataFrame({'A' : [np.nan]})\n\nIn [9]: df.empty\nOut[9]: False\n\nIn [10]: df.dropna().empty\nOut[10]: True\n```\n",
      "That is exactly the behavior I was questioning, I think out[9] should be True. It seems to me that a dataframe containing nothing by na cells is \"empty\" according to most definitions.....\n\nPhil\n",
      "_I_ certainly expected that df.empty would be true if a data frame contained nothing but na cells.\n",
      "no, that is not a normal definition of empty which is 0-len. nulls are real values which are placeholders. The key here is that you actually have a valid index. Changing this would involve the definition dependent on the data itself which is not a good thing. welcome to have a doc update with some examples though.\n",
      "Well its certainly not the case that df.empty is the same as len(df.index==0) e.g. \n\n```\ndf = pd.DataFrame([], index=[0,1,2])\nprint df.empty #True\nprint len(df.index==0) #False\n```\n\nAlso\n\n```\ndf = pd.DataFrame([], index=[0,1,2], columns=['A','B'])\ndf.empty #False\n```\n",
      "So not only do I dispute that len=0 is the semantic definition of empty, that doesn't appear to be the implementation anyway.\n",
      "Also stuff like: \n\n```\ndf = pd.DataFrame([], index=[0,1,2], columns=['A','B'])\ndf.set_value(1, 'A', 17)\ndf['B'].empty # False\n```\n\nwhich just seems plain wrong. If I have a column that I have never added any data to, it should not return false when asked if its empty. I guess under the hood when you specify an index and columns it autofills the dataframe somehow, and that results in this behaviour, but the definition of empty should really play nicely with the default dataframe constructor in these examples imo. Else its just confusing.\n",
      "its very simple. its empty only if all axes are len 0\n",
      "It sounds like you're looking for some other collection of `Series`. A `DataFrame` is a tabular collection, and it makes sense to look at the shape. \n",
      "@phil20686 You can eg use:\n\n```\nIn [15]: df['B'].isnull().all()\nOut[15]: True\n```\n",
      "@jrebeck my example shows that that is not the implemented behavior:\n\n```\ndf = pd.DataFrame([], index=[0,1,2])\nprint df.empty #True \nsum(df.shape) == 0 #false\ndf = pd.DataFrame([], columns=['A','B'])\nprint df.empty # True\nsum(df.shape) == 0 #false\n```\n\nI really find it super weird that pre-allocation should result in empty=False, e.g. if you concatenate an empty series with a non empty dataframe it will get preallocated and then extracting it means empty has changed from True to False. This seems very strange to me, in some abstract sense series C is the same object, but merely moving it around has changed its properties.\n\n```\ndf = pd.DataFrame([], index=[0,1,2], columns=['A','B'])\ndf.set_value(1, 'A', 17)\nseries = pd.Series(name=\"C\")\nprint series.empty\ndf2 = pd.concat([df,series], axis=1)\nprint df2[\"C\"].empty\n```\n\nAnyway, my main point is that this behavior should be documented, because its quite counter-intuitive, not to argue about definitions of empty.\n",
      "@phil20686 every one of those results is correct, what exactly is counter intuitive here?\n\nthere isn't any 'pre-allocation' at all. You have indices. If the indicies are 0 in any way (could be 1 dim or not) then you are empty, otherwise you are not.\n\nWhat exactly are you using `.empty`? To be honest I have rarely needed this. Most operations in pandas just work regardless if things are empty or not. I think the docs are fairly clear on [this](http:\/\/pandas.pydata.org\/pandas-docs\/stable\/basics.html#boolean-reductions).\n",
      "Um. I had a series that had .empty = True, I concat it with a dataframe, and then I extract the series, and then magically .empty=False? Even though at no time has the user added data to it?\n\nSimilarly, you can create a dataframe with either an index or columns but no values and its \"empty\", but if it has both and no values its \"non-empty\". \n\nYou don't think that is counter intuitive behavior? \n\nAnyway, I think most people would assume that empty == contains no data. That clearly isn't the case as it looks like its the same as \n\n```\nnot any(df.shape)\n```\n\nAnyway, my main point was that the documentation of Dataframe.empty should note these behaviors. \n",
      "@phil20686 one of the highlites of pandas is that it aligns data. When you put in a series it was empty, however, the concat realigned the Series to the other values in the DataFrame\n\n```\nIn [19]: df2\nOut[19]: \n     A    B   C\n0  NaN  NaN NaN\n1   17  NaN NaN\n2  NaN  NaN NaN\n\nIn [20]: df2.columns\nOut[20]: Index([u'A', u'B', u'C'], dtype='object')\n\nIn [21]: df2['C']\nOut[21]: \n0   NaN\n1   NaN\n2   NaN\nName: C, dtype: float64\n```\n\nthen [21] is clearly NOT empty; yes it is all null. Which is a MUCH more common operation. `.empty` is a very blunt instrument and not really used much in practice for this very reason. It is correct as far as it goes.\n",
      "@phil20686 In trying to clear some things up, I think we have to make a distinction between two points:\n- The definition of `empty` is clear**: it returns True if the length of one or all of the axes is 0 (you could see it as \"len(index) x len(columns) == 0\" for a dataframe). It is by defintion _not_ about having all NaNs or not. It is quite possible you find this not the best definition, but taking this definition as a starting point, all the return values in the examples you showed are consistent and as expected.\n- What _is_ maybe more surprising in some cases, leading to the unintuitive behaviour you described, is the way pandas fills Series\/DataFrames with NaNs (and once filled with NaNs, it is not empty anymore). Pandas will fill a DataFrame with NaNs once it has both an index and columns. And there are indeed operation (eg concat) where index\/columns can be added, leading to filling with NaNs. \n\nI just want to point out that I think the confusion you get has another root cause than the `empty` method. \nAnd the current `empty` method is just not the method you are looking for I think. It would be more something like `allnan` or `allnull`, which you can obtain with `isnull().all()`\n\n** _I don't say it is 'clear' in the docs, I mean in implementation_\n\nBut indeed, the docs of empty can certainly point that out. Do you want to do a PR to specify that this is not about NaNs ?\n",
      "I agree that the docs of empty could point this out (I actually had a student just ask me about this). Since it's been a few days, I can do a quick PR.\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced"
    ],
    "changed_files":1,
    "additions":37,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12176,
    "reporter":"wojdyr",
    "created_at":1454080558000,
    "closed_at":1456583285000,
    "resolver":"mfarrugi",
    "resolved_in":"1d6d7d4667806f1b7bca640ffc4eb368441e2471",
    "resolver_commit_num":0,
    "title":"confusing ImportError message in `__init__.py`",
    "body":"In some cases the error message could be more clear, as discussed here:\n#commitcomment-15762471\n\nmy example:\n\n\n",
    "labels":[
      "Build",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "yeh, prob need to go thru the deps (`pytz`, `dateutil`) and try importing them and give a nice message.\n`numpy` is already checked. Then if this is hit it would be a good message.\n"
    ],
    "events":[
      "commented",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":2,
    "additions":16,
    "deletions":3
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":8020,
    "reporter":"cpcloud",
    "created_at":1407946222000,
    "closed_at":1456584388000,
    "resolver":"jylin",
    "resolved_in":"c81d03b0bd98f0e946b4cc03b23da39b8949cedd",
    "resolver_commit_num":0,
    "title":"Make empty data construction error message less dev-y",
    "body":"When one tries to construct a `DataFrame` where the data column count and the passed in column count differ the error message is completely unintuitive unless you know something about the internal representation of the data inside of `NDFrame`s.\n\nI think we should at the very least make the dimensions in the error message consistent with how the frame _looks_ rather than how it's _implemented_.\n\nthis code:\n\n\n\nraises this exception:\n\n\n\nsee here for another example: \n",
    "labels":[
      "Error Reporting",
      "Good as first PR"
    ],
    "comments":[
      "sure\n"
    ],
    "events":[
      "labeled",
      "milestoned",
      "commented",
      "assigned",
      "milestoned",
      "demilestoned",
      "labeled",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":4,
    "additions":9,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":8848,
    "reporter":"jreback",
    "created_at":1416313564000,
    "closed_at":1456588970000,
    "resolver":"andrew-rosenfeld-ts",
    "resolved_in":"91967c89baffa03baa875934521c9e2fe978b124",
    "resolver_commit_num":0,
    "title":"ENH: add missing methods to .dt for Period",
    "body":"methods\n- to_timestamp\n- asfreq\n\nproperties\n- start_time\/end_time\n",
    "labels":[
      "Good as first PR",
      "API Design",
      "Period",
      "Compat",
      "Effort Low"
    ],
    "comments":[
      "@jreback I just want to make sure I understand this issue. If there's a series of Period objects, there are some additional methods desired on the .dt accessor? And these would be the same for PeriodIndex, as from looking at the code it seems that PeriodIndex is what's really behind the scenes of the .dt accessor?\n\nI should note there's a whole host of methods that are available to datetime_series.dt that aren't on period_series.dt, including: ceil\/floor\/round, is_{month,quarter,year}_{start,end}, microsecond, nanosecond, normalize, tz, tz_convert, tz_localize.\n",
      "Now that I've played with this code a bit, not sure a lot of those methods make sense for PeriodIndex\/period_series.dt. I think a new issue can be raised if some of them should be implemented. (a harder task than what this issue was, which was mostly just exposing on .dt what was already done on PeriodIndex)\n"
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "referenced",
      "commented"
    ],
    "changed_files":4,
    "additions":23,
    "deletions":3
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12386,
    "reporter":"jreback",
    "created_at":1455832456000,
    "closed_at":1456922760000,
    "resolver":"paul-reiners",
    "resolved_in":"eba78032e927d2680850c88190f3bafe18cac6ba",
    "resolver_commit_num":0,
    "title":"CLN: remove pandas.util.testing.choice",
    "body":"xref #12384 \n\nso I think this was for numpy < 1.7 compat.\n\nwe should remove `pandas.util.testing.choice`\n#L173\n\nthere might be some latent doc references\n",
    "labels":[
      "Clean",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "cc @carroux \n",
      "```\npandas\/tests\/frame\/test_query_eval.py\n99:        a = tm.choice(['red', 'green'], size=10)\n100:        b = tm.choice(['eggs', 'ham'], size=10)\n152:        a = tm.choice(['red', 'green'], size=10)\n153:        b = tm.choice(['eggs', 'ham'], size=10)\n246:        a = tm.choice(['red', 'green'], size=10)\n978:        a = Series(tm.choice(list('abcde'), 20))\n\npandas\/tests\/test_graphics_others.py\n638:            gender = tm.choice(['male', 'female'], size=n)\n712:            gender_int = tm.choice([0, 1], size=n)\n\npandas\/tests\/test_graphics.py\n63:            gender = tm.choice(['Male', 'Female'], size=n)\n64:            classroom = tm.choice(['A', 'B', 'C'], size=n)\n3864:            gender = tm.choice(['male', 'female'], size=n)\n\npandas\/tools\/tests\/test_merge.py\n239:            df = DataFrame({'a': tm.choice(['m', 'f'], size=3),\n241:            df2 = DataFrame({'a': tm.choice(['m', 'f'], size=10),\n248:            df = DataFrame({'a': tm.choice(['m', 'f'], size=3),\n251:            df2 = DataFrame({'a': tm.choice(['m', 'f'], size=10),\n257:            df = DataFrame({'a': tm.choice(['m', 'f'], size=3),\n259:            df2 = DataFrame({'a': tm.choice(['m', 'f'], size=10),\n\nvb_suite\/groupby.py\n146:obj = tm.choice(list('ab'), size=n).astype(object)\n```\n",
      "I think I have this finished.  See #12490.\n",
      "This is fixed in pull request #12505.  Tests have passed, it's all in one commit, and whatsnew has been added.\n"
    ],
    "events":[
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "milestoned",
      "demilestoned",
      "commented",
      "cross-referenced",
      "commented",
      "milestoned",
      "demilestoned",
      "referenced",
      "cross-referenced",
      "commented"
    ],
    "changed_files":7,
    "additions":22,
    "deletions":30
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":10789,
    "reporter":"max-sixty",
    "created_at":1439225034000,
    "closed_at":1457276279000,
    "resolver":"evectant",
    "resolved_in":"547c784c927a1affadc551d368e5ee0b2a1d32cb",
    "resolver_commit_num":0,
    "title":"ENH: Allow exponential weighing functions to specify alpha, in addition to span \/ com \/ halflife",
    "body":"Currently the exponential functions take one of three arguments to specify the length of backhistory. Each of these numerically converts to alpha. Outlined here: -docs\/stable\/computation.html#exponentially-weighted-moment-functions\n\nIs there a reason we don't allow users to just pass in `alpha`? This is how I think about weighing some of the time.\n\nI'm happy to do a PR in time; this issue is to solicit feedback.\n",
    "labels":[
      "Difficulty Novice",
      "API Design",
      "Numeric",
      "Effort Low"
    ],
    "comments":[
      "This would be useful - I also find it strange that alpha is always indirectly parameterized.\n",
      "this would be a pretty easy add. I don't know the original reason why this is, nor do I see the harm. As long as the docs are updated, ok by me.\n\nBut maybe need a note about why so many ways to specify the same thing (e.g. because like to think about it differently, maybe their is a reference somewhere, e.g. wikipedia or somesuch)\n"
    ],
    "events":[
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files":6,
    "additions":171,
    "deletions":76
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":10041,
    "reporter":"multiloc",
    "created_at":1430495998000,
    "closed_at":1457297063000,
    "resolver":"thejohnfreeman",
    "resolved_in":"60b307f2ea2c6ccdd58dfeaa0c1d6b308bd2b74b",
    "resolver_commit_num":1,
    "title":"Loss of nanosecond resolution when constructing Timestamps from str",
    "body":"Looks like a bug: When passing a YYYYMMDDTHHMMSS.f string to `pd.Timestamp`, the resolution seems limited to micro- rather than nano-seconds:\n\n\n\nThe behavior seems correct when using a different string format:\n\n\n\nUsing pandas 0.16.0, python 2.7.6\n",
    "labels":[
      "Timeseries",
      "Bug",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments":[
      "Because dateutil doesn't support nanosecond, it looks little difficult to cover all the cases to nanoseconds (see #7907).\n\nOne option is to prepare a nanosecond parser to support some popular date formats. For example, change (or prepare separate function like) `np_datetime_string.c` to allow formats with other separators or without separators (like suggested in #9714).\n\nA PR would be appreciated :)\n"
    ],
    "events":[
      "labeled",
      "commented",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "closed",
      "reopened",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":168,
    "deletions":140
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12572,
    "reporter":"ctyler-wapiti",
    "created_at":1457542629000,
    "closed_at":1457660344000,
    "resolver":"tonypartheniou",
    "resolved_in":"71b7237f233824deae249b26bf98fdb6d9ae26a3",
    "resolver_commit_num":1,
    "title":"SignedJwtAssertionCredentials has been removed from oauth2client version 2.00 breaking read_gbq",
    "body":"In pandas\/io\/gbq.py there is the function:\n\n`def _test_google_api_imports():\n\n\n\nThis will fail for users with oauth2client version higher than 2.0.0 since SignedJwtAssertionCredentials  has been removed from the module. \n\nGoogle suggests other ways of handling service account credentials. See:\n#v200\n",
    "labels":[
      "Google I\/O"
    ],
    "comments":[
      "can you `pd.show_versions()`\n",
      "cc @tworec \ncc @parthea \ncc @andrewryno\ncc @andrioni \ncc @jacobschaer\n",
      "> > > pd.show_versions()\n\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Windows\nOS-release: 8.1\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.18.0rc1\nnose: 1.3.7\npip: 8.1.0\nsetuptools: 20.2.2\nCython: None\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: None\nxarray: None\nIPython: None\nsphinx: None\npatsy: None\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: None\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: 0.9.2\napiclient: 1.3.1\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\n"
    ],
    "events":[
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "labeled",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "milestoned",
      "referenced",
      "referenced"
    ],
    "changed_files":7,
    "additions":74,
    "deletions":12
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12577,
    "reporter":"nickeubank",
    "created_at":1457560630000,
    "closed_at":1458134241000,
    "resolver":"OXPHOS",
    "resolved_in":"f7faee0865d682c81f07134570b863e7e1d75f85",
    "resolver_commit_num":0,
    "title":"BUG: Crosstab margins ignoring dropna",
    "body":"`crosstab` also has a bug -- it counts np.nan in margin totals even when `dropna=True`.\n\nAppears independent of #12569 and #4003\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.4.4.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.3.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.17.1\nnose: 1.3.7\npip: 8.1.0\nsetuptools: 20.2.2\nCython: 0.23.4\nnumpy: 1.10.4\nscipy: 0.16.1\nstatsmodels: None\nIPython: 4.0.1\nsphinx: 1.3.1\npatsy: 0.4.0\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.4.4\nmatplotlib: 1.4.3\nopenpyxl: 2.2.6\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.7.7\nlxml: 3.4.4\nbs4: 4.4.1\nhtml5lib: 0.999\n None\napiclient: None\nsqlalchemy: 1.0.9\npymysql: None\npsycopg2: None\nJinja2: 2.8\n",
    "labels":[
      "Bug",
      "Missing-data",
      "Reshaping",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "Shouldn't the expected output be?\n\n```\nOut[233]: \nb    3  4  All\na             \n1.0  1  0    1\n2.0  1  3    4\nAll  2  3    5\n```\n",
      "@sakulkar how is that different from issue report? Looks like we're saying same thing. \n",
      "I edited the top @nickeubank (it was incorrect)\n",
      "Oops thanks\n"
    ],
    "events":[
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented"
    ],
    "changed_files":3,
    "additions":68,
    "deletions":0
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12574,
    "reporter":"chris-b1",
    "created_at":1457547146000,
    "closed_at":1458136782000,
    "resolver":"drewfustin",
    "resolved_in":"a5670f2043251b4c9bd7f80573f7983722b4b2a8",
    "resolver_commit_num":0,
    "title":"BUG: Series with Categorical and dtype",
    "body":"xref #discussion_r55561598\n\n\n",
    "labels":[
      "Bug",
      "Reshaping",
      "Difficulty Novice",
      "Categorical",
      "Effort Low"
    ],
    "comments":[
      "@jreback I'm new to contributing to OS, let alone pandas. I think this is the fix that you're referencing. Let me know if not and I'll fix.\n"
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files":3,
    "additions":18,
    "deletions":2
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12591,
    "reporter":"xflr6",
    "created_at":1457690304000,
    "closed_at":1458222494000,
    "resolver":"xflr6",
    "resolved_in":"1893ffd3e97303a77b30ee05ab4e0033caa80b6a",
    "resolver_commit_num":1,
    "title":"API: rename Index.sym_diff to Index.symmetric_difference",
    "body":"Following builtin `set`, I would expect the method name to be `symmetric_difference`.\n\nHow about deprecating `sym_diff` in favour of this (see also #6016, #8226)?\n",
    "labels":[
      "API Design",
      "Difficulty Novice",
      "Effort Low",
      "Deprecate"
    ],
    "comments":[
      "@xflr6, well said, We should have included `sym_diff` -> `symmetric_difference` along with the `diff` -> `difference` change.\n\nWould you be interested in submitting a pull requests to deprecate `sym_diff` in favor of `symmetric_difference`?\n",
      "Sounds manageable, sure.\n"
    ],
    "events":[
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "unlabeled",
      "milestoned",
      "commented",
      "labeled",
      "cross-referenced"
    ],
    "changed_files":6,
    "additions":28,
    "deletions":20
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":10331,
    "reporter":"thrasibule",
    "created_at":1434041105000,
    "closed_at":1458563276000,
    "resolver":"markroth8",
    "resolved_in":"7e71a44a93cdcf9a1a4716ad9fdd4d83764ea4b3",
    "resolver_commit_num":0,
    "title":"Partial string matching for timestamps with multiindex",
    "body":"I'm trying to get a slice from a multiindex. I find the behavior pretty inconsistent. Here is a simple dataframe:\n\n\n\nWith a single index, I can select all the data for a given day as follows:\n\n\n\nBut it doesn't work for a multiindex:\n\n\n\nSo I can make it work if I specify the slice explicitely, but it would be nice if the behavior for the 1D index carried over to Multiindices.\n",
    "labels":[
      "Enhancement",
      "Timeseries",
      "Indexing",
      "MultiIndex",
      "API Design",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments":[
      "this is almost a dupe of #9732 \nThis is simply not implemented (though not that difficult). See that issue for an easy work-around.\n",
      "Added some unit tests that should pass once this feature works. @jreback did a quick review pass on the tests.\n\n@jreback, you said you'd have a look at the code path and give some recommendations on how to fix this issue.\n",
      "@jreback, per our discussion, I have an implementation that seems to fix this issue (in the private branch above), but it does not fail the test for `df.loc[('2016-01-01', 'a'), :]` which you thought it should fail. Before I make a formal pull request, can you have a look at the branch?\n\nThese tests all pass:\n`$ nosetests -A \"not slow\" tests\/frame tests\/series tests\/indexes tests\/test_indexing.py`\n",
      "Additionally, `df_swap.loc[idx[:, '2016-01-02':], :]` fails with an error, but I believe this can be considered a separate bug, as it did so before my proposed fix.\n\nThe cause is that the code in `multi.py` is trying to call `.stop` on an `int` instead of first checking to see if `stop` is a `slice`:\n\n``` python\nif isinstance(start, slice) or isinstance(stop, slice):\n                # we have a slice for start and\/or stop\n                # a partial date slicer on a DatetimeIndex generates a slice\n                # note that the stop ALREADY includes the stopped point (if\n                # it was a string sliced)\n                return convert_indexer(start.start, stop.stop, step)\n```\n\nPlease let me know how you'd like me to proceed with the pull request.\n",
      "thanks @markroth8 I will be visting this next week.\n",
      "While trying to update the docs, I think I found an issue with the implementation. I'm not sure if it's new:\n\n``` python\ndft2 = DataFrame(randn(200000,1),columns=['A'],index=MultiIndex.from_product([dft.index, ['a', 'b']]))\n```\n\n```\nIn [32]: dft2.loc['2013-03']\nOut[32]: \n                              A\n2013-01-01 00:00:00 a  0.145858\n2013-01-01 00:01:00 a  0.007413\n2013-01-01 00:02:00 a  0.286948\n2013-01-01 00:03:00 a -0.695290\n2013-01-01 00:04:00 a -0.948675\n2013-01-01 00:05:00 a -2.454434\n2013-01-01 00:06:00 a  0.920254\n2013-01-01 00:07:00 a -0.614170\n2013-01-01 00:08:00 a  2.039961\n2013-01-01 00:09:00 a -1.326936\n2013-01-01 00:10:00 a  0.500509\n2013-01-01 00:11:00 a -0.433509\n2013-01-01 00:12:00 a  0.466115\n2013-01-01 00:13:00 a  1.245106\n2013-01-01 00:14:00 a  1.077567\n2013-01-01 00:15:00 a  0.603533\n2013-01-01 00:16:00 a -1.153203\n2013-01-01 00:17:00 a -0.932540\n2013-01-01 00:18:00 a -0.942356\n2013-01-01 00:19:00 a  0.078370\n2013-01-01 00:20:00 a -0.421003\n2013-01-01 00:21:00 a -0.700034\n2013-01-01 00:22:00 a  0.956662\n2013-01-01 00:23:00 a -1.009236\n2013-01-01 00:24:00 a -1.325032\n2013-01-01 00:25:00 a -1.633084\n2013-01-01 00:26:00 a -0.281456\n2013-01-01 00:27:00 a  0.394617\n2013-01-01 00:28:00 a -1.875757\n2013-01-01 00:29:00 a  1.056043\n...                         ...\n2013-03-11 10:25:00 a  0.212931\n                    b  0.419044\n2013-03-11 10:26:00 a  1.571333\n                    b  0.059788\n2013-03-11 10:27:00 a -0.271262\n                    b -0.167976\n2013-03-11 10:28:00 a -1.111855\n                    b -0.525583\n2013-03-11 10:29:00 a -0.801815\n                    b  0.557208\n2013-03-11 10:30:00 a -0.004837\n                    b -0.452653\n2013-03-11 10:31:00 a  1.270449\n                    b -0.775152\n2013-03-11 10:32:00 a -0.773299\n                    b  0.258476\n2013-03-11 10:33:00 a  0.376850\n                    b  1.430959\n2013-03-11 10:34:00 a  0.566823\n                    b -0.464938\n2013-03-11 10:35:00 a  0.889925\n                    b  1.714770\n2013-03-11 10:36:00 a -1.593098\n                    b -0.938460\n2013-03-11 10:37:00 a  0.046490\n                    b  0.543553\n2013-03-11 10:38:00 a  0.527221\n                    b  0.117193\n2013-03-11 10:39:00 a -0.459528\n                    b -1.791615\n\n[115040 rows x 1 columns]\n```\n\nNote how rows with 'b' are only present for '2013-03' rows but 'a' appears for all rows.\n",
      "Confirmed the same behavior exists in master branch with:\n\n`dft2.loc[IndexSlice['2013-03':'2013-03',:],:]`\n\nIs this a bug we should file separately?\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "labeled",
      "cross-referenced",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced"
    ],
    "changed_files":4,
    "additions":146,
    "deletions":10
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12663,
    "reporter":"agartland",
    "created_at":1458245788000,
    "closed_at":1458754598000,
    "resolver":"agartland",
    "resolved_in":"286e7a366ea23d364ab0834725f7a1d51f880d04",
    "resolver_commit_num":0,
    "title":"Copy method does not make truly deep copies of dtype object arrays",
    "body":"The `copy` method of `pd.Series` and `pd.DataFrame` has a parameter `deep` which claims to [Make a deep copy, i.e. also copy data](-docs\/stable\/generated\/pandas.Series.copy.html). The example below seems to show that this isn't a truly deep copy (as in `from copy import deepcopy`) I can't seem to find the implementation in the source. I am wondering if this behavior below is expected, if something different should be done for `dtype=object` to make it a truly deep copy, or if we could at least add a note to the documentation that notes this behavior?\n\nThanks!\n\n\n",
    "labels":[
      "Usage Question",
      "Compat",
      "Docs",
      "Difficulty Intermediate"
    ],
    "comments":[
      "this is not supported. pandas objects are stored in numpy arrays (generally) which if they have `object` dtype are simply pointers to python objects. deep-copying them does not imply that numpy array is deep-copied itself (I don't even know if that is actually supported). Deep refers to the indexes themselves being copied.\n\nIt would be expensive to do this. Not really even sure of a usecase for it; generally the actual data are scalar type data (e.g. float, int, string), not actual python objects themselves. \n\nThis is an anti-pattern to store python objects here. I suppose you could add a note to the doc-string.\n",
      "Thanks, that's helpful, I understand why the copy method behaves as it does. I'm wondering if it could be made more clear in the documentation since the word \"deep\" seems at least slightly ambiguous here.\n\nI typically use a DataFrame for numbers, but sometimes I like to have a column that holds some other kind of meta-data in an object. This way I get all the benefits of indexing and mergeing and can keep the meta-data associated with the data (even if this effectively disables many of numpy's nice features and efficiencies)\n",
      "@agartland sure, a doc-string update would be fine. you can even put your example there to make it clear (maybe slim it down a bit).\n",
      "I added a note in the copy method docstring that should help. I didn't know where\/how to add an example but here's a slimmed down version if you think its useful:\n\n```\nimport copy\n\na = pd.Series([1, 'a', [4,5,6]])\nb = a.copy(deep=True)\nc = copy.deepcopy(a)\n\nprint a\n\n\"\"\"Changes to the copy.deepcopy don't affect the original.\"\"\"\nc.loc[2].append(0)\nprint a.loc[2], b.loc[2], c.loc[2]\n\n\"\"\"Changes to the a.copy(deep=True) are reflected in the original.\"\"\"\nb.loc[2].append(-9)\nprint a.loc[2], b.loc[2], c.loc[2]\n```\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "labeled",
      "labeled",
      "cross-referenced",
      "commented",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":1,
    "additions":8,
    "deletions":2
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":11852,
    "reporter":"ohadle",
    "created_at":1450258521000,
    "closed_at":1458765497000,
    "resolver":"terfilip",
    "resolved_in":"247fe0718c3b09a2de4d1af834cff8efb9f8edcc",
    "resolver_commit_num":0,
    "title":"xz compression in to_csv()",
    "body":"I use compression directly in DataFrame.to_csv() to save on disk space \/ IO. Would be nice to have support for xz compression there.\n\nSimilar to , but I think in to_csv this has more added value.\n",
    "labels":[
      "Enhancement",
      "CSV",
      "Effort Medium",
      "Difficulty Novice"
    ],
    "comments":[
      "I suppose if someone wanted to add additional compressor to read\/write csv, this would be ok.\n\ncan you point to the canonical package which implements this compressor?\n",
      "I haven't tried it myself, but it looks like for python 3 it would be https:\/\/docs.python.org\/3\/library\/lzma.html. For 2.7 this looks like it: https:\/\/github.com\/peterjc\/backports.lzma\n",
      "sure that looks reasonable.\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "unlabeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files":12,
    "additions":105,
    "deletions":10
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12610,
    "reporter":"chinskiy",
    "created_at":1457889098000,
    "closed_at":1458912615000,
    "resolver":"chinskiy",
    "resolved_in":"26c7d8d7e613890d680a4d4c77464c806facde27",
    "resolver_commit_num":0,
    "title":"Bug: Pandas Series name attribute can be array",
    "body":"#### Code Sample\n\n\n#### Expected Output\n\nget error when result_df try to be created with array name attribute,\nor give error when try to create Series object with array argument in name attribute.\n#### output of `pd.show_versions()`\n\n---\n\ncommit: None\npython: 2.7.6.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.2.0-30-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_GB.UTF-8\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.1.0\nsetuptools: 20.2.2\nCython: 0.23.4\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.1.2\nsphinx: 1.3.6\npatsy: 0.4.1\ndateutil: 2.5.0\npytz: 2015.7\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.5.1\nopenpyxl: 2.3.3\nxlrd: 0.9.4\nxlwt: None\nxlsxwriter: 0.8.4\nlxml: 3.5.0\nbs4: 4.4.1\nhtml5lib: 1.0b8\n 0.9.2\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\njinja2: 2.8\nboto: None\n",
    "labels":[
      "Bug",
      "Compat",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "there was an old issue somewhat related: https:\/\/github.com\/pydata\/pandas\/pull\/9193\n\nbut this _should_ be caught. want to investigate and see where this is happening?\n",
      "Okay, I'll try.\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files":7,
    "additions":41,
    "deletions":7
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12706,
    "reporter":"jreback",
    "created_at":1458769492000,
    "closed_at":1458952377000,
    "resolver":"tdhopper",
    "resolved_in":"5870731f32ae569e01e3c0a8972cdd2c6e0301f8",
    "resolver_commit_num":0,
    "title":"ERR: validation options that accept callables",
    "body":"#issuecomment-200554983\n\n\n\nThis should raise\n\n\n",
    "labels":[
      "Difficulty Novice",
      "Error Reporting",
      "Effort Low"
    ],
    "comments":[
      "the issue is that in `core\/config_init.py` `float_format` doesn't have a validator.\n",
      "Right: [here](https:\/\/github.com\/pydata\/pandas\/blob\/eb87db9e3dffe7febb575b82bf26065e807c028b\/pandas\/core\/config_init.py#L282). \n\nIs [`callable`](https:\/\/docs.python.org\/2\/library\/functions.html#callable) sufficient?\n",
      "you could prob just create a `is_callable = callable` [here](https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/core\/config.py#L800)\n\nand `validator=is_callable` would work I think\n"
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "referenced"
    ],
    "changed_files":4,
    "additions":42,
    "deletions":6
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12727,
    "reporter":"has2k1",
    "created_at":1459224863000,
    "closed_at":1459430647000,
    "resolver":"has2k1",
    "resolved_in":"0d58446503abc0f4299df8644c7d64163664a885",
    "resolver_commit_num":0,
    "title":"pd.timedelta has smaller than expected range",
    "body":"The pandas timedelta class has less than expected minimum and maximum timedeltas. The range is `[-99999, 99999]`  days instead of [`-999999999, 999999999]` days\n#### Code Sample\n\n\n#### Expected Output\n\n\n\nThe problem is illustrated at points `2` (silent overflow) and `4`(overflow error)\n#### output of `pd.show_versions()`\n\n\n\nI get similar results on python `3.5.1`.\n",
    "labels":[
      "Timedelta",
      "Compat"
    ],
    "comments":[
      "Unlike in numpy, pandas.Timdelta always uses nanosecond precision, so anything more than [about 100000 days](https:\/\/www.google.com\/search?q=2%5E64+nanoseconds+in+years&rlz=1C9BKJA_enUS592US592&oq=2%5E64+nano&aqs=chrome.1.69i57j0.9678j0j7&hl=en-US&sourceid=chrome-mobile&ie=UTF-8#hl=en-US&q=2%5E63+nanoseconds+in+days) cannot be represented. Something does seem to be going wrong with the numeric overflow, though. \n",
      "@has2k1 I guess. This is by definition as to the implementation as @shoyer comments, and in-line with the dateranges represented by `Timestamps`.\n",
      "I suppose a doc-section along the lines of: http:\/\/pandas.pydata.org\/pandas-docs\/stable\/gotchas.html#gotchas-timestamp-limits would be fine. Maybe we should move these to the `Timeseries\/Timedelta` section though. @jorisvandenbossche ?\n\n```\nIn [8]: pd.Timedelta(np.iinfo(np.int64).max)\nOut[8]: Timedelta('106751 days 23:47:16.854775')\n\nIn [9]: pd.Timedelta(np.iinfo(np.int64).min)\nOut[9]: NaT\n\nIn [10]: pd.Timedelta(np.iinfo(np.int64).min+1)\nOut[10]: Timedelta('-106752 days +00:12:43.145224')\n```\n",
      "@jreback, yes that would be helpful. In PR #12728 I'll include a section to that end in the\n `timedelta` documentation.\n"
    ],
    "events":[
      "renamed",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned"
    ],
    "changed_files":6,
    "additions":65,
    "deletions":22
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12564,
    "reporter":"pganssle",
    "created_at":1457476151000,
    "closed_at":1459692360000,
    "resolver":"facaiy",
    "resolved_in":"ad8ade8d8bfe87d7d82e7cbecfc1dd88c54d77a1",
    "resolver_commit_num":0,
    "title":"Categorical equality check raises ValueError in DataFrame",
    "body":"Apparently there's an issue when comparing the equality of a scalar value against a categorical column as part of a DataFrame. In the example below, I'm checking against `-np.inf`, but comparing to a string or integer gives the same results.\n\nThis raises `ValueError: Wrong number of dimensions`.\n#### Code Sample\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels":[
      "Bug",
      "Indexing",
      "Difficulty Novice",
      "Categorical",
      "Effort Low"
    ],
    "comments":[
      "here's a simpler example. Yeah I suppose this should work. Note that comparing a vs a DataFrame is not typically useful. You almost always compare against a Series (and then inddex).\n\npull-requests are welcome.\n\n```\nIn [23]: df = DataFrame({'A' : ['foo','bar','baz']})\n\nIn [24]: df['B'] = df['A'].astype('category')\n\nIn [25]: df['A'] == 'foo'\nOut[25]: \n0     True\n1    False\n2    False\nName: A, dtype: bool\n\nIn [26]: df['B'] == 'foo'\nOut[26]: \n0     True\n1    False\n2    False\nName: B, dtype: bool\n\nIn [27]: df[['A']] == 'foo'\nOut[27]: \n       A\n0   True\n1  False\n2  False\n\nIn [28]: df[['B']] == 'foo'\nValueError: Wrong number of dimensions\n```\n",
      "My use case, if you want to know, was that I just wanted to discard rows with `np.inf` in any column (though you could imagine the same thing with 0 or something). I find it easier and more readable to just broadcast the comparison across the entire DataFrame, even though logically speaking I know that the comparison really only needs to be applied to the float columns.\n",
      "``` python\n>>> df[['A']].ndim\n2\n>>> df[['A']]._data.blocks[0].values\narray([['foo', 'bar', 'baz']], dtype=object)\n>>> df[['A']]._data.blocks[0].values.ndim\n2\n>>> df[['B']].ndim\n2\n>>> df[['B']]._data.blocks[0].values\n[foo, bar, baz]\nCategories (3, object): [bar, baz, foo]\n>>> df[['B']]._data.blocks[0].values.ndim\n1\n```\n\ninteresting, \nI think that's why pandas raises `ValueError: Wrong number of dimensions.`\nright?\n",
      "@ningchi that is only a manifestation of the issue, not the cause. `CategoricalBlocks` only hold a 1-d structure. The comparison path goes thru `core\/internals\/Block\/eval`.\n\nyou can prob get away with changing this:\n\n`transf = (lambda x: x.T) if is_transposed else (lambda x: x)`\n\nto something like\n\n```\ndef transf(x):\n     transposer = lambda x: x.T if is_transposed: lambda x: x\n     return lambda x: _block_shape(transposer(x), ndim=self.ndim)\n```\n",
      "Thanks, @jreback\n\nbecause `CategoricalBlocks` only hold a 1-d structure, I have no idea how to extend its `ndim`, except using `to_dense()` like `NonConsolidatableMixIn.get_values`.\n\nOn the another hand, is it strange to convert `self.dim` to `self.value.dim`?  as we expect a Dataframe, rather than a Series.\n",
      "did you try the code above?\n",
      "@jreback Sorry, I didn't test your suggestion completely. Many thanks.\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "referenced"
    ],
    "changed_files":4,
    "additions":29,
    "deletions":0
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12768,
    "reporter":"sebov",
    "created_at":1459526032000,
    "closed_at":1459693988000,
    "resolver":"jonaslb",
    "resolved_in":"64977f185747016e6939307b4f1a8258edcd7f89",
    "resolver_commit_num":0,
    "title":"BUG: filter (with dropna=False) when there are no groups fulfilling the condition",
    "body":"For a DataFrame I want to preserve rows that belong to groups that fulfil specific condition and replace other rows with NaN. I have used a combination of 'groupby' and 'filter' (with dropna=False). In a special case when there are no groups fulfilling the condition an exception occured.\n\n\n\nThe problem I have identified is in the _apply_filter method of _GroupBy class (core\/groupby.py) -- line with \"mask[indices.astype(int)] = True\" throws because in my case indices is equal to []; shouldn't it be \"indices = np.array([])\" instead of \"indices = []\" in the case when len(indices) == 0\n\n\n#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels":[
      "Bug",
      "Groupby",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments":[
      "yep looks buggy.\n",
      "pull-requests welcome\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":16,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":10208,
    "reporter":"jseabold",
    "created_at":1432649526000,
    "closed_at":1459698074000,
    "resolver":"dukebody",
    "resolved_in":"8776596346b2e7717a81ecd19bb78bb399f5621e",
    "resolver_commit_num":0,
    "title":"Regex C Engine Warning",
    "body":"Using `pd.read_csv(..., sep=\", \", ...)` I'm now getting a warning about falling back to the C engine because regex parsing isn't supported in the C engine. That's fine, but this isn't actually using regex.\n\nI don't have an idea for a good transition strategy, and maybe the ship has sailed, but perhaps there should be a separate `read_regex` or a `regex` keyword instead of emitting this warning for any string greater than length 1.\n\nPandas 0.16.0.\n",
    "labels":[
      "CSV",
      "Docs",
      "Difficulty Novice",
      "Error Reporting",
      "Effort Low"
    ],
    "comments":[
      "The warning has to do with the fact that your separator is > 1 character, which is not supported by the parser (Prob isn't that hard to implement, but would need someone to do it). Here is a nice way to do this (and use the c-parser). \n\n```\nIn [38]: data = \"\"\"a, b, c\\n1, 2, 3\"\"\"\n\nIn [39]: read_csv(StringIO(data),sep=\",\",engine='c',skipinitialspace=True)\nOut[39]: \n   a  b  c\n0  1  2  3\n\nIn [40]: read_csv(StringIO(data),sep=\", \",engine='python')\nOut[40]: \n   a  b  c\n0  1  2  3\n```\n",
      "From the documentation it is not clear when a separator is considered a regex and when it isn't. I was trying to use '::' as separator (MovieLens dataset) when reading a file and pandas was interpreting it as a regex, when it really isn't.\n\nI think a separate `sep_regex` keyword would be cleaner. For the time being, we can also raise an exception \"non-regex separators of more than 1 character are not supported\". If it's the C engine that doesn't support >1 char separators, we can warn \"C engine doesn't support separators longer than 1 character, falling back to Python engine\".\n",
      "I don't think there's any need to adjust the API, just a clearer warning message.\n",
      "I think documentation should also be amended.\n\n> **sep**: Delimiter to use. If sep is None, will try to automatically determine this. Regular expressions are accepted and will force use of the python parsing engine and will ignore quotes in the data.\n\nWhen I first read this I wondered how pandas knows when am I using a regexp as delimiter and when am I using a normal string. I would change this by:\n\n> **sep**: Delimiter to use. If sep is None, will try to automatically determine this. If it is longer than 1 character, it will be interpreted as a regular expression, will force use of the python parsing engine and will ignore quotes in the data.\n\nAnyhow I still believe that accepting string separators larger than 1 character is a good feature, but might need a separate ticket\/issue.\n",
      "IIRC if its > 1 length, then it by defintion defers to the python engine.\n",
      "no need to add any more options to the parsers. But as @TomAugspurger points out a clearer error message would be fine.\n",
      "@dukebody pull-requests welcome.\n"
    ],
    "events":[
      "commented",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files":2,
    "additions":10,
    "deletions":6
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12467,
    "reporter":"tsdlovell",
    "created_at":1456515214000,
    "closed_at":1459947610000,
    "resolver":"tsdlovell",
    "resolved_in":"e04f3438c362777fc2fea24994caf388639214d8",
    "resolver_commit_num":0,
    "title":"Concat of tz-aware and tz-unaware dataframes fails",
    "body":"This snippet\n\n\n\ncauses\n\n> ...\/pandas\/tseries\/common.py in _concat_compat(to_concat, axis)\n>     282         if 'datetime' in typs or 'object' in typs:\n>     283             to_concat = [convert_to_pydatetime(x, axis) for x in to_concat]\n> --> 284             return np.concatenate(to_concat, axis=axis)\n>     285 \n>     286         # we require ALL of the same tz for datetimetz\n> \n> ValueError: all the input arrays must have same number of dimensions\n\nWe would expect it to return something like this\n\n> pd.DataFrame(dict(time=pd.Series([pd.Timestamp('2015-01-01', tz=None), pd.Timestamp('2015-01-01', tz='UTC')], dtype=object)))\n> Out[18]: \n>                         time\n> 0        2015-01-01 00:00:00\n> 1  2015-01-01 00:00:00+00:00\n\noutput of `pd.show_versions()`\n### INSTALLED VERSIONS\n\ncommit: fe584e7829dc4dd9fa585a7fbf5fd47fc6f4b057\npython: 2.7.11.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.13.0-53-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.0rc1+47.gfe584e7\nnose: 1.3.7\npip: 8.0.3\nsetuptools: 20.1.1\nCython: 0.23.4\nnumpy: 1.10.4\nscipy: None\nstatsmodels: None\nxarray: None\nIPython: 4.1.1\nsphinx: 1.3.5\npatsy: None\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 0.9.4\nsqlalchemy: 0.9.4\npymysql: None\npsycopg2: None\njinja2: 2.8\n",
    "labels":[
      "Bug",
      "Reshaping",
      "Timezones"
    ],
    "comments":[
      "can you rebase\/update\n",
      "rebased, looking into the test failures\n"
    ],
    "events":[
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "commented"
    ],
    "changed_files":3,
    "additions":27,
    "deletions":2
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12911,
    "reporter":"jseabold",
    "created_at":1460902679000,
    "closed_at":1460993174000,
    "resolver":"mdboom",
    "resolved_in":"ab4ac1209c4b7517e8579e6e75fa176d59ca8d6c",
    "resolver_commit_num":0,
    "title":"pandas logo has a latex typo",
    "body":"Someone pointed this out on twitter (sorry, forgot who), but it you should be `y_{it} =` not `y_it` as it looks like it might be now. The `t` should also be subscript.\n",
    "labels":[
      "Community"
    ],
    "comments":[
      "cc @mdboom\n",
      "I just put up a gist with the matplotlib script I used to create the (new) logo, with the corrected LaTeX expression:\n\nhttps:\/\/gist.github.com\/mdboom\/06578d2edbb21807d0e216c8553bdf82\n\nIf you have a good suggestion about where in the pandas tree this should live permanently, I can make a PR with this stuff.\n",
      "thanks @mdboom why don't you do a PR with this in `pandas\/doc\/logo` (new dir). If you can put the generated logo there as well. thanks.\n",
      "Sure: see #12919.\n"
    ],
    "events":[
      "renamed",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "cross-referenced",
      "commented"
    ],
    "changed_files":4,
    "additions":924,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12902,
    "reporter":"elinoreroebber",
    "created_at":1460672481000,
    "closed_at":1460999408000,
    "resolver":"adneu",
    "resolved_in":"a6d76981efc94ae4dcb0ea917aff624d9b497e37",
    "resolver_commit_num":0,
    "title":"BUG: Complex types are coerced to float when summing along level of MultiIndex",
    "body":"When trying to sum complex numbers along one level of a MultiIndex, I get a `ComplexWarning` and the resulting datatypes are `float64` with imaginary components discarded.  Summing without specifying the level or by unstacking the MultiIndex works as expected.  \n\nBased on the error message, I assume that groupby is being called internally, and that the bug is actually there.  I'm using pandas version 17.1.\n#### Code Sample\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 14.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_CA.UTF-8\n\npandas: 0.17.1\nnose: 1.3.7\npip: 7.1.2\nsetuptools: 19.2\nCython: 0.23.5\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: None\nIPython: 4.1.2\nsphinx: 1.4\npatsy: None\ndateutil: 2.5.1\npytz: 2016.3\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.5.1\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\nJinja2: None\n",
    "labels":[
      "Bug",
      "Groupby",
      "Complex",
      "Effort Low",
      "Difficulty Novice"
    ],
    "comments":[
      "This is about [here](https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/core\/groupby.py#L1746)\n\npull-requests are welcome. For now we need to coerce this to `object` (rather than treat this as a float)\n\nThis does it. just need some tests and such.\n\n```\ndiff --git a\/pandas\/core\/groupby.py b\/pandas\/core\/groupby.py\nindex 6996254..e2a4482 100644\n--- a\/pandas\/core\/groupby.py\n+++ b\/pandas\/core\/groupby.py\n@@ -1747,7 +1747,7 @@ class BaseGrouper(object):\n             values = _algos.ensure_float64(values)\n         elif com.is_integer_dtype(values):\n             values = values.astype('int64', copy=False)\n-        elif is_numeric:\n+        elif is_numeric and not com.is_complex_dtype(values):\n             values = _algos.ensure_float64(values)\n         else:\n             values = values.astype(object)\n```\n",
      "The bug also occurs when using a Series and invoking `groupby` with `level=0` (so it is not specific to MultiIndex, only indirectly via `groupby`, as suggested).\n\n```\nIn [1]: series = pd.Series(data=np.arange(4)*(1+2j),index=[0,0,1,1])\nIn [2]: print series\n0        0j\n0    (1+2j)\n1    (2+4j)\n1    (3+6j)\ndtype: complex128\n\nIn [3]: series.groupby(level=0).sum()\nOut[3]: \n0    1.0\n1    5.0\ndtype: float64\n```\n\nExpected:\n\n```\nOut[3]: \n0     (1+2j)\n1    (5+10j)\ndtype: complex128\n```\n"
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "labeled",
      "unlabeled",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":14,
    "deletions":2
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12934,
    "reporter":"brandon-rhodes",
    "created_at":1461163429000,
    "closed_at":1461344609000,
    "resolver":"brandon-rhodes",
    "resolved_in":"ed324e8692002db49e8d587a9bb73d28d8776ffe",
    "resolver_commit_num":0,
    "title":"Add parameter defaults for swaplevel",
    "body":"I will be happy to write up a pull request, but first wanted to gauge the sanity of my suggestion:\n\nI think that `swaplevel()` deserves default values for its parameters, just like its friends like `stack()` and `unstack()` and `sortlevel()` that also all take an initial `level` argument. I suggest:\n\n\n\nThis provides the greatest symmetry with the other methods that operate on levels: they all, if no level is specified, operate on the innermost level as their default.\n\nIn the very common case where there are only two levels to the multi-index anyway, this would reduce this frequent operation to simply `.swaplevel()` or `.swaplevel(axis='columns')` without, I don't think, any more loss of readability than when `stack()` or `unstack()` fail to specify the level upon which they are operating.\n",
    "labels":[
      "Reshaping",
      "API Design",
      "MultiIndex",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments":[
      "@brandon-rhodes yes this does seem ok\n"
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "labeled",
      "labeled",
      "cross-referenced"
    ],
    "changed_files":7,
    "additions":57,
    "deletions":18
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12981,
    "reporter":"ajenkins-cargometrics",
    "created_at":1461594671000,
    "closed_at":1461676975000,
    "resolver":"ajenkins-cargometrics",
    "resolved_in":"cc67b72cc7cd37302068135ececabb0efe16f8c5",
    "resolver_commit_num":0,
    "title":"Timezone lost on DataFrame assignments with realignment",
    "body":"Starting from pandas 0.17, certain assignments to DataFrames cause offset-aware datetime columns to be converted to offset-naive columns.  Specifically, it seems that if any data realignment is required when assigning the RHS to a a slice of the DataFrame, then timezone info is lost.  Here's an example: \n\n\n\nThe output I'd expect, which is what I get from pandas 0.16.2, is:\n\n\n\nHowever when I run this with pandas 0.18.0, after the assignment the timezone info is lost:\n\n\n\nIt seems the custom timezone-aware dtype that pandas started using for timezone-aware time series in 0.17.x doesn't get correctly propagated in this operation.\n#### output of `pd.show_versions()`\n\n\n",
    "labels":[
      "Bug",
      "Indexing",
      "Timezones",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments":[
      "yep looks like a bug. pull-requests are welcome!\n",
      "After a little digging, I believe I've found the fix.  In `DataFrame._santize_column`, there is a statement which accesses the `values` property, which should access `_values`.  This statement:\n\n``` python\nvalue = value.reindex(self.index).values\n```\n\nshould be\n\n``` python\nvalue = value.reindex(self.index)._values\n```\n\nThe `values` property returns a numpy array, which loses the custom dtype, whereas `_values` returns a DateTimeIndex which preserves the dtype.  I'll submit a PR.\n",
      "@ajenkins-cargometrics great!\n"
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":17,
    "deletions":2
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":11831,
    "reporter":"jorisvandenbossche",
    "created_at":1449933638000,
    "closed_at":1461698479000,
    "resolver":"rs2",
    "resolved_in":"7fbc600e6f11ab256dc873175b971ca06d5ce456",
    "resolver_commit_num":0,
    "title":"Performance issue with timeseries plotting on py3?",
    "body":"I noticed a performance issue with plotting timeseries. After some trying with different environments (different pandas, matplotlib and python versions), it seems there is a problem on python 3 -> up to 10 x slowdown compared to python 2.7:\n\nPython 2 and pandas 0.16.2 and 0.17.1:\n\n\n\n\n\nWith python 3, pandas 0.16.2 and 0.17.1:\n\n\n\n\n",
    "labels":[
      "Performance",
      "Regression",
      "Visualization"
    ],
    "comments":[
      "this is the period index conversion, which IIRC you fixed?\n",
      "You are referring to https:\/\/github.com\/pydata\/pandas\/pull\/11194 I think. This fixed a perf regression before 0.17.0 was released (introduced between 0.16.2 and 0.17.0). Do you see a possible reason that this fix would not work on py3?\n",
      "Has this issue been fixed or are there workarounds?\nI do have them same Problem (python3 is about 16 times slower).\n\nAre things moving on this issue?\n",
      "@Rittmeister123 Not that I know (I haven't had time myself to dive into it).\n\nIf you want to try to profile, to see where the slowdown is coming from, that would be very welcome!\n",
      "I'm new here (my first entry :) ), so please excuse possible format-issues or something else.\n\n@jorisvandenbossche I did some profiling with the example from above:\n\nMy Setup for Python 3:\n\n```\nIn [1]: sys.version\nOut[1]: '3.5.1 |Anaconda 2.4.1 (64-bit)| (default, Dec  7 2015, 15:00:12)\n[MSC v.1900 64 bit (AMD64)]\n\nIn[2]: pd.__version__\nOut[2]: '0.17.1'\n\nIn[3]: matplotlib.__version__\nOut[3]: '1.5.0'\n\nIn[4]: np.__version__\nOut[4]: '1.10.1'\n\n```\n\nMy Setup for Python 2:\n\n```\nIn [1]: sys.version\nOut[1]: '2.7.11 |Continuum Analytics, Inc.| (default, Dec  7 2015, 14:10:42)\n[MSC v.1500 64 bit (AMD64)]'\n\nIn[2]: pd.__version__\nOut[2]: u'0.17.1'\n\nIn[3]: matplotlib.__version__\nOut[3]: '1.5.0'\n\nIn[4]: np.__version__\nOut[4]: '1.10.1'\n\n```\n\nThe dataframe is generated with:\n\n```\nIn [1]: N = 2000\n        df = pd.DataFrame(np.random.randn(N, 5), index=pd.date_range('1\/1\/1975', periods=N))\n```\n\nTimeit on python2:\n\n```\nIn [1]: %timeit df.plot()\nOut[1]: 1 loops, best of 3: 111 ms per loop\n\n```\n\nTimeit on python3:\n\n```\nIn [1]: %timeit df.plot()\nOut[1]: 1 loops, best of 3: 1.01 s per loop\n```\n\nAttached you can find the Profiling files generated with\n\n```\nIn[1]: %prun -D python2_df_plot.prof df.plot()\n```\n\nand\n\n```\nIn[1]: %prun -D python3_df_plot.prof df.plot()\n\n```\n\n[profiling_pandas_plot.zip](https:\/\/github.com\/pydata\/pandas\/files\/82356\/profiling_pandas_plot.zip)\n\nPlease have a look at it, since i have no knowledge on profiling\n",
      "@Rittmeister123 any luck with this?\n",
      "Unfortunately not!\nI'm quiet new in python and do not had time to take a close look whats going on and read the profiling..\n",
      "@jorisvandenbossche any thoughts on this?\n",
      "FYI: \nA few days ago I tried to plot a dataframe with the use_index Parameter set to false...and recognized a speed up...but hadn't had time to exactly verify this and proof it...\n",
      "Did some quick profiling, and one of the elements is in any case the difference in performance of `PeriodIndex._mpl_repr()`, which is just a call to `PeriodIndex._get_object_array`:\n\n```\nIn [8]: sys.version\nOut[8]: '3.5.0 |Anaconda 2.4.0 (64-bit)| (default, Nov  7 2015, 13:15:24) [MSC v.1900 64 bit (AMD64)]'\n\nIn [9]: pd.__version__\nOut[9]: '0.17.1'\n\nIn [10]: pidx = pd.period_range('1975-01-01', periods=2000)\n\nIn [11]: %timeit pidx._mpl_repr()\n1 loops, best of 3: 461 ms per loop\n```\n\nvs\n\n```\nIn [6]: sys.version\nOut[6]: '2.7.11 |Anaconda 1.7.0 (64-bit)| (default, Jan 19 2016, 12:08:31) [MSCv.1500 64 bit (AMD64)]'\n\nIn [7]: pd.__version__\nOut[7]: '0.16.2'\n\nIn [8]: pidx = pd.period_range('1975-01-01', periods=2000)\n\nIn [9]: %timeit pidx._mpl_repr()\n100 loops, best of 3: 5.25 ms per loop\n```\n\nIn turn, this boils down to calls to `Period.from_ordinal()`:\n\n```\nIn [12]: sys.version\nOut[12]: '3.5.0 |Anaconda 2.4.0 (64-bit)| (default, Nov  7 2015, 13:15:24) [MSCv.1900 64 bit (AMD64)]'\n\nIn [13]: pd.__version__\nOut[13]: '0.17.1'\n\nIn [14]: %timeit pd.Period._from_ordinal(ordinal=1, freq='D')\n1000 loops, best of 3: 476 \u00b5s per loop\n```\n\nvs\n\n```\nIn [6]: sys.version\nOut[6]: '2.7.11 |Anaconda 1.7.0 (64-bit)| (default, Jan 29 2016, 14:26:21) [MSCv.1500 64 bit (AMD64)]'\n\nIn [7]: pd.__version__\nOut[7]: '0.17.1+315.g62363d2'\n\nIn [8]: %timeit pd.Period._from_ordinal(ordinal=1, freq='D')\n10000 loops, best of 3: 42.1 \u00b5s per loop\n```\n\nNow, what would cause the dramatic difference in performance in the `from_ordinal` method between python 2 and 3, is still a mystery to me (and I also don't have time to look into further). \n@jreback any idea?\n\n@blbradley you did some work on the Period cython code. Do you have by any chance an idea where this peformance difference could be coming from?\n",
      "I think https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/src\/period.pyx#L658\n\nneeds this instead of doing the string interpretation each time\n\n```\nif isinstance(freq, offsets.DateOffset):\n    return freq\n```\n\nalso the import can be replace by `offsets.to_offset` (below)\n\nas when `_mpl_repr` is called an already constructed freq object is passed (and not the string as in the case above).\n\nThere maybe something else going on in the actual `to_offset` to explain the py2\/3 diff though (as string conversion should be similar perf)\n",
      "I actually did a wrong timeit, as the `freq` is already an offset in the case of plotting:\n\n```\n# python 3\nIn [24]: %timeit pd.Period._from_ordinal(ordinal=1, freq=pidx.freq)\n1000 loops, best of 3: 233 \u00b5s per loop\n\nIn [25]: type(pidx.freq)\nOut[25]: pandas.tseries.offsets.Day\n```\n\nvs\n\n```\nIn [11]: %timeit pd.Period._from_ordinal(ordinal=1, freq=pidx.freq)\nThe slowest run took 6.19 times longer than the fastest. This could mean that an\n intermediate result is being cached\n100000 loops, best of 3: 5.5 \u00b5s per loop\n```\n\nwhich makes the difference even larger\n",
      "Strange, I don't see a difference in performance of `to_offset`, while there is in `_maybe_convert_freq`:\n\npython 3:\n\n```\nIn [43]: %timeit pd.Period._maybe_convert_freq(pidx.freq)\n1000 loops, best of 3: 200 \u00b5s per loop\n\nIn [44]: %timeit to_offset(pidx.freq)\nThe slowest run took 10.28 times longer than the fastest. This could mean that a\nn intermediate result is being cached\n1000000 loops, best of 3: 479 ns per loop\n```\n\nvs python 2:\n\n```\nIn [19]:  %timeit pd.Period._maybe_convert_freq(pidx.freq)\nThe slowest run took 6.87 times longer than the fastest. This could mean that an\n intermediate result is being cached\n100000 loops, best of 3: 4.42 \u00b5s per loop\n\nIn [20]: %timeit to_offset(pidx.freq)\nThe slowest run took 13.08 times longer than the fastest. This could mean that a\nn intermediate result is being cached\n1000000 loops, best of 3: 502 ns per loop\n```\n",
      "try removing the import in `_maybe_convert_freq` (use `offsets.to_offset` instead)\nwould still add the check for `DateOffset` as its doing extra work\n"
    ],
    "events":[
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":5,
    "additions":56,
    "deletions":53
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":9815,
    "reporter":"jreback",
    "created_at":1428257126000,
    "closed_at":1461765673000,
    "resolver":"leifwalsh",
    "resolved_in":"1e0b2286cd341da2cbb6ba42ae5bbc186485ef2c",
    "resolver_commit_num":0,
    "title":"DOC: expanding comparison with R section",
    "body":"might be worth expanding the [comparison with R section](-docs\/stable\/comparison_with_r.html); maybe have a basics section?\n\nwith some of the verbs from [here]()\n\n@TomAugspurger \n",
    "labels":[
      "Docs",
      "Difficulty Novice",
      "Effort Medium"
    ],
    "comments":[

    ],
    "events":[
      "labeled",
      "mentioned",
      "subscribed",
      "milestoned",
      "labeled",
      "labeled",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":1,
    "additions":73,
    "deletions":0
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":10001,
    "reporter":"a59",
    "created_at":1430164698000,
    "closed_at":1462536640000,
    "resolver":"kordek",
    "resolved_in":"1296ab39d32acaf2c77ed0c185fafe4ebcfedcb3",
    "resolver_commit_num":0,
    "title":"pandas.ExcelFile ignore parse_dates=False",
    "body":"I am trying to read an Excel file which someone else created and the wrongly formatted a column as \"date\" when it is not. It has a large integer in it, which triggers an error\n\nOverflowError: normalized days too large to fit in a C int\n\nBut I have \"parse_dates=False\" so I thought pandas.ExcelFile would not try to parse the dates and return a string instead. Is this a bug?\n",
    "labels":[
      "Excel",
      "Bug"
    ],
    "comments":[
      "you would have to show an example and `pd.show_versions()`\n",
      "In Excel (2013 Windows 7), I created a new Workbook. In Sheet1, I entered A in A1 and 10000000 in B1. I then formatted B1 to be a Short Date which displays the cell as #################. I saved the files as 'test.xlsx'.\n\nI then ran the following python code\n\n``` python\nimport pandas as pd\npd.show_versions()\nxl_file = pd.ExcelFile('test.xlsx')\nsheet = xl_file.parse('Sheet1',parse_dates=False)\n```\n\nwhich gives me the following output\n\n> ## INSTALLED VERSIONS\n> \n> commit: None\n> python: 2.7.9.final.0\n> python-bits: 64\n> OS: Darwin\n> OS-release: 14.3.0\n> machine: x86_64\n> processor: i386\n> byteorder: little\n> LC_ALL: None\n> LANG: en_US.UTF-8\n> \n> pandas: 0.16.0\n> nose: 1.3.6\n> Cython: 0.22\n> numpy: 1.9.2\n> scipy: 0.15.1\n> statsmodels: 0.6.1\n> IPython: 3.1.0\n> sphinx: 1.2.3\n> patsy: 0.3.0\n> dateutil: 2.4.2\n> pytz: 2015.2\n> bottleneck: None\n> tables: 3.1.1\n> numexpr: 2.3.1\n> matplotlib: 1.4.3\n> openpyxl: 2.0.2\n> xlrd: 0.9.3\n> xlwt: 1.0.0\n> xlsxwriter: 0.7.2\n> lxml: 3.4.3\n> bs4: 4.3.2\n> html5lib: None\n> httplib2: None\n> apiclient: None\n> sqlalchemy: 1.0.1\n> pymysql: None\n> psycopg2: None\n> Traceback (most recent call last):\n>  File \"test.py\", line 5, in <module>\n>    sheet = xl_file.parse('Sheet1',parse_dates=False)\n>  File \"\/Users\/myname\/anaconda\/lib\/python2.7\/site-packages\/pandas\/io\/excel.py\", line 287, in parse\n>    **kwds)\n>  File \"\/Users\/myname\/anaconda\/lib\/python2.7\/site-packages\/pandas\/io\/excel.py\", line 418, in _parse_excel\n>    row.append(_parse_cell(value,typ))\n>  File \"\/Users\/myname\/anaconda\/lib\/python2.7\/site-packages\/pandas\/io\/excel.py\", line 342, in _parse_cell\n>    epoch1904)\n>  File \"\/Users\/myname\/anaconda\/lib\/python2.7\/site-packages\/xlrd\/xldate.py\", line 130, in xldate_as_datetime\n>    return epoch + datetime.timedelta(days, seconds, 0, milliseconds)\n> OverflowError: date value out of range\n",
      "`parse_dates` is not implemented. But I think could be in the `_should_parse` function. pull-requests are welcome.\n",
      "Hi, I am adding this patch here, in case it's useful for those who do not want to parse dates from excel file by setting parse_dates=False.  Please review.  I had trouble parsing the following excel file from Crunchbase Excel Export which had really old dates which gave OverflowError.\n\n```\nFrom aae19c65e2a4b3a965f91bdffa5bd4595b0b7d7b Mon Sep 17 00:00:00 2001\nFrom: Kwan Lee <kwan@openviewpartners.com>\nDate: Tue, 29 Mar 2016 14:52:03 +0300\nSubject: [PATCH] parse_dates=False in read_excel should prevent from parsing\n dates.  It was throwing overflowerror when the dates were too old. update\n #10001\n\n---\n pandas\/io\/excel.py | 6 +++---\n 1 file changed, 3 insertions(+), 3 deletions(-)\n\ndiff --git a\/pandas\/io\/excel.py b\/pandas\/io\/excel.py\nindex 5656c36..b4b8996 100644\n--- a\/pandas\/io\/excel.py\n+++ b\/pandas\/io\/excel.py\n@@ -321,11 +321,11 @@ class ExcelFile(object):\n\n         epoch1904 = self.book.datemode\n\n-        def _parse_cell(cell_contents, cell_typ):\n+        def _parse_cell(cell_contents, cell_typ, parse_dates=True):\n             \"\"\"converts the contents of the cell into a pandas\n                appropriate object\"\"\"\n\n-            if cell_typ == XL_CELL_DATE:\n+            if cell_typ == XL_CELL_DATE and parse_dates:\n                 if xlrd_0_9_3:\n                     # Use the newer xlrd datetime handling.\n                     cell_contents = xldate.xldate_as_datetime(cell_contents,\n@@ -405,7 +405,7 @@ class ExcelFile(object):\n                         should_parse[j] = self._should_parse(j, parse_cols)\n\n                     if parse_cols is None or should_parse[j]:\n-                        row.append(_parse_cell(value, typ))\n+                        row.append(_parse_cell(value, typ, parse_dates))\n                 data.append(row)\n\n             if sheet.nrows == 0:\n-- \n2.7.2\n\n```\n",
      "@kwantopia It'll be easier to review that if you put it up as a pull request. Then we can comment inline.\n\nWhat's the desired behavior here? `read_csv` seems to silently \"fail\" to parse the columns that can't be represented as datetime64s.\n\n``` python\nIn [5]: !cat foo.csv\ndate,val\n1500-01-01,1\n1600-01-02,2\n1700-01-01,3\n1800-01-01,4\n1900-01-01,5\n2000-01-01,6\n\nIn [1]: pd.read_csv('foo.csv', parse_dates='date')\nOut[1]:\n         date  val\n0  1500-01-01    1\n1  1600-01-02    2\n2  1700-01-01    3\n3  1800-01-01    4\n4  1900-01-01    5\n5  2000-01-01    6\n```\n",
      "actually maybe @jorisvandenbossche can comment here. IIRC the `parse_dates` kw only matters if in excel its NOT a date already (and in your case these are out-of-range, so they are strings and will be parsed to `object` even in pandas).\n",
      "this is related to the issue in #11544 and looks to be a dupe of these (there is a somewhat convoluted chain as to what the original issues actually though). maybe someone can figure this chain out and we can create a master issue so its more clear.\n",
      "@TomAugspurger it's a problem in read_excel, but I guess I was also misunderstanding parse_dates field.  I was assuming that parse_dates=True means parse the dates and parse_dates=False means do not parse the dates for pandas.read_excel\n"
    ],
    "events":[
      "commented",
      "labeled",
      "commented",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "demilestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "milestoned",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":6,
    "additions":23,
    "deletions":4
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12947,
    "reporter":"DavidEscott",
    "created_at":1461257342000,
    "closed_at":1462632695000,
    "resolver":"zhangxiangnick",
    "resolved_in":"40ba6eb3772f841625534a602447e3e2c9865511",
    "resolver_commit_num":0,
    "title":"Confusing behavior with (multi-)assignment and _LocIndexer\/_IXIndexer",
    "body":"\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Linux\nOS-release: 2.6.32-431.29.2.el6.x86_64\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.1.1\nsetuptools: 20.4\nCython: None\nnumpy: 1.11.0\nscipy: None\nstatsmodels: None\nxarray: None\nIPython: 4.1.2\nsphinx: None\npatsy: None\ndateutil: 2.5.2\npytz: 2016.3\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: None\nopenpyxl: 2.3.4\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: None\njinja2: None\nboto: None\n",
    "labels":[
      "Usage Question",
      "Indexing",
      "Docs",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "you are missing the point here, when you use multiple columns, pandas will align for you. so you need to give it a raw array\/list if you are doing this.\n\n```\nIn [29]: df.loc[df.A== 1, [\"B\", \"C\"]] = df.loc[df.A == 1, [\"D\", \"E\"]].values\n\nIn [30]: df\nOut[30]: \n   A  B  C  D  E\n0  1  4  5  4  5\n```\n",
      "I suppose you could do a warning section in the docs. interested in that?\n",
      "I don't follow at all. Here is a little more strangeness:\n\n```\nIn [42]: df = pandas.DataFrame([[1,2,3,4,5]], columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\nIn [43]: type(df[[\"B\",\"C\"]])\nOut[43]: pandas.core.frame.DataFrame\n\nIn [44]: type(df.loc[df.A==1, [\"B\",\"C\"]])\nOut[44]: pandas.core.frame.DataFrame\n\nIn [45]: df[[\"B\", \"C\"]] = df[[\"D\", \"E\"]]\n\nIn [46]: df\nOut[46]:\n   A  B  C  D  E\n0  1  4  5  4  5\n```\n\nSo I can assign a DataFrame to another DataFrame (of compatible dimension just fine) \nUNLESS one is a .loc or .ix of the other (and then stuff gets nulled out).\n\nI don't understand the NaNs at all. LHS=RHS shouldn't result in LHS being None when RHS is not None. That doesn't sound like correct behavior at all.\n\nAnother weird thing that happens:\n\n```\nIn [93]: df = pandas.DataFrame([[1,2,3,4,5]], columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\nIn [94]: df2 = df.loc[:,[\"B\",\"C\"]]\n\nIn [95]: df3 = df.loc[:,[\"D\",\"E\"]]\n\nIn [96]: df2.loc[:,:] is df2\nOut[96]: True\n\nIn [97]: df2.loc[:,:] = df3\n\nIn [98]: df2\nOut[98]:\n    B   C\n0 NaN NaN\n\nIn [99]: df\nOut[99]:\n   A  B  C  D  E\n0  1  2  3  4  5\n```\n\nbut since `df2.loc[:,:] is df2` this should be equivalent to: `df.loc[:,[\"B\",\"C\"]] = df3` which of course we have seen is not the case.\n\nTherefore with Pandas `X.foo().bar()` is not the same thing as `_ = X.foo(); _.bar()`. That is something I find super scary.\n",
      "you are doing 2 different things, in `[45]` you are saying take these columns and assign to these, this ignores alignment because its a column asssignment.\n\nwhile above in my `[29]` you are assigning part of a frame, this is a conceptual difference and as expected.\n"
    ],
    "events":[
      "commented",
      "labeled",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files":1,
    "additions":20,
    "deletions":0
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13057,
    "reporter":"dancsi",
    "created_at":1462204824000,
    "closed_at":1462633183000,
    "resolver":"dancsi",
    "resolved_in":"5541fd7c8dc0ad017057cad00ce70e7f9d9ee1f8",
    "resolver_commit_num":0,
    "title":"ENH: add option to tz_localize to return NaT instead of raising a NonExistentTimeError",
    "body":"It would be nice if the `tz_localize` function of a `DatetimeIndex` had an optional flag for silently returning `NaT` instead of throwing a `NonExistentTimeError`, if the timestamp is not valid in the given timezone (for example due to DST changes).\nI ran into this problem while trying to `tz_localize` a large index, and it seems to me that this would be a much better solution than manually handling the exception with a lambda expression in a (slow) python loop.\n",
    "labels":[
      "Timezones",
      "API Design",
      "Missing-data",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments":[
      "pls show an example\n\ntz_localize already has the ambiguous argument for this purpose\n",
      "and pd.show_versions()\n",
      "Here is a minimal example\n\n``` Python\nimport pandas as pd\n\ndf = pd.DataFrame({'large_series': [pd.Timestamp('2015-03-08 02:30:00')]})\nind = pd.DatetimeIndex(df['large_series']) \nind = ind.tz_localize('America\/Los_Angeles')\n```\n\n(imagine that `large_series` is indeed a long column, with some timestamps that are invalid)\nThe error that is thrown:\n\n```\nTraceback (most recent call last):\n  File \"C:\/Dev\/temp\/pandas_demo.py\", line 5, in <module>\n    ind = ind.tz_localize('America\/Los_Angeles')\n  File \"C:\\Dev\\Python35\\lib\\site-packages\\pandas\\util\\decorators.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Dev\\Python35\\lib\\site-packages\\pandas\\tseries\\index.py\", line 1843, in tz_localize\n    ambiguous=ambiguous)\n  File \"pandas\\tslib.pyx\", line 3914, in pandas.tslib.tz_localize_to_utc (pandas\\tslib.c:67511)\npytz.exceptions.NonExistentTimeError: 2015-03-08 02:30:00\n```\n\nNote that the exception is `pytz.exceptions.NonExistentTimeError`, and not `pytz.AmbiguousTimeError`, that is handled by the `ambiguous` flag. It seems that in the current master, [this line](https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/tslib.pyx#L4072) is responsible.\nFinally, the output of `pd.show_versions()`\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Windows\nOS-release: 10\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\nbyteorder: little\nLC_ALL: en_US.UTF_8\nLANG: en_US.UTF-8\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.1.1\nsetuptools: 20.10.1\nCython: 0.23.1\nnumpy: 1.11.0\nscipy: 0.17.0\nstatsmodels: None\nxarray: 0.7.2\nIPython: 4.2.0\nsphinx: None\npatsy: None\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: 1.0.0\ntables: None\nnumexpr: 2.5.2\nmatplotlib: 1.4.3\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: 4.4.0\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\n```\n",
      "so you want a `errors='coerce'` with the default being `'raise'`. which will `NaT` the datetime.\n\nok I suppose, though this indicates a fundamental issue that you have. I don't think hiding this is the right answer. How did you generate this in the first place?\n\ncc @rockg\ncc @ischwabacher\ncc @adamgreenhall\n",
      "Exactly. I got the data from an external source ([here](http:\/\/www.transtats.bts.gov\/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time), if you are interested). There are just a few timestamps out of 500k that are a few minutes after 2am on the day when DST becomes active, so I believe they are just an error in the dataset\n",
      "ok, unless other objections, I don't see adding a coercion option as a problem. pull-requests welcome!\n",
      "Here it is #13058. Hopefully, I didn't miss anything, as it is my first time contributing to a large OSS project.\n"
    ],
    "events":[
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "milestoned",
      "labeled",
      "labeled",
      "referenced",
      "referenced",
      "cross-referenced",
      "commented"
    ],
    "changed_files":5,
    "additions":76,
    "deletions":9
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":10486,
    "reporter":"DSLituiev",
    "created_at":1435794649000,
    "closed_at":1462641150000,
    "resolver":"tsstchoi",
    "resolved_in":"881a690c370c411c555e10ea7665688fd9014912",
    "resolver_commit_num":0,
    "title":"BUG: query with invalid dtypes should fallback to python engine",
    "body":"The following call of `df.query()` produces an error:\n\n\n\nin pandas 0.15.2\n\n\n",
    "labels":[
      "Bug",
      "Dtypes",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments":[
      "well, `numexpr` does not support this, nor are string ops actually passed. You should simply use regular indexing\n\n`df[df.gene == \"Actb\"]`\n\nI suppose this is a bug in that this should fall back to the python engine.\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "renamed",
      "labeled",
      "labeled",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":1,
    "additions":1,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":11981,
    "reporter":"ChristopherShort",
    "created_at":1452161044000,
    "closed_at":1463005107000,
    "resolver":"yaduart",
    "resolved_in":"d0734ba4d0f4c228110dc3974943ce4ec2adeea4",
    "resolver_commit_num":0,
    "title":"interation of set_eng_float_format and pivot_tables",
    "body":"There appears to be an interaction between the option for float display length and pivot_table methods on integers. This issue arose when working on a dataset with both floats and integers - but this arose on operations on integers part of the dataset.\n\nThe following example throws an error from the `format.py` module.\n\nSwap the comments on the formatting options  (undoing the setting for floats) and it runs fine.\n\nIn my dataset - there were no missing values following the pivot table operation and the error still occurs. In the sample code below, NaNs may be causing the formatting issue, but that didn't appear to be the case for me.\n\n\n\ncheers\nChris\n",
    "labels":[
      "Bug",
      "Missing-data",
      "Output-Formatting",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "not so friendly, needs a nan-check before handing off to `Decimal`.\n\nwant to do a pull-request?\n",
      "OK - I'll have a go since you've classified it as a novice level issue. Last time I tried (with a csv issue in 2014), I got lost very quickly.  I'll get back to you in a week or so.\n\ncheers\n",
      "@ChristopherShort ok thanks. lmk.\n",
      "Hi,\nThis is my first bug fix. If this bug has been schedule to released in the next major release, which document should I update in doc\/source\/whatsnew\/vx.y.z.txt (would it be the document v0.18.2)\n\nRegards,\nYadu\n",
      "0.18.2\n",
      "yadu - thanks - this has been on my conscience for a couple of months (the 1 week blew right past).\n\nI'll find something else I can contribute too.\n",
      "No worries. cheers.\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "commented",
      "commented",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files":3,
    "additions":21,
    "deletions":2
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13029,
    "reporter":"DavidDobr",
    "created_at":1461930655000,
    "closed_at":1463181283000,
    "resolver":"Xndr7",
    "resolved_in":"01dd11109a0d1def8bc3b03d06c533817cc273f2",
    "resolver_commit_num":0,
    "title":"DOC: additional join examples in \"10 Minutes to pandas\"",
    "body":"---\n\nTutorial at [pandas.pydata.org\/pandas-docs\/stable\/10min.html](-docs\/stable\/10min.html) has the following code, where the obvious mistake is having only 1 key \"foo\" instead of 2 keys: \"foo\" and \"bar\" throughout the whole example \n\n---\n# Join\n## SQL style merges. See the Database style joining\n\n\n\nHere is how it **should** be (replaced second 'foo' with 'bar':\n\n![screenshot from 2016-04-29 14-49-45](-0e19-11e6-92f4-093f879824e4.png)\n",
    "labels":[
      "Docs",
      "Reshaping",
      "Difficulty Novice"
    ],
    "comments":[
      "@DavidDobr this is not a mistake, but probably a common case. You can certainly _add_ to this to show the example with foo & bar.\n"
    ],
    "events":[
      "renamed",
      "renamed",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "cross-referenced"
    ],
    "changed_files":1,
    "additions":11,
    "deletions":0
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":11773,
    "reporter":"michaelaye",
    "created_at":1449405987000,
    "closed_at":1463400591000,
    "resolver":"quintusdias",
    "resolved_in":"62bed0e33397132bd4340c8da54c3feeb22e5083",
    "resolver_commit_num":0,
    "title":"read_hdf not supporting pathlib.Path",
    "body":"According to the Enhancements text for 17.1:\n\n> pd.read_\\* functions can now also accept pathlib.Path, or py._path.local.LocalPath objects for the filepath_or_buffer argument. (GH11033) \n\nall read_\\* functions should support pathlib.Path now. It works for me with `read_csv` but not for `read_hdf`. \nInterestingly, I find in the squashed PR for this issue, 0d3dcbb, only a comment regarding `read_csv` and `read_table`, while the merge 82f0033 talks about `read_*`. Is there something missing for `read_hdf` ?\n",
    "labels":[
      "Testing",
      "IO HDF5",
      "Compat",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "yeh, this was not tested on all of the IO formats. pretty sure that `csv\/excel\/stat\/msgpack\/json` should work as they all use the same io calls. HDF uses a slightly different one as it doesn't support reading from buffers in the same way.\n\nSo need tests for each of these formats (and not just the IO call itself) on read\/to\nHDF test\/fix\n\ncc @flying-sheep \n",
      "jup, [you told me](https:\/\/github.com\/pydata\/pandas\/issues\/11033#issuecomment-138912214)\n\n> [here](https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/io\/common.py#L127) is where all of this path inference is done (well not 100% sure of all but vast majority)\n\nso it only was the vast majority, not all :smile: \n\nPS: the link is broken by now (tip: press <kbd>y<\/kbd> on some github page to get a canonical url so that this doesn\u2019t happen anymore)\n",
      "`to_csv` also does not support pathlib.Path yet. (v0.17.1), while `read_csv` does, interestingly.\n"
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced"
    ],
    "changed_files":3,
    "additions":47,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13172,
    "reporter":"starplanet",
    "created_at":1463156689000,
    "closed_at":1463492450000,
    "resolver":"starplanet",
    "resolved_in":"20ea4064b0c94f99c275bfc4217664cc8aea75c5",
    "resolver_commit_num":0,
    "title":"COMPAT: unicode_literals conflict with to_records",
    "body":"#### Code Sample, a copy-pastable example if possible\n\n\n\nthe code above will report following error:\n\n\n\nIf I comment out `from __future__ import unicode_literals`, the code above will work fine.\n\n\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.4.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: zh_CN.UTF-8\n\npandas: 0.18.1\nnose: None\npip: 8.1.1\nsetuptools: 19.4\nCython: None\nnumpy: 1.11.0\nscipy: None\nstatsmodels: None\nxarray: None\nIPython: 4.2.0\nsphinx: 1.3.5\npatsy: None\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: None\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: 4.4.1\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\npandas_datareader: None\n",
    "labels":[
      "Unicode",
      "2\/3 Compat",
      "Compat"
    ],
    "comments":[
      "https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/core\/frame.py#L1068 should prob be\n\n`lmap(compat.u, self.columns)`\n\nwant to do a PR?\n",
      "OK, I will have a try.\n\n\u539f\u59cb\u90ae\u4ef6\n\u53d1\u4ef6\u4eba:Jeff Rebacknotifications@github.com\n\u6536\u4ef6\u4eba:pydata\/pandaspandas@noreply.github.com\n\u6284\u9001:starplanetzhangjinjie@yimian.com.cn; Authorauthor@noreply.github.com\n\u53d1\u9001\u65f6\u95f4:2016\u5e745\u670814\u65e5(\u5468\u516d)\u200700:56\n\u4e3b\u9898:Re: [pydata\/pandas] unicode_literals conflict with to_records(#13172)\n\nhttps:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/core\/frame.py#L1068 should prob be\nlmap(compat.u, self.columns)\nwant to do a PR?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files":3,
    "additions":10,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13213,
    "reporter":"fmarczin",
    "created_at":1463575407000,
    "closed_at":1463663686000,
    "resolver":"fmarczin",
    "resolved_in":"eeccd058a5199c3e4fd9900b95e00672f701b3e9",
    "resolver_commit_num":0,
    "title":"json_normalize() can't deal with non-ascii characters in unicode keys",
    "body":"Example code:\n\n\n\nOutput:\n\n\n\nExpected output\n\n\n\nThe cause are probably\n#L618\nand #L620\n\nThose lines seemingly were introduced to deal with numeric types, but fail when `k` is a Unicode object containing non-ascii characters.\n\nIt seems to be the same bug in principle as \n",
    "labels":[
      "Bug",
      "Unicode"
    ],
    "comments":[

    ],
    "events":[
      "referenced",
      "referenced",
      "cross-referenced",
      "labeled",
      "labeled"
    ],
    "changed_files":3,
    "additions":27,
    "deletions":2
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13149,
    "reporter":"mao-liu",
    "created_at":1463020979000,
    "closed_at":1464036345000,
    "resolver":"pijucha",
    "resolved_in":"afde7187e22b2013147d0a15911f6ec72e056a43",
    "resolver_commit_num":0,
    "title":"NaNs in Float64Index are converted to silly integers using index.astype('int')",
    "body":"#### Code Sample, a copy-pastable example if possible\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels":[
      "Bug",
      "Dtypes",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "This is numpy behaviour:\n\n```\nIn [22]: np.array([np.nan, 1.]).astype(int)\nOut[22]: array([-2147483648,           1])\n```\n\nBut, we should probably check for the occurence of NaNs, just as we do for Series:\n\n```\nIn [29]: df.iloc[0,0] = np.nan\n\nIn [30]: df.a\nOut[30]:\nNaN   NaN\n 1      2\nName: a, dtype: float64\n\nIn [31]: df.a.astype(int)\n...\n\nC:\\Anaconda\\lib\\site-packages\\pandas\\core\\common.pyc in _astype_nansafe(arr, dty\npe, copy)\n   2726\n   2727         if np.isnan(arr).any():\n-> 2728             raise ValueError('Cannot convert NA to integer')\n   2729     elif arr.dtype == np.object_ and np.issubdtype(dtype.type, np.intege\nr):\n   2730         # work around NumPy brokenness, #1987\n\nValueError: Cannot convert NA to integer\n```\n",
      "I wanted to fix this bug but noticed a similar behaviour of other objects: DatetimeIndex, TimedeltaIndex, Categorical, CategoricalIndex. Namely (all four of them behave identically):\n\n```\nA = pd.DatetimeIndex([1e10,2e10,None])\nA\nOut[76]: DatetimeIndex(['1970-01-01 00:00:10', '1970-01-01 00:00:20', 'NaT'], dtype='datetime64[ns]', freq=None)\nA.astype(int)\nOut[77]: array([         10000000000,          20000000000, -9223372036854775808])\n```\n\nHowever, unlike with Float64Index, this is invertible:\n\n```\npd.DatetimeIndex(A.astype(int))\nOut[78]: DatetimeIndex(['1970-01-01 00:00:10', '1970-01-01 00:00:20', 'NaT'], dtype='datetime64[ns]', freq=None)\n```\n\nMy question: is this behaviour also a bug and should be fixed the same way (raising a ValueError)? And if so, should all the fixes be placed into one commit\/pull request?\n\nBy the way, there might be other objects with the same issue, which call numpy.ndarray.astype(). And numpy is also a bit inconsistent here:\n\n```\nnp.array([1,np.nan]).astype(int)\nOut[84]: array([                   1, -9223372036854775808])\nnp.array([1,np.nan], dtype = int)\nTraceback...\nValueError: cannot convert float NaN to integer\n```\n",
      "@ch41rmn these are all as expected. converting to `int` converts to the underlying integer based representation. \n\nThe _only_ issue is that `Float64Index.astype(int)` should raise (as its effectively non-convertible).\n",
      "@jreback I actually think we should raise in the datetimeindex case as well (ideally). A `NaT` cannot be converted to int (just as float nan cannot be converted). There is the `asi8` attribute if you want this. \nBut, of course, that is not really back compat. Internally I think we consequently use `asi8`? But not sure about external use of course\n",
      "Raising for CategoricalIndex seems less of a problem (not a common thing to do)\n",
      "This is excactly what should be returned (and is useful). yes its equivalen to internal `.asi8`, but I dont' see a good reason to NOT do this.\n\n```\nIn [20]: pd.DatetimeIndex([1e10,2e10,None]).astype(int)\nOut[20]: array([         10000000000,          20000000000, -9223372036854775808])\n```\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files":17,
    "additions":330,
    "deletions":132
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13235,
    "reporter":"roycoding",
    "created_at":1463730077000,
    "closed_at":1464178492000,
    "resolver":"roycoding",
    "resolved_in":"87492737f2f81183700849d453c38d507f128811",
    "resolver_commit_num":0,
    "title":"label keyword argument for resample causes an error when used with a groupby object",
    "body":"#### Code Sample, a copy-pastable example if possible\n\n\n#### The dataframe\n\n\n#### Expected Output\n\n\n#### Actual ouput\n\n\n\nWithout the `label` keyword, I get this (with expected right labeled dates):\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels":[
      "Bug",
      "Resample",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "So the `**kwargs` should not be passed [here](https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/tseries\/resample.py#L913)\n\nsimple fix. These args are all eaten by the `TimeGrouper` except those explicity named.\n\nwant to do a PR?\n",
      "> want to do a PR?\n\nI'll take a shot at it. Thanks.\n"
    ],
    "events":[
      "renamed",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files":3,
    "additions":27,
    "deletions":4
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13282,
    "reporter":"nparley",
    "created_at":1464170625000,
    "closed_at":1464197710000,
    "resolver":"nparley",
    "resolved_in":"b4e2d34edcbc404f6c90f76b67bcc5fe26f0945f",
    "resolver_commit_num":0,
    "title":"pandas.show_versions causing malloc_error_break",
    "body":"If blosc is installed show_versions() will caused python to produce a malloc_error_break (double free) error on exiting. This can be seen in the travis runs under `source activate pandas && ci\/print_versions.py` section but can also be easily replicated on Linux and Mac. The fix for this is to replace imp (which has been deprecated) with importlib. I will create and link a PR.\n\n\n\n>>> \npython(1726,0x7fff77074000) malloc: *** error for object 0x102573a00: pointer being freed was not allocated\n*** set a breakpoint in malloc_error_break to debug\nAbort trap: 6\n\n\n\n``````\n",
    "labels":[

    ],
    "comments":[
      "excellent @nparley \n"
    ],
    "events":[
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files":1,
    "additions":2,
    "deletions":6
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12188,
    "reporter":"edublancas",
    "created_at":1454207151000,
    "closed_at":1464267262000,
    "resolver":"edublancas",
    "resolved_in":"57ea76fb9a1d0b23943c700a6129d37de6df6adc",
    "resolver_commit_num":0,
    "title":"Confusing interpretation of what DataFrame.join does",
    "body":"From the docs I see that the difference between .merge and .join is that .join operates using indexes by default, but it also lets you use columns,  so I tried to use it for that since it sounds natural when coming from the SQL world.\n\nFrom the [docs](-docs\/version\/0.17.1\/generated\/pandas.DataFrame.join.html):\n\n> on : column name, tuple\/list of column names, or array-like\n> Column(s) to use for joining, otherwise join on index. If multiples columns given, the passed DataFrame must have a MultiIndex. Can pass an array as the join key if not already contained in the calling DataFrame. Like an Excel VLOOKUP operation\n\nFrom my understanding, if on is absent a join operation is performed on the index, if on is present, it would be reasonable to think that the same operation would be performed.\n\nHaving said that:\n\n\n\nOutput:\n\n\n\nAnd if I add id as index:\n\n\n\nOutput:\n\n\n\nIs that the correct behavior? If yes, I think the documentation is misleading. It took me a lot to find the bug in my code and I ended up using merge since .join works in an unexpected way.\n\nI don't think I'm the [only one](-simple-join-not-working) with this issue, so maybe a change in the documentation would help to clarify how .join works.\n",
    "labels":[
      "Docs",
      "Reshaping",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "xref to #6336 \n",
      "So the above join operation does this\n\n```\nIn [3]: a.join(b, on='id', lsuffix='_x',  rsuffix='_y', how='left')\nOut[3]: \n           address_x  id_x          address_y  id_y\n0   1820 SOME STREET     1    1140 BIG STREET     3\n1  32 ANOTHER STREET     2    20 SMALL STREET     4\n2    1140 BIG STREET     3   1820 SOME STREET     1\n3    20 SMALL STREET     4  32 ANOTHER STREET     2\n4   1090 AVENUE NAME     5                NaN   NaN\n```\n\nthis merge\n\n```\nIn [4]: pd.merge(a, b, left_on='id', right_index=True, suffixes=('_x','_y'), how='left')\nOut[4]: \n           address_x  id_x          address_y  id_y\n0   1820 SOME STREET     1    1140 BIG STREET     3\n1  32 ANOTHER STREET     2    20 SMALL STREET     4\n2    1140 BIG STREET     3   1820 SOME STREET     1\n3    20 SMALL STREET     4  32 ANOTHER STREET     2\n4   1090 AVENUE NAME     5                NaN   NaN\n```\n\nWhereas I think you _think_ it should do this one\n\n```\nIn [5]: pd.merge(a, b, left_on='id', right_on='id', suffixes=('_x','_y'), how='left')\nOut[5]: \n           address_x  id          address_y\n0   1820 SOME STREET   1   1820 SOME STREET\n1  32 ANOTHER STREET   2  32 ANOTHER STREET\n2    1140 BIG STREET   3    1140 BIG STREET\n3    20 SMALL STREET   4    20 SMALL STREET\n4   1090 AVENUE NAME   5   1090 AVENUE NAME\n```\n\nI agree that is a bit confusing a bit, but I think the rationale is that you are joining from left, and the `on` controls what you are joining (on the left). There is no way in the `.join` to control the rhs, which is pre-supposed to be the index.\n\nSo this is what\n\n```\nIn [9]: a.join(b.set_index('id'), on='id', lsuffix='_x',  rsuffix='_y', how='left')\nOut[9]: \n           address_x  id          address_y\n0   1820 SOME STREET   1   1820 SOME STREET\n1  32 ANOTHER STREET   2  32 ANOTHER STREET\n2    1140 BIG STREET   3    1140 BIG STREET\n3    20 SMALL STREET   4    20 SMALL STREET\n4   1090 AVENUE NAME   5   1090 AVENUE NAME\n```\n\nSo I think a doc-update, maybe with some more examples would help?\n",
      "Thanks for the clarification! I agree that a documentation update will be really helpful.\n\nBesides an update in the [join method doc](http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.17.1\/generated\/pandas.DataFrame.join.html), a brief mention of this behavior would be useful at the bottom of Database-style DataFrame joining\/merging section [here](http:\/\/pandas.pydata.org\/pandas-docs\/stable\/merging.html#database-style-dataframe-joining-merging).\n",
      "sure - pull requests would be welcome!\n"
    ],
    "events":[
      "commented",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":2,
    "additions":84,
    "deletions":13
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13260,
    "reporter":"rladeira",
    "created_at":1464028642000,
    "closed_at":1464286544000,
    "resolver":"pfrcks",
    "resolved_in":"5d6772074a89c1ed7a5c24b078215cb7f9cc6eb3",
    "resolver_commit_num":0,
    "title":"DOC: Strange behavior when combining astype and loc",
    "body":"I am trying to use `astype` together with `loc` to convert just a subset of columns to a specified dtype. However, I can't understand what is actually happening. Is this the expected behavior when combining \n`astype` and `loc`?\n#### Code Sample, a copy-pastable example if possible\n\n\n\nOutput:\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.6.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.2.0-36-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.0\nnose: None\npip: 8.1.1\nsetuptools: 20.6.7\nCython: None\nnumpy: 1.11.0\nscipy: 0.17.0\nstatsmodels: None\nxarray: None\nIPython: 4.1.2\nsphinx: None\npatsy: None\ndateutil: 2.5.2\npytz: 2016.3\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\n",
    "labels":[

    ],
    "comments":[
      "This is as expected; use getitem if you want to re-assign w\/o masking. In this case your mask is ':' (meaning the entire columns), but upcasting is done when used in this way (and that's why its upcast to `int64`).\n\nIOW, `.loc` will try to fit in what you are assigning to the current dtypes, while `[]` will overwrite them taking the dtype from the rhs. subtle point.\n\n```\nIn [1]: df = pd.DataFrame({'a': [1,2,3], 'b': [4,5,6], 'c': [7, 8, 9]})\n\nIn [2]: df\nOut[2]: \n   a  b  c\n0  1  4  7\n1  2  5  8\n2  3  6  9\n\nIn [3]: df[['a','b']] = df[['a','b']].astype(np.uint8)\n\nIn [4]: df\nOut[4]: \n   a  b  c\n0  1  4  7\n1  2  5  8\n2  3  6  9\n\nIn [5]: df.dtypes\nOut[5]: \na    uint8\nb    uint8\nc    int64\ndtype: object\n```\n",
      "This is a bit related to the docs we added here: https:\/\/github.com\/pydata\/pandas\/pull\/13070\n\nIf you'd like to propose an example for this section would take it: http:\/\/pandas-docs.github.io\/pandas-docs-travis\/basics.html#astype (e.g. showing pitfalls, e.g. this example)\n",
      "Subtle point, indeed. Thanks for the clarification.\n",
      "let me reopen as a doc-issue.\n"
    ],
    "events":[
      "commented",
      "closed",
      "commented",
      "commented",
      "commented",
      "reopened",
      "milestoned",
      "renamed",
      "cross-referenced"
    ],
    "changed_files":1,
    "additions":22,
    "deletions":0
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12907,
    "reporter":"educhana",
    "created_at":1460807335000,
    "closed_at":1464308129000,
    "resolver":"camilocot",
    "resolved_in":"0f1666d8adfa8e121a935309b1d7ca6effec813c",
    "resolver_commit_num":0,
    "title":"read_html: support \"decimal\" argument for parsing numbers, like read_csv",
    "body":"read_csv has support for declaring the decimal and thousands separator.\n\nread_html is missing the 'decimal' parameter. it'd be useful and more consistent to accept it too.\n\nExample:\n\n\n",
    "labels":[
      "Enhancement",
      "Difficulty Novice",
      "IO HTML",
      "Effort Low"
    ],
    "comments":[
      "this is a dupe of #8200 but we'll close that issue. This should be straightforward and just need to pass this thru to the parser. Pull-requests are welcome.\n",
      "I don't think that `TextParser` supports passing the decimal keyword in python. The functionality probably needs to be added to PythonParser\n",
      "This defaults to the c-engine, so I don't really see a problem. If it actually is, then you could defer to the python engine if the `decimal` kw is not-None.\n\nhttps:\/\/github.com\/pydata\/pandas\/issues\/12933 to add the `decimal` option (I don't know if we have that particular issue).\n\nAlternatively, you could post-process.\n\n```\nIn [7]: s = Series(['1,2','2,3'])\n\nIn [8]: s\nOut[8]: \n0    1,2\n1    2,3\ndtype: object\n\nIn [9]: pd.to_numeric(s.str.replace(',','.'),errors='coerce')\nOut[9]: \n0    1.2\n1    2.3\ndtype: float64\n```\n"
    ],
    "events":[
      "commented",
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":49,
    "deletions":12
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12388,
    "reporter":"jennolsen84",
    "created_at":1455846280000,
    "closed_at":1464619174000,
    "resolver":"jennolsen84",
    "resolved_in":"cc1025a62019215a0fa38a891e07e6ca6ba656f1",
    "resolver_commit_num":0,
    "title":"pd.eval division operation upcasts float32 to float64 ",
    "body":"The current behavior is inconsistent with normal python division of two `DataFrame`s (see code sample).\n\nPandas upcasts both terms to 64-bit floats when it detects a division, see:\n\n#L453\n\nI think numexpr can handle different types too, and upcast automatically, though I am not 100% sure.  I can submit a PR, but how do you recommend fixing this?  Something like the following?\n\n\n\nThe downside is that if someone does `2 + df`, they'll probably still end up upcasting it.  But this proposal is still better than what we have today\n\nI might re-write the above using `filter` too, but at this time I just wanted to discuss the general approach\n#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels":[
      "Dtypes",
      "Numeric"
    ],
    "comments":[
      "```\nIn operations implying a scalar and an array, the normal rules of casting are used in Numexpr, in contrast with NumPy, where array types takes priority. For example, if 'a' is an array of type `float32` and 'b' is an scalar of type `float64` (or Python `float` type, which is equivalent), then 'a*b' returns a `float64` in Numexpr, but a `float32` in NumPy (i.e. array operands take priority in determining the result type). If you need to keep the result a `float32`, be sure you use a `float32` scalar too.\n```\n\n(this is different that what you are saying, but should prob handle non-the-less). I would do this test\/casting in `_cast_inplace` itself.\n",
      "`numpy` behavior seems to make more sense.  \n\n```\npd.eval('3.5 \/ float32array')\n```\n\nis much easier to write than:\n\n```\ns = np.float32('3.5')\npd.eval('s \/ float32array')\n```\n\nAlso, if someone that didn't read the `numexpr` docs super carefully, they would've missed the little detail.\n\nTherefore, should we mimic `numpy` behavior?\n\nAs for `_cast_inplace`, should we modify the signature?  After the changes, it would be much more specialized function.  It looks like it is only used once, so we have that going for us.\n",
      "Thought about it some more\n\nWe could look at the whole expression, and come up with an output datatype:\n\n```\nIf all array elements in an expression are floats32 and ints:\nthen\n    output type = float32\nelse:\n    output type = float64\n```\n\nThis still has corner cases like adding two int32 arrays will result in float64.  It is unclear what the solution of adding two int32 arrays should be: If the numbers are small, then int32 array as an output array is OK, but if the numbers are big you need int64 arrays.  A way around this would be to let the user specify an `out` parameter.  We could do extra checks to warn the user in case there are incompatiblities, like if two float64s are being added, but the output type is float32, etc.\n\nSo, the proposal now becomes:\n1.  Add `out` parameter to let user specify the destination of the datatype.  must be ndarray or a pandas object (so either has `.dtype` or `.values.dtype`)\n2.  Choose an output array dtype to be one of `{float64, float32}`, depending on datatypes of arrays in the expression.  `float32` is chosen if all arrays in the expression have dtypes of float32 or any of the ints, otherwise `float64` is chosen.\n3.  Warn if `out` is specified, and is `float32` array, but input contains `float64` array.\n",
      "I don't recall why we are casting in the first place. I would ideally like to defer this entirey to the engine. \n@chris-b1 @cpcloud any recall?\n\nif not, then would be ok with passing a `dtype=` argument for casting and default to the minimum casting needed (though this just adds another layer of indirection but I guess needs to be done).\n",
      "Should we go with `numpy` casting behavior (instead of `numexpr`)?  `numpy` behavior is consistent `pandas` when `numexpr` is not used.\n\nSo, what we'd have to do here is to down-cast constants from float64 to float32, if and only if all arrays are float32s.  E.g., `numpy` and `pandas` will use float64 as output dtype when int32 arrays are multiplied with float32 constant.  So, it seems like float32 array case is the main thing we have to worry about.\n\ne.g.\n\n```\nIn [1]: import pandas as pd\nIn [2]: import numpy as np\nIn [3]: pd.Series(np.arange(5, dtype=np.float32)) * 2.0\nOut[3]: \n0    0\n1    2\n2    4\n3    6\n4    8\ndtype: float32\n\nIn [11]: a = pd.Series(np.arange(5, dtype=np.int32)) * np.float32(1.1)\nIn [12]: a\nOut[12]: \n0    0.0\n1    1.1\n2    2.2\n3    3.3\n4    4.4\ndtype: float64\n\nIn [13]: np.arange(5, dtype=np.int32) * np.float32(1.1)\nOut[13]: array([ 0.        ,  1.10000002,  2.20000005,  3.30000007,  4.4000001 ])\nIn [14]: z = np.arange(5, dtype=np.int32) * np.float32(1.1)\nIn [15]: z.dtype\nOut[15]: dtype('float64')\n\n```\n",
      "I think you have to upcast by default, the only way I wouldn't would be if the users indicated (with `dtype=`) that its ok to proceed and then I would simply cast things to the passed dtype so the underlying wouldn't then upcast.\n",
      "but wouldn't this result in inconsistent behavior between normal pandas binary operations (like `s * 2.0`, which does not upcast s if it is a float32 series) and `pd.eval('s * 2.0')`, which will end up upcasting?\n",
      "@jennolsen84 hmm. that is a good point. just trying to avoid pandas do _any_ casting here. What if we remove that and just let the engine do it? (I don't really recall why this is special cased here). Or if we are forced to do it, then I guess you are right would have to do a lowest-common denonimator cast (maybe use `np.find_common_type`\n",
      "how about this as a start?  https:\/\/github.com\/jennolsen84\/pandas\/commit\/c82819fe483bb7dd218e94caabc4cd806b488275\n\nI manually tested it, and the behavior is now consistent with non-numexpr related code.  I am trying to avoid casting un-necessarily as you recommended, and letting the lower-level libraries take care of a lot of things.\n\nI did run the nosetests, and they all pass on existing tests.\n\nIf the commit looks good to you, I can add in some tests, add to docs, etc. and submit a PR.\n",
      "@jreback can you please take another look at the commit?  I addressed your comment, and I am not sure if you missed it.\n",
      "@jennolsen84 yeh just getting back to this.\n\nyour soln seems fine. However I still don't understand _why_ it is necessary to upcast (and only for division); what does numexpr do (if you don't upcast)? is it wrong?\n",
      "We're casting to float32 in all ops (not just division).\n\nThe division thing was another case where `pandas` was casting to `float`(64), so I had to make a change there as well.\n\nThe reason why the cast happens at all is for some reason `numexpr` would cast a scalar 64 bit float \\* array 32 bit float to 64-bit floats.  I am not sure why.  This is inconsistent with `numpy`, and un-necessarily slower and takes up more RAM.\n\nI will submit a PR (with whatsnew and tests)\n",
      "thanks @jennolsen84 why don't you submit and we'll go from there\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "milestoned"
    ],
    "changed_files":4,
    "additions":57,
    "deletions":2
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13219,
    "reporter":"PeterKucirek",
    "created_at":1463589322000,
    "closed_at":1464779621000,
    "resolver":"hassanshamim",
    "resolved_in":"fcd73ad2e7482414b61d47056c6c9c220b11702c",
    "resolver_commit_num":0,
    "title":"Unicode not acceptable input for `usecols` kwarg in `read_csv()`",
    "body":"I caught this bug while updating from version 0.18.0 to 0.18.1. The kwarg `usecols` no longer accepts unicode column labels.\n\nExample below:\n\n\n\nI note that 0.18.1 introduced the requirement that `usecols` be all string or all ints. This makes sense but it looks like the implementation also throws away unicode strings.\n",
    "labels":[
      "Bug",
      "Difficulty Novice",
      "Unicode",
      "Effort Low",
      "CSV"
    ],
    "comments":[
      "makes sense, pull-requests welcome to fix\n\ncc @gfyoung \n",
      "@PeterKucirek: good catch!  PR is definitely in order here.  Shouldn't be too bad to implement.\n"
    ],
    "events":[
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced"
    ],
    "changed_files":3,
    "additions":111,
    "deletions":9
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13306,
    "reporter":"dlpd",
    "created_at":1464388697000,
    "closed_at":1464890421000,
    "resolver":"uwedeportivo",
    "resolved_in":"ce56542d1226adf8b3439c51f0c34b49dd53bb28",
    "resolver_commit_num":0,
    "title":"Hour overflow in tz-aware datetime conversions",
    "body":"Comparison of tz-aware timestamps fails across DST boundaries. The comment in tslib.pyx:3845\n\n\n\nperhaps implies this is a known problem that was never resolved, so apologies if a new issue is not appropriate.\n#### Self contained example\n\n\n#### Output\n\nExpected output:\n\n\n\nActual output of timedelta computation:\n\n\n\nComputed timedelta for rows after the 2008-12-12 date are off by an hour.\n\nOutput of tz_convert:\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels":[

    ],
    "comments":[
      "I get exactly the output you have above. What exactly is the issue?\n",
      "Sorry for being unclear, the output is wrong - these timestamps are not an hour apart. The output above is the actual (buggy) output, not what it should be.\n",
      "maybe the example is not clear. Can you show something and have an expected which is obvious. \n",
      "The expected output is the correct timedelta - updated to reflect that.\n",
      "```\nIn [36]: df = DataFrame(\n{'A' : pd.to_datetime(['2008-05-12 13:50:33',\n                       '2008-12-12 14:50:35',\n                       '2008-05-12 13:50:32']).tz_localize('US\/Eastern'), \n'B' : Timestamp('2008-05-12 13:50:33',tz='US\/Eastern')}\n)\n\nIn [37]: df\nOut[37]: \n                          A                         B\n0 2008-05-12 13:50:33-04:00 2008-05-12 13:50:33-04:00\n1 2008-12-12 14:50:35-05:00 2008-05-12 13:50:33-04:00\n2 2008-05-12 13:50:32-04:00 2008-05-12 13:50:33-04:00\n\nIn [39]: df.dtypes\nOut[39]: \nA    datetime64[ns, US\/Eastern]\nB    datetime64[ns, US\/Eastern]\ndtype: object\n\nIn [40]: df.B-df.A\nOut[40]: \n0       0 days 00:00:00\n1   -215 days +22:59:58\n2       0 days 01:00:01\nName: B, dtype: timedelta64[ns]\n```\n",
      "So we are not converting to UTC somewhere. Should be a straightforward fix. want to do a pull-request?\n\n```\nIn [41]: df2 = df.astype('M8[ns]')\n\nIn [42]: df2\nOut[42]: \n                    A                   B\n0 2008-05-12 17:50:33 2008-05-12 17:50:33\n1 2008-12-12 19:50:35 2008-05-12 17:50:33\n2 2008-05-12 17:50:32 2008-05-12 17:50:33\n\n# this also seems to have lost the name of the Series (B), oddly.\nIn [43]: df2.B-df2.A\nOut[43]: \n0       0 days 00:00:00\n1   -215 days +21:59:58\n2       0 days 00:00:01\ndtype: timedelta64[ns]\n```\n",
      "Please look at the second half of my comment. The (perhaps known?) bug is in tz_convert() itself, whose implementation assumes the array is sorted.\n",
      "@dlpd you are reaching into the implementation. So not really sure what you are doing. You would have to demonstrate a bug using the public API which you did for the case above but not for anything else.\n",
      "@jreback Thanks for the comments. You wrote:\n\n> So we are not converting to UTC somewhere\n\nMy point is that the bug is in the tz-conversion itself, not lack thereof.\n\nIf pandas.tslib.tz_convert() is not considered public, I'm happy to reproduce with DatetimeIndex.tz_convert() which is. If you trace the execution of the timedelta computation, you will see that these two examples are actually the same.\n",
      "well if you'd like to submit a pull request would be great\n"
    ],
    "events":[
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files":3,
    "additions":99,
    "deletions":20
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12453,
    "reporter":"ghost",
    "created_at":1456490037000,
    "closed_at":1465255234000,
    "resolver":"brandys11",
    "resolved_in":"67b72e3cbbaeb89a5b9c780b2fe1c8d5eaa9c505",
    "resolver_commit_num":0,
    "title":"BUG: excel export merge margin ",
    "body":"#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n|  | SALARY |  |  |  | DAYS |  |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| YEAR |  | 2013 | 2014 |  | All | 2013 | 2014 |  | All |\n| MONTH |  | 12 | 1 | 2 |  | 12 | 1 | 2 |  |\n| JOB | NAME |  |  |  |  |  |  |  |  |\n| Employ | Mary | 23 | 200 | 190 | 413 | 5 | 15.0 | 5.0 | 8.333333 |\n| Worker | Bob | 17 | 210 | 80 | 307 | 3 | 10.5 | 8.0 | 8.000000 |\n| All |  | 40 | 410 | 270 | 720 | 4 | 12.0 | 6.5 | 8.142857 |\n\nBut when exporting to Excel, the Month 2, on SALARY and DAYS, merges with the next column and All above.\n\n2014 .......... | All ...........|\n1 ..... | 2 ....................... |\n15 ... | 5 ..... | 8.333333 | \n\nWhen exporting to HTML it's ok with the header but, if the value cell has no value then shows cuts the columns.\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Windows\nOS-release: 2012ServerR2\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 47 Stepping 2, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.17.1\nnose: None\npip: 8.0.2\nsetuptools: 20.1.1\nCython: None\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: None\nIPython: None\nsphinx: None\npatsy: None\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 0.9.4\nxlwt: None\nxlsxwriter: 0.8.4\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\nJinja2: None\nNone\n\n![image](-dc85-11e5-960e-d6fe6ac85ee8.png)\n",
    "labels":[
      "IO Excel",
      "Bug",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "try `merge_cells=False`\n",
      "cc @chris-b1 \n",
      "merge_cells affects all the cells and not just the \"margins\".\n\n![image](https:\/\/cloud.githubusercontent.com\/assets\/976878\/13352739\/4c86f01e-dc87-11e5-9c87-2a89a9377abd.png)\n",
      "Something going awry in the logic [here](https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/core\/format.py#L1786) when there's an empty level, I'll take a look.\n",
      "Here a picture of when exporting the same dataframe to html:\n\n![image](https:\/\/cloud.githubusercontent.com\/assets\/976878\/13372797\/9b70b790-dd4b-11e5-9fc7-c8fc0ba76e26.png)\n",
      "I have just opened PR for this. The only problem could be, that empty name is changed to [Unnamed x_level_y](https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/io\/parsers.py#L1848) when reading back form excel.\n",
      "I think the expected output above is wrong\n\n```\nIn [11]: df\nOut[11]: \n            SALARY                      DAYS                     \nYEAR          2013   2014           All 2013  2014            All\nMONTH           12      1      2          12     1    2          \nJOB    NAME                                                      \nEmploy Mary   23.0  200.0  190.0  413.0  5.0  15.0  5.0  8.333333\nWorker Bob    17.0  210.0   80.0  307.0  3.0  10.5  8.0  8.000000\nAll           40.0  410.0  270.0  720.0  4.0  12.0  6.5  8.142857\n```\n\nThis is correct (as pandas prints it in the console)\n\n```\nIn [12]: df.index\nOut[12]: \nMultiIndex(levels=[[u'All', u'Employ', u'Worker'], [u'', u'Bob', u'Mary']],\n           labels=[[1, 2, 0], [2, 1, 0]],\n           names=[u'JOB', u'NAME'])\n\nIn [13]: df.columns\nOut[13]: \nMultiIndex(levels=[[u'DAYS', u'SALARY'], [2013, 2014, u'All'], [1, 2, 12, u'']],\n           labels=[[1, 1, 1, 1, 0, 0, 0, 0], [0, 1, 1, 2, 0, 1, 1, 2], [2, 0, 1, 3, 2, 0, 1, 3]],\n           names=[None, u'YEAR', u'MONTH'])\n```\n",
      "Have you been referring to my PR? If yes, is it ok to change behavior so that empty index names would stay empty instead \"Unnamed: %d_level_%d\" as it is right now?\n",
      "yes I referred to the PR. My point is that the example in the issue is wrong.\n",
      "Sorry, but I am still not sure if I fully understand what you are saying. The html table in first comment looks exactly as yours. And the one in the comment from 27th February is wrong, although I get correct output when calling df.to_html(). \n\nRunning it after applying my PR would yield:\n\n```\nIn [11]: excel = pd.read_excel(file, header=[0,1,2], index_col=[0,1])\n\nIn [12]: excel\nOut[12]: \n            DAYS                               SALARY            \nYEAR        2013  2014                     All   2013 2014        All\nMONTH         12     1    2 Unnamed: 5_level_2     12    1    2   Unnamed: 9_level_2\nJOB    NAME                                                       \nEmploy Mary    5  15.0  5.0           8.333333     23  200  190   413\nWorker Bob     3  10.5  8.0           8.000000     17  210   80   307\nAll    Bob     4  12.0  6.5           8.142857     40  410  270   720\n\nIn [13]: excel.index\nOut[13]: \nMultiIndex(levels=[['All', 'Employ', 'Worker'], ['Bob', 'Mary']],\n           labels=[[1, 2, 0], [1, 0, 0]],\n           names=['JOB', 'NAME'])\n\nIn [14]: excel.columns\nOut[14]: \nMultiIndex(levels=[['DAYS', 'SALARY'], [2013, 2014, 'All'], [1, 2, 12, 'Unnamed: 5_level_2', 'Unnamed: 9_level_2']],\n           labels=[[0, 0, 0, 0, 1, 1, 1, 1], [0, 1, 1, 2, 0, 1, 1, 2], [2, 0, 1, 3, 2, 0, 1, 4]],\n           names=[None, 'YEAR', 'MONTH'])\n\n```\n\nThe problem is that \"Unnamed\" cells are not merged and we get more Indexes on the 3rd level. I have tried to empty strings insted of \"Unnamed\" cells. Some tests have failed -> [test_unnamed_columns](https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/io\/tests\/parser\/common.py#L230) as the column names have to be unique. We could count the number of empty cells and use the \"Unnamed cells\" only if there is more of them or be Ok with having \"Unnamed\" cells all the time. \n\nDo you have any other sugestions, or should I try implementing one of mentioned?\n",
      "@brandys11 The tables are not the same, salary is misplaced in the top table. The top row is 1 column off. \n\nAs far as your PR, you have something wrong. You should't have any unnamed columns. This conversation should move to the PR itself, not the issue.\n",
      "Sorry, but I'm not fully understanding. Is it a bug or am I doing something wrong?\n"
    ],
    "events":[
      "commented",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "commented",
      "renamed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":96,
    "deletions":23
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":1543,
    "reporter":"JWCornV",
    "created_at":1340828962000,
    "closed_at":1465939336000,
    "resolver":"adrienemery",
    "resolved_in":"bd66592d7d1c10d88749c9fe42f770ded5d6a0d3",
    "resolver_commit_num":0,
    "title":"Support for bimonthly\/weekly timerules",
    "body":"Any change of getting bi-monthly (1st and 15th) of the month, or bi-weekly (every two weeks) supported?\n\nI could probably do this.\n",
    "labels":[
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments":[
      "I'd be happy to accept a patch for bi-monthly. Bi-weekly is already supported but not in \"anchored form\":\n\n```\nIn [2]: date_range('6\/26\/2012', periods=10, freq='2W-TUE')\nOut[2]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2012-06-26 00:00:00, ..., 2012-10-30 00:00:00]\nLength: 10, Freq: 2W-TUE, Timezone: None\n\nIn [3]: list(date_range('6\/26\/2012', periods=10, freq='2W-TUE'))\nOut[3]: \n[<Timestamp: 2012-06-26 00:00:00>,\n <Timestamp: 2012-07-10 00:00:00>,\n <Timestamp: 2012-07-24 00:00:00>,\n <Timestamp: 2012-08-07 00:00:00>,\n <Timestamp: 2012-08-21 00:00:00>,\n <Timestamp: 2012-09-04 00:00:00>,\n <Timestamp: 2012-09-18 00:00:00>,\n <Timestamp: 2012-10-02 00:00:00>,\n <Timestamp: 2012-10-16 00:00:00>,\n <Timestamp: 2012-10-30 00:00:00>]\n```\n",
      "(Edited)  The Bi-unit form (multiplied by two) is already [supported](http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.16.2\/timeseries.html#dateoffset-objects) for any `DateOffset` object.  What you asking for is the Semi-unit form (divided by two), which (AFAIK) is not supported.  Having a semi-\\* form would be useful for many business applications.  \n",
      "Any word on if this will be included in an upcoming release? If not I can put together a patch for Semi-Monthly.\n\nI propose:\n\n`SM` for Semi-Monthly Month End (15th and last day of month)\n`SMS` for Semi-Monthly Month Start (1st and 15th)\n",
      "This would be a straightforward extension. patches are welcome.\n"
    ],
    "events":[
      "subscribed",
      "commented",
      "subscribed",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files":7,
    "additions":750,
    "deletions":13
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13438,
    "reporter":"jreback",
    "created_at":1465906440000,
    "closed_at":1466110941000,
    "resolver":"priyankjain",
    "resolved_in":"fca35fbe40e7749e8202c64292f10a24c5effa19",
    "resolver_commit_num":0,
    "title":"ERR: invalid input to .str.replace does not raise",
    "body":"This should raise; `None` is not an allowed option for the replacement.\n\n\n\ncc @kwsmith\n",
    "labels":[
      "Difficulty Novice",
      "Error Reporting",
      "Strings",
      "Effort Low"
    ],
    "comments":[
      "It's not only None that gives non-sensical output, also eg numerical values:\n\n```\nIn [115]: s.str.replace('a', 3)\nOut[115]:\n0   NaN\n1   NaN\n2   NaN\ndtype: float64\n```\n\nSo rather than checking for None, maybe just check that the replacement is string-like? \nOr are there arguments that do work that are not string like?\n",
      "Gotcha, is the following output valid:\n\n```\ns.str.replace('','a')\n\n0     aaa\n1     aba\n2    None\ndtype: object\n```\n",
      "Hmm, not sure about that one. At first, it seems invalid as well, but this is the standard behaviour of `str.replace`:\n\n```\nIn [125]: 'a'.replace('', 'b')\nOut[125]: 'bab'\n```\n",
      "I think it makes more sense to consider this as valid, since that's how str.replace works. Also it would make sense to throw TypeError instead of ValueError:\n\n```\n'abc'.replace('a', None)\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-19-51f0617eaa9d> in <module>()\n----> 1 'abc'.replace('a', None)\n\nTypeError: Can't convert 'NoneType' object to str implicitly\n\n```\n"
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "mentioned",
      "subscribed",
      "milestoned",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":13,
    "deletions":2
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13451,
    "reporter":"tdhopper",
    "created_at":1466001401000,
    "closed_at":1466201074000,
    "resolver":"ravinimmi",
    "resolved_in":"9d33c7be38fd09fa493c68ab81c50ee7a681de34",
    "resolver_commit_num":0,
    "title":"to_datetime can't handle int16 or int8",
    "body":"#### Code sample\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n\n``````\n",
    "labels":[
      "Bug",
      "Timeseries",
      "Dtypes",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "yeah I think these overflow (as we are multiplying by a factor relative to position). should convert first (use `.astype('int64', copy=False)`)\n\npull-requests welcome!\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":34,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13354,
    "reporter":"ekeydar",
    "created_at":1464940605000,
    "closed_at":1466263515000,
    "resolver":"cmazzullo",
    "resolved_in":"35bb1a1c2d915d862ca0daadbe1d32180a998ccf",
    "resolver_commit_num":0,
    "title":"BUG: df.pivot_table: margins_name is ignored when there aggfunc is list",
    "body":"#### Code Sample, a copy-pastable example if possible\n\n\n\nWill ignore the margins_name\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels":[
      "Bug",
      "Reshaping",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "yep, looks like this is not being passed thru [here](https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/tools\/pivot.py#L86)\n\npull-request welcomed!\n"
    ],
    "events":[
      "renamed",
      "renamed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":24,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13398,
    "reporter":"mbrucher",
    "created_at":1465391900000,
    "closed_at":1467487918000,
    "resolver":"mbrucher",
    "resolved_in":"30d710f4c8a07cb7ea3bc91f6eb05c4bbdfa2f24",
    "resolver_commit_num":0,
    "title":"TemporaryFile as input to read_table raises TypeError: '_TemporaryFileWrapper' object is not an iterator",
    "body":"Although the requirement in the doc says that the input can be a file like object, it doesn't work with objects from tempfile. On Windows, they can't be reopened, so I need to pass the object itself.\n#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\nNot an exception!\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\n\npandas: 0.18.0\n",
    "labels":[
      "Bug",
      "Difficulty Novice",
      "CSV",
      "Effort Low"
    ],
    "comments":[
      "this is only with `engine='python'` as the sep you gave is a regex (if you use `sep='\\s+'` which is a more typical whitespace) it works as expected.\n",
      "in the future, pls show the entire `show_versions()`. you are missing crucial information there (the platform); though you did put it in the comments. we ask for these things to make it easier for people to look.\n",
      "pull-requests are welcome\n",
      "Do you mean that if I used sep='\\s+', there is no exception?\nYes, I removed some info because it's not relevant here (except the platform, I didn't see I removed the OS) + there are some things that I can't send as well.\n",
      "yes if u were splitting on white space it would use the c engine which would give u an error that the data file is empty\n\nsince u used a regex it went to the python engine and gives that weird error (only on Windows)\n",
      "Oh, OK. The thing is that I may have several spaces between columns, so I have to use the regex :(\n",
      "\\s+ is white space with at least a single space having 0 spaces is very weird\n",
      "Yes, agreed that 0 spaces is weird :)\nBTW, the data file is not empty, I'm passing the file like object, it shouldn't fail in any case!\n",
      "oh the example above it IS empty\n\nin any case I'd u would like to debug - I think it's a simple fix \n",
      "Oh yes, sorry. I forgot I had to remove the data as it is confidential!\n",
      "The issue is that you can't call next() on a file apparently.\n",
      "@mbrucher : \n\n1) If you can't provide the original data, create dummy data that can trigger the exception, particularly example data that could be reproduced by just calling `read_table(new_file)`.\n\n2) If you have confidentiality issues, can you try reproducing the issue on another machine?  Full version output is extremely useful when trying to debug.\n\n3) How does your tempfile have data?  Are you calling `new_file.write` before you call `read_table`?  If so, make sure to call `new_file.seek(0)` first so as to reset the stream position.  Otherwise, none of your written data will be read (you can see this for yourself if you call `new_file.read()` before and after calling `new_file.seek(0)`).\n\nI should add that this advise also applies to normal file objects (i.e. those created by calling `open(...)`), so this issue with tempfiles is not unique IIUC.\n",
      "@gfyoung this repros exactly as above with an empty file \n",
      "I know but I thought @mbrucher said the file contained data, and I was addressing that.  In any case, unless a more convincing example can provided, I think this is safe to close, as the function does work with tempfiles in the manner I described , data or no data.\n",
      "no it doesn't on Windows \n",
      "```\nIn [2]: import pandas as pd\n\nIn [3]: pd.__version__\nOut[3]: '0.18.1+139.ge24ab24'\n\nIn [4]: import pandas as pd\n\nIn [5]: from tempfile import TemporaryFile\n\nIn [6]: new_file = TemporaryFile(\"w+\")\n\nIn [7]: dataframe = pd.read_table(new_file, skiprows=3, header=None, sep=r\"\\s*\")\nC:\\Miniconda2\\envs\\pandas3.5\\Scripts\\ipython-script.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not\n support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying\nengine='python'.\n  if __name__ == '__main__':\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-7-43d01852f446> in <module>()\n----> 1 dataframe = pd.read_table(new_file, skiprows=3, header=None, sep=r\"\\s*\")\n\nC:\\Users\\conda\\Documents\\pandas3.5\\pandas\\io\\parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, s\nqueeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_va\nlues, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, itera\ntor, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols,\nerror_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lin\nes, memory_map, float_precision)\n    627                     skip_blank_lines=skip_blank_lines)\n    628\n--> 629         return _read(filepath_or_buffer, kwds)\n    630\n    631     parser_f.__name__ = name\n\nC:\\Users\\conda\\Documents\\pandas3.5\\pandas\\io\\parsers.py in _read(filepath_or_buffer, kwds)\n    380\n    381     # Create the parser.\n--> 382     parser = TextFileReader(filepath_or_buffer, **kwds)\n    383\n    384     if (nrows is not None) and (chunksize is not None):\n\nC:\\Users\\conda\\Documents\\pandas3.5\\pandas\\io\\parsers.py in __init__(self, f, engine, **kwds)\n    710             self.options['has_index_names'] = kwds['has_index_names']\n    711\n--> 712         self._make_engine(self.engine)\n    713\n    714     def close(self):\n\nC:\\Users\\conda\\Documents\\pandas3.5\\pandas\\io\\parsers.py in _make_engine(self, engine)\n    894             elif engine == 'python-fwf':\n    895                 klass = FixedWidthFieldParser\n--> 896             self._engine = klass(self.f, **self.options)\n    897\n    898     def _failover_to_python(self):\n\nC:\\Users\\conda\\Documents\\pandas3.5\\pandas\\io\\parsers.py in __init__(self, f, **kwds)\n   1742         # infer column indices from self.usecols if is is specified.\n   1743         self._col_indices = None\n-> 1744         self.columns, self.num_original_columns = self._infer_columns()\n   1745\n   1746         # Now self.columns has the set of columns that we will process.\n\nC:\\Users\\conda\\Documents\\pandas3.5\\pandas\\io\\parsers.py in _infer_columns(self)\n   2068         else:\n   2069             try:\n-> 2070                 line = self._buffered_line()\n   2071\n   2072             except StopIteration:\n\nC:\\Users\\conda\\Documents\\pandas3.5\\pandas\\io\\parsers.py in _buffered_line(self)\n   2136             return self.buf[0]\n   2137         else:\n-> 2138             return self._next_line()\n   2139\n   2140     def _empty(self, line):\n\nC:\\Users\\conda\\Documents\\pandas3.5\\pandas\\io\\parsers.py in _next_line(self)\n   2164             while self.pos in self.skiprows:\n   2165                 self.pos += 1\n-> 2166                 next(self.data)\n   2167\n   2168             while True:\n\nC:\\Users\\conda\\Documents\\pandas3.5\\pandas\\io\\parsers.py in _read()\n   1869         else:\n   1870             def _read():\n-> 1871                 line = next(f)\n   1872                 pat = re.compile(sep)\n   1873                 yield pat.split(line.strip())\n\nTypeError: '_TemporaryFileWrapper' object is not an iterator\n```\n",
      "So if the file is populated, of course same issue:\n\n```\nimport pandas as pd\nfrom tempfile import TemporaryFile\nnew_file = TemporaryFile(\"w+\")\nnew_file.write(\"0 0\")\nnew_file.flush()\nnew_file.seek(0)\ndataframe = pd.read_table(new_file, header=None, sep=r\"\\s+\", engine=\"python\")\nprint(dataframe)\n```\n\nTested on OS X with Python 2.7 (brew version), works like a charm, so there must be a difference in the implementation. I don't have a 3.5 on my Mac, so can't try it to see if it's the OS or the Python version :\/\n\n@gfyoung I know perfectly well how files work, thank you very much. I've been writing Python for more than a decade now, I hit all these issues in the past and obviously I know how to avoid them. But I guess you haven't tried my code before posting your message.\n\nAs @jreback said, it should be \"easy\" to fix, so I'll have a try when I have time.\nA completely different question, be can't use a list of strings to generate a DataFrame? (for instance a filtered file would end up being a list of strings that could be read in pandas, that's actually my use use case. Using a TemporaryFile because I couldn't figure another way).\n",
      "@mbrucher what do you mean a 'list of strings', do you mean?\n\nyou can! The difference is that this is not very efficient as have to be introspected (to figure out what exactly you are passing, as there are many possibilities), and then converted to a storage format (e.g. numpy). These may not necessarily be cheap; hence from the parser has more info available (e.g. it already knows the layout and can infer dtypes directly).\n\n```\nIn [12]: DataFrame(['foo', 'bar', 'baz'])\nOut[12]: \n     0\n0  foo\n1  bar\n2  baz\n\nIn [13]: DataFrame([['foo', 'bar', 'baz']])\nOut[13]: \n     0    1    2\n0  foo  bar  baz\n```\n",
      "Actually I was thinking of something like pd.read_table([\"0 0\", \"1 1\"], header=None, sep=r\"\\s+\", engine=\"python\") as the data is not yet parsed in my case (reading a report file that mixes lots of things together, only looking for specific tables that I then append to a list).\n",
      "Much more efficient to do this with the c-engine, you have whitespace separating. Introduce line separation and you are set. \n\n```\nIn [5]: pd.read_csv(StringIO('\\n'.join([\"0 0\", \"1 1\"])), header=None, sep=\"\\s+\")\nOut[5]: \n   0  1\n0  0  0\n1  1  1\n```\n",
      "OK, thanks.\n\nIt seems that file like object don't implement **next**(). The issue comes from the fact that to select the type of reader, we check the attribute readline which is used for separators of length 1, but pandas uses next() for the other separators.\n",
      "@mbrucher : Whoa, slow down there, aren't we letting our ego get bit in the way of rationale conversation?  First of all, your code gave no indication that you were aware of this, so if you would like to update your code example in the initial post, go right ahead and do so.\n\nSecond, I did in fact try it out on a newly-acquired Windows 7 machine using Python 2.7.11 using `v0.18.1` and could not reproduce the Exception.  In addition, I tested the new examples that were later posted and also got not Exception.\n",
      "@gfyoung Which is why I specified the Python version, as there is a change in the API AFAIK on the behavior of next. Anyway, the pull request fixes it and I'm adding a test as we speek.\n",
      "@mbrucher : fair enough - but it's worthwhile to note since this issue you raise isn't then a general Windows bug but rather a change in the way `TemporaryFile` is written between Python versions.\n",
      "They must have forgotten when they changed the **next** API :(\n",
      "@jreback : this issue should have been closed with @mbrucher 's commit (I think it didn't because the commit says \"dcloses\" instead of \"closes\")\n"
    ],
    "events":[
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "milestoned",
      "demilestoned",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed"
    ],
    "changed_files":3,
    "additions":16,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":10690,
    "reporter":"shishirsharma",
    "created_at":1438108845000,
    "closed_at":1467991006000,
    "resolver":"haleemur",
    "resolved_in":"ba82b511c76d87421c8900348efebe4577548ec6",
    "resolver_commit_num":0,
    "title":"to_html() formatters does not work for objects with 'datetime64[ns]' type",
    "body":"I am using a to_html() to convert a dataframe into a table. i have various datatypes in Dataframe including float, int, datetime and timedelta. \n\nI am using formatters to control outputs, It seems to be working for int and float but not for datetime. \n\n\n",
    "labels":[
      "Timeseries",
      "IO HTML"
    ],
    "comments":[
      "pls show a small but minimally reproducible example, along with `pd.show_versions()`\n",
      "I encountered the same problem, and then did some digging. The issue is reproduced with the following example.\n\n```\nimport pandas as pd\ndf = pd.DataFrame({'months': ['2015-01-01', '2015-10-10', '2016-01-01']})\ndf.months = pd.to_datetime(df.months)\ndf.to_html(formatters={'months': lambda x: x.strftime('%Y-%m')})\n```\n\nthis prints:\n\n```\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>months<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>0<\/th>\n      <td>2015-01-01<\/td>\n    <\/tr>\n    <tr>\n      <th>1<\/th>\n      <td>2015-10-10<\/td>\n    <\/tr>\n    <tr>\n      <th>2<\/th>\n      <td>2016-01-01<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>\n```\n\nAs you can see, the dates are not formatted using the specified function.\n\nThe class `pandas.formats.format.Datetime64Formatter` is responsible for the formatting, however it does nothing if a `formatter` is passed to it's init. Browsing through the file, I found a class `pandas.formats.format.Datetime64TZFormatter` which inherits from Datetime64Formatter, which does respect `formatter`.\n\n```\nfrom pandas.formats.format import Datetime64Formatter, Datetime64TZFormatter\n\nDatetime64TZFormatter(df.months, formatter=lambda x: x.strftime('%Y-%m')).get_result()\n#prints\n['2015-01', '2015-10, '2016-01']\n\nDatetime64Formatter(df.months, formatter=lambda x: x.strftime('%Y-%m')).get_result()\n#prints\n['2015-01-01', '2015-10-10', '2016-01-01']\n```\n\nI propose to change method `Datetime64Formatter._format_strings` from \n\n```\n    def _format_strings(self):\n        \"\"\" we by definition have DO NOT have a TZ \"\"\"\n\n        values = self.values\n        if not isinstance(values, DatetimeIndex):\n            values = DatetimeIndex(values)\n\n        fmt_values = format_array_from_datetime(\n            values.asi8.ravel(),\n            format=_get_format_datetime64_from_values(values,\n                                                      self.date_format),\n            na_rep=self.nat_rep).reshape(values.shape)\nreturn fmt_values.tolist()\n```\n\nto\n\n```\n    def _format_strings(self):\n        \"\"\" we by definition have DO NOT have a TZ \"\"\"\n\n       if self.formatter is not None:\n           return [formatter(x) for x in values]\n\n        values = self.values\n        if not isinstance(values, DatetimeIndex):\n            values = DatetimeIndex(values)\n\n        fmt_values = format_array_from_datetime(\n            values.asi8.ravel(),\n            format=_get_format_datetime64_from_values(values,\n                                                      self.date_format),\n            na_rep=self.nat_rep).reshape(values.shape)\nreturn fmt_values.tolist()\n```\n\nHere's the output of `pd.show_versions()` on my system.\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.5.1.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.3.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_CA.UTF-8\n\npandas: 0.18.1\nnose: None\npip: 8.1.2\nsetuptools: 23.0.0\nCython: 0.24\nnumpy: 1.11.0\nscipy: 0.17.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.2.0\nsphinx: None\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: 2.6.0\nmatplotlib: 1.5.1\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.13\npymysql: None\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\njinja2: 2.8\nboto: None\npandas_datareader: None\n```\n"
    ],
    "events":[
      "commented",
      "labeled",
      "commented",
      "cross-referenced",
      "milestoned",
      "labeled",
      "labeled",
      "unlabeled"
    ],
    "changed_files":3,
    "additions":133,
    "deletions":0
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13598,
    "reporter":"toobaz",
    "created_at":1468077728000,
    "closed_at":1468890678000,
    "resolver":"wcwagner",
    "resolved_in":"5a521713f3892539b648bc2735d3cc502feb2b48",
    "resolver_commit_num":0,
    "title":"Series.str.zfill() doesn't check type",
    "body":"#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\nA `ValueError`.\n#### output of `pd.show_versions()`\n\n\n",
    "labels":[
      "Bug",
      "Error Reporting",
      "Strings",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "yep that's an error\n",
      "Rather than adding type check logics on everywhere, adding a dummy call (like `\" \".zfill(width)` may be easier.\n",
      "not sure about that. Most strings can only accept string-likes, OR integers. Having a couple of common routines might help readablity.\n\n`_assure_string`, `_assure_integer`\n",
      "@jreback I'd like to fix this one as well. Can I go ahead and put a check in `zfill` or should I create separate routine as you mentioned?\n",
      "I think make a separate check\n",
      "Is there any place where such common routines are kept? If not, where should I keep this one?\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "cross-referenced"
    ],
    "changed_files":3,
    "additions":16,
    "deletions":2
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13652,
    "reporter":"bdrosen96",
    "created_at":1468493983000,
    "closed_at":1469051649000,
    "resolver":"bdrosen96",
    "resolved_in":"210fea9d4dc4314f9bc4ddb5f7dab6fa87912ca9",
    "resolver_commit_num":0,
    "title":"Read CSV using c engine silently swallows useful exceptions",
    "body":"#### Code Sample, a copy-pastable example if possible\n\n\n#### Actual Output\n\n\n#### Expected Behavior\n\nThe C engine should behave like python engine. This should be possible by using PyErr_Occurred .\n",
    "labels":[
      "Unicode",
      "Error Reporting",
      "IO CSV"
    ],
    "comments":[
      "you have a pretty old version of pandas, current is 0.18.1 and 0.19.0 releasing soon.\n\nand you can simply pass in the the `encoding` argument if you need to.\n\nI am closing, but if you can provide a copy-pastable example that reproduces a non-obvious error on latest, then pls reopen.\n",
      "This should not have been closed.\n\n1 Even though the version I have is old, I think this issue still exists in latest version.\n\n2 The encoding issue was just an example that was easy to produce and should not be dismised because of the existence of the encoding option. If the file handle was a socket and the connection was reset, it would also raise an exception and there would not be a workaround\n",
      "I just verified this with same code using newer pandas\n\n```\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.4.3.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.13.0-55-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: 1.3.7\npip: 7.1.0\nsetuptools: 20.2.2\nCython: 0.24.0a0\nnumpy: 1.9.2\nscipy: 0.16.0\nstatsmodels: None\nxarray: None\nIPython: 4.1.2\nsphinx: None\npatsy: None\ndateutil: 2.4.2\npytz: 2015.4\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: 2.4.3\nmatplotlib: None\nopenpyxl: None\nxlrd: 0.9.4\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: 0.9999999\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: None\nboto: 2.38.0\npandas_datareader: None\nPandas version: 0.18.1\n\nPython version: sys.version_info(major=3, minor=4, micro=3, releaselevel='final', serial=0)\n\nShowing stream error on read\n\nTraceback (most recent call last):\n  File \"pandas_bug.py\", line 34, in <module>\n    data = stream.read()\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/codecs.py\", line 798, in read\n    data = self.reader.read(size)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/codecs.py\", line 497, in read\n    newchars, decodedbytes = self.decode(data, self.errors)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x83 in position 12: invalid start byte\nShowing stream error on read_csv (python engine)\n\nTraceback (most recent call last):\n  File \"pandas_bug.py\", line 41, in <module>\n    stream = test_pandas(True)\n  File \"pandas_bug.py\", line 28, in test_pandas\n    df = pandas.read_csv(stream, engine=engine)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas\/io\/parsers.py\", line 562, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas\/io\/parsers.py\", line 315, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas\/io\/parsers.py\", line 645, in __init__\n    self._make_engine(self.engine)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas\/io\/parsers.py\", line 805, in _make_engine\n    self._engine = klass(self.f, **self.options)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas\/io\/parsers.py\", line 1608, in __init__\n    self.columns, self.num_original_columns = self._infer_columns()\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas\/io\/parsers.py\", line 1823, in _infer_columns\n    line = self._buffered_line()\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas\/io\/parsers.py\", line 1975, in _buffered_line\n    return self._next_line()\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas\/io\/parsers.py\", line 2006, in _next_line\n    orig_line = next(self.data)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/codecs.py\", line 820, in __next__\n    data = next(self.reader)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/codecs.py\", line 638, in __next__\n    line = self.readline()\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/codecs.py\", line 551, in readline\n    data = self.read(readsize, firstline=True)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/codecs.py\", line 497, in read\n    newchars, decodedbytes = self.decode(data, self.errors)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x83 in position 0: invalid start byte\nShowing missing stream error on read_csv (python c)\n\nTraceback (most recent call last):\n  File \"pandas_bug.py\", line 48, in <module>\n    stream = test_pandas(False)\n  File \"pandas_bug.py\", line 28, in test_pandas\n    df = pandas.read_csv(stream, engine=engine)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas\/io\/parsers.py\", line 562, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas\/io\/parsers.py\", line 315, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas\/io\/parsers.py\", line 645, in __init__\n    self._make_engine(self.engine)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas\/io\/parsers.py\", line 799, in _make_engine\n    self._engine = CParserWrapper(self.f, **self.options)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas\/io\/parsers.py\", line 1213, in __init__\n    self._reader = _parser.TextReader(src, **kwds)\n  File \"pandas\/parser.pyx\", line 520, in pandas.parser.TextReader.__cinit__ (pandas\/parser.c:5129)\n  File \"pandas\/parser.pyx\", line 671, in pandas.parser.TextReader._get_header (pandas\/parser.c:7259)\n  File \"pandas\/parser.pyx\", line 868, in pandas.parser.TextReader._tokenize_rows (pandas\/parser.c:9602)\n  File \"pandas\/parser.pyx\", line 1865, in pandas.parser.raise_parser_error (pandas\/parser.c:23325)\npandas.io.common.CParserError: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n```\n",
      "I do not have permissions to reopen this issue. Can you do so?\n",
      "@bdrosen96 then pls show an example which can be copy-pasted. e.g. your file is not there. you need to create it to repro (e.g. write out a test csv file or something), better yet is to use StringIO\n",
      "and you are testing on 0.18.1, I am pretty sure these are already fixed on master.\n",
      "``` python\nimport pandas\nimport codecs\nimport traceback\nimport sys\nimport io\n\n# Data in UTF-8\n\nDATA = '''num, text\n1,\u30b5\u30a6\u30ed\u30f3\uff08Sauron\u3001\u30a2\u30a4\u30cc\u30a2\u306e\u5275\u9020\u306e\u6642 - \u7b2c\u4e09\u7d003019\u5e743\u670825\u65e5\uff09\u306f\u3001J\u30fbR\u30fbR\u30fb\u30c8\u30fc\u30eb\u30ad\u30f3\u306e\u4e2d\u3064\u56fd\u3092\u821e\u53f0\u3068\u3057\u305f\u5c0f\u8aac\n\u300e\u30db\u30d3\u30c3\u30c8\u306e\u5192\u967a\u300f\u300e\u6307\u8f2a\u7269\u8a9e\u300f\u300e\u30b7\u30eb\u30de\u30ea\u30eb\u306e\u7269\u8a9e\u300f\u306e\u767b\u5834\u4eba\u7269\u3002\n2,\u300e\u30db\u30d3\u30c3\u30c8\u306e\u5192\u967a\u300f\u306b\u8a00\u53ca\u306e\u3042\u308b\u300c\u6b7b\u4eba\u3046\u3089\u306a\u3044\u5e2b\u300d\uff08\u6620\u753b\u300e\u30db\u30d3\u30c3\u30c8\u30b7\u30ea\u30fc\u30ba\u300f\u306e\u5b57\u5e55\u3067\u306f\u300c\u6b7b\u4eba\u9063\u3044\uff08\u30cd\u30af\u30ed\u30de\u30f3\u30b5\u30fc\uff09\u300d\uff09\u3068\u306f\u5f7c\u306e\u3053\u3068\u3067\u3042\u308b\u3002\n3,\u305d\u306e\u7d9a\u7de8\u3067\u3042\u308b\u300e\u6307\u8f2a\u7269\u8a9e\u300f\u306b\u304a\u3044\u3066\u306f\u300c\u4e00\u3064\u306e\u6307\u8f2a\uff08the One Ring\uff09\u300d\u306e\u4f5c\u308a\u4e3b\u3001\u300c\u51a5\u738b\uff08Dark Lord\uff09\u300d\u3001\u300c\u304b\u306e\u8005\n\uff08the One\uff09[1]\u300d\u3068\u3057\u3066\u767b\u5834\u3059\u308b\u3002\u524d\u53f2\u306b\u3042\u305f\u308b\u300e\u30b7\u30eb\u30de\u30ea\u30eb\u306e\u7269\u8a9e\u300f\u3067\u306f\u3001\u521d\u4ee3\u306e\u51a5\u738b\u30e2\u30eb\u30b4\u30b9\u306e\u6700\u3082\u529b\u3042\u308b\u5074\u8fd1\u3067\u3042\u3063\u305f\u3002\n4,\u30b5\u30a6\u30ed\u30f3\u306f\u5143\u6765\u3001\u30a2\u30eb\u30c0\uff08\u5730\u7403\uff09\u306e\u5275\u9020\u3092\u62c5\u3063\u305f\u5929\u4f7f\u7684\u7a2e\u65cf\u30a2\u30a4\u30cc\u30a2\u306e\u4e00\u54e1\u3067\u3042\u3063\u305f\u304c\u3001\u4e3b\u30e1\u30eb\u30b3\u30fc\u30eb\u306e\u53cd\u9006\u306b\u52a0\u62c5\u3057\u3066\u5815\u843d\u3057\u3001\u30a2\u30eb\u30c0\u306b\u5bb3\u3092\u306a\u3059\u5b58\u5728\u3068\u306a\u3063\u305f\u3002\n5,\u300c\u30b5\u30a6\u30ed\u30f3\u300d\u3068\u306f\u30af\u30a6\u30a7\u30f3\u30e4\u3067\u300c\u8eab\u306e\u6bdb\u306e\u3088\u3060\u3064\u3082\u306e\u300d\u3068\u3044\u3046\u610f\u5473\u3067\u3042\u308a\u3001\u30b7\u30f3\u30c0\u30ea\u30f3\u3067\u540c\u69d8\u306e\u610f\u5473\u3067\u3042\u308b\u540d\u524d\u300c\u30b4\u30eb\u30b5\u30a6\u30a2\u300d\u3068\u547c\u3070\u308c\u308b\u3053\u3068\u3082\u3042\u308b\u3002\n6,\u3053\u308c\u3089\u306f\u3001\u30b5\u30a6\u30ed\u30f3\u3092\u6050\u308c\u3001\u5fcc\u307f\u5acc\u3063\u305f\u30a8\u30eb\u30d5\u306b\u3088\u308b\u540d\u3067\u3042\u308a\u3001\u300e\u6307\u8f2a\u7269\u8a9e\u300f\u4f5c\u4e2d\u306b\u304a\u3044\u3066\u30a2\u30e9\u30b4\u30eb\u30f3\u306f\u300c\u304b\u308c\uff08\u30b5\u30a6\u30ed\u30f3\uff09\u306f\u81ea\u5206\u306e\u672c\u5f53\u306e\u540d\u306f\u4f7f\u308f\u306a\u3044\u3057\u3001\u305d\u308c\u3092\u5b57\u306b\u66f8\u3044\u305f\u308a\u53e3\u306b\u51fa\u3057\u305f\u308a\u3059\u308b\u3053\u3068\u3082\u8a31\u3055\u306a\u3044\u300d\u3068\u767a\u8a00\u3057\u3066\u3044\u308b\u3002\n7,\u305d\u306e\u307b\u304b\u3001\u7b2c\u4e8c\u7d00\u306b\u30a8\u30eb\u30d5\u306b\u5bfe\u3057\u3066\u81ea\u79f0\u3057\u305f\u3068\u3055\u308c\u308b\u540d\u306b\u3001\u300c\u30a2\u30f3\u30ca\u30bf\u30fc\u30eb\uff08\u7269\u8d08\u308b\u541b\uff09\u300d\u3001\u300c\u30a2\u30eb\u30bf\u30ce\uff08\u9ad8\u8cb4\u306a\u7d30\u5de5\u5e2b\uff09\u300d\u3001\u300c\u30a2\u30a6\u30ec\u30f3\u30c7\u30a3\u30eb\uff08\u30a2\u30a6\u30ec\u306e\u4e0b\u50d5\uff09\u300d\u304c\u3042\u308b\u3002\n8,\u7b2c\u4e00\u7d00\u306e\u9803\u306e\u30b5\u30a6\u30ed\u30f3\u306f\u3001\u81ea\u5728\u306b\u5909\u8eab\u3059\u308b\u80fd\u529b\u3092\u6301\u3063\u3066\u3044\u305f\u3002\n9,\u305d\u306e\u80fd\u529b\u3092\u4f7f\u3048\u3070\u898b\u76ee\u9e97\u3057\u3044\u7acb\u6d3e\u306a\u5916\u898b\u3092\u88c5\u3046\u3053\u3068\u3084\u3001\u307e\u305f\u5de8\u5927\u306a\u72fc\u3084\u5438\u8840\u3053\u3046\u3082\u308a\u3068\u3044\u3063\u305f\u602a\u7269\u306b\u5909\u3058\u308b\u3053\u3068\u3082\u3067\u304d\u3001\u30a8\u30eb\u30d5\u304b\u3089\u6050\u308c\u3089\u308c\u305f\u3002\n10,\u7b2c\u4e8c\u7d00\u306b\u4e00\u3064\u306e\u6307\u8f2a\u3092\u4f5c\u308a\u4e0a\u3052\u305f\u30b5\u30a6\u30ed\u30f3\u306f\u3001\u4ed6\u306e\u529b\u306e\u6307\u8f2a\u3067\u6210\u3055\u308c\u308b\u4e8b\u67c4\u3084\u305d\u306e\u6240\u6709\u8005\u3092\u652f\u914d\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u3063\u305f\u3002\n11,\u307e\u305f\u3001\u8089\u4f53\u304c\u6ec5\u3073\u3066\u3082\u6307\u8f2a\u304c\u3042\u308b\u9650\u308a\u4f55\u5ea6\u3067\u3082\u8607\u308b\u3053\u3068\u304c\u3067\u304d\u305f\u3002\n12,\u305f\u3060\u3057\u30cc\u30fc\u30e1\u30ce\u30fc\u30eb\u6ca1\u843d\u306e\u969b\u306b\u7f8e\u3057\u3044\u8089\u4f53\u3092\u7834\u58ca\u3055\u308c\u305f\u5f8c\u306f\u3001\u4e8c\u5ea6\u3068\u7f8e\u3057\u304f\u5909\u8eab\u3059\u308b\u3053\u3068\u306f\u3067\u304d\u306a\u304f\u306a\u308a\u3001\u305d\u306e\u60aa\u610f\u306e\n\u5177\u73fe\u306e\u3088\u3046\u306a\u898b\u308b\u3082\u6050\u308d\u3057\u3044\u59ff\u3057\u304b\u3068\u308c\u306a\u304f\u306a\u3063\u305f\u3068\u3044\u3046\u3002\n13,\u307e\u305f\u3057\u3070\u3057\u3070\u300c\u307e\u3076\u305f\u306e\u306a\u3044\u706b\u306b\u7e01\u53d6\u3089\u308c\u305f\u76ee\u300d\u3068\u3044\u3063\u305f\u5fc3\u8c61\u8868\u73fe\u3067\u6349\u3048\u3089\u308c\u305f\u3002\n'''\n\npandas.show_versions()\n\nprint(\"Pandas version: {}\\n\".format(pandas.__version__))\nprint(\"Python version: {}\\n\".format(sys.version_info))\n\ndef build_stream():\n\n    bytes_data = DATA.encode(\"shift-jis\")\n    handle = io.BytesIO(bytes_data)\n    codec = codecs.lookup(\"utf-8\")\n    utf8 = codecs.lookup('utf-8')\n    # stream must be binary UTF8\n    stream = codecs.StreamRecoder(\n        handle, utf8.encode, utf8.decode, codec.streamreader, codec.streamwriter)\n    return stream\n\ndef test_pandas(use_python):\n    stream = build_stream()\n\n    if use_python:\n        engine = 'python'\n    else:\n        engine = 'c'\n    df = pandas.read_csv(stream, engine=engine)\n\n\nprint(\"Showing stream error on read\\n\")\ntry:\n    stream = build_stream()\n    data = stream.read()\nexcept Exception as exc:\n    traceback.print_exc(file=sys.stdout)\n\n\nprint(\"Showing stream error on read_csv (python engine)\\n\")\ntry:\n    stream = test_pandas(True)\nexcept Exception as exc:\n    traceback.print_exc(file=sys.stdout)\n\n\nprint(\"Showing missing stream error on read_csv (python c)\\n\")\ntry:\n    stream = test_pandas(False)\nexcept Exception as exc:\n    traceback.print_exc(file=sys.stdout)\n```\n\n```\n```\n",
      "I just ran this again on master and got same behavior.\n\nPandas version: 0.18.1+198.g3f6d4bd\n",
      "pls show the output from master\n",
      "again, using a non-decoded stream is really really odd; this is not supported\n",
      "I'll reopen. If you can provide a PR which 'fixes' this I think it will be easier to look\/test.\n\ncc @gfyoung \n",
      "```\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.4.3.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.13.0-55-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\nLOCALE: en_US.UTF-8\n\npandas: 0.18.1+198.g3f6d4bd\nnose: 1.3.7\npip: 7.1.0\nsetuptools: 20.2.2\nCython: 0.24.0a0\nnumpy: 1.9.2\nscipy: 0.16.0\nstatsmodels: None\nxarray: None\nIPython: 4.1.2\nsphinx: None\npatsy: None\ndateutil: 2.4.2\npytz: 2015.4\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: 2.4.3\nmatplotlib: None\nopenpyxl: None\nxlrd: 0.9.4\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: 0.9999999\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: None\nboto: 2.38.0\npandas_datareader: None\nPandas version: 0.18.1+198.g3f6d4bd\n\nPython version: sys.version_info(major=3, minor=4, micro=3, releaselevel='final', serial=0)\n\nShowing stream error on read\n\nTraceback (most recent call last):\n  File \"pandas_bug.py\", line 57, in <module>\n    data = stream.read()\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/codecs.py\", line 798, in read\n    data = self.reader.read(size)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/codecs.py\", line 497, in read\n    newchars, decodedbytes = self.decode(data, self.errors)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x83 in position 12: invalid start byte\nShowing stream error on read_csv (python engine)\n\nTraceback (most recent call last):\n  File \"pandas_bug.py\", line 64, in <module>\n    stream = test_pandas(True)\n  File \"pandas_bug.py\", line 51, in test_pandas\n    df = pandas.read_csv(stream, engine=engine)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg\/pandas\/io\/parsers.py\", line 631, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg\/pandas\/io\/parsers.py\", line 384, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg\/pandas\/io\/parsers.py\", line 714, in __init__\n    self._make_engine(self.engine)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg\/pandas\/io\/parsers.py\", line 898, in _make_engine\n    self._engine = klass(self.f, **self.options)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg\/pandas\/io\/parsers.py\", line 1746, in __init__\n    self.columns, self.num_original_columns = self._infer_columns()\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg\/pandas\/io\/parsers.py\", line 1988, in _infer_columns\n    line = self._buffered_line()\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg\/pandas\/io\/parsers.py\", line 2140, in _buffered_line\n    return self._next_line()\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg\/pandas\/io\/parsers.py\", line 2171, in _next_line\n    orig_line = next(self.data)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/codecs.py\", line 820, in __next__\n    data = next(self.reader)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/codecs.py\", line 638, in __next__\n    line = self.readline()\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/codecs.py\", line 551, in readline\n    data = self.read(readsize, firstline=True)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/codecs.py\", line 497, in read\n    newchars, decodedbytes = self.decode(data, self.errors)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x83 in position 0: invalid start byte\nShowing missing stream error on read_csv (python c)\n\nTraceback (most recent call last):\n  File \"pandas_bug.py\", line 71, in <module>\n    stream = test_pandas(False)\n  File \"pandas_bug.py\", line 51, in test_pandas\n    df = pandas.read_csv(stream, engine=engine)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg\/pandas\/io\/parsers.py\", line 631, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg\/pandas\/io\/parsers.py\", line 384, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg\/pandas\/io\/parsers.py\", line 714, in __init__\n    self._make_engine(self.engine)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg\/pandas\/io\/parsers.py\", line 892, in _make_engine\n    self._engine = CParserWrapper(self.f, **self.options)\n  File \"\/home\/brett\/.virtualenvs\/datasets-service\/lib\/python3.4\/site-packages\/pandas-0.18.1+198.g3f6d4bd-py3.4-linux-x86_64.egg\/pandas\/io\/parsers.py\", line 1340, in __init__\n    self._reader = _parser.TextReader(src, **kwds)\n  File \"pandas\/parser.pyx\", line 527, in pandas.parser.TextReader.__cinit__ (pandas\/parser.c:5137)\n  File \"pandas\/parser.pyx\", line 701, in pandas.parser.TextReader._get_header (pandas\/parser.c:7700)\n  File \"pandas\/parser.pyx\", line 898, in pandas.parser.TextReader._tokenize_rows (pandas\/parser.c:10058)\n  File \"pandas\/parser.pyx\", line 1890, in pandas.parser.raise_parser_error (pandas\/parser.c:24033)\npandas.io.common.CParserError: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n```\n",
      "```\ndiff --git a\/pandas\/parser.pyx b\/pandas\/parser.pyx\nindex 3928bc8..61a1e03 100644\n--- a\/pandas\/parser.pyx\n+++ b\/pandas\/parser.pyx\n@@ -10,7 +10,9 @@ import warnings\n from csv import QUOTE_MINIMAL, QUOTE_NONNUMERIC, QUOTE_NONE\n from cpython cimport (PyObject, PyBytes_FromString,\n                       PyBytes_AsString, PyBytes_Check,\n-                      PyUnicode_Check, PyUnicode_AsUTF8String)\n+                      PyUnicode_Check, PyUnicode_AsUTF8String,\n+                      PyErr_Occurred, PyErr_Fetch)\n+from cpython.ref cimport PyObject, Py_XDECREF\n from io.common import CParserError, DtypeWarning, EmptyDataError\n\n\n@@ -1878,6 +1880,17 @@ cdef kh_float64_t* kset_float64_from_list(values) except NULL:\n\n\n cdef raise_parser_error(object base, parser_t *parser):\n+    cdef:\n+        object old_exc\n+        PyObject *type, *value, *traceback\n+    if PyErr_Occurred():\n+        PyErr_Fetch(&type, &value, &traceback);\n+        Py_XDECREF(type)\n+        Py_XDECREF(traceback)\n+        if value != NULL:\n+            old_exc = <object> value\n+            Py_XDECREF(value)\n+            raise old_exc\n     message = '%s. C error: ' % base\n     if parser.error_msg != NULL:\n         if PY3:\n```\n",
      "@bdrosen96 : thanks for pointing this out!  You can submit a PR for this and make sure to include a test as well (I would think in `common.py` if possible but otherwise `c_parser_only.py`)!\n",
      "I cannot submit a PR without creating a fork first (permissions issue)\n",
      "Of course.  It clearly says that in the documentation for contributing.\n",
      "https:\/\/github.com\/pydata\/pandas\/pull\/13693\n"
    ],
    "events":[
      "commented",
      "closed",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "reopened",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files":5,
    "additions":59,
    "deletions":7
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":7271,
    "reporter":"TomAugspurger",
    "created_at":1401327708000,
    "closed_at":1469052289000,
    "resolver":"StephenKappel",
    "resolved_in":"63a1e5c58af8ddc8dec192f39a0999aad74acaf9",
    "resolver_commit_num":0,
    "title":"ENH: `df.astype` could accept a dict of {col: type}",
    "body":"This would be consistent with other pandas methods.\n\nThe reason I'm running into it is having some NaNs scattered across some `int` and `bool` columns, which converts to float \/ objects. If I discard those NaNs, it would be nice to do\n\n`df = df.astype({'my_bool', 'bool', 'my_int': 'int'})`\n\ninstead of\n\n\n",
    "labels":[
      "Dtypes",
      "Enhancement",
      "Good as first PR"
    ],
    "comments":[
      "that's a nice idea!\n",
      ":+1:\n"
    ],
    "events":[
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "labeled",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":5,
    "additions":130,
    "deletions":6
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13599,
    "reporter":"toobaz",
    "created_at":1468077871000,
    "closed_at":1469053522000,
    "resolver":"sahildua2305",
    "resolved_in":"1ce8f8e0b8540252dac25497f29d4de66a8bea3f",
    "resolver_commit_num":0,
    "title":"MultiIndex.from_arrays() does not check lenghts",
    "body":"#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\nThe first previous command should already raise an error.\n#### output of `pd.show_versions()`\n\n\n",
    "labels":[
      "Error Reporting",
      "MultiIndex",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "@jreback Hi, I'd like to contribute to this library. Can you please help me in starting with it as well as tagging me in some task perfect for a beginner?\n\nThanks! :smile: \n",
      "@sahildua2305 see contributing docs [here](http:\/\/pandas.pydata.org\/pandas-docs\/stable\/contributing.html)\n\nthings tagged a 'Difficulty Novice' are generally pretty straightforward (like this issue)\n",
      "@jreback Thanks for pointers!\n\nHowever [this](http:\/\/pandas.pydata.org\/pandas-docs\/stable\/contributing.html) link doesn't work for me. Can you please check once?\n",
      "seems ok to me.\n",
      "Accessible now. Thanks once again! \n"
    ],
    "events":[
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":24,
    "deletions":0
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":10806,
    "reporter":"jreback",
    "created_at":1439392896000,
    "closed_at":1469113526000,
    "resolver":"IamJeffG",
    "resolved_in":"bb6b5e54edaf046389e8cce28e7cd27ee87f5fcc",
    "resolver_commit_num":1,
    "title":"BUG\/ENH: sort_values(by=index_label, axis=1)",
    "body":"xref #10726 \n\nThis is possible, just not implemented\n\n\n",
    "labels":[
      "Enhancement",
      "Reshaping",
      "Difficulty Intermediate",
      "Effort Low",
      "Prio-low"
    ],
    "comments":[
      "Old issue was #2940, will close that one\n"
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "labeled",
      "unlabeled",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":4,
    "additions":45,
    "deletions":10
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":9180,
    "reporter":"okdistribute",
    "created_at":1420155985000,
    "closed_at":1469369646000,
    "resolver":"aterrel",
    "resolved_in":"6efd743ccbf2ef6c13ea0c71b7b2e2a022a99455",
    "resolver_commit_num":0,
    "title":"Support ndjson -- newline delimited json -- for streaming data.",
    "body":"Hey all,\n\nI'm a developer on [dat project]() (git for data) and we are building a python library to interact with the data store. \n\nEverything in dat is streaming, and we use newline delimited json as the official transfer format between processes. \n\n[Take a look at the specification for newline delimited json here](-spec\/)\n\nDoes pandas support this yet, and if not, would you consider adding a `to_ndjson` function to the existing output formats?\n\nFor example, the following table:\n\n\n\nWould be converted to \n\n\n\nFor general streaming use cases, it might be nice to also consider other ways of supporting this format, like a generator function that outputs ndjson-able objects\n",
    "labels":[
      "JSON",
      "Enhancement",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments":[
      "something like this, perhaps:\n\n``` python\ndef iterndjson(df):\n    generator = df.iterrows()\n    ndjson = []\n    row = True\n    while row:\n        try:\n            row = next(generator)\n            ndjson.append(row[1].to_dict())\n        except StopIteration:\n            row = None\n\n    return ndjson\n```\n\n``` python\n> df = pd.DataFrame({'one': [1,2,3,4], 'two': [3,4,5,6]})\n> iterndjson(df)\n[{'one': 1, 'two': 3},\n {'one': 2, 'two': 4},\n {'one': 3, 'two': 5},\n {'one': 4, 'two': 6}]\n```\n",
      "have a look here http:\/\/pandas.pydata.org\/pandas-docs\/stable\/io.html#json\n",
      "this would also be great for things like BigQuery, which outputs JSON files as new line delimited JSON.  The issue is that it's not actually valid JSON (since it ends up as multiple objects).\n\n@karissa maybe you could hack around this by reading the file row by row, using ujson\/json to read each row into a python dictionary and then passing the whole thing to the DataFrame constructor?\n",
      "@mrocklin and I looked into this. The simplest solution we came up was loading the file into a buffer, add the appropriate commas and brackets then passing back to read_json. \n\nBelow are a few timings on this approach, it seems the current implementation of read_json is a bit slower than ujson, so we felt the simplicity of this approach didn't make anything too slow.\n\n```\nIn [3]: def parse():\n    with open(\"reviews_Clothing_Shoes_and_Jewelry_5.json\") as fp:\n        list(map(json.loads, fp))\n   ...:\n\nIn [4]: %timeit parse()\n1 loop, best of 3: 4.05 s per loop\n\nIn [18]: import ujson as json\n\nIn [19]: def parse():\n    with open(\"reviews_Clothing_Shoes_and_Jewelry_5.json\") as fp:\n        list(map(json.loads, fp))\n   ....:\n\nIn [20]: %timeit parse()\n1 loop, best of 3: 1.43 s per loop\n\nIn [22]: %time _ = pd.read_json('reviews_Clothing_Shoes_and_Jewelry_5_comma.json', convert_dates=False)\nCPU times: user 4.75 s, sys: 520 ms, total: 5.27 s\nWall time: 5.49 s\n```\n\nI'll try to get a patch together unless someone thinks there is a better solution. The notion would be to add a flag 'line=True' to the reader.\n",
      "@karissa is there any difference between ndjson and jsonlines (http:\/\/jsonlines.org\/)  I've never heard of ndjson but it seems to be the same thing.\n",
      "Looks like jsonlines includes a few more rules, including a specification about UTF-8 encoding. http:\/\/jsonlines.org\/ vs http:\/\/ndjson.org\/\n"
    ],
    "events":[
      "commented",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "milestoned",
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "assigned",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented"
    ],
    "changed_files":5,
    "additions":142,
    "deletions":11
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13703,
    "reporter":"ivannz",
    "created_at":1468922055000,
    "closed_at":1469616588000,
    "resolver":"ivannz",
    "resolved_in":"31f8e4dc8af8f0d109f366d0b726aef210bf7904",
    "resolver_commit_num":0,
    "title":"Unexpected segmentation fault in pd.read_csv C-engine",
    "body":"Dear developers,\n\nI am using pandas in an application where I need to process large **csv** files (around 1Gb each) which have approximately 800k records and 400+ columns of mixed type. That is why I decided to use data iterator functionality of **pd.read_csv()**. When experimenting with `chunksize` my application seems to crash somewhere inside **TextReader__string_convert** call.\n\nHere is an archive with a sample CSV data file that seems to cause the crash (it also includes crash dump reports, a copy of the example, and a snapshot of versions of installed python packages).\n[read_csv_crash.tar.gz]()\n#### Code Sample\n\nTo run this example you would have to extract `dataset.csv` from the supplied archive.\n\n\n\nPlease, note that this crash does not seem to occur when the file is less than 260Kib. Also note that playing with `low_memory` setting did not alleviate the problem.\n#### Expected Output\n\nThis code sample outputs this:\n\n\n#### output of `pd.show_versions()`\n\nThe output of this call is attached to this issue.\n[pd_show_versions.txt]()\n#### Python greetings string\n\n\n#### OSX version\n\n\n",
    "labels":[
      "Bug",
      "IO CSV"
    ],
    "comments":[
      "1) For the expected output section, put what you're actually expecting to see, not what you actually saw.  Underneath that, you should then put what you saw.\n\n2) Can you try your code sample again with the `master` branch?\n\n3) While I cannot reproduce this (either with `0.18.1` or `master`) on Linux (sorry, no access to Mac ATM), the fact that it's crashing with `string` and `object` dtype bares resemblance to an earlier segfault we were seeing in a different part of the code.\n\nIf the issue persists on `master`, in <a href=\"https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/parser.pyx\">parser.pyx<\/a>, you can find the `string_convert` function <a href=\"https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/parser.pyx#L1197\">here<\/a>.  Judging from your versions output, I suspect the segfault is occurring in this function <a href=\"https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/parser.pyx#L1221\">here<\/a> in fact.  If my suspicion is correct, can you further specify which method call is causing the crash?\n",
      "Hello,  @gfyoung !\n\nI cloned the master branch, but still the problem persisted.\n\nI've managed to trace the source to a read access beyond the allocated large memory block in either **kh_get_str** or **kh_get_strbox** (defined in **src\/klib\/khash.h**) in **_string_box_factorize** (from **parser.pyx**) called from the [very last branch](https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/parser.pyx#L1221) in **_string_convert** for the remaining 5 lines of the text input. The particular function depends on the **na_filter** setting.\n\nI `debug-patched` **COLITER_NEXT** in **tokenizer.h** with:\n\n``` C\n#define COLITER_NEXT(iter, word) do { \\\n    const int i = *iter.line_start++ + iter.col; \\\n    word = i < *iter.line_start ? iter.words[i]: \"\"; \\\n    printf(\"%d, %p\\n\", i, (const void*) iter.words[i]); \\\n    } while(0)\n```\n\n-- to print out the last address just before the crash. It printed `0` and the address at which the `EXC_BAD_ACCESS` happens. Any attempt to print the **word** returned by **COLITER_NEXT** results in **segfault**. Note that the **word** is later fed into both  **kh_&ast;** functions.\n\nI also added this to **tokenizer.c**:\n\n``` C\nvoid _dump(const void *addr, size_t len) \n{\n    size_t i;\n    unsigned char buff[17];\n    unsigned char *pc = (unsigned char*)addr;\n\n    printf(\"%p:\\n\", addr);\n    for (i = 0; i < len; i++) {\n        if ((i % 16) == 0) {\n            if (i != 0)\n                printf(\"  %s\\n\", buff);\n            printf(\"  %04x \", i);\n        }\n        printf(\" %02x\", pc[i]);\n        if ((pc[i] < 0x20) || (pc[i] > 0x7e)) {\n            buff[i % 16] = '.';\n        } else {\n            buff[i % 16] = pc[i];\n        }\n        buff[(i % 16) + 1] = '\\0';\n    }\n    while ((i % 16) != 0) {\n        printf(\"   \");\n        i++;\n    }\n    printf(\"  %s\\n\", buff);\n}\n```\n\nI borrowed it with simplifications from [this gist](https:\/\/gist.github.com\/domnikl\/af00cc154e3da1c5d965) to dump memory contents of **parser->words** and **parser->stream** in a call to **_string_box_factorize**. It turns out that just before the crash the pointers in **parser->words** point to a memory region starting with the address that causes the crash.\n\nI strongly suspect that this problem is specific to OS X memory allocation.\n\nPS: It seems that [**end_field**](https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/src\/parser\/tokenizer.c#L411) records the pointers to words in a memory region into **parser->word_starts**, which later becomes unaccessible.\n\nPPS: I suspect [**parser_trim_buffers**](https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/src\/parser\/tokenizer.c#L1217) changes allocated memory but does not re-initialize **parser->word_starts**.\n\nPPPS: Here is a snippet which does not use the `dataset.csv` file, and instead uses StringIO, but still crashes with **segfault**.\n\n``` python\nimport pandas as pd\nfrom cStringIO import StringIO\nrecord_ = \"\"\"9999-9,99:99,,,,ZZ,ZZ,,,ZZZ-ZZZZ,.Z-ZZZZ,-9.99,,,9.99,ZZZZZ,,-99,9,ZZZ-ZZZZ,ZZ-ZZZZ,,9.99,ZZZ-ZZZZZ,ZZZ-ZZZZZ,ZZZ-ZZZZ,ZZZ-ZZZZ,ZZZ-ZZZZ,ZZZ-ZZZZ,ZZZ-ZZZZ,ZZZ-ZZZZ,999,ZZZ-ZZZZ,,ZZ-ZZZZ,,,,,ZZZZ,ZZZ-ZZZZZ,ZZZ-ZZZZ,,,9,9,9,9,99,99,999,999,ZZZZZ,ZZZ-ZZZZZ,ZZZ-ZZZZ,9,ZZ-ZZZZ,9.99,ZZ-ZZZZ,ZZ-ZZZZ,,,,ZZZZ,,,ZZ,ZZ,,,,,,,,,,,,,9,,,999.99,999.99,,,ZZZZZ,,,Z9,,,,,,,ZZZ,ZZZ,,,,,,,,,,,ZZZZZ,ZZZZZ,ZZZ-ZZZZZZ,ZZZ-ZZZZZZ,ZZ-ZZZZ,ZZ-ZZZZ,ZZ-ZZZZ,ZZ-ZZZZ,,,999999,999999,ZZZ,ZZZ,,,ZZZ,ZZZ,999.99,999.99,,,,ZZZ-ZZZ,ZZZ-ZZZ,-9.99,-9.99,9,9,,99,,9.99,9.99,9,9,9.99,9.99,,,,9.99,9.99,,99,,99,9.99,9.99,,,ZZZ,ZZZ,,999.99,,999.99,ZZZ,ZZZ-ZZZZ,ZZZ-ZZZZ,,,ZZZZZ,ZZZZZ,ZZZ,ZZZ,9,9,,,,,,ZZZ-ZZZZ,ZZZ999Z,,,999.99,,999.99,ZZZ-ZZZZ,,,9.999,9.999,9.999,9.999,-9.999,-9.999,-9.999,-9.999,9.999,9.999,9.999,9.999,9.999,9.999,9.999,9.999,99999,ZZZ-ZZZZ,,9.99,ZZZ,,,,,,,,ZZZ,,,,,9,,,,9,,,,,,,,,,ZZZ-ZZZZ,ZZZ-ZZZZ,,ZZZZZ,ZZZZZ,ZZZZZ,ZZZZZ,,,9.99,,ZZ-ZZZZ,ZZ-ZZZZ,ZZ,999,,,,ZZ-ZZZZ,ZZZ,ZZZ,ZZZ-ZZZZ,ZZZ-ZZZZ,,,99.99,99.99,,,9.99,9.99,9.99,9.99,ZZZ-ZZZZ,,,ZZZ-ZZZZZ,,,,,-9.99,-9.99,-9.99,-9.99,,,,,,,,,ZZZ-ZZZZ,,9,9.99,9.99,99ZZ,,-9.99,-9.99,ZZZ-ZZZZ,,,,,,,ZZZ-ZZZZ,9.99,9.99,9999,,,,,,,,,,-9.9,Z\/Z-ZZZZ,999.99,9.99,,999.99,ZZ-ZZZZ,ZZ-ZZZZ,9.99,9.99,9.99,9.99,9.99,9.99,,ZZZ-ZZZZZ,ZZZ-ZZZZZ,ZZZ-ZZZZZ,ZZZ-ZZZZZ,ZZZ-ZZZZZ,ZZZ,ZZZ,ZZZ,ZZZ,9.99,,,-9.99,ZZ-ZZZZ,-999.99,,-9999,,999.99,,,,999.99,99.99,,,ZZ-ZZZZZZZZ,ZZ-ZZZZ-ZZZZZZZ,,,,ZZ-ZZ-ZZZZZZZZ,ZZZZZZZZ,ZZZ-ZZZZ,9999,999.99,ZZZ-ZZZZ,-9.99,-9.99,ZZZ-ZZZZ,99:99:99,,99,99,,9.99,,-99.99,,,,,,9.99,ZZZ-ZZZZ,-9.99,-9.99,9.99,9.99,,ZZZ,,,,,,,ZZZ,ZZZ,,,,,\"\"\"\ncsv_data = \"\\n\".join([record_]*173) + \"\\n\"\n\nfor _ in range(2):\n    iterator_ = pd.read_csv(StringIO(csv_data), header=None, engine=\"c\",\n                            dtype=object, chunksize=84, na_filter=True)\n    for chunk_ in iterator_:\n        print chunk_.iloc[0, 0], chunk_.iloc[-1, 0]\n    print \">>>NEXT\"\n```\n",
      "The problems seems to be in the [**parser_trim_buffers**](https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/src\/parser\/tokenizer.c#L1217) as it appears not to move word pointers.\n\nIf I swap blocks **L1224 -- L1237** (**\/&ast; trim stream &ast;\/**) and **L1239 -- L1256**(**\/&ast; trim words, word_starts &ast;\/**) and then copy the initialization of **parser->words** from [**make_stream_space**](https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/src\/parser\/tokenizer.c#L289) to the **trim stream** block (as shown in the snippet below), the problem goes away.\n\nHere is a new version of **parser_trim_buffers**\n\n``` C\n\nint parser_trim_buffers(parser_t *self) {\n    \/*\n      Free memory\n     *\/\n    size_t new_cap;\n    void *newptr;\n\n    int i;\n\n    \/* trim words, word_starts *\/\n    new_cap = _next_pow2(self->words_len) + 1;\n    if (new_cap < self->words_cap) {\n        TRACE((\"parser_trim_buffers: new_cap < self->words_cap\\n\"));\n        newptr = safe_realloc((void*) self->words, new_cap * sizeof(char*));\n        if (newptr == NULL) {\n            return PARSER_OUT_OF_MEMORY;\n        } else {\n            self->words = (char**) newptr;\n        }\n        newptr = safe_realloc((void*) self->word_starts, new_cap * sizeof(int));\n        if (newptr == NULL) {\n            return PARSER_OUT_OF_MEMORY;\n        } else {\n            self->word_starts = (int*) newptr;\n            self->words_cap = new_cap;\n        }\n    }\n\n    \/* trim stream *\/\n    new_cap = _next_pow2(self->stream_len) + 1;\n    TRACE((\"parser_trim_buffers: new_cap = %zu, stream_cap = %zu, lines_cap = %zu\\n\",\n           new_cap, self->stream_cap, self->lines_cap));\n    if (new_cap < self->stream_cap) {\n        TRACE((\"parser_trim_buffers: new_cap < self->stream_cap, calling safe_realloc\\n\"));\n        newptr = safe_realloc((void*) self->stream, new_cap);\n        if (newptr == NULL) {\n            return PARSER_OUT_OF_MEMORY;\n        } else {\n            \/\/ realloc sets errno when moving buffer?\n            if (self->stream != newptr) {\n                \/\/ uff\n                \/* TRACE((\"Moving word pointers\\n\")) *\/\n\n                self->pword_start = newptr + self->word_start;\n\n                for (i = 0; i < self->words_len; ++i)\n                {\n                    self->words[i] = newptr + self->word_starts[i];\n                }\n            }\n\n            self->stream = newptr;\n            self->stream_cap = new_cap;\n\n        }\n    }\n\n...\n\n    return 0;\n}\n\n```\n",
      "Awesome that you were able to fix your segfault!  Here's what I would do now:\n\n1) run all of the unit tests to see if your changes break any existing functionality\n\n2) if they don't, then submit this as a PR so that all of us can have a look!\n\n3) if they do cause failures, then I'll leave it up to you whether you want to investigate the failure causes. Feel free to provide a patch that we can clone and figure out.\n",
      "I issued a pull request (#13788), however there was one failed test and several _deprecation warnings_:\n\n```\nFAIL: test_round_trip_frame_sep (pandas.io.tests.test_clipboard.TestClipboard)\n```\n",
      "that test is ok, its not currently engaged on travis and fails on linux\/macosx (and already has an outstanding issue).\n\ndeprecation warnings are ok (though I do try to eliminate them periodically).\n"
    ],
    "events":[
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "labeled",
      "labeled"
    ],
    "changed_files":3,
    "additions":101,
    "deletions":14
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13884,
    "reporter":"jzwinck",
    "created_at":1470164917000,
    "closed_at":1470344670000,
    "resolver":"jzwinck",
    "resolved_in":"9c1e738df7effbf89c98dea59b0482b057c8c8b8",
    "resolver_commit_num":0,
    "title":"Round trip through HDF5 with format=table and localized DatetimeIndex discards index name",
    "body":"This should work, but the assert fails:\n\n\n\nIt works fine if you don't localize the DatetimeIndex, or if you don't use format='table'.\n\nThe index name \"expected\" is actually stored in the \"info\" attribute inside the HDF5 file whether it's localized or not.  But the format is slightly different.  If not localized:\n\n\n\nIf localized:\n\n\n\nI don't know enough to say whether the bug is in read_hdf(), to_hdf(), or PyTables.\n\nI'm using Pandas 0.18.1.\n",
    "labels":[
      "Bug",
      "Indexing",
      "Difficulty Novice",
      "IO HDF5",
      "Effort Low"
    ],
    "comments":[
      "thought there was an issue about this, but can't seem to find it.\n\nSo the attribute is saved. Must not be set on the read-back somehow.\n\n```\nIn [38]: store.root.world._v_attrs\nOut[38]:\n\/world._v_attrs (AttributeSet), 15 attributes:\n   [CLASS := 'GROUP',\n    TITLE := '',\n    VERSION := '1.0',\n    data_columns := [],\n    encoding := 'UTF-8',\n    index_cols := [(0, 'index')],\n    info := {1: {'type': 'Index', 'names': [None]}, 'index': {'tz': <UTC>, 'index_name': 'expected'}},\n    levels := 1,\n    metadata := [],\n    nan_rep := 'nan',\n    non_index_axes := [(1, ['a'])],\n    pandas_type := 'frame_table',\n    pandas_version := '0.15.2',\n    table_type := 'appendable_frame',\n    values_cols := ['values_block_0']]\n```\n",
      "@jzwinck HDFStore is a meta-data layer on top of PyTables. pull-requests are welcome.\n"
    ],
    "events":[
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files":3,
    "additions":18,
    "deletions":5
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13891,
    "reporter":"lia-simeone",
    "created_at":1470254438000,
    "closed_at":1470665188000,
    "resolver":"agraboso",
    "resolved_in":"72be37bcc855ef1fb01ebda78f6aa4ef3bcc6315",
    "resolver_commit_num":1,
    "title":"BUG: allow describe() for on boolean-only columns",
    "body":"I know I can obtain the expected output by using `include=['bool']`, but that feels bad to me as a user. I want `describe()` to know that I'm only asking for boolean columns and not freak out.\n\nThank you!\n#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n\n#### Actual output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels":[
      "Bug",
      "Numeric",
      "Dtypes",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "must be something in the exlusion logic, as an all object dtype works. PR's are welcome.\n",
      "The problem is in [`generic.py#L5141-L5143`](https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/core\/generic.py#L5141):\n\n``` python\nif len(self._get_numeric_data()._info_axis) > 0:\n    # when some numerics are found, keep only numerics\n    data = self.select_dtypes(include=[np.number])\n```\n\n`_get_numeric_data()` keeps boolean columns ([`BoolBlock`](https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/core\/internals.py#L1749) inherits from [`NumericBlock`](https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/core\/internals.py#L1506)), but `select_dtypes(include=[np.number])` does not.\n\n``` python\n>>> test_df._get_numeric_data()\n  test_ind_1 test_ind_2\n0      False      False\n1      False       True\n2       True       True\n3       True      False\n4      False      False\n5       True       True\n6       True       True\n7      False       True\n>>> test_df.select_dtypes(include=[np.number])\nEmpty DataFrame\nColumns: []\nIndex: [0, 1, 2, 3, 4, 5, 6, 7]\n```\n\nI'm working on a fix, but I'm worried I may be getting too deep into the internals of pandas...\n"
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "renamed",
      "milestoned",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":4,
    "additions":58,
    "deletions":4
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12879,
    "reporter":"randomgambit",
    "created_at":1460463161000,
    "closed_at":1470672654000,
    "resolver":"bkandel-picwell",
    "resolved_in":"b7abef4949fb1ba7fd1004feba4f47ace7004282",
    "resolver_commit_num":0,
    "title":"DOC: OrderedDict example in groupby aggregation",
    "body":"Hello,\n\nIt's me, again. ;-) I would like to submit to your attention another possible improvement your could implement. \n\nLet me start by saying that 90% of the time, data processing in my field involves some form of groupby at the time frequency, panel frequency, etc. \n\nSo aggregating data efficiently is definitely something important and very useful. \n\nI clearly prefer pandas' `groupby` over stata `collapse` (or others) because it is so much faster.\nHowever, a key functionality seems to be missing in Pandas. Usually, people want to apply functions to several columns, and be able to rename the output. In stata, you would write something like\n\n`collapse (firstnm) jreback=pandas, by(time)`\n\nto create a variable named `jreback`, that contains the first non missing value of the column `pandas` for every group in `time`.\n\nIn Pandas, a similar process seem unnecessarily complex.\n\nI can only use the syntax `group=df.groupby('group').agg({'A':'mean', 'B':['mean','sum']})`\nwhich has a **major disadvantage**\n- **no predictability over the sorting order of the columns**. That is, there is no guarantee that in group, the first column will be A and the second one will be B. I need to `group.column.tolist()` manually to understand which column corresponds to what. That is clearly not efficient (or maybe I am missing something here)\n\nIt would be therefore useful to add an option `column_names` that allows the user to chose columns names at the `agg` level, with the guarantee that the first name correspond to the first column and so on. For instance, in the example above I could specify `col_names=['mean_A','mean_B','this is a sum']`\n\nWhat do you think?\n",
    "labels":[
      "Usage Question",
      "Groupby",
      "Docs",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "@randomgambit there is this\n\n``` python\nIn [60]: df = pd.DataFrame({'A': [1, 2, 3], 'B': ['a', 'a', 'b'], 'C': [3, 4, 5]})\n\nIn [61]: df\nOut[61]:\n   A  B  C\n0  1  a  3\n1  2  a  4\n2  3  b  5\n\nIn [62]: df.groupby('B').agg({'A': {'mean1': 'mean', 'med1': 'median'}, 'C': {'mean2': 'mean', 'med2': 'median'}})\nOut[62]:\n      C          A\n  mean2 med2 mean1 med1\nB\na   3.5  3.5   1.5  1.5\nb   5.0  5.0   3.0  3.0\n```\n\nSo you pass a dict like `{original_column: {'output_name': aggfunc}}`. As you see this is unordered since python's dicts are undordered. So I'd suggest using an OrderedDict if order is important\n\n``` python\n# from collections import OrderedDict\nIn [99]: agg_funcs = OrderedDict([('A', OrderedDict([('mean1', 'mean'), ('med1', 'median')])), ('C', OrderedDict([('mean2', 'mean'), ('med2', 'median')]))])\n\nIn [100]: df.groupby('B').agg(agg_funcs)\nOut[100]:\n      A          C\n  mean1 med1 mean2 med2\nB\na   1.5  1.5   3.5  3.5\nb   3.0  3.0   5.0  5.0\n```\n",
      "excellent! I suggest you add this to the cookbook or to the tutorial. I know many people that are confused by this simple renaming thing. Thanks\n",
      "just a follow up: if I remember well this only works with `agg`, right? what about `transform`?\n",
      "you could put a section at the end of http:\/\/pandas-docs.github.io\/pandas-docs-travis\/groupby.html#aggregation\n"
    ],
    "events":[
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "labeled",
      "renamed",
      "milestoned",
      "labeled",
      "labeled",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":1,
    "additions":12,
    "deletions":0
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13577,
    "reporter":"mhaseebtariq",
    "created_at":1467882246000,
    "closed_at":1471517370000,
    "resolver":"mhaseebtariq",
    "resolved_in":"1919e26ead5d156c2b505a0ad8d233b02eb1b573",
    "resolver_commit_num":0,
    "title":"GbqConnector should be able to fetch default credentials on Google Compute Engine",
    "body":"\n\n\n\nGoogle Compute Engine and Google Dataproc etc. already have default application credentials on them. Therefore, there is no need of running `OAuth2WebServerFlow` while the method `get_user_account_credentials` is being called - instead `return GoogleCredentials.get_application_default()` is all you need. This change will allow you to run `GbqConnector` on commandline-only systems on Google Compute Engine without a need of a service account json key.\n",
    "labels":[
      "Google I\/O"
    ],
    "comments":[
      "cc @parthea @aaront @jacobschaer\n",
      "Fix for this: https:\/\/github.com\/pydata\/pandas\/pull\/13608\n"
    ],
    "events":[
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "milestoned"
    ],
    "changed_files":4,
    "additions":124,
    "deletions":9
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13135,
    "reporter":"aliabbasjp",
    "created_at":1462950543000,
    "closed_at":1471786674000,
    "resolver":"rkern",
    "resolved_in":"ce61b3f1c85c1541cfbe1b3bb594431b38689946",
    "resolver_commit_num":0,
    "title":"df.query bug giving RuntimeWarning: divide by zero encountered in log10 in align.py\", line 98, in _align_core     ordm = np.log10(abs(reindexer_size - term_axis_size)) ",
    "body":"Getting the following bug:\n\nRuntimeWarning: divide by zero encountered in log10\nC:\\Users\\Nick\\Anaconda3\\lib\\site-packages\\pandas\\computation\\align.py:98: \n  ordm = np.log10(abs(reindexer_size - term_axis_size))\nwhile running query().\n\ndtypes:\n\nCGI            int64\nSITEID         int32\nLONGITUDE    float64\nLATITUDE     float32\ndtype: object\nQuery:\n\ntowerData = towerData.query('CGI != 0')\n\nonce you get this runtime warning it just halts everything  and you have to restart the program.\n\nThis bug happens only in multithreaded calls to query function. I am using the concurrent.futures module for the same.\n\nruns fine for non threaded application\n",
    "labels":[
      "Numeric",
      "Bug"
    ],
    "comments":[
      "Thanks for the report. Do you have minimal copy-pastable example?\n",
      "Note: I'll be avoiding that warning explicitly in my fix for #13109. The numpy error state is thread-local IIRC, so the current fire-and-forget `np.seterr(all='ignore')` probably only takes effect in the main thread. Why the OP's program halts after the warning, I don't know.\n",
      "I'm having a similar issue even without threading.  See example code below.\n\n`\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame.from_items([('A', [1, 2, 3, 4, 5]), \n                              ('B', [14, 15, 16, 17, 18]), \n                              ('C', list(np.random.randn(5)))])\ndf.set_index(['A', 'B'], inplace = True)\ndf.query('15 <= B <= 17')\n\nC:\\Anaconda3\\lib\\site-packages\\pandas\\computation\\align.py:98: \nRuntimeWarning: divide by zero encountered in log10\nordm = np.log10(abs(reindexer_size - term_axis_size))\nOut[1]: \n             C\nA B \n2 15 -0.852411\n3 16 -0.665470\n4 17  0.132162\n`\n\nWindows 7 64 bit\nSpyder 3.0.0.dev0\nPython 3.5.2\nPandas 0.18.1\nNumpy 1.11.1\n"
    ],
    "events":[
      "commented",
      "labeled",
      "commented",
      "cross-referenced",
      "commented",
      "milestoned",
      "labeled"
    ],
    "changed_files":35,
    "additions":449,
    "deletions":314
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13977,
    "reporter":"kernc",
    "created_at":1471028082000,
    "closed_at":1472241860000,
    "resolver":"kernc",
    "resolved_in":"042b6f00ad691345812e61bb7e86e52476805602",
    "resolver_commit_num":0,
    "title":"df.iterrows() constructs Series instead of its proper subclass",
    "body":"#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n`row` would be of proper overriden `_constructor_sliced` type (i.e. `SubclassedSeries`).\n",
    "labels":[
      "Bug",
      "Dtypes"
    ],
    "comments":[

    ],
    "events":[
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files":4,
    "additions":12,
    "deletions":3
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13844,
    "reporter":"VelizarVESSELINOV",
    "created_at":1469826210000,
    "closed_at":1472244200000,
    "resolver":"tom-bird",
    "resolved_in":"0db43045508c474f1fcaf8c3f10a306c0e571c91",
    "resolver_commit_num":0,
    "title":"Empty dataset: typerror ufunc add cannot use operands with types dtype('<M8[ns]') and dtype('O')",
    "body":"#### Code Sample, a copy-pastable example if possible\n\n\n#### Current Output\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels":[
      "Bug",
      "Timeseries",
      "Timedelta",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments":[
      "Here is a simpler replication.\n\n```\nIn [63]: df = pd.DataFrame({'A' : Series(dtype='M8[ns]'), 'B' : Series(dtype='m8[ns]')})\n\nIn [64]: df\nOut[64]: \nEmpty DataFrame\nColumns: [A, B]\nIndex: []\n\nIn [65]: df.dtypes\nOut[65]: \nA     datetime64[ns]\nB    timedelta64[ns]\ndtype: object\n\nIn [66]: df.A+df.B\nTypeError: ufunc add cannot use operands with types dtype('<M8[ns]') and dtype('O')\n```\n\nYeah this empty case is prob address by coercion, maybe @sinhrks has a better idea.\n",
      "The op is performed in `_TimeOp` class, and it regards empty input as `offset` even if it has a `dtype`. \n- https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/core\/ops.py#L596\n\nThen, offset is coerced to `object` here. \n- https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/core\/ops.py#L532\n\nFixing `_TimeOp._is_offset` should solve the problem. PR is appreciated!\n",
      "Happy to take a look at this and submit a PR\n",
      "So, we shouldn't regard something as `offset` if its empty? This is a three line fix, and certainly resolves the error in the above two examples.\n"
    ],
    "events":[
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":12,
    "deletions":3
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":14171,
    "reporter":"ponomarevvl90",
    "created_at":1473238242000,
    "closed_at":1473460156000,
    "resolver":"josh-howes",
    "resolved_in":"289cd6d0df66b812921ff4c5cbade937b875406d",
    "resolver_commit_num":0,
    "title":"Series.str.contains doesn't correct if series contains only nan values.",
    "body":"#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.10.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.2.0-42-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.17.1\nnose: None\npip: 8.0.3\nsetuptools: 18.0.1\nCython: 0.23.4\nnumpy: 1.10.1\nscipy: None\nstatsmodels: None\nIPython: 4.0.1\nsphinx: None\npatsy: None\ndateutil: 2.4.2\npytz: 2015.7\nblosc: None\nbottleneck: None\ntables: 3.2.2\nnumexpr: 2.4.6\nmatplotlib: None\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\nJinja2: None\n",
    "labels":[
      "Bug",
      "Difficulty Novice",
      "Strings",
      "Effort Low"
    ],
    "comments":[
      "Here's a simpler example. This is casting only when `na` flag is passed.\n\n```\nIn [20]: Series([np.nan, np.nan, np.nan],dtype='object').str.contains('foo')\nOut[20]: \n0   NaN\n1   NaN\n2   NaN\ndtype: float64\n\nIn [21]: Series([np.nan, np.nan, np.nan],dtype='object').str.contains('foo',na=False)\nOut[21]: \n0    0.0\n1    0.0\n2    0.0\ndtype: float64\n\nIn [22]: Series([np.nan, np.nan, np.nan],dtype='object').str.contains('foo',na=True)\nOut[22]: \n0    1.0\n1    1.0\n2    1.0\ndtype: float64\n```\n\nworks fine if not all-nan `object` dtype.\n\n```\nIn [23]: Series([np.nan, np.nan, np.nan, 'a'],dtype='object').str.contains('foo')\nOut[23]: \n0      NaN\n1      NaN\n2      NaN\n3    False\ndtype: object\n\nIn [25]: Series([np.nan, np.nan, np.nan, 'a'],dtype='object').str.contains('foo', na=False)\nOut[25]: \n0    False\n1    False\n2    False\n3    False\ndtype: bool\n\nIn [26]: Series([np.nan, np.nan, np.nan, 'a'],dtype='object').str.contains('foo', na=True)\nOut[26]: \n0     True\n1     True\n2     True\n3    False\ndtype: bool\n```\n",
      "pull-requests to fix are welcome!\n",
      "First time contributor here.  I'll take this on.\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":4,
    "additions":24,
    "deletions":2
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13797,
    "reporter":"ygriku",
    "created_at":1469498140000,
    "closed_at":1473805532000,
    "resolver":"ygriku",
    "resolved_in":"fb25cca654db9b626a3993bb62a01857807794b4",
    "resolver_commit_num":0,
    "title":"BUG: iloc fails with non lex-sorted MultiIndex",
    "body":"#### Code Sample, a copy-pastable example if possible\n\n\n\ndf1 (whose index is not lex sorted) fails with iloc access:\n\n\n#### Expected Output\n\ndf2 (whose index is lex sorted) works as expected:\n\n\n\nThis attributeError does not occur when the DataFrame.values consist of numpy objects (e.g. numpy.int32) because they have the ndim attribute. (Although the performance warning remains, it may be another issue).\n\nI found that the addition of an **if** statement can remedy this in pandas\/core\/indexing.py. This just makes the **_getitem_tuple(self, tup)** be aware of objects without the ndim attribute, as **_getitem_nested_tuple(self, tup)** is (I will prepare a pull request if it is helpful.)\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels":[
      "Bug",
      "Indexing",
      "MultiIndex"
    ],
    "comments":[
      "@YG-Riku actually the fix should start [here](https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/core\/indexing.py#L905)\n\n`.iloc` should not hit the `MultiIndex` branch testing at all. If you want to give this a try would be great.\n",
      "@jreback \nThank you very much for your response.\n\nThat's the point where I stumbled on too. Just commenting out the lines:\n\n``` python\n#        ax0 = self.obj._get_axis(0)\n#        if isinstance(ax0, MultiIndex):\n#            result = self._handle_lowerdim_multi_index_axis0(tup)\n#            if result is not None:\n#                return result\n```\n\ndiminishes the error (and the warning too). But I am not sure about the consequence of this removal of the codes (any side-effect?). \n",
      "almost certainly needs to be more selective\neg dispatch on self.name == 'iloc'\n\nlots and lots of indexing tests must still pass\n",
      "I think I can do some trials-and-errors with the existing test cases of pandas\/tests. But if there is a need to prepare a decent set of new test codes, that will be too much burden for me. (Anyway, I will start looking into the test cases ... )\n",
      "no as I said there are hundreds of tests already\nyou can just add this case\n",
      "Thank you for the comment. I will try. It is exciting to have a chance to contribute!. (But, please don't wait for me. Probably the time I can spare for this would be mostly on weekends.)\n",
      "I found that the following simple amendment works as jreback suggested.\nThis passed `nosetests pandas\/tests\/test_multilevel.py` and no significant performance change was detected (at least in some trial). \n\n``` python\n@@ -903,7 +903,7 @@ class _NDFrameIndexer(object):\n\n         # we maybe be using a tuple to represent multiple dimensions here\n         ax0 = self.obj._get_axis(0)\n-        if isinstance(ax0, MultiIndex):\n+        if isinstance(ax0, MultiIndex) and self.name != 'iloc':\n             result = self._handle_lowerdim_multi_index_axis0(tup)\n             if result is not None:\n                 return result\n```\n\nWhat is the next step?\n",
      "you submit a PR with tests and the fix\n\nsee here: http:\/\/pandas.pydata.org\/pandas-docs\/stable\/contributing.html\n"
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":32,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":14373,
    "reporter":"aixtools",
    "created_at":1475837349000,
    "closed_at":1475926268000,
    "resolver":"aixtools",
    "resolved_in":"794f79295484298525dbc8dd3b8ab251ad065e61",
    "resolver_commit_num":0,
    "title":"problem with building pandas - not using gcc as compiler",
    "body":"\"\"\"the example is merely\npip install pandas - which fails, so download and run\n\npython .\/setup.py build\n\n\n",
    "labels":[
      "Build"
    ],
    "comments":[
      "please let me know how to attach a text or zip file - if interested!\n",
      "pls show architecture, version of cython & python. as well as how you got the source.\n\nand version of gcc.\n",
      "AIX 5.3, (not gcc, but xlc), 64-bit\nroot@x064:[\/]pip list\nCython (0.24.1)\nnumpy (1.11.2)\npip (8.1.1)\npython-dateutil (2.5.3)\npytz (2016.7)\nrequests (2.11.1)\nsetuptools (20.10.1)\nsix (1.10.0)\n\npip install pandas\n\npip download pandas\n\nroot@x065:[\/data\/prj\/python\/pipbuilds]ls *.gz\nnumpy-1.11.2.tar.gz     pandas-0.19.0.tar.gz\n",
      "I repeated the process, on AIX 6.1 - 32-bit mode, python built using gcc rather xlc.\n\nAIX 6.1, python-2.7.12, gcc-4.7.4\nroot@x065:[\/data\/prj\/python\/pipbuilds]pip list\nCython (0.24.1)\nnumpy (1.11.2)\npip (8.1.1)\npython-dateutil (2.5.3)\npytz (2016.7)\nsetuptools (20.10.1)\nsix (1.10.0)\n\nThis ends one(?) file earlier - I think the first .cpp (_packer.cpp), with different warnings:\n    gcc -pthread -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -D_LARGE_FILES -D__BIG_ENDIAN__=1 -Ipandas\/src\/msgpack -Ipandas\/src\/klib -Ipandas\/src -I\/opt\/lib\/python2.7\/site-packages\/numpy\/core\/include -I\/opt\/include\/python2.7 -c pandas\/msgpack\/_packer.cpp -o build\/temp.aix-6.1-2.7\/pandas\/msgpack\/_packer.o -Wno-unused-function\n    cc1plus: warning: command line option '-Wstrict-prototypes' is valid for C\/ObjC but not for C++ [enabled by default]\n    pandas\/msgpack\/_packer.cpp: In function 'Py_ssize_t __Pyx_PyIndex_AsSsize_t(PyObject*)':\n    pandas\/msgpack\/_packer.cpp:7517:9: error: call of overloaded 'abs(const Py_ssize_t&)' is ambiguous\n    pandas\/msgpack\/_packer.cpp:7517:9: note: candidates are:\n    In file included from \/opt\/include\/python2.7\/Python.h:42:0,\n                     from pandas\/msgpack\/_packer.cpp:4:\n    \/opt\/lib\/gcc\/powerpc-ibm-aix5.3.7.0\/4.7.4\/include-fixed\/stdlib.h:309:14: note: int abs(int)\n    In file included from \/opt\/include\/python2.7\/pyport.h:325:0,\n                     from \/opt\/include\/python2.7\/Python.h:58,\n                     from pandas\/msgpack\/_packer.cpp:4:\n    \/opt\/lib\/gcc\/powerpc-ibm-aix5.3.7.0\/4.7.4\/include-fixed\/math.h:947:14: note: float abs(float)\n    \/opt\/lib\/gcc\/powerpc-ibm-aix5.3.7.0\/4.7.4\/include-fixed\/math.h:977:20: note: long double abs(long double)\n    error: command 'gcc' failed with exit status 1\n\nThe \"error\" being this line:\n   pandas\/msgpack\/_packer.cpp:7517:9: error: call of overloaded 'abs(const Py_ssize_t&)' is ambiguous\n\nFYI: when using xlc - _packer.o completed.\n\n45482770  116 -rw-r-----  1 root      system      117901 Oct  7 10:46 .\/pandas-0.19.0\/build\/temp.aix-5.3-2.7\/pandas\/msgpack\/_packer.o\nroot@x065:[\/data\/prj\/python\/pipbuilds]oslevel\n6.1.0.0\n\nNote the directory above is build\/temp.aix5.3-2.7 not aix6.1-2.7\n",
      "we don't support aix directly, meaning that it's not tested at all\nso would accept compatible patches\n\nwe also make no guarantees about big endian (powerpc)\nagain would accept patches \n"
    ],
    "events":[
      "commented",
      "commented",
      "labeled",
      "commented",
      "commented",
      "milestoned"
    ],
    "changed_files":1,
    "additions":1,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":14369,
    "reporter":"AlbertDeFusco",
    "created_at":1475783748000,
    "closed_at":1476561593000,
    "resolver":"brandonmburroughs",
    "resolved_in":"fd3be00bc46b416437e8cfafcf5661ec57385e2f",
    "resolver_commit_num":0,
    "title":"concat with axis='rows'",
    "body":"#### A small, complete example of the issue\n\nIs there something special going on here that `pd.concat` seems to interpret `axis='rows'` as `axis=1`?\n\n\n#### Expected Output\n\nI would think that `axis='rows'` and `axis=0` do the same thing.\n#### Output of `pd.show_versions()`\n\n<details>\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.2.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.6.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\nLOCALE: en_US.UTF-8\n\npandas: 0.19.0\nnose: None\npip: 8.1.2\nsetuptools: 27.2.0\nCython: None\nnumpy: 1.11.2\nscipy: None\nstatsmodels: None\nxarray: None\nIPython: 5.1.0\nsphinx: None\npatsy: None\ndateutil: 2.5.3\npytz: 2016.7\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: None\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\npandas_datareader: None\n\n<\/details>\n",
    "labels":[
      "Bug",
      "Reshaping",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "It seems that the conversion from 'rows'\/'columns' to 0\/1 does not happen in `concat`, so any string will be evaluated as True in a boolean check ..\n",
      "@jreback  I use pandas pretty extensively in most of my projects. I'd like to make some contributions back. \ud83d\ude03  Mind if I take a stab at this?\n",
      "sure the more the merrier\n"
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":67,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13139,
    "reporter":"gaow",
    "created_at":1462977313000,
    "closed_at":1477347204000,
    "resolver":"tserafim",
    "resolved_in":"13088842a7218e8e4626ab68f0c4f204f25f0ba4",
    "resolver_commit_num":0,
    "title":"ERR: better error message on invalid .query input",
    "body":"#### Code Sample\n\n\n#### Expected output\n\nIf I run query with an empty string I should expect no output, however:\n\n\n\nThe problem lies on the last line of \n\nI'm not sure of a proper fix so I'm opening this issue instead of pull request. \n#### output of `pd.show_versions()`\n\n\n",
    "labels":[
      "Difficulty Novice",
      "Error Reporting",
      "Effort Low"
    ],
    "comments":[
      "I supppose this could be a better error message. That is invalid input.\n",
      "There is already a helper function for that: [_check_expression(expr)](https:\/\/github.com\/pandas-dev\/pandas\/blob\/65362aa4f06f01efdc20ca487c1c3c1f090613ee\/pandas\/computation\/eval.py#L85-L99)\n\nProblem is, this function is only executed in the for loop: [eval.py#L246-L247](https:\/\/github.com\/pandas-dev\/pandas\/blob\/65362aa4f06f01efdc20ca487c1c3c1f090613ee\/pandas\/computation\/eval.py#L246-L247)\n\nA simple `_check_expression(expr)`  before the for loop can fix it. Or maybe on the beginning of the eval function. Is there a better way, according to pandas code style?\n\nI could send a PR(first OS contribution, ever)\n",
      "yes u can move it\npls add a test as well\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":4,
    "additions":22,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":14448,
    "reporter":"AnkurDedania",
    "created_at":1476829152000,
    "closed_at":1477348097000,
    "resolver":"keshavramaswamy",
    "resolved_in":"18fba53089fdfa3075cb9faa1f3ac57a2146be9b",
    "resolver_commit_num":0,
    "title":"DOC\/BUG: pd.to_datetime example produces different results from documentation",
    "body":"#### A small, complete example of the issue\n\nreferring too -docs\/stable\/generated\/pandas.to_datetime.html\n\nThe example provides different results.\n\n\n\nraises...\n\n\n#### Expected Output\n\n(according to example in docs)\n\n\n#### Output of `pd.show_versions()`\n\n<details>\n\n> > > pd.show_versions()\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.2.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 63 Stepping 2, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\nLOCALE: None.None\n\npandas: 0.19.0\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 28.0.0\nCython: 0.24.1\nnumpy: 1.11.2\nscipy: 0.18.1\nstatsmodels: 0.6.1\nxarray: 0.8.2\nIPython: 5.1.0\nsphinx: 1.4.8\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.7\nblosc: 1.4.1\nbottleneck: 1.1.0\ntables: None\nnumexpr: 2.6.0\nmatplotlib: 1.5.3\nopenpyxl: 2.4.0\nxlrd: 1.0.0\nxlwt: 1.1.2\nxlsxwriter: 0.9.3\nlxml: 3.6.4\nbs4: 4.5.0\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.1.0\npymysql: 0.7.9.None\npsycopg2: None\njinja2: 2.8\nboto: None\npandas_datareader: None\n\n<\/details>\n",
    "labels":[
      "Timeseries",
      "Docs",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "I'm guessing this was a side effect of #10674.  I'm actually in favor of the new behavior - makes it much more obvious there is a problem, but I suppose this behavior might have been useful in some cases.\n",
      "the doc-string needs to be updated to use `ignore`\n\n```\nIn [2]: import pandas as pd\n   ...: pd.to_datetime('13000101', format='%Y%m%d',errors='ignore')\nOut[2]: datetime.datetime(1300, 1, 1, 0, 0)\n```\n",
      "@jreback - oh, I see, I had tried without the format, which I guess never returned a plain datetime, so agree it's just docs\n\n```\nIn [26]: pd.to_datetime('13000101', errors='ignore')\nOut[26]: '13000101'\n```\n",
      "It is a bit inconsistent that the return type changes on whether `format` is specified or not:\n\n```\nIn [1]: pd.to_datetime('13000101', errors='ignore')\nOut[1]: '13000101'\n\nIn [2]: pd.to_datetime('13000101', errors='ignore', format='%Y%m%d')\nOut[2]: datetime.datetime(1300, 1, 1, 0, 0)\n```\n"
    ],
    "events":[
      "renamed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "labeled",
      "unlabeled",
      "unlabeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "commented",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":1,
    "additions":12,
    "deletions":5
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":14323,
    "reporter":"Liam3851",
    "created_at":1475166274000,
    "closed_at":1477348461000,
    "resolver":"Liam3851",
    "resolved_in":"bee90a7c50576b0160db55fb325908040233e92d",
    "resolver_commit_num":0,
    "title":"DatetimeIndex union fails in 0.19rc1 when constructed from differences in DatetimeIndexes",
    "body":"When constructing a union of 2 DatetimeIndex objects that themselves were constructed from differences from a third DatetimeIndex, the union operator is ignored. This appears to be new behavior in 0.19rc1; the code functioned correctly under 0.18.1.\n\n\n#### Output of `pd.show_versions()`\n\n<details>\n## INSTALLED VERSIONS                                                                                  \n\ncommit: None  \npython: 2.7.11.final.0  \npython-bits: 64  \nOS: Windows  \nOS-release: 7  \nmachine: AMD64  \nprocessor: Intel64 Family 6 Model 62 Stepping 4, GenuineIntel  \nbyteorder: little  \nLC_ALL: None  \nLANG: None  \nLOCALE: None.None                                                                                   \n\npandas: 0.19.0rc1+0.g497a3bc.dirty  \nnose: 1.3.7  \npip: 8.1.2  \nsetuptools: 27.2.0  \nCython: 0.24.1  \nnumpy: 1.11.1  \nscipy: 0.18.1  \nstatsmodels: 0.6.1  \nxarray: 0.8.2  \nIPython: 5.1.0  \nsphinx: 1.4.6  \npatsy: 0.4.1  \ndateutil: 2.5.3  \npytz: 2016.6.1  \nblosc: None  \nbottleneck: 1.1.0  \ntables: 3.2.2  \nnumexpr: 2.6.1  \nmatplotlib: 1.5.3  \nopenpyxl: 2.3.2  \nxlrd: 1.0.0  \nxlwt: 1.1.2  \nxlsxwriter: 0.9.3  \nlxml: 3.6.4  \nbs4: 4.5.1  \nhtml5lib: None  \n 0.9.2  \napiclient: 1.4.2  \nsqlalchemy: 1.0.13  \npymysql: None  \npsycopg2: None  \njinja2: 2.8  \nboto: 2.42.0  \npandas_datareader: 0.2.1  \n<\/details>\n",
    "labels":[
      "Regression",
      "Bug",
      "Timeseries",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments":[
      "@Liam3851 Thanks for the report! I can confirm it is indeed a bug\/regression.\n\nSeems it has something to do with `offset` being now defined:\n\n```\nIn [36]: pd.__version__\nOut[36]: '0.19.0rc1+34.gc128626'\n\nIn [37]: a\nOut[37]: \nDatetimeIndex(['2016-09-21', '2016-09-22', '2016-09-25', '2016-09-26',\n               '2016-09-27', '2016-09-28', '2016-09-29', '2016-09-30'],\n              dtype='datetime64[ns]', freq='D')\n\nIn [38]: a.offset\nOut[38]: <Day>\n```\n\nvs\n\n```\nIn [12]: pd.__version__\nOut[12]: '0.18.1'\n\nIn [13]: a\nOut[13]: \nDatetimeIndex(['2016-09-21', '2016-09-22', '2016-09-25', '2016-09-26',\n               '2016-09-27', '2016-09-28', '2016-09-29', '2016-09-30'],\n              dtype='datetime64[ns]', freq=None)\n\nIn [15]: a.offset is None\nOut[15]: True\n```\n",
      "It looks to me like this was introduced with #13514. `Index.difference` now returns a shallow copy of the original index with the differenced values:\n\n`return this._shallow_copy(the_diff, name=result_name)`\n\nBefore, this code returned a new DatetimeIndex:\n\n`return Index(theDiff, name=result_name)`\n\nI'm not too familiar with this code, but it looks to me like _shallow_copy is copying all the attributes, including the freq (which is no longer valid after the differencing operation). The `Index` constructor version would have re-computed the frequency during the construction of the new Index (thus determining there was no valid frequency).\n",
      "@Liam3851 I think that is a perfect assessment of the situation. We could use `_simple_new` instead of `_shallow_copy` (which does not retain the attributes), but then eg the timezone of a DatetimeIndex would get lost.\n\n@sinhrks do you think of a generic approach without adding code to specifically invalidate the `freq` in case of a DatetimeIndex\n",
      "you can just pass freq=None to _shallow_copy\ndon't directly use _simple_new\n",
      "Ah, I thought that would not work for PeriodIndex (which wants to keep its `freq`), but apparently it does.\n\n@Liam3851 Wants to do a PR for this change?\n",
      "I'm doing a PR (sorry, still a newb at Git and pandas building). Just want to get the requirements straight for the unit tests:\n1. DatetimeIndex and TimedeltaIndex should be None freq after the differencing, not an inferred frequency (it appears this was the old behavior, counter to what I said above).\n2. PeriodIndex should retain its frequency after differencing, even without the missing datapoints (because the Periods themselves have frequency)\n3. All three should have a union of differences matching a union of indexes.\n\nThat about right?\n"
    ],
    "events":[
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced"
    ],
    "changed_files":3,
    "additions":77,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":14626,
    "reporter":"evectant",
    "created_at":1478726499000,
    "closed_at":1478966933000,
    "resolver":"MykolaGolubyev",
    "resolved_in":"3552dc0c4533a5eafafe859f5afd29a7ce063e03",
    "resolver_commit_num":0,
    "title":"TST: indexes\/test_base.py:TestIndex.test_format is flaky",
    "body":"`indexes\/test_base.py:TestIndex.test_format` compares `index.format()` and `str(index[0])`, where `index = Index([datetime.now()])`. These won't match if the current timestamp ends with zeros:\r\n\r\n\r\n\r\nSame thing for real: -ci.org\/pandas-dev\/pandas\/jobs\/173440088#L1355",
    "labels":[
      "Difficulty Novice",
      "Effort Low",
      "Testing"
    ],
    "comments":[
      "yeah this does occasionally fail\n\nit's not a great test, but is testing the default behavior so no easy way to control the formatting\nmaybe put this in a while loop and get a new time if the current time ends with 000 \n\nput a short explanation there \n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":1,
    "additions":13,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":14652,
    "reporter":"luca-s",
    "created_at":1479126108000,
    "closed_at":1479386983000,
    "resolver":"luca-s",
    "resolved_in":"2fc0c68ace1cb447f1fa6f016295575a2024db3d",
    "resolver_commit_num":0,
    "title":"BUG: pandas.cut and negative values",
    "body":"Here is an example of pandas.cut ran on a pandas.Series with only one positive element and then on a pandas.Series with only one negative element. In the second scenario pandas.cut is not able to insert the single value on the only one bin.\r\n\r\nI might be wrong but I expected pandas.cut to behave on negative values the same as with positive \r\nvalues. \r\n\r\n\r\n#### A small, complete example of the issue\r\n\r\n\r\n\r\n#### Expected Output\r\n\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.3.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-45-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: 1.3.1\r\npip: 1.5.4\r\nsetuptools: 28.0.0\r\nCython: 0.20.1post0\r\nnumpy: 1.11.1\r\nscipy: 0.18.0\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.4.8\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.6.1\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999\r\n 0.8\r\napiclient: None\r\nsqlalchemy: 1.0.14\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: 0.2.1\r\n\r\n\r\n<\/details>\r\n",
    "labels":[
      "Bug",
      "Difficulty Novice",
      "Effort Low",
      "Groupby",
      "Reshaping"
    ],
    "comments":[
      "hmm, that does look wrong.\n\nwelcome for you to have a look!\n",
      "Ok, I had a look and the problem is probably in [here](https:\/\/github.com\/pandas-dev\/pandas\/blob\/master\/pandas\/tools\/tile.py#L101), the code should be:\n`mn -= .001 * abs(mn)`\nand also [here](https:\/\/github.com\/pandas-dev\/pandas\/blob\/master\/pandas\/tools\/tile.py#L102), the code  should be:\n`mx += .001 * abs(mx)`\n\nShould I submit a PR? I am not familiar with Pandas development\n",
      "@luca-s perfect. if you'd like to do a PR would be great.\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "renamed",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":17,
    "deletions":4
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":14692,
    "reporter":"dragonator4",
    "created_at":1479503784000,
    "closed_at":1479508374000,
    "resolver":"discort",
    "resolved_in":"d02ef6f04466e4a74f67ad584cf38cdc6df56e42",
    "resolver_commit_num":1,
    "title":"BUG: HDF5 Files cannot be read concurrently",
    "body":"#### A small, complete example of the issue\r\n\r\n\r\n\r\nThe above code either fails loudly with the following error:\r\n\r\n\r\n\r\nOr with the following error:\r\n\r\n\r\nBut in this case, `object 7` clearly exists in the table. Any help?\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-47-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.0\r\nnose: None\r\npip: 8.1.2\r\nsetuptools: 27.2.0\r\nCython: 0.25.1\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: 0.8.2\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: 1.1.0\r\ntables: 3.3.0\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: 0.9.3\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n\r\n<\/details>\r\n",
    "labels":[
      "Duplicate",
      "IO HDF5"
    ],
    "comments":[
      "duplicate https:\/\/github.com\/pandas-dev\/pandas\/issues\/12236\n\npull-request to fix are welcome. This is not actually that hard to fix.\n"
    ],
    "events":[
      "labeled",
      "labeled",
      "milestoned"
    ],
    "changed_files":3,
    "additions":19,
    "deletions":5
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":14682,
    "reporter":"j-santander",
    "created_at":1479421337000,
    "closed_at":1479814179000,
    "resolver":"jsantander",
    "resolved_in":"9f2e45378cbce5532a8edf2484d62a802369634e",
    "resolver_commit_num":0,
    "title":"AmbiguousTimeError on groupby when including a DST change",
    "body":"#### A small, complete example of the issue\r\n\r\n\r\n#### Expected Output\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n# Paste the output here pd.show_versions() here\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-47-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.1\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 28.6.1\r\nCython: 0.25.1\r\nnumpy: 1.11.2\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: None\r\nsphinx: 1.4.8\r\npatsy: None\r\ndateutil: 2.4.2\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: 2.6.1\r\nmatplotlib: None\r\nopenpyxl: 2.2.6\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: 3.5.0\r\nbs4: 4.4.1\r\nhtml5lib: 0.999\r\n 0.9.1\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n<\/details>\r\n\r\nThe above code raises an `AmbiguousTimeError` exception, when grouping by a time-date series including a DST change. In the above example the unix timestamps are for the recent DST change in Europe.\r\n\r\nThe stack trace is:\r\n\r\n\r\nCode works if the series does not include a DST change (e.g. one day earlier):\r\n\r\n\r\n\r\ngets:\r\n\r\n",
    "labels":[
      "Bug",
      "Difficulty Advanced",
      "Effort Low",
      "Groupby",
      "Timezones"
    ],
    "comments":[
      "xref https:\/\/github.com\/pandas-dev\/pandas\/issues\/10668 (though this looks separate).\n\nyeah, prob need to specify `ambiguous` when creating the bins. a pull-request to fix would make the fix happen sooner.\n",
      "I've been trying to debug the above issue. \n\nTried adding the ambiguous keyword to the constructor of the Timestamps... but I wasn't sure how to set it (as `infer`) didn't seem to be a valid option.\n\nThe code raising the exception seems to have been modified with commit dcc68d7c5a06df85fe9fec568566bee1e9936b10 where the `_adjust_dates_anchored()` function at `pandas.tseries.resample` module first drops the tz information at the beginning of the function and then adds it back on the return statement.\n\nI've modified the code to not do that... but then I had to modify an assert at pandas.tseries.index.py that it is checking for equality of time zones... but it turns that Europe\/Madrid on DST is considered different from Europe\/Madrid not on DST.\n\nI'll try to create a pull request with my changes so that you can comment.\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":4,
    "additions":40,
    "deletions":7
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":7626,
    "reporter":"ChristopherShort",
    "created_at":1404183323000,
    "closed_at":1480988885000,
    "resolver":"jeffcarey",
    "resolved_in":"4378f82967f59097055eef17ede50aa515525551",
    "resolver_commit_num":0,
    "title":"nrows limit fails reading well formed csv files from Australian electricity market data",
    "body":"Reading Australian electricity market data files, read_csv reads past the nrows limit for certain nrows values and consequently fails.\n\nThese market data files are 4 csv files combined into a single csv file and so the file has multiple headers and variable field size across the rows. \n\nThe first set of data is from rows 1-1442.\n\n Intent was to extract first set of data with nrows = 1442. \n\nTesting several arbitrary CSV files from this data source shows well formed CSV - 120 fields between rows 1 to 1442  (with a 10 field at row 0)\n\n\n\nreturns\n    120    1441\n    10        1\n    dtype: int64\n\nOther python examples of reading the market data using csv module [work fine](-json\/blob\/master\/script\/extract-historic-public-prices.py)\n\nIn the reproducible example below, code works for nrows< 824, but fails on any value above it. \n\nTesting on arbitrary files suggests the 824 limit is variable - sometimes a few more rows, sometimes a few less rows. \n\n\n",
    "labels":[
      "CSV",
      "Bug"
    ],
    "comments":[
      "I meant to say - the sort of error returned is:\n\n```\nCParserError: Error tokenizing data. C error: Expected 120 fields in line 1443, saw 130\n```\n\nWhich is expected - the field size changes past row 1442 - but for these files, the nrows limit reads past the 1442 (or 823+) value.\n\nI also tested nrows on arbitrarily created csv files via numpy arrays but couldn't reproduce the error from the real data I was working with.\n\n(And apologies for poorly formed markdown above - first time posting  :-)\n",
      "why don't you create a test. Pull the header and 2 rows from each section (then limit the number of fields). Then try this using nrows to skip. If this is a bug, would need to create a reproducible example.\n",
      "thanks - but I'm unclear on your request - that is, I thought I did what you asked already.\n\nI created a reproducible example with the code at the bottom of my post - admittedly with iPython rather than a straight python file.\n\nI'm trying to extract the first section (rows 1-1442 of a 3366 row file) - this is where my problem occurs.\n\nWas my code example unclear?\n\nFor reproducibility purposes, the bulk of the code deals with downloading a zip file,  but the test is in the five lines from 'with thezipfile.open(fname) as csvFile:' onwards\n\nI'm expecting it to be a subtle bug (or I'm doing something very wrong) - nrows parameter clearly works on the various examples I threw at it that were much larger in row length.\n\nBut at the same time, these electricity market files are well formed CSV files (they are part of the market data process in a live electricity market where auctions are run every 5 minutes for the past 15 years)  - and pandas is failing to parse the files I used in developing the code.\n",
      "no, what I mean is we need an example that you can simply copy and paste (and not use an external URL).\n",
      "thanks - you've given me a thought that I can test it by just breaking the relevant part of the CSV file out.\n\nBut if it turns out to be related to the file structure itself - not sure how to provide a test without a link to a sample file.  Would creating a github repo with some sample csv files and a few lines of code be suitable as the test?\n",
      "see what you come up with. this is either a bug, which can be reproduced via a generated type of test file (e.g. create a specific structure), a problem with the csv, or incorrect use of the options. \n\nWe need a self-contained test in order to narrow down the problem.\n\nlmk what you find.\n",
      "Thanks.\n\nIf the file is only the relevant section (or rows to skip at the front) - no error.\n\nImplies not a problem with use of options too I think.\n\nIf the file structure includes the very next line (no. 1443) - the 130 field header for the next section - it fails with any nrows>823.\n\nI also experimented with deleting arbitrary number of rows (but small number) at the end of the section before the next header row - to see if the issue related to that particular line ending.  Again fail.\n\nI'm not sure I can create a test file - other than the sample files I've been experimenting with.\n\nI'll go and figure out how to make a github repo and perhaps we can take it from there.\n\nFor info - the full error at the fail point is:\n\n\/Users\/ChristopherShort\/anaconda\/lib\/python3.4\/site-packages\/pandas\/io\/parsers.py in read(self, nrows)\n   1128 \n   1129         try:\n-> 1130             data = self._reader.read(nrows)\n   1131         except StopIteration:\n   1132             if nrows is None:\n\n\/Users\/ChristopherShort\/anaconda\/lib\/python3.4\/site-packages\/pandas\/parser.so in pandas.parser.TextReader.read (pandas\/parser.c:7146)()\n\n\/Users\/ChristopherShort\/anaconda\/lib\/python3.4\/site-packages\/pandas\/parser.so in pandas.parser.TextReader._read_low_memory (pandas\/parser.c:7547)()\n\n\/Users\/ChristopherShort\/anaconda\/lib\/python3.4\/site-packages\/pandas\/parser.so in pandas.parser.TextReader._read_rows (pandas\/parser.c:7979)()\n\n\/Users\/ChristopherShort\/anaconda\/lib\/python3.4\/site-packages\/pandas\/parser.so in pandas.parser.TextReader._tokenize_rows (pandas\/parser.c:7853)()\n\n\/Users\/ChristopherShort\/anaconda\/lib\/python3.4\/site-packages\/pandas\/parser.so in pandas.parser.raise_parser_error (pandas\/parser.c:19604)()\n\nCParserError: Error tokenizing data. C error: Expected 120 fields in line 1443, saw 130\n",
      "how about this for a test.  \n\nCreate two CSV files ( 1442 rows by 120 cols and 5 rows by 130 cols)\nConcatenate them\nRead the joined CSV file back into a dataframe with nrows option <= 1442\n\nIt fails in the same way - though the nrows parameter was much larger before failure occurred relative to the examples above (where the files contained more strings)\n\nIn the example below - it fails for nrow>1360.  Works fine for lower values.\n\n```\npd.DataFrame(np.random.uniform(size=1442*120).reshape(1442,120)).to_csv('test120.csv',index=False)\n\npd.DataFrame(np.random.uniform(size=5*130).reshape(5, 130)).to_csv('test130.csv',index=False)\n\n\nfilenames = ['test120.csv', 'test130.csv']\nwith open('testNrows.csv', 'w') as outfile:\n    for fname in filenames:\n        with open(fname) as infile:\n            for line in infile:\n                outfile.write(line)\n\ndf = pd.read_csv('testNrows.csv', nrows=1361)\n\n```\n",
      "ok great, must be a bug somewhere\n",
      "I have some time now to try and look at this bug, but not much experience.\n\nDo you have any recommendations on things I should know first?\n",
      "well it's going to be in parser.pyx\n\nIMHO not so easy to debug cython\n\nI would start by putting print statements to figure out what it is doing on this file\n",
      "OK - thanks \n",
      "This is a small update (and to see if any thoughts occur to you).\n\nBefore I went to look at parser.py, I tried to generalise the test file above in order to explore row\/column variations to see if there was a boundary to the error.\n\nI didn't get far in exploring row parameters before realising the error appears to occur randomly. \n\nIn the code below, it loops over the 'test' 3 times, printing out the number of rows in the failed example, as well as the memory size of the dataframe in the failed run.\n\nDifferent number of errors across different runs (I've seen one where there was no error at all).\n\nThe dataframe memory size doesn't appear relevant - when i printed it for all tests, bigger ones passed, smaller ones failed, nothing obvious to look for.\n\nAnd indicating that it's not a memory thing - a typo that set the number of columns to 12, instead of 120, got that error each and every time read_csv was called.\n\nI'll go look at parser.py to see where I could put some print statements - but, as you say, probably in cython call somewhere - and I'm an economist (not a programer) who last used C sparingly 20 years ago.\n\n``` python\ndef test_RowCount (size_1=(1442,120), rowCount=1361):   #original parameters where failure occurred\n\n    df_1 = pd.DataFrame (np.random.uniform(size=size_1))\n    df_1.to_csv('test120.csv',index=False)\n\n    #create file of combined csv file ('testNrows.csv') of different record lengths\n    filenames = ['test120.csv', 'test130.csv']\n    with open('testNrows.csv', 'w') as outfile:\n        for fname in filenames:\n            with open(fname) as infile:\n                for line in infile:\n                    outfile.write(line)\n\n\n    try:\n        df = pd.read_csv('testNrows.csv', header=0, nrows=rowCount)\n\n    except (pd.parser.CParserError) as error:\n        print (error)\n        print ('Rows: ', size_1[0])\n        print ('Memory (MB): ', df_1.memory_usage(index=True).sum()\/1024\/1024, '\\n')\n\n#    except:\n#        print (\"Unexpected error: \", sys.exc_info()[0])\n\n\n\n### Write out 1 file of different record length for later use in test_RowCount function \nsize_2 = (1,130)\ndf_2 = pd.DataFrame(np.random.uniform(size=size_2))\ndf_2.to_csv('test130.csv',index=False)       \n\n\n### Loop for testing various row counts and record lengths\nfor j in range(3):\n    print ('Run ', j)\n    for i in range(1442, 1361, -1): \n        #print (i)\n        test_RowCount(size_1=(i,120), rowCount=1360)\n```\n",
      "@jreback This is a real problem.  It's still present in 0.19.  It can be worked around by `engine='python'` but this is not a real solution.  Stack Overflow has now discovered this problem at least twice:\r\n\r\n1. http:\/\/stackoverflow.com\/questions\/25985817\/why-is-pandas-read-csv-not-reading-the-right-number-of-rows\r\n2. http:\/\/stackoverflow.com\/questions\/37040634\/pandas-read-csv-with-engine-c-issue-bug-or-feature\r\n",
      "well if u have a reproducible example pls show it",
      "@jreback OK, the input file is 516 KB.  Where would you like me to put it?  I tried removing \"unnecessary\" rows from it but this bug doesn't reproduce if you shrink the file a lot.",
      "best to put this up on a separate repo or gist, and use a URL to access.",
      "@jreback I have uploaded a file which reproduces this error: https:\/\/gist.githubusercontent.com\/jzwinck\/838882fbc07f7c3a53992696ef364f66\r\n\r\nSimply download that file and run this:\r\n\r\n    import pandas as pd\r\n    pd.read_table('pandas_issue_7626.txt', skiprows=2195, nrows=100)\r\n\r\nIt fails, saying:\r\n\r\n    File \"pandas\/parser.pyx\", line 846, in pandas.parser.TextReader.read (pandas\/parser.c:9884)\r\n    File \"pandas\/parser.pyx\", line 880, in pandas.parser.TextReader._read_low_memory (pandas\/parser.c:10347)\r\n    File \"pandas\/parser.pyx\", line 922, in pandas.parser.TextReader._read_rows (pandas\/parser.c:10870)\r\n    File \"pandas\/parser.pyx\", line 909, in pandas.parser.TextReader._tokenize_rows (pandas\/parser.c:10741)\r\n    File \"pandas\/parser.pyx\", line 2018, in pandas.parser.raise_parser_error (pandas\/parser.c:25878)\r\n    pandas.io.common.CParserError: Error tokenizing data. C error: Expected 6 fields in line 2355, saw 14\r\n\r\nSince we told Pandas to read from line 2195 for 100 rows, it should never have seen line 2355."
    ],
    "events":[
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":21,
    "deletions":5
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":14861,
    "reporter":"jreback",
    "created_at":1481506665000,
    "closed_at":1481541375000,
    "resolver":"gr8h",
    "resolved_in":"96b171a6593fdab6b4b20157bf4d2e8bd72c5fb2",
    "resolver_commit_num":0,
    "title":"DOC: groupby.rst examples broken",
    "body":"the df for the groupby.resample examples are hiding the correct frame that was previously being used by the docs below \r\n\r\n-docs.github.io\/pandas-docs-travis\/groupby.html#dispatching-to-instance-methods",
    "labels":[
      "Difficulty Novice",
      "Docs",
      "Groupby"
    ],
    "comments":[
      "Can i contribute to this?\r\n\r\nShould i use `dff` instead `df` since `dff = pd.DataFrame({'A': np.arange(8), 'B': list('aabbbbcc')})` and we need to groupby **A**?",
      "I have renamed _resample()_ `df` to `df_re` under **New syntax to window and resample operations** section. [pull request](https:\/\/github.com\/pandas-dev\/pandas\/pull\/14863)\r\n\r\n"
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented"
    ],
    "changed_files":1,
    "additions":13,
    "deletions":13
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":14778,
    "reporter":"erikcs",
    "created_at":1480548675000,
    "closed_at":1481671835000,
    "resolver":"erikcs",
    "resolved_in":"86233e15193c3bcd0f646915891ca6c7892335d9",
    "resolver_commit_num":0,
    "title":"ENH\/DOC: wide_to_long performance and docstring clarification",
    "body":"I had to massage some messy data recently, and a big bottleneck turned out to be [wide_to_long](-dev\/pandas\/blob\/4814823903b862c411caf527271e384df0d0d7e7\/pandas\/core\/reshape.py#L878)\r\n\r\n\r\nSome test data with many id variables and time variables\r\n\r\n\r\n\r\nReshaping with `wide_to_long` takes around 2 secs.\r\n\r\n\r\n\r\nI modified `wide_to_long` slightly (regex on categorical column \/ avoid copying many \"idvariables\", postpone type coercion) and the runtime is now \r\n\r\n\r\n\r\nThe result is the same\r\n\r\n\r\n\r\n#### Docstring clarification\r\n\r\nThe `wide_to_long` docstring also contains an unused last parameter `stubend : str`, which should be removed. \r\n\r\nA docstring `Note` addtion about escaping special characters (with for example `re.escape`) in `stubnames` could also perhaps be informative, since if the user passes a dataframe with messy stubnames, the function fails with a pretty uninformative error message. \r\n\r\nI can send a PR for this if that is wanted?\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: 06f26b51e97a0e81e8bd7fca4bba18e57659d963\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.1.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.0+124.g06f26b5.dirty\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.24.1\r\nnumpy: 1.11.1\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.4.6\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.6.1\r\nblosc: None\r\nbottleneck: 1.1.0\r\ntables: 3.2.3.1\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.3\r\nopenpyxl: 2.3.2\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.3\r\nlxml: 3.6.4\r\nbs4: 4.5.1\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: 1.0.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.42.0\r\npandas_datareader: None\r\n\r\n<\/details>\r\n",
    "labels":[
      "Docs",
      "Performance",
      "Reshaping"
    ],
    "comments":[
      "@nuffe sure that would be great! (also you could add your example in the docs if that is useful).",
      "Thanks, sorry for the premature PR, it turned out I had some edges cases that broke the routine. I will get back to this and fix this later. Sorry. "
    ],
    "events":[
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":5,
    "additions":402,
    "deletions":39
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12651,
    "reporter":"jreback",
    "created_at":1458182525000,
    "closed_at":1481727261000,
    "resolver":"adrian-stepien",
    "resolved_in":"43928d49171750c8827f1c6e02c416c0f50fdbeb",
    "resolver_commit_num":0,
    "title":"DOC: improve cross-links in docs between .expanding and .cum*",
    "body":"xref #12648 \n- cross-link doc-strings for .cum\\* to the .expanding().\\* (for sum,prod,max,min, which are only supported).\n- show an example (in computation.rst wher eexpanding is shown? or basics.rst (where cum\\* are shown)) how these are related (w.r.t. NaN filling).\n\n\n",
    "labels":[
      "Docs",
      "Numeric",
      "Algos",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "@jreback I looked into this but am confused as to how to implement the cross-linking in the docstrings. The expanding function for dataframes and series don't have a doc page for .sum, .product, etc. As I understand it this is just chaining the .expanding method with the .sum method, so there shouldn't be a separate doc page right?\n\nAs an alternative we could just cross-link .expanding to all of the .cum\\* and vice-versa. But I'm not sure if that's what you were going for. \n",
      "just show an example as indicated above\n"
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "cross-referenced",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "milestoned",
      "demilestoned",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files":3,
    "additions":52,
    "deletions":17
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":12766,
    "reporter":"denfromufa",
    "created_at":1459525532000,
    "closed_at":1481930760000,
    "resolver":"nateyoder",
    "resolved_in":"6f4e36a0f8c3638fe5dfe7bf68af079a2f034d00",
    "resolver_commit_num":0,
    "title":"API: Index.map should return Index rather than array",
    "body":"\n\n\n\n\n\n\n\nI think `map` should accept `inplace`\n\n\n",
    "labels":[
      "API Design",
      "Compat",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "`.map` return an array, not an `Index`. So if that is fixed then this would work. \n\n@denfromufa pr's welcome.\n",
      "`inplace` is not acceptable for an immutable object.\n",
      "Not sure if it's that easy, but seemed like a good start to contribute something.\nPutting in a PR, I just turn it into `Index` before returning?\n",
      "Go for it! :)\n\nYes it should be that simple!\n\nOn Monday, April 4, 2016, Joerg Rings notifications@github.com wrote:\n\n> Not sure if it's that easy, but seemed like a good start to contribute\n> something.\n> Putting in a PR, I just turn it into Index before returning?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https:\/\/github.com\/pydata\/pandas\/issues\/12766#issuecomment-205583011\n"
    ],
    "events":[
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "renamed",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files":13,
    "additions":188,
    "deletions":61
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":14872,
    "reporter":"brendene",
    "created_at":1481600180000,
    "closed_at":1482090148000,
    "resolver":"opensourceworkAR",
    "resolved_in":"f3c5a427cc632e89cc5d2cb0df5b5e875cb6e23b",
    "resolver_commit_num":0,
    "title":"fillna() containing timezone aware datetime64 rounded",
    "body":"#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n\r\n\r\n#### Problem description\r\n\r\nThis is similar to bug #6587 but difference is a timezone aware datetime.  It looks like the dtype is being stripped and the value is converted to a float and back.\r\n\r\n#### Expected Output\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\npandas: 0.19.1\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\npytz: 2016.7\r\n<\/details>\r\n",
    "labels":[
      "Bug",
      "Missing-data",
      "Timeseries",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "yep, these prob just need a ``is_datetime64tz_dtype`` in the same path (see #6587 for where to add).\r\n\r\nPR's welcome!"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files":4,
    "additions":28,
    "deletions":10
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":14908,
    "reporter":"pbreach",
    "created_at":1482089438000,
    "closed_at":1482179421000,
    "resolver":"pbreach",
    "resolved_in":"8e630b63010ffdb4ed3c9b47309dc237f8141fdc",
    "resolver_commit_num":0,
    "title":"List required for `percentiles` kwarg in `DataFrame.describe` when median is not present as opposed to array-like",
    "body":"#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nIn the documentation the kwarg `percentiles` is expecting an array-like input, however when passing in a numpy array, an attribute error is thrown as if it were expecting a list. If a list is being expected in the case that the median is not found should there be an explicit conversion to `list` before the median is appended?\r\n\r\n#### Expected Output\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nIn [7]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.19.1\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 23.0.0\r\nCython: 0.24\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: 0.8.2\r\nIPython: 4.2.0\r\nsphinx: 1.3.1\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.4\r\nblosc: None\r\nbottleneck: 1.1.0\r\ntables: 3.2.2\r\nnumexpr: 2.6.0\r\nmatplotlib: 1.5.3\r\nopenpyxl: 2.3.2\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.2\r\nlxml: 3.6.0\r\nbs4: 4.4.1\r\nhtml5lib: 0.999\r\n None\r\napiclient: None\r\nsqlalchemy: 1.0.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.40.0\r\npandas_datareader: None\r\n\r\n<\/details>\r\n",
    "labels":[
      "Bug",
      "Dtypes",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "this is a bug. should have a ``list()`` conversion first.\r\n\r\npull-requests are welcome."
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files":3,
    "additions":11,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":14826,
    "reporter":"ischurov",
    "created_at":1481190513000,
    "closed_at":1482255892000,
    "resolver":"ischurov",
    "resolved_in":"50930a9879b580ab4f30d8b741229391e41afa76",
    "resolver_commit_num":0,
    "title":"Inconsistent behavior of DatetimeIndex Partial String Indexing on Series and DataFrames",
    "body":"This bugreport is related to [this](-is-pandas-not-returing-a-scalar-string-instead-of-a-series-when-accessing-a) SO question and the discussion there.\r\n\r\n#### Summary\r\nI believe that current [DatetimeIndex Partial String Indexing](-docs\/stable\/timeseries.html#datetimeindex-partial-string-indexing) behavior is either inconsistent or underdocumented as the result depends nontrivially on whether we are working with `Series` or `DataFrame` and whether `DateTimeIndex` is periodic or not.\r\n\r\n#### `Series` vs. `DataFrame`\r\n\r\n\r\n\r\n\r\n\r\nHere we see that the behaviour depends on what we are indexing: `Series` returns scalar while `DataFrame` raises an exception. This exception is consistent with the documentation notice:\r\n> **Warning** The following selection will raise a KeyError; otherwise this selection methodology would be inconsistent with other selection methods in pandas (as this is not a slice, nor does it resolve to one)\r\n\r\nWhy we do not get the same exception for `Series` object?\r\n\r\n#### Periodic vs. Non-periodic\r\n\r\nIn contrast with the previous example, we get an instance of `Series` here, so the same timestamp is considered as a slice, not index. Why it depends in such a way on periodicity of the index?\r\n\r\n\r\nNo exceptions here, in contrast with periodic case.\r\n\r\nIs it intended behavior? If yes, I believe that this should be clearly documented and rationale provided.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.1.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.0+157.g2466ecb\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.1\r\nnumpy: 1.11.2\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: None\r\npandas_datareader: None\r\n<\/details>\r\n",
    "labels":[
      "Bug",
      "Difficulty Advanced",
      "Effort Low",
      "Indexing",
      "Timeseries"
    ],
    "comments":[
      "The first \"series vs dataframe\" issue is as expected \/ follows from the second issue. `__getitem__`\/`[]` indexing does not work exactly the same for series vs dataframe (http:\/\/pandas.pydata.org\/pandas-docs\/stable\/indexing.html#basics). When provided a single key, Series will get a single value, while dataframe will try to get a column. And there is no column named \"2016-12-07 09:00:00\", hence the KeyError. \r\nSo this behaviour follows from the fact that in your first example, \"2016-12-07 09:00:00\" is not interpreted as a slice, but as a single key. Given that, the behaviour is as expected.\r\n\r\nBut, you are correct there might be an inconsistency in determining whether the string is a single key or a slice between the regular and irregular datetimeindex. The problem is that it is very difficult for pandas to guess\/determine this when the index has no frequency.  ",
      "For the first part, I completely agree. I forgot that `[]` for dataframe select columns if argument is not a slice (shame on me), this explains the behavior nicely. (And now I understand the rationale behind the Warning in the docs I quoted.)\r\n\r\nFor the second part, is it true that for irregular indexes any string is considered to be slice, and for regular ones only those strings that provide date-time specification with precision less then frequency is considered to be slice?\r\n\r\nWhy it is not possible to consider string a key (not a slice) if it is casted to date-time that is exactly the same as one of keys in the index for irregular indexes?",
      "> And now I understand the rationale behind the Warning in the docs I quoted.\r\n\r\nIf you have ideas to rephrase this to make it clearer, very welcome!\r\n\r\n> Why it is not possible to consider string a key (not a slice) if it is casted to date-time that is exactly the same as one of keys in the index for irregular indexes?\r\n\r\nI am not exactly sure how it is implemented in the code, but imagine the following case: you have a timeseries with index [\"2016-01-01 00:00\", \"2016-01-01 12:00\", \"2016-01-01 23:00\", \"2016-01-02 05:00\", \"2016-01-02 18:00\"] (some irregular hours over two days). \r\nIf you would index this with the key \"2016-01-01\", this can be interpreted as \"give me all the data of the 1st of January\", so in this case: a slice (which is what pandas does). But if you would also want to check if the provided key exactly matches with one of the labels of the index, pandas parses this key -> `pd.Timestamp(\"2016-01-01\")`, which gives you \"2016-01-01 00:00:00\" (because pandas does not have different resolution in its datetime data type) and this would match with one of the elements (so: not a slice). So which one of the two would the user want? \r\n(to put it in other words, pandas cannot make a distinction between `s[\"2016-01-01\"]` (logically a slice) and `s[\"2016-01-01 00:00\"]` (logically a single element))",
      "Yes, I'm going to improve the docs according to our discussion after I understand all the details.\r\n\r\nYour explanation sounds reasonable, but I cannot get why all these arguments do not apply to the case of regular index? In fact, *pandas* can detect the resolution of a string-represented timestamp, this is done in function `pandas.tseries.tools.parse_time_string()` and it allows `pandas` to distinct `s[\"2016-01-01\"]` and `s[\"2016-01-01 00:00\"]` in case `s` is indexed with regular index:\r\n\r\n```python\r\nseries = pd.Series([1, 2, 3], pd.DatetimeIndex(['2016-12-07 00:00:00',\r\n                                                '2016-12-07 01:00:00',\r\n                                                '2016-12-07 02:00:00']))\r\nprint(series[\"2016-12-07 00:00:00\"])\r\n# 1\r\nprint(series[\"2016-12-07 00:00\"])\r\n# 1\r\nprint(series[\"2016-12-07\"])\r\n# 2016-12-07 00:00:00    1\r\n# 2016-12-07 01:00:00    2\r\n# 2016-12-07 02:00:00    3\r\n# dtype: int64\r\n```",
      "Finally, it seems that I got it. The code I'm interested in is [the following](https:\/\/github.com\/pandas-dev\/pandas\/blob\/master\/pandas\/tseries\/index.py#L1296):\r\n\r\n```python\r\n    def _partial_date_slice(self, reso, parsed, use_lhs=True, use_rhs=True):\r\n        is_monotonic = self.is_monotonic\r\n        if ((reso in ['day', 'hour', 'minute'] and\r\n             not (self._resolution < Resolution.get_reso(reso) or\r\n                  not is_monotonic)) or\r\n            (reso == 'second' and\r\n             not (self._resolution <= Resolution.RESO_SEC or\r\n                  not is_monotonic))):\r\n            # These resolution\/monotonicity validations came from GH3931,\r\n            # GH3452 and GH2369.\r\n            raise KeyError\r\n```\r\n\r\nraising `KeyError` here means that the timestamp cannot be coerced to a slice. The condition basically says that if the resolution of the timestamp (that we remember from the string) is less precise than the resolution of the index, it is slice, otherwise it is not. That sounds very reasonable.\r\n\r\nI'm not sure yet, why `second` resolution uses different condition (non-strict inequality instead of strict one), but basically I understand what's going on here.\r\n\r\nI'll try to improve the docs soon and prepare PR that will refer to this issue.",
      "Btw, could anybody tell, why `second` resolution is treated in such a different way here? I tried to figure it out myself (looking at PRs mentioned near the code), but didn't succeed.",
      "I finally discovered that this was PR #3931\r\n\r\n@jreback, could you please comment on this? Why do we introduce the inconsistence like this:\r\n\r\n```python\r\nseries = pd.Series([1, 2, 3, 4], pd.DatetimeIndex(['2016-12-06 23:59:00',\r\n                                                   '2016-12-07 01:00:00',\r\n                                                   '2016-12-07 01:01:00',\r\n                                                   '2016-12-07 01:02:01']))\r\n\r\nprint(type(series[\"2016-12-07 01:01:00\"]))\r\n# <class 'pandas.core.series.Series'>\r\n\r\nseries = pd.Series([1, 2, 3, 4], pd.DatetimeIndex(['2016-12-07',\r\n                                                   '2016-12-08',\r\n                                                   '2016-12-09',\r\n                                                   '2016-12-10']))\r\nprint(type(series[\"2016-12-07\"]))\r\n\r\n# <class 'numpy.int64'>\r\n```\r\nWhy `second` frequency is treated in different way?",
      "@ischurov Thanks for digging in! So indeed, it had in the end nothing to do with the irregular\/regular index (only the resolution of the index is different due to the irregularity, and this impact how the slice is determined), but the different treatment of second resolution or higher resolutions.\r\n\r\nI am not sure why this was added differently as the other resolution, and this seems rather inconsistent to me.\r\n\r\nBy the way, apart from clarification in the docs, some comprehensive tests looping over some combinations of different resolutions is also welcome",
      "if you look at the PR, DataFrames need to have a slice here (and not a single indexer). So I think this could fix the inconsistent case you enumerate above (e.g. seconds is an exact match in which case you raise ``KeyError``; this may be counter intuitive in *how* to do it, but it goes to another path that converts to a timestamp and just looks for an exact match, if its there it returns the value, otherwise raises).\r\n\r\nSo obviously this is not tested on series.\r\n\r\nBut I think you'd have to introduce some logic  to actually return a slice when selecting for a DataFrame, which in this case IS a slice.\r\n\r\nThe other resolutions, day, hour, minute, cannot by definition ever have an exact match directly (because they always have a seconds component attached which you don't know). However seconds is special in that it *could* fully represented and actually be an exact match.\r\n",
      "> if you look at the PR, DataFrames need to have a slice here (and not a single indexer).\r\n\r\nWhy is that? \r\nThe example you give at the top of that PR is:\r\n\r\n```\r\nIn [11]: df = DataFrame(randn(5,5),columns=['open','high','low','close','volume'],index=date_range('2012-01-02 18:01:00',periods=5,tz='US\/Central',freq='s'))\r\n\r\nIn [12]: df\r\nOut[12]: \r\n                               open      high       low     close    volume\r\n2012-01-02 18:01:00-06:00  0.131243  0.301542  0.128027  0.804162  1.296658\r\n2012-01-02 18:01:01-06:00  0.341487  1.548695  0.703234  0.904201  1.422337\r\n2012-01-02 18:01:02-06:00 -1.050453 -1.884035  1.537788 -0.821058  0.558631\r\n2012-01-02 18:01:03-06:00  0.846885  1.045378 -0.722903 -0.613625 -0.476531\r\n2012-01-02 18:01:04-06:00  1.186823 -0.018299 -0.513886 -1.103269 -0.311907\r\n\r\nIn [14]: df['2012-01-02 18:01:02']\r\nOut[14]: \r\n                               open      high       low     close    volume\r\n2012-01-02 18:01:02-06:00 -1.050453 -1.884035  1.537788 -0.821058  0.558631\r\n```\r\n\r\nSo AFAIK the PR made possible to slice the above dataframe with that string index. However, I would argue that in this case this is no slice at all, but a single key (as both the indexer key as the index is of second resolution, so the result of such a string key will always be slice of length 1 ?)\r\n",
      "for a dataframe by definition is IS a slice always as it cannot be an exact match (wrong axis for exact matching); it can only ever be a slice\r\n\r\nwhile for a series both are possible",
      "As the issue is marked as *bug*, may I ask, what is the desired behavior?\r\n\r\nActually, I believe this is not `Series` issue, it's `DataFrame` issue as well. I believe the following logic is consistent:\r\n\r\n- If timestamp resolution is strictly greater (less precise) than index resolution, timetamp is a slice as it can (in theory) correspond to more than one elements in the index. For `Series`, `[]` should return `Series`, for `DataFrame` \u2014 `DataFrame`.\r\n- If timestamp resolution is equal to index resolution, then timestamp is considered as an attempt to get a kind of \"exact match\". For `Series`, `[]` should return scalar, for `DataFrame` \u2014 try to find column with this key (if any), and most probably raise `KeyError`.\r\n- If timestamp resolution is strictly less than index resolution, `KeyError` have to be raised in both cases.\r\n\r\nOne can argue that if the resolution is greater than `second`, no exact match possible. However, I believe that it's an implementation detail \u2014 how timestamps are presented internally \u2014 and from user's point of view if the resolution of index is e.g. hour, then \"2016-01-01 01\" is in fact exact match.",
      "@ischurov so can you show some short test cases that replicate the logic you have presented (and show what is changing from current).",
      "@jreback see PR #14856. I added several [tests](https:\/\/github.com\/pandas-dev\/pandas\/pull\/14856\/files#diff-db69b090c3ef26a0c2111e1c0a5909e3) to check partial string indexing with respect to the logic stated above. With current code, only `test_partial_slice_second` fails. The proposed solution is as simple as [this](https:\/\/github.com\/pandas-dev\/pandas\/pull\/14856\/files#diff-23ecb29e7ceba52109a365e447400d2e).",
      "@jreback Here is a super short summary of what's changed:\r\n\r\nLet\r\n```python\r\ndf = DataFrame({'a': [1, 2, 3]},\r\n                       DatetimeIndex(['2011-12-31 23:59:59',\r\n                                      '2012-01-01 00:00:00',\r\n                                      '2012-01-01 00:00:01']),\r\n                       dtype=np.int64)\r\n```\r\nThen `df['a']['2011-12-31 23:59:59']` should return `np.int64` object `1` (now returns `Series`) and `df['2011-12-31 23:59:59']` should raise `KeyError` (now return `DataFrame`).",
      "I reported the behaviour on StackOverflow (and @ischurov raised it here), so I believe I am not influenced by Pandas design decision\/culture. As a new comer, I expected the indexing\/selection to return a consistent datatype (using the example in the response above):\r\n\r\n- if the index value (eg: `'2012-01-01 00:00:00'`) has the same resolution as the values in the DataFrame's index and the index value has an exact match then return a single scalar value.\r\n- Otherwise, return a Series containing all scalar values that can match the index value's resolution. This means that if the index value is has a higher resolution than the index value then a empty series is returned.\r\n\r\nMy suggestion differs from @ischurov's [proposition](https:\/\/github.com\/pandas-dev\/pandas\/issues\/14826#issuecomment-266037484) on KeyError. IMHO, if KeyError is too be used then I would expect it to be raised when the index value is not the same resolution as the DataFrame's Index resolution.\r\n",
      "> if KeyError is too be used then I would expect it to be raised when the index value is not the same resolution as the DataFrame's Index resolution.\r\n\r\nI don't really understand this comment. If the resolution of the indexer is lower than of the Index, you get a slice, if it is higher, you get a KeyError. So KeyError is already used in certain cases where the resolutions differ.\r\n\r\n"
    ],
    "events":[
      "renamed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":4,
    "additions":173,
    "deletions":28
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":14922,
    "reporter":"uweschmitt",
    "created_at":1482161834000,
    "closed_at":1482527054000,
    "resolver":"uweschmitt",
    "resolved_in":"6bea8275e504a594ac4fee71b5c941fb520c8b1a",
    "resolver_commit_num":0,
    "title":"BUG: sorting with large float and multiple columns incorrect",
    "body":"#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n\r\nproduces\r\n\r\n\r\n\r\nAs you can see the first two calls (only sorting by `int` or by `float`) sort correctly whereas `df.sort_values([\"int\", \"float\"])`  does not seem to sort at all. Row with index `1` should appear first, and then row with index `0`.\r\n\r\nUsing `pandas 0.17.0` I get the  correct result:\r\n\r\n\r\n\r\n####\r\n\r\nThis issue was discussed at -by-broken-in-pandas-0-18-0\r\n    \r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS                                                                                                                                                         [1\/193]\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.1\r\nnose: None\r\npip: 7.1.2\r\nsetuptools: 18.2\r\nCython: None\r\nnumpy: 1.10.4\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: None\r\npandas_datareader: None\r\n<\/details>\r\n",
    "labels":[
      "Bug",
      "Difficulty Intermediate",
      "Effort Low",
      "Reshaping"
    ],
    "comments":[
      "pls show a copy-pastable example.",
      "yes, pls update the top message. idea is that the repro is just a copy-paste",
      "also pls try to make a more useful than 'not working'  (e.g. what is the problem)",
      "Ok now ? I was in a hurry...",
      "ok, confirmed on master. ty. if you'd like to digin and see what's happening would be great."
    ],
    "events":[
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":50,
    "deletions":2
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":7751,
    "reporter":"yonil7",
    "created_at":1405363674000,
    "closed_at":1483124521000,
    "resolver":"ashishsingal1",
    "resolved_in":"8051d61223b10002a7b4a4f66373e7cfeb976095",
    "resolver_commit_num":0,
    "title":"qcut() should make sure the bins bounderies are unique before passing them to _bins_to_cuts",
    "body":"xref #8309\n\nfor example:\n\n\n\nwill raise \"ValueError: Bin edges must be unique: array([ 1.,  1.])\" exception\n\nFix suggestion - add one new line:\n\n\n",
    "labels":[
      "Error Reporting",
      "Reshaping",
      "Good as first PR"
    ],
    "comments":[
      "why would you not just catch the `ValueError` ?  better to be informed of the issue, right?\n",
      "I think this should not throw exception as the is legitimate use.\nmaybe a better example:\n\n``` python\npd.qcut([1,2,3,4,4], [0, .25, .5, .75, 1.])\n```\n\nalso throw: ValueError: Bin edges must be unique: array([1, 2, 3, 4, 4], dtype=int64)\n",
      "## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.5.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\n\npandas: 0.14.0\nnose: 1.3.0\nCython: 0.19.1\nnumpy: 1.8.1\nscipy: 0.14.0\nstatsmodels: 0.5.0\nIPython: 1.0.0\nsphinx: 1.1.3\npatsy: 0.2.1\nscikits.timeseries: None\ndateutil: 2.2\npytz: 2014.4\nbottleneck: 0.8.0\ntables: 3.1.1\nnumexpr: 2.3.1\nmatplotlib: 1.3.1\nopenpyxl: 1.8.5\nxlrd: 0.9.3\nxlwt: 0.7.5\nxlsxwriter: 0.5.5\nlxml: 3.3.5\nbs4: 4.3.1\nhtml5lib: None\nbq: None\napiclient: None\nrpy2: None\nsqlalchemy: 0.9.6\npymysql: None\npsycopg2: None\n",
      "maybe you misunderstand me. This is a user error (in that you generally pass unique bins, and you CAN make them unique before passing them in). Yes it could be done in the function, but wouldn't you as a user want to know that you are passing non-unique bins?\n",
      "The issue is regarding the **pd.qcut()** function (not pd.cut()). It should get array of samples and quantiles\/ranks.\nIn the prev example, I passed array of samples [1,2,3,4,4] and would like it to calculate for each sample in that array to which of the 4 quantiles it belongs. (quantile 1,2,3 or 4)\n",
      "its the same issue. If you uniqfy in one, you would in the other. That's not the question though. _why_ should this happen silently? (automatically). maybe I am missing something.\n\n@jseabold ?\n",
      "I would expect this to raise an informative error to the user. \"The quantiles you selected do not result in unique bins. Try selecting fewer quantiles.\" I.e., if your 25th quantile is the same as your median, then you should be told this and you shouldn't ask for both. Trying to get fancy under the hood is asking for confusion IMO. \n",
      "@jseabold ok, I can buy that this could produce a better msg.\n\n@yonil7 want to take this on with a pull-request?\n",
      "In addition to just raising a more informative error, I think that there should be an option to automatically handle duplicates in the following way: \n\nSuppose we have a list with too many duplicates, say we want to split [1,2,3,3,3,3,3,3,4,5,6,7] into quartiles. Right now qcut fails, because the second-lowest quartile consists entirely of '3's, duplicating the bin edges. But there is a natural way to assign the quartiles if we allow duplicate values to be split among different bins: [1,2,3], [3,3,3], [3,3,4], [5,6,7]. \n\nNow this is not completely ideal -- the assignment is arbitrary, and there is no way of knowing what to do by looking at the bin edges alone. But in the real world it can be convenient to have an option that 'just works', and I think this warrants a 'split_dup=True' option that is false by default. \n\nWhat do people think? I'll add this if people support it.  \n",
      "@edjoesu start with the error msg. If you want to propose a new issue for discussion that is ok. I think if you show several test cases, some of which you CAN distinguish and some of which have to raise then prob ok, but needs discussion.\n",
      "Is there any work around for this until it's changed?\n",
      "this is not a bug, just a more informative error message.\n",
      "Got it, I thought it was being changed to a warning instead of an error.  I personally would like the option to allow non-unique bins, but I also understand the case against it.\n\nI just commented out those lines in tile.py and it seems to have allowed me to use duplicates.\n",
      "Does anyone own this change? Or has it already been done?\n",
      "See http:\/\/stackoverflow.com\/questions\/20158597\/how-to-qcut-with-non-unique-bin-edges for a discussion about this. \n\n@edjoesu I agree with your proposal, but I foresee that the implementation won't be trivial, since the way the current code assigns values to bins I think some bins would become empty if duplicates are allowed.\n",
      "I think we could add a \"duplicate_edges\" parameter, with the following options:\n- 'raise' (default). Raise an error if duplicate bin edges are found.\n- 'drop'. Delete duplicate edges, adding the `bins = np.unique(bins)` line as in https:\/\/github.com\/pydata\/pandas\/issues\/7751#issue-37814702. This would result in less bins than specified, and some larger (with more elements) than others.\n- 'unique'. Perform the binning based on the unique values present in the input array. This is essentialy the first solution in http:\/\/stackoverflow.com\/questions\/20158597\/how-to-qcut-with-non-unique-bin-edges and would imply unevenly sized bins, without altering the total number of bins.\n\nDoes it look reasonable? What do you think?\n",
      "dukebody's suggestion looks good to me.\n\nFor my use case, I would prefer for qcut to just return the (non-unique) bin list, and let me handle it. \n",
      "hi guys let me jump in and push for that to happen as well.  Solutions already exist in many other languages, its just a matter of choosing one of them.  See also https:\/\/stackoverflow.com\/questions\/38594277\/how-to-compute-quantile-bins-in-pandas-when-there-are-ties-in-the-data\/38596578?noredirect=1#comment64582414_38596578\n",
      "see for instance here for a possible solution adopted in SAS\n\nhttps:\/\/support.sas.com\/documentation\/cdl\/en\/proc\/61895\/HTML\/default\/viewer.htm#a000146840.htm\n",
      "@randomgambit its not about a soln. This is quite straightforward. Its about someone actually implementing it. There are many many many issues. If someone wants to submit a PR that moves things WAY faster.\n",
      "i get it jeff, no worries. unfortunately i dont have the python skills to help you and get the job done. I can only give hints for the good samaritan that wants to improve the current work in this direction\n"
    ],
    "events":[
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "labeled",
      "milestoned",
      "labeled",
      "cross-referenced",
      "cross-referenced",
      "labeled",
      "referenced",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":33,
    "deletions":6
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":14973,
    "reporter":"sadruddin",
    "created_at":1482513128000,
    "closed_at":1483125273000,
    "resolver":"kamal94",
    "resolved_in":"298b241be32365e3e2c8d53fbad098a05de044c9",
    "resolver_commit_num":0,
    "title":"Using the python power operator on numerical Index objects yields unexpected results",
    "body":"#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nUsing the pow '**' operator on an index object results in a behaviour different than when using the '*' operator. The pow result implies that the values being used are not index.values, which one would expect given the behaviour of other operators (*, -, +, \/ ...).\r\nThis problem only arises when the index object is the second argument.\r\n\r\n#### Expected Output\r\nexpected output would be the result of 2.0**index.values\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\ncommit: None\r\npython: 2.7.13.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 42 Stepping 7, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.19.1\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 32.2.0\r\nCython: None\r\nnumpy: 1.12.0rc1\r\nscipy: 0.18.1\r\nstatsmodels: 0.8.0rc1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.4.8\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.3\r\nopenpyxl: 2.4.1\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: None\r\nlxml: 3.7.0\r\nbs4: None\r\nhtml5lib: 0.9999999\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\n\r\n<\/details>\r\n",
    "labels":[
      "Bug",
      "Difficulty Novice",
      "Effort Low",
      "Numeric"
    ],
    "comments":[
      "https:\/\/github.com\/pandas-dev\/pandas\/blob\/master\/pandas\/indexes\/base.py#L3537\r\n\r\n``rpow`` needs to be a separate function (with ``reversed=True``) as this is not a communative operation (unlike mul).\r\n\r\nwant to do a PR?",
      "I can work on this if no one is..?",
      "That would be great! I wouldn't have been able to look at it to try and fix it until the next couple of weeks."
    ],
    "events":[
      "renamed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":13,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":14992,
    "reporter":"gcbeltramini",
    "created_at":1482771571000,
    "closed_at":1483126881000,
    "resolver":"gcbeltramini",
    "resolved_in":"859c80f327cf4ac25b3aaac8bfb05de34bc3ef8e",
    "resolver_commit_num":0,
    "title":"Reindex function applied to DataFrame over columns ignores method",
    "body":"#### Code Sample\r\n\r\n\r\n\r\n#### Problem description\r\n\r\nI want to apply the `reindex` function to the columns using the `\"ffill\"` method, but the holes are filled with NaN. The workaround is to transpose the DataFrame, apply the function and then transpose again.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.1.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\n\r\npandas: 0.18.1\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: None\r\nnumpy: 1.11.1\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.38.0\r\npandas_datareader: None\r\n<\/details>\r\n",
    "labels":[
      "Bug",
      "Difficulty Novice",
      "Effort Low",
      "Missing-data",
      "Reshaping"
    ],
    "comments":[
      "```\r\nIn [3]: df.reindex(columns=range(7)).ffill(axis=1)\r\nOut[3]: \r\n    0     1     2     3     4     5     6\r\n1 NaN  11.0  11.0  12.0  12.0  13.0  13.0\r\n3 NaN  21.0  21.0  22.0  22.0  23.0  23.0\r\n5 NaN  31.0  31.0  32.0  32.0  33.0  33.0\r\n```\r\nis an idiomatic way to do this,\r\n\r\nbut I see that the ``method`` kwarg is not getting propogated to the reindexing methods for columns. Pull requests to fix are welcome!"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":5,
    "additions":121,
    "deletions":9
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":14580,
    "reporter":"navin-k80",
    "created_at":1478207797000,
    "closed_at":1483133723000,
    "resolver":"nathalier",
    "resolved_in":"7dd451d881964d958acbd078e8dba505906b01bf",
    "resolver_commit_num":0,
    "title":"BUG: iloc misbehavior with pd.Series: sometimes returns pd.Categorical instead",
    "body":"#### A small, complete example of the issue\r\n\r\n\r\n\r\n#### Expected Output\r\nBoth should return a pandas Series object\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\npandas version = 0.18\r\n\r\n<\/details>\r\n",
    "labels":[
      "Bug",
      "Indexing",
      "Difficulty Novice",
      "Categorical",
      "Effort Low"
    ],
    "comments":[
      "in 0.19.0\n\n```\nIn [18]: pd.Series([1,2,3]).astype('category').iloc[0:1]\nOut[18]: \n0    1\ndtype: category\nCategories (3, int64): [1, 2, 3]\n\nIn [19]: pd.Series([1,2,3]).astype('category').iloc[np.array([0, 1])]\nOut[19]: \n[1, 2]\nCategories (3, int64): [1, 2, 3]\n```\n\ncould be #12531 (in 0.18.1) or in 0.19.0\n",
      "actually, I stand corrected. This is a different treatment of slices vs list-like\nso this is a bug.\n\n```\nIn [16]: s = pd.Series([1,2,3]).astype('category')\n\nIn [17]: type(s.iloc[[0,1]])\nOut[17]: pandas.core.categorical.Categorical\n\nIn [18]: type(s.iloc[0:1])\nOut[18]: pandas.core.series.Series\n```\n"
    ],
    "events":[
      "renamed",
      "commented",
      "closed",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "reopened",
      "demilestoned",
      "milestoned",
      "labeled",
      "labeled",
      "labeled",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":54,
    "deletions":17
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":15096,
    "reporter":"nkorlin",
    "created_at":1484028551000,
    "closed_at":1484338208000,
    "resolver":"rouzazari",
    "resolved_in":"1a18420d97f1988b8628e438ad83f44bdb618465",
    "resolver_commit_num":0,
    "title":"to_json() line separation broken by backslash in content",
    "body":"#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nI noticed this issue when I saw that a ~8500 row dframe became a ~3400 line file, even though in theory \"lines=True\" should mean that the number of rows and number of lines are equal. I dug into the code a bit, and it turns out that convert_json_to_lines() does not correctly insert newlines if the json contains a backslash before a double quote, even if the backslash itself is escaped.\r\n(-dev\/pandas\/blob\/v0.19.2\/pandas\/lib.pyx#L1114)\r\n\r\nHere are the test files I used with the code sample above, and the outputs I got:\r\n\r\n\r\n#### Expected Output\r\n\r\n\"-out\" files should be identical to originals.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-68-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.1\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: None\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.7\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: 2.43.0\r\npandas_datareader: None\r\n\r\n<\/details>",
    "labels":[
      "Bug",
      "Difficulty Novice",
      "Effort Low",
      "IO JSON",
      "Output-Formatting"
    ],
    "comments":[
      "so there is a PR #14693 which fixes this, though it was closed by its author. would welcome to pick it up."
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files":3,
    "additions":15,
    "deletions":2
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":15155,
    "reporter":"watercrossing",
    "created_at":1484764595000,
    "closed_at":1484857313000,
    "resolver":"watercrossing",
    "resolved_in":"4c65d5fc79ea435bc4e47d8af2914cba324117bd",
    "resolver_commit_num":0,
    "title":"Groupby level fails to enumerate groups",
    "body":"#### Code Sample\r\n\r\n\r\n#### Problem description\r\n\r\ninstead of returning the data as expected, an error is thrown:\r\n\r\n\r\n#### Expected Output\r\n\r\nI guess an alternative way to get this output is `test.loc[\"a\"]` - which yields the expected output below:\r\n\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.10.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.32-431.11.2.el6.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: en_GB.utf8\r\nLANG: en_GB.utf8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 32.3.1\r\nCython: None\r\nnumpy: 1.11.3\r\nscipy: 0.18.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.5.1\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.3\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.9999999\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.4\r\nboto: None\r\npandas_datareader: None\r\n\r\n<\/details>\r\n",
    "labels":[
      "Bug",
      "Categorical",
      "Groupby",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "This is a bug, which is hidden because of this missing method on ``CategoricalIndex``. If you would like to do a PR which adds this (and tests of course).\r\n\r\n```\r\ndiff --git a\/pandas\/indexes\/category.py b\/pandas\/indexes\/category.py\r\nindex 2c89f72..e3ffa40 100644\r\n--- a\/pandas\/indexes\/category.py\r\n+++ b\/pandas\/indexes\/category.py\r\n@@ -255,6 +255,9 @@ class CategoricalIndex(Index, base.PandasDelegate):\r\n     def ordered(self):\r\n         return self._data.ordered\r\n \r\n+    def _reverse_indexer(self):\r\n+        return self._data._reverse_indexer()\r\n+\r\n     def __contains__(self, key):\r\n         hash(key)\r\n         return key in self.values\r\n```"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "referenced"
    ],
    "changed_files":3,
    "additions":24,
    "deletions":0
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":15140,
    "reporter":"Rufflewind",
    "created_at":1484543761000,
    "closed_at":1484915492000,
    "resolver":"Rufflewind",
    "resolved_in":"681e6a9b07271a0955e6780e476ab2d7101e549c",
    "resolver_commit_num":0,
    "title":"Segmentation fault with `read_csv(io.StringIO(\"a\\na\"), float_precision=\"round_trip\")`",
    "body":"#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n\r\nThe input needs to be at least two lines and must contain non-numerical data.\r\n\r\nExperienced this problem on Arch Linux with Python 3.\r\n\r\n#### Problem description\r\n\r\n*Why is the current behaviour a problem?* (1) I can't parse a CSV file containing text with `round_trip` precision (2) Possible security vulnerability (3) It fills up my hard drive with core dumps\r\n\r\n#### Expected Output\r\n\r\nNothing.\r\n\r\n#### Actual Output\r\n\r\n<details>\r\n<pre>\r\n#0  0x00007ffff7440350 in PyErr_Restore () from \/usr\/lib\/libpython3.6m.so.1.0\r\n#1  0x00007ffff7440a62 in PyErr_FormatV () from \/usr\/lib\/libpython3.6m.so.1.0\r\n#2  0x00007ffff7440b24 in PyErr_Format () from \/usr\/lib\/libpython3.6m.so.1.0\r\n#3  0x00007ffff73c2c60 in PyOS_string_to_double ()\r\n   from \/usr\/lib\/libpython3.6m.so.1.0\r\n#4  0x00007fffe5359a73 in ?? ()\r\n   from \/usr\/lib\/python3.6\/site-packages\/pandas\/parser.cpython-36m-x86_64-linux-gnu.so\r\n#5  0x00007fffe53675c6 in ?? ()\r\n   from \/usr\/lib\/python3.6\/site-packages\/pandas\/parser.cpython-36m-x86_64-linux-gnu.so\r\n#6  0x00007fffe535c74a in ?? ()\r\n   from \/usr\/lib\/python3.6\/site-packages\/pandas\/parser.cpython-36m-x86_64-linux-gnu.so\r\n#7  0x00007fffe537db89 in ?? ()\r\n   from \/usr\/lib\/python3.6\/site-packages\/pandas\/parser.cpython-36m-x86_64-linux-gnu.so\r\n#8  0x00007ffff74220b6 in PyCFunction_Call () from \/usr\/lib\/libpython3.6m.so.1.0\r\n#9  0x00007fffe534fe29 in ?? ()\r\n   from \/usr\/lib\/python3.6\/site-packages\/pandas\/parser.cpython-36m-x86_64-linux-gnu.so\r\n#10 0x00007fffe536f78e in ?? ()\r\n   from \/usr\/lib\/python3.6\/site-packages\/pandas\/parser.cpython-36m-x86_64-linux-gnu.so\r\n#11 0x00007fffe5344cf4 in ?? ()\r\n   from \/usr\/lib\/python3.6\/site-packages\/pandas\/parser.cpython-36m-x86_64-linux-gnu.so\r\n#12 0x00007ffff7421ddc in _PyCFunction_FastCallDict ()\r\n   from \/usr\/lib\/libpython3.6m.so.1.0\r\n#13 0x00007ffff740c21f in ?? () from \/usr\/lib\/libpython3.6m.so.1.0\r\n#14 0x00007ffff73cd067 in _PyEval_EvalFrameDefault ()\r\n   from \/usr\/lib\/libpython3.6m.so.1.0\r\n#15 0x00007ffff740ade9 in ?? () from \/usr\/lib\/libpython3.6m.so.1.0\r\n#16 0x00007ffff740bf9a in ?? () from \/usr\/lib\/libpython3.6m.so.1.0\r\n#17 0x00007ffff740c303 in ?? () from \/usr\/lib\/libpython3.6m.so.1.0\r\n#18 0x00007ffff73cd067 in _PyEval_EvalFrameDefault ()\r\n   from \/usr\/lib\/libpython3.6m.so.1.0\r\n#19 0x00007ffff740aaa1 in ?? () from \/usr\/lib\/libpython3.6m.so.1.0\r\n#20 0x00007ffff740bf9a in ?? () from \/usr\/lib\/libpython3.6m.so.1.0\r\n#21 0x00007ffff740c303 in ?? () from \/usr\/lib\/libpython3.6m.so.1.0\r\n#22 0x00007ffff73cd067 in _PyEval_EvalFrameDefault ()\r\n   from \/usr\/lib\/libpython3.6m.so.1.0\r\n#23 0x00007ffff740bd4a in ?? () from \/usr\/lib\/libpython3.6m.so.1.0\r\n#24 0x00007ffff740c303 in ?? () from \/usr\/lib\/libpython3.6m.so.1.0\r\n#25 0x00007ffff73cd067 in _PyEval_EvalFrameDefault ()\r\n   from \/usr\/lib\/libpython3.6m.so.1.0\r\n#26 0x00007ffff740ade9 in ?? () from \/usr\/lib\/libpython3.6m.so.1.0\r\n#27 0x00007ffff740bf9a in ?? () from \/usr\/lib\/libpython3.6m.so.1.0\r\n#28 0x00007ffff740c303 in ?? () from \/usr\/lib\/libpython3.6m.so.1.0\r\n#29 0x00007ffff73cde87 in _PyEval_EvalFrameDefault ()\r\n   from \/usr\/lib\/libpython3.6m.so.1.0\r\n#30 0x00007ffff740c757 in PyEval_EvalCodeEx () from \/usr\/lib\/libpython3.6m.so.1.0\r\n#31 0x00007ffff73ccd4b in PyEval_EvalCode () from \/usr\/lib\/libpython3.6m.so.1.0\r\n#32 0x00007ffff74ae112 in ?? () from \/usr\/lib\/libpython3.6m.so.1.0\r\n#33 0x00007ffff74b097d in PyRun_FileExFlags () from \/usr\/lib\/libpython3.6m.so.1.0\r\n#34 0x00007ffff74b0b67 in PyRun_SimpleFileExFlags ()\r\n   from \/usr\/lib\/libpython3.6m.so.1.0\r\n#35 0x00007ffff74a4a91 in Py_Main () from \/usr\/lib\/libpython3.6m.so.1.0\r\n#36 0x0000000000400a5d in main ()\r\n<\/pre>\r\n<\/details>\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n<pre>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.0.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.8.13-1-ARCH\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8<br\/>\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 33.0.0\r\nCython: 0.25.2\r\nnumpy: 1.11.3\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 2.0.0rc2+2914.g1fa4dd705.dirty\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n 0.9.2\r\napiclient: 1.5.5\r\nsqlalchemy: 1.1.4\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.4\r\nboto: None\r\npandas_datareader: None\r\n<\/pre>\r\n<\/details>\r\n",
    "labels":[
      "Bug",
      "Difficulty Intermediate",
      "Effort Low",
      "IO CSV"
    ],
    "comments":[
      "It appears to have died [while trying to read the thread state](https:\/\/github.com\/python\/cpython\/blob\/a967fce5f634e5c90e3951887d27cfd3bb70e2f2\/Python\/errors.c#L42):\r\n\r\n~~~c\r\noldtype = tstate->curexc_type;\r\n~~~\r\n\r\n`tstate` is null because [the global interpreter lock has been released](https:\/\/github.com\/pandas-dev\/pandas\/blob\/52aee10acc390fc230eead4287e3341692a3500a\/pandas\/src\/parser\/tokenizer.c#L2292-L2300):\r\n\r\n~~~py\r\nwith nogil:\r\n    error = _try_double_nogil(parser, col, line_start, line_end,\r\n                              na_filter, na_hashset, use_na_flist,\r\n                              na_fset, NA, data, &na_count)\r\n~~~\r\n\r\nThis is problematic, as the `round_trip` converter does [call back into Python](https:\/\/github.com\/pandas-dev\/pandas\/blob\/52aee10acc390fc230eead4287e3341692a3500a\/pandas\/src\/parser\/tokenizer.c#L1774-L1781):\r\n\r\n~~~c\r\ndouble round_trip(const char *p, char **q, char decimal, char sci, char tsep,\r\n                  int skip_trailing) {\r\n#if PY_VERSION_HEX >= 0x02070000\r\n    return PyOS_string_to_double(p, q, 0);\r\n#else\r\n    return strtod(p, q);\r\n#endif\r\n}\r\n~~~\r\n\r\nThe funny thing is that even if I delete `with nogil:`, it still fails with a Python exception, because the code in [_try_double_nogil](https:\/\/github.com\/pandas-dev\/pandas\/blob\/7ad6c65c3ce6c8fd5e3b82306c028d4fb115483c\/pandas\/parser.pyx#L1744) was never even designed to handle Python exceptions to begin with.",
      "This appears to work around the problem (kept the GIL and silenced the Python exception):\r\n\r\n~~~diff\r\ndiff --git a\/pandas\/parser.pyx b\/pandas\/parser.pyx\r\nindex bd793c98e..dc0292e5b 100644\r\n--- a\/pandas\/parser.pyx\r\n+++ b\/pandas\/parser.pyx\r\n@@ -1699,10 +1699,9 @@ cdef _try_double(parser_t *parser, int col, int line_start, int line_end,\r\n     result = np.empty(lines, dtype=np.float64)\r\n     data = <double *> result.data\r\n     na_fset = kset_float64_from_list(na_flist)\r\n-    with nogil:\r\n-        error = _try_double_nogil(parser, col, line_start, line_end,\r\n-                                  na_filter, na_hashset, use_na_flist,\r\n-                                  na_fset, NA, data, &na_count)\r\n+    error = _try_double_nogil(parser, col, line_start, line_end,\r\n+                              na_filter, na_hashset, use_na_flist,\r\n+                              na_fset, NA, data, &na_count)\r\n     kh_destroy_float64(na_fset)\r\n     if error != 0:\r\n         return None, None\r\ndiff --git a\/pandas\/src\/parser\/tokenizer.c b\/pandas\/src\/parser\/tokenizer.c\r\nindex 87e17fe5f..77c36ef8a 100644\r\n--- a\/pandas\/src\/parser\/tokenizer.c\r\n+++ b\/pandas\/src\/parser\/tokenizer.c\r\n@@ -1774,7 +1774,9 @@ double precise_xstrtod(const char *str, char **endptr, char decimal, char sci,\r\n double round_trip(const char *p, char **q, char decimal, char sci, char tsep,\r\n                   int skip_trailing) {\r\n #if PY_VERSION_HEX >= 0x02070000\r\n-    return PyOS_string_to_double(p, q, 0);\r\n+    double r = PyOS_string_to_double(p, q, 0);\r\n+    PyErr_Clear();\r\n+    return r;\r\n #else\r\n     return strtod(p, q);\r\n #endif\r\n~~~",
      "yeah this looks to be violating gil holding. Welcome for you to add a test \/ fix.\r\n\r\nprob simply best just hold the gil if float precision is specified, since this is not the default.",
      "I made a pull request here: https:\/\/github.com\/pandas-dev\/pandas\/pull\/15148"
    ],
    "events":[
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented"
    ],
    "changed_files":5,
    "additions":49,
    "deletions":17
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":3715,
    "reporter":"gerdemb",
    "created_at":1369918357000,
    "closed_at":1485039021000,
    "resolver":"nchmura4",
    "resolved_in":"2540d5ac7dd772daa307395be982dc5ce86c3c3f",
    "resolver_commit_num":0,
    "title":"ENH: Add fill_value option to resample()",
    "body":"see also #3707\n\nAdd a `fill_value` option to `resample()` so that it is possible to resample a `TimeSeries` without creating `NaN` values. If the series is an int dtype and an int is passed to `fill_value`, it should be possible to resample the series without casting the values to floats.\n\n\n",
    "labels":[
      "API Design"
    ],
    "comments":[
      "wouldn't this be the right way to do this?\ns.resample('M').sum().fillna(0)\n",
      "@nchmura4 that is a way, but this issue is about filling _before_ reindexing, like `.reindex` provides.\n\ne.g. compare this\n\n```\nIn [9]: s.reindex(pd.date_range(s.index.min(), s.index.max(), freq='D'), fill_value=0).resample('M').sum()\nOut[9]:\n2013-01-31    3\n2013-02-28    0\n2013-03-31    4\nFreq: M, dtype: int64\n\nIn [10]: s.reindex(pd.date_range(s.index.min(), s.index.max(), freq='D'), fill_value=1).resample('M').sum()\nOut[10]:\n2013-01-31    32\n2013-02-28    28\n2013-03-31     4\nFreq: M, dtype: int64\n```\n",
      "got it. thanks for the clarification!\n"
    ],
    "events":[
      "labeled",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":6,
    "additions":164,
    "deletions":21
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":15240,
    "reporter":"sdementen",
    "created_at":1485494145000,
    "closed_at":1485628151000,
    "resolver":"sdementen",
    "resolved_in":"66d8c41db4e1cb39e06c9e1f712f7d1f105c4478",
    "resolver_commit_num":0,
    "title":"Timestamp.replace raises ValueError instead of TypeError when given wrong argument",
    "body":"#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nThe reimplementation of Timestamp.replace (-dev\/pandas\/commit\/f8bd08e9c2fc6365980f41b846bbae4b40f08b83#diff-05de2950b2c86c9828531957e02d1b87L662) raises a ValueError when a keyword argument does not exist.\r\nPrevious implementation used datetime.replace that raises a TypeError when an argument is not valid.\r\nThis is a regression and it is probably better to keep API compatibility with the datetime.replace function (modulo extra functionalities).\r\nFor autodocumentation\/autocomplete purposes, it could also be useful to have explicitly in the function signature the keywords, so instead of::\r\n\r\n`  def replace(self, **kwds):\r\n`\r\nhave::\r\n \r\n`  def replace(self, year=None, month=None, day=None, hour=None, minute=None, second=None, microsecond=None, nanosecond=None, tzinfo=None)\r\n`\r\n\r\n#### Actual Output\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"pandas\\tslib.pyx\", line 715, in pandas.tslib.Timestamp.replace (pandas\\tslib.c:14832)\r\nValueError: invalid name apple passed\r\n\r\n#### Expected Output\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"pandas\\tslib.pyx\", line 715, in pandas.tslib.Timestamp.replace (pandas\\tslib.c:14832)\r\nTypeError: 'apple' is an invalid keyword argument for this function\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 61 Stepping 4, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: None\r\nnumpy: 1.11.3\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.5.2\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.4\r\nboto: None\r\npandas_datareader: None\r\n<\/details>\r\n",
    "labels":[
      "Difficulty Novice",
      "Effort Low",
      "Error Reporting",
      "Timeseries"
    ],
    "comments":[
      "if you want to do a PR wouldn be great",
      "I am having a try :-)\r\nCould you tell me where I can add an additional test on Timestamp.replace ? I have a hard time locating the tests...",
      "I can't really test my patch on my machine (as I have not the tool suite necessary to compile pandas). Can I start a PR (even unfinished) to test via the Travis\/Appveyor tests ? or is it \"bad practice\" ?",
      "@sdementen the tests are exactly in the above link, IOW in pandas\/tseries\/test_timezones.py\r\n\r\nI would recommend you install a proper dev version of pandas to test: http:\/\/pandas.pydata.org\/pandas-docs\/stable\/contributing.html",
      "@jreback I can't find any test of the Timestamp.replace method (except tests testing the replace(tzinfo=xxx)). Are there specific tests for Timestamp.replace ? I was expecting\/looking for tests like \r\n```\r\ndt = pandas.Timestamp(\"2016\")\r\ntm.assert(dt.hour,0)\r\ntm.assert(dt.replace(hour=3).hour,3)\r\n```",
      "https:\/\/github.com\/pandas-dev\/pandas\/blob\/master\/pandas\/tseries\/tests\/test_timezones.py#L1170\r\n\r\nyou could add some in ``test_timeseries.py`` if you want (and just leave these for the actual timezone replacement), but doesn't really matter.",
      "Hi All, I've been looking to contribute to pandas for a while, so I've made a PR that I think closes this. followed the guide as best I could."
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "commented",
      "referenced"
    ],
    "changed_files":3,
    "additions":41,
    "deletions":38
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":15234,
    "reporter":"david-hoffman",
    "created_at":1485406916000,
    "closed_at":1485981937000,
    "resolver":"david-hoffman",
    "resolved_in":"48fc9d613323ada9702a7d5c78c23eb0e8cae8a8",
    "resolver_commit_num":0,
    "title":"BUG: groupby.count fails on windows with large categorical index",
    "body":"#### Problem description\r\n\r\nOn windows machines the native `int` is `int32` which causes an overflow error in `cartesian_product` in `tools.util`. The problem line is `lenX = np.fromiter((len(x) for x in X), dtype=int)`\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 63 Stepping 2, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.2\r\nnumpy: 1.11.2\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: 0.8.2\r\nIPython: 5.1.0\r\nsphinx: 1.3.1\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.2.2\r\nnumexpr: 2.6.1\r\nmatplotlib: 2.0.0\r\nopenpyxl: 2.4.1\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 0.9.6\r\nlxml: 3.7.2\r\nbs4: 4.5.3\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: 1.1.5\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.4\r\nboto: 2.45.0\r\npandas_datareader: None\r\n\r\n<\/details>\r\n",
    "labels":[
      "Groupby",
      "Windows",
      "Bug",
      "Difficulty Intermediate",
      "Effort Low",
      "Categorical"
    ],
    "comments":[
      "show a reproducible example",
      "can you show a specific example where this fails?",
      "```python\r\nln[1]: import pandas as pd\r\n\r\nIn [2]: import numpy as np\r\n\r\nIn [3]: data = np.random.randn(1000000, 3)\r\n\r\nIn [4]: rangex = np.linspace(-2, 2, 2048)\r\n\r\nIn [5]: rangey = np.linspace(-1, 1, 1024)\r\n\r\nIn [6]: rangez = np.linspace(-3, 3, 4096)\r\n\r\nIn [7]: np.iinfo(int).max > rangex.size * rangey.size * rangez.size\r\nOut[8]: False\r\n\r\nIn [8]: df = pd.DataFrame(data, columns=[\"x\", \"y\", \"z\"])\r\n\r\nIn [9]: grouped = df.groupby([pd.cut(df.x, rangex), pd.cut(df.y, rangey), pd.cut(df.z, rangez)])\r\n\r\nIn [10]: grouped.count()\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-11-220cef394518> in <module>()\r\n----> 1 grouped.count()\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py in count(self)\r\n   3886         blk = map(make_block, map(counter, val), loc)\r\n   3887\r\n-> 3888         return self._wrap_agged_blocks(data.items, list(blk))\r\n   3889\r\n   3890\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py in _wrap_agged_blocks(self, items, blocks)\r\n   3801             result = result.T\r\n   3802\r\n-> 3803         return self._reindex_output(result)._convert(datetime=True)\r\n   3804\r\n   3805     def _reindex_output(self, result):\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py in _reindex_output(self, result)\r\n   3823         levels_list = [ping.group_index for ping in groupings]\r\n   3824         index, _ = MultiIndex.from_product(\r\n-> 3825             levels_list, names=self.grouper.names).sortlevel()\r\n   3826\r\n   3827         if self.as_index:\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\multi.py in from_product(cls, iterables, sortorder, names)\r\n   1023\r\n   1024         labels, levels = _factorize_from_iterables(iterables)\r\n-> 1025         labels = cartesian_product(labels)\r\n   1026\r\n   1027         return MultiIndex(levels=levels, labels=labels, sortorder=sortorder,\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\tools\\util.py in cartesian_product(X)\r\n     70     return [np.tile(np.repeat(np.asarray(com._values_from_object(x)), b[i]),\r\n     71                     np.product(a[i]))\r\n---> 72             for i, x in enumerate(X)]\r\n     73\r\n     74\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\tools\\util.py in <listcomp>(.0)\r\n     70     return [np.tile(np.repeat(np.asarray(com._values_from_object(x)), b[i]),\r\n     71                     np.product(a[i]))\r\n---> 72             for i, x in enumerate(X)]\r\n     73\r\n     74\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py in repeat(a, repeats, axis)\r\n    394     except AttributeError:\r\n    395         return _wrapit(a, 'repeat', repeats, axis)\r\n--> 396     return repeat(repeats, axis)\r\n    397\r\n    398\r\n\r\nValueError: negative dimensions are not allowed\r\n```",
      "ok thanks. I think this is the *same* as #14942, though on windows this fails before getting to the *eats-all-memory* part.\r\n\r\nwelcome for you to do a PR to fix this.",
      "Is it possible to do a PR without forking the whole repository?",
      "@david-hoffman theoretically (for a very very simple thing, you can do it via github), but not for this, this will require some investigation, debugging, and tests.\r\n\r\ncontribution docs are here: http:\/\/pandas.pydata.org\/pandas-docs\/stable\/contributing.html"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced"
    ],
    "changed_files":2,
    "additions":2,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":15289,
    "reporter":"ExpHP",
    "created_at":1486010509000,
    "closed_at":1486067215000,
    "resolver":"ExpHP",
    "resolved_in":"f6cfaabad9b9de6d0382e51a77b080723f84d778",
    "resolver_commit_num":0,
    "title":"series.replace with empty dictlike raises ValueError: not enough values to unpack",
    "body":"#### Problem description\r\n\r\n`DataFrame.replace` and `Series.replace` fail unceremoniously on an empty Series or dict argument; I would expect it to simply do nothing.\r\n\r\nWhile there is a fair bit of type-based wizardry going on in this function, I don't suppose there is any actual ambiguity in how this edge case should behave?\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n\r\n#### Output\r\n\r\n\r\n\r\n#### Expected Output\r\n\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-59-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 8.1.2\r\nsetuptools: 27.2.0\r\nCython: 0.24.1\r\nnumpy: 1.11.1\r\nscipy: 0.18.0\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.6.1\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: None\r\nnumexpr: 2.6.2\r\nmatplotlib: 1.5.1\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: 0.7.3\r\nlxml: None\r\nbs4: 4.4.1\r\nhtml5lib: 0.999\r\n 0.9.1\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n<\/details>\r\n",
    "labels":[
      "Difficulty Novice",
      "Effort Low",
      "Missing-data",
      "Reshaping"
    ],
    "comments":[
      "yep, this look like a bug. pull-requests to fix welcome!",
      "I am on it"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "renamed",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files":4,
    "additions":20,
    "deletions":3
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":15342,
    "reporter":"stephenrauch",
    "created_at":1486533097000,
    "closed_at":1486659936000,
    "resolver":"stephenrauch",
    "resolved_in":"3c9fec39d502cf7a24d4a9e16e3c5733560dc05c",
    "resolver_commit_num":0,
    "title":"Multiline Eval broken for local variables after first line",
    "body":"#### Problem description\r\n\r\nAs discussed [here](-local-variables-with-multiple-assignments-with-pandas-eval-function), using local variables with a multi-line eval does not work for locals not on the first line. \r\n\r\nThis: \r\n\r\n\r\n\r\nfails with:\r\n\r\n    error: pandas.computation.ops.UndefinedVariableError: local variable 'y' is not defined\r\n\r\nNOTE: I will be submitting a PR shortly.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.1.final.0\r\npython-bits: 32\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel\r\nbyteorder: little\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 8.1.2\r\nsetuptools: 18.2\r\nCython: None\r\nnumpy: 1.12.0\r\ndateutil: 2.6.0\r\npytz: 2016.6.1\r\n<\/details>\r\n",
    "labels":[
      "Bug",
      "Numeric"
    ],
    "comments":[

    ],
    "events":[
      "cross-referenced",
      "labeled",
      "labeled",
      "milestoned"
    ],
    "changed_files":3,
    "additions":18,
    "deletions":7
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":15344,
    "reporter":"tobgu",
    "created_at":1486540998000,
    "closed_at":1486735924000,
    "resolver":"tobgu",
    "resolved_in":"e8840725447859531ddcc4b878266f2043fb6465",
    "resolver_commit_num":0,
    "title":"Calling DataFrame.to_json() increases data frame memory usage in Python 3.6",
    "body":"#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n\r\nCompared to Python 2.7.12\r\n\r\n\r\n#### Problem description\r\nCalling `to_json` should not have any impact on the reported memory usage of a DataFrame. Just like in Python 2. The observed increase above is 32% which is really high.\r\n\r\nThis only seems to happen with dataframes that have strings in them.\r\n\r\nI've also tested calling `to_csv`, that does not trigger this behaviour.\r\n\r\nFurthermore it seems like the memory usage is quite a lot higher in Python 3 compared to the equivalent data frame in Python 2 (~25% in the example above). I guess this is more related to strings in Python 2 vs Python 3 than Pandas though?\r\n\r\n#### Expected Output\r\nNo change in reported memory usage after calling `to_json`.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.0.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.13.0-83-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 34.1.0\r\nCython: None\r\nnumpy: 1.12.0\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: 2.6.2\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: None\r\npandas_datareader: None\r\n\r\n<\/details>\r\n",
    "labels":[
      "2\/3 Compat",
      "IO JSON",
      "Performance"
    ],
    "comments":[
      "Partly guessing, but I think https:\/\/www.python.org\/dev\/peps\/pep-0393\/ could be the culprit.  Python 3 can choose a compact string representation, then the C API calls the json code is using could force it into a less compact representation.\r\n\r\nhttps:\/\/docs.python.org\/3\/whatsnew\/3.3.html#pep-393-flexible-string-representation\r\n",
      "the absolute sizes of memory used are dependent on py2\/py3 as @chris-b1 indicates.\r\n\r\nSo strings are held in python space, backed by pointers from numpy. This IS using the c-api; I would hazard a guess that strings that were formerly interned are now no longer. This is pretty deep. \r\n\r\nIf anyone wants to investigate please do so. Though I would say this is out of pandas hands.",
      "It does look like `ujson` was updated to use the new C API - so if you wanted to try porting this back in, may not be too hard.\r\n\r\nours - https:\/\/github.com\/pandas-dev\/pandas\/blob\/bf1a5961a09a6f5237a681f9f1c9a698b1a13918\/pandas\/src\/ujson\/python\/objToJSON.c#L402\r\n\r\ntheirs - https:\/\/github.com\/esnme\/ultrajson\/blob\/2f1d4874f4f4d2a40a460678004c80e69387c663\/python\/objToJSON.c#L143\r\n",
      "Yes, there seems to be quite a few updates in `uson` since the version that is present in Pandas. It does not seem to me like it has with interning to do but rater that single byte ascii characters are turned into full blown 4 byte unicode characters based on the basic experiment below.\r\n\r\nHaving a dataframe dominated by long strings would hence mean that the memory usage would be quadrupled after calling `to_json` compared to before.\r\n\r\n```python\r\n>>> df = pd.DataFrame({'a': [str(1)]})\r\n>>> df.memory_usage(index=True, deep=True)\r\nIndex    80\r\na        58\r\ndtype: int64\r\n>>> df.to_json()\r\n'{\"a\":{\"0\":\"1\"}}'\r\n>>> df.memory_usage(index=True, deep=True)\r\nIndex    80\r\na        66\r\ndtype: int64\r\n>>> df = pd.DataFrame({'a': [str(11)]})\r\n>>> df.to_json()\r\n'{\"a\":{\"0\":\"11\"}}'\r\n>>> df = pd.DataFrame({'a': [str(11)]})\r\n>>> df.memory_usage(index=True, deep=True)\r\nIndex    80\r\na        59\r\ndtype: int64\r\n>>> df.to_json()\r\n'{\"a\":{\"0\":\"11\"}}'\r\n>>> df.memory_usage(index=True, deep=True)\r\nIndex    80\r\na        71\r\ndtype: int64\r\n>>> df = pd.DataFrame({'a': [str(111)]})\r\n>>> df.memory_usage(index=True, deep=True)\r\nIndex    80\r\na        60\r\ndtype: int64\r\n>>> df.to_json()\r\n'{\"a\":{\"0\":\"111\"}}'\r\n>>> df.memory_usage(index=True, deep=True)\r\nIndex    80\r\na        76\r\ndtype: int64\r\n>>> \r\n\r\n```"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "commented",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":24,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":15421,
    "reporter":"yegulalp",
    "created_at":1487248432000,
    "closed_at":1487251490000,
    "resolver":"gwpdt",
    "resolved_in":"37e5f78b4e9ff03cbff4dea928445cc3b1f707c8",
    "resolver_commit_num":0,
    "title":"Unexpected string->float conversion in DataFrame.groupby().apply()",
    "body":"#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n\r\n\r\n#### Problem description\r\ngroupby.apply() does an unexpected conversion from string to float for column 'B' in the example above.  The bug is triggered only when both of the following happen:\r\n1.  A column ('B' in the example above) has string values, some of which are parseable as numbers and some which are not.\r\n2. Another column ('T' in the example above) in the dataframe has timestamps.\r\n\r\n\r\n#### Expected Output\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n# Paste the output here pd.show_versions() here\r\n\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.13.final.0\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 7\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nLOCALE: None.None\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.2\r\nnumpy: 1.11.3\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.5.1\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: 1.2.0\r\ntables: 3.2.2\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.1\r\nopenpyxl: 2.4.0\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.6\r\nlxml: 3.7.2\r\nbs4: 4.5.3\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: 1.1.4\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8.1\r\nboto: 2.45.0\r\npandas_datareader: 0.2.1\r\n<\/details>\r\n",
    "labels":[

    ],
    "comments":[
      "@yegulalp Thanks for the report!\r\n\r\nThis is a duplicate of https:\/\/github.com\/pandas-dev\/pandas\/issues\/14849, and also related to https:\/\/github.com\/pandas-dev\/pandas\/issues\/14423\r\n\r\nContributions to try to fix this are always welcome!"
    ],
    "events":[
      "commented",
      "mentioned"
    ],
    "changed_files":3,
    "additions":54,
    "deletions":2
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13453,
    "reporter":"jcrist",
    "created_at":1466017516000,
    "closed_at":1487255327000,
    "resolver":"aiguofer",
    "resolved_in":"5a8883b965610234366150897fe8963abffd6a7c",
    "resolver_commit_num":0,
    "title":"Resampler.nunique counting data more than once",
    "body":"xref addtl example in #13795 \n\nPandas `Resampler.nunique` appears to be putting the same data in multiple bins:\n\n\n\nIn pandas 0.18.1 and 0.18.0 these don't give the same results, when they should\n\n\n\nIn pandas 0.17.0 and 0.17.1 (adjusting to old style resample syntax), the `nunique` one fails due to a \"`ValueError: Wrong number of items passed 4, placement implies 5`\" somewhere in the depths of `internals.py`. If I go back to 0.16.2, I do get the same result for each.\n\nI'm not sure what's going on here. Since the `nunique` results sum to larger than the length, it appears data is being counted more than once.\n\n---\n\n\n",
    "labels":[
      "Resample",
      "Bug",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments":[
      "May be related to https:\/\/github.com\/pydata\/pandas\/issues\/10914.\n",
      "Interestingly everything seems to work fine if `agg` is used with `pd.Series.nunique` instead:\n\n```\nIn [11]: r = s.resample('M')\n\nIn [12]: r.agg(pd.Series.nunique)\nOut[12]:\n2000-01-31    744\n2000-02-29    337\n2000-03-31      0\n2000-04-30    384\n2000-05-31    337\nFreq: M, dtype: int64\n\nIn [13]: r.nunique()    # same result as r.agg('nunique')\nOut[13]:\n2000-01-31    337\n2000-02-29    744\n2000-03-31      0\n2000-04-30    744\n2000-05-31    337\nFreq: M, dtype: int64\n```\n",
      "CC:  @behzadnouri \n",
      "I think the root cause of the problem is in groupby.nunique(), which I believe is eventually is called by resample.nunique().  Note that groupby.nunique() has the same bug:\n\n``` python\nimport pandas as pd\nfrom pandas import Timestamp\n\ndata = ['1', '2', '3']\ntime = time = [Timestamp('2016-06-28 09:35:35'), Timestamp('2016-06-28 16:09:30'), Timestamp('2016-06-28 16:46:28')]\ntest = pd.DataFrame({'time':time, 'data':data})\n\n#wrong counts\nprint test.set_index('time').groupby(pd.TimeGrouper(freq='h'))['data'].nunique(), \"\\n\"\n#correct counts\nprint test.set_index('time').groupby(pd.TimeGrouper(freq='h'))['data'].apply(pd.Series.nunique)\n```\n\nThis gives\n\n```\ntime  \n2016-06-28 09:00:00    1  \n2016-06-28 10:00:00    0  \n2016-06-28 11:00:00    0  \n2016-06-28 12:00:00    0  \n2016-06-28 13:00:00    0  \n2016-06-28 14:00:00    0  \n2016-06-28 15:00:00    0  \n2016-06-28 16:00:00    1  \nFreq: H, Name: data, dtype: int64   \n\ntime  \n2016-06-28 09:00:00    1  \n2016-06-28 10:00:00    0  \n2016-06-28 11:00:00    0  \n2016-06-28 12:00:00    0  \n2016-06-28 13:00:00    0  \n2016-06-28 14:00:00    0  \n2016-06-28 15:00:00    0  \n2016-06-28 16:00:00    2  \nFreq: H, Name: data, dtype: int64  \n```\n\nI believe the problem is in the second to last line of groupby.nunique(), \ni.e. line 2955 in [groupby.py](https:\/\/github.com\/pydata\/pandas\/blob\/master\/pandas\/core\/groupby.py)\n\n``` python\nres[ids] = out\n```\n\nI suspect `ids` should not be used for the indexing--it has different dimensions than `out`.\n\n``` python\npd.show_versions()\n```\n\n```\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.12.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 14.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\nLOCALE: None.None\n\npandas: 0.19.0\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 27.2.0\nCython: 0.24.1\nnumpy: 1.11.2\nscipy: 0.18.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 5.1.0\nsphinx: 1.4.6\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.6.1\nblosc: None\nbottleneck: 1.1.0\ntables: 3.2.3.1\nnumexpr: 2.6.1\nmatplotlib: 1.5.3\nopenpyxl: 2.3.2\nxlrd: 1.0.0\nxlwt: 1.1.2\nxlsxwriter: 0.9.3\nlxml: 3.6.4\nbs4: 4.5.1\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: 1.0.13\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.42.0\npandas_datareader: 0.2.1\n```\n",
      "@mgalbright why don't you submit a pull-request with your test examples (and those from the issue), and the proposed fix. See if that breaks anything else. Would be greatly appreciated!\n",
      "Hey, is there any advancement on this? I just realized that a report that I've been building is giving the wrong results and I believe it's due to this. I can't share all the code but here's a comparison of `groupby.unique` and `groupby.nunique`:\n\n``` python\nIn [216]: ents.groupby(pd.Grouper(freq='1W-SAT', key='startdate'))['ent_id'].unique().tail(1)\nOut[216]: \n\nstartdate\n2016-11-12    [550A00000033DHUIA2]\nFreq: W-SAT, Name: ent_id, dtype: object\n\nIn [217]: ents.groupby(pd.Grouper(freq='1W-SAT', key='startdate'))['ent_id'].nunique().tail(1)\nOut[217]: \n\nstartdate\n2016-11-12    7\nFreq: W-SAT, Name: ent_id, dtype: int64\n\nIn [218]: ents.groupby(pd.Grouper(freq='1W-SAT', key='startdate'))['ent_id'].count().tail(1)\nOut[221]: \n\nstartdate\n2016-11-12    1\nFreq: W-SAT, Name: ent_id, dtype: int64\n```\n",
      "@aiguofer pull-requests are welcome to fix.\n",
      "Not really adding anything, but I just ran into this issue for a work report as well (pandas version 0.19.2). Passing to .agg(pd.Series.nunique) works great - thanks for the tip",
      "Took a look at @mgalbright coment and suggestion and if I'm understanding the code correctly, the above PR should fix it. I ran `nosetests pandas\/tests\/groupby` and only had one unrelated test fail (`test_series_groupby_value_counts() takes exactly 2 arguments (0 given)`)."
    ],
    "events":[
      "commented",
      "commented",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "referenced"
    ],
    "changed_files":4,
    "additions":38,
    "deletions":4
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":15328,
    "reporter":"jesrael",
    "created_at":1486454150000,
    "closed_at":1487266887000,
    "resolver":"abaldenko",
    "resolved_in":"c7300ea9ccf6c8b4eeb5a4ae59dc2419753c9b18",
    "resolver_commit_num":0,
    "title":"BUG: Concat with inner join and empty DataFrame",
    "body":"Function `concat` with parameter `join='inner'` not return empty `DataFrame`:\r\n\r\n    DF_empty = pd.DataFrame()\r\n    DF_a = pd.DataFrame({'a': [1, 2]}, index=[0, 1])\r\n    print (DF_empty)\r\n    Empty DataFrame\r\n    Columns: []\r\n    Index: []\r\n    \r\n    print (DF_a)\r\n       a\r\n    0  1\r\n    1  2\r\n    \r\n    print(pd.concat([DF_empty, DF_a], axis=1, join='inner'))\r\n       a\r\n    0  1\r\n    1  2\r\n\r\nBut `merge` works nice:\r\n\r\n    print (pd.merge(DF_empty, DF_a, left_index=True, right_index=True))\r\n    Empty DataFrame\r\n    Columns: [a]\r\n    Index: []\r\n\r\n[SO question](-concat-inner-join-a-join-with-an-empty-dataframe-does-not-yield-an-empty)\r\n\r\n---\r\n\r\n    print (pd.show_versions())\r\n\r\n<details>\r\n    INSTALLED VERSIONS\r\n    ------------------\r\n    commit: None\r\n    python: 3.5.1.final.0\r\n    python-bits: 64\r\n    OS: Windows\r\n    OS-release: 7\r\n    machine: AMD64\r\n    processor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\r\n    byteorder: little\r\n    LC_ALL: None\r\n    LANG: sk_SK\r\n    LOCALE: None.None\r\n    \r\n    pandas: 0.19.2\r\n    nose: 1.3.7\r\n    pip: 9.0.1\r\n    setuptools: 20.3\r\n    Cython: 0.23.4\r\n    numpy: 1.11.0\r\n    scipy: 0.17.0\r\n    statsmodels: 0.6.1\r\n    xarray: None\r\n    IPython: 4.1.2\r\n    sphinx: 1.3.1\r\n    patsy: 0.4.0\r\n    dateutil: 2.5.1\r\n    pytz: 2016.2\r\n    blosc: None\r\n    bottleneck: 1.0.0\r\n    tables: 3.2.2\r\n    numexpr: 2.5.2\r\n    matplotlib: 1.5.1\r\n    openpyxl: 2.3.2\r\n    xlrd: 0.9.4\r\n    xlwt: 1.0.0\r\n    xlsxwriter: 0.8.4\r\n    lxml: 3.6.0\r\n    bs4: 4.4.1\r\n    html5lib: 0.999\r\n     None\r\n    apiclient: None\r\n    sqlalchemy: 1.0.12\r\n    pymysql: None\r\n    psycopg2: None\r\n    jinja2: 2.8\r\n    boto: 2.39.0\r\n    pandas_datareader: 0.2.1\r\n    None\r\n<\\details>",
    "labels":[
      "Bug",
      "Difficulty Novice",
      "Effort Low",
      "Reshaping"
    ],
    "comments":[
      "ok, this is a bug, we normally filter out completely empty frames, but in this case we need to skip that condition\r\n\r\n```diff --git a\/pandas\/tools\/merge.py b\/pandas\/tools\/merge.py\r\nindex 3fbd83a..5bf0546 100644\r\n--- a\/pandas\/tools\/merge.py\r\n+++ b\/pandas\/tools\/merge.py\r\n@@ -1689,7 +1689,7 @@ class _Concatenator(object):\r\n                            if sum(obj.shape) > 0 or isinstance(obj, Series)]\r\n \r\n             if (len(non_empties) and (keys is None and names is None and\r\n-                                      levels is None and join_axes is None)):\r\n+                                      levels is None and join_axes is None and not self.intersect)):\r\n                 objs = non_empties\r\n                 sample = objs[0]\r\n \r\n```\r\n\r\npull-request to add some tests  (there are *very* little tests for using ``join=`` with ``concat``) and fix would be great.",
      "happy to work on that.  does it amount to fixing `\/pandas\/tools\/merge.py` and writing some related tests in `pandas\/pandas\/tests\/types\/test_concat.py`?",
      "tests are in ``pandas\/tools\/tests\/test_concat.py``\r\n\r\nand (shortly) this code is in ``pandas\/tools\/concat.py`` (just moving it)."
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "commented",
      "commented",
      "cross-referenced"
    ],
    "changed_files":4,
    "additions":22,
    "deletions":2
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":15370,
    "reporter":"bmcfee",
    "created_at":1486850568000,
    "closed_at":1487618651000,
    "resolver":"bmcfee",
    "resolved_in":"0b4fdf988e3125f7c55aaf6e08a2dfa7d9e2e8a0",
    "resolver_commit_num":0,
    "title":"deepcopy failure on empty dataframes with non-empty column set (numpy 1.12 compatibility)",
    "body":"#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nThis only occurs with numpy 1.12 (and, presumably above): when deepcopying an empty dataframe with a non-empty column set, it fails with the following:\r\n\r\n\r\n\r\nIf the column set is also empty, everything works as expected.\r\n\r\nOn older numpy versions (1.11), it also works as expected.\r\n\r\n#### Expected Output\r\n\r\nNot failing.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n# Paste the output here pd.show_versions() here\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.0.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-62-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 9.0.1\r\nsetuptools: 32.3.1.post20170108\r\nCython: 0.25.2\r\nnumpy: 1.12.0\r\nscipy: 0.18.1\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 2.0.0\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: None\r\npandas_datareader: None\r\n<\/details>\r\n",
    "labels":[
      "Compat",
      "Difficulty Novice",
      "Effort Low",
      "Reshaping"
    ],
    "comments":[
      "not shocked this doesn't work, see #8571 \r\n\r\nwe are not using an override (which we should be), to simply call our custom methods.\r\n\r\ne.g. basically\r\n\r\n```\r\ndef __copy__(self):\r\n    return self.copy()\r\n```\r\n\r\nif you'd like to take a crack at this issue (and add this as a test) would be appreciated!",
      "> if you'd like to take a crack at this issue (and add this as a test) would be appreciated!\r\n\r\nNot sure I know how best to fix it, as I haven't mucked around the internals of dataframes much.  It's not obvious to me that this should be fixed in dataframe, ndframe, or somewhere else?",
      "@bmcfee simply need to define ``__copy__`` and ``__deepcopy__``, can be done in ``pandas\/core\/generic\/NDFrame``. These are *already* defined for ``Index`` (in ``pandas\/indexes\/base.py`` (it may simply be possible to move those to ``pandas\/core\/base`` though as we have a mixin that goes thru the entire hierarchy ``PandasObject``."
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "referenced"
    ],
    "changed_files":4,
    "additions":41,
    "deletions":7
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":6322,
    "reporter":"wangshun98",
    "created_at":1392130489000,
    "closed_at":1487684337000,
    "resolver":"tzinckgraf",
    "resolved_in":"bb2144a32cb30bc7428b117389a280b2515e9cf1",
    "resolver_commit_num":1,
    "title":"BUG: reset_index with NaN in MultiIndex",
    "body":"This is related to the following commit.\n\n\nWhen one level in the MultiIndex is all NaN, line 2819 in DataFrame.reset_index() and subfunction _maybe_cast, will have a problem, because values is empty.\n2819:    values = values.take(labels)\nIndexError: cannot do a non-empty take from an empty axes\n\nA possible fix is to add the following lines before 2819:\n                if mask.all():\n                    values = labels \\* np.nan\n                    return values\n",
    "labels":[
      "Bug",
      "Missing-data",
      "MultiIndex"
    ],
    "comments":[
      "can U give an example of your use case\nan all nan level in a multi index is really really hard to support properly \n",
      "This comes out of a database query of mixed fields of dates and values. And I want to use the combination of dates as index so I can easily join the values from multiple queries. however, sometimes all dates in one column are all null.  \n\nNaN in MultiIndex is already supported. That 3 lines I suggested would make it work with a level with all NaN.  We can test case other functionalities. But after I made that change in my local copy, I haven't experienced other issues with all NaN MultiIndex yet.\n",
      "go ahead and do a PR (with a test case) and i'll take a look\n",
      "I can confirm I am also experiencing precisely this issue with an all-NaN level in my multi-index. Looks to me like those three lines should work if included.\n",
      "@jreback\n\nThe code below returns the error: `IndexError: cannot do a non-empty take from an empty axes.`\n\n```\nimport pandas as pd\nimport numpy as np\nimport random\nimport string\n\n\n\n# Create 12 random strings 3 char long \nrndm_strgs = [''.join(random.SystemRandom().choice(string.ascii_uppercase + string.digits) for _ in range(3)) for i in range(12)]            \nrndm_strgs[0] = None\nrndm_strgs[5] = None\n\n# Make Dataframe\ndf = pd.DataFrame({'A' : list('pandasisgood'),\n                   'B' : np.nan,\n                   'C' : rndm_strgs,\n                   'D' : np.random.rand(12)})\n\n# Set an Index -> Columns have Nans\ndf_w_idx = df.set_index(['A','B','C'])\n\n# This is where it fails\ndf_w_idx.reset_index()\n```\n\nThis is the desired result:\n\n```\nfor clmNm in df_w_idx.index.names:\n    print(clmNm)\n\n    print(clmNm)\n\n    df_w_idx[clmNm] = df_w_idx.index.get_level_values(clmNm)\n\n\ndf_w_idx = df_w_idx.reset_index(drop=True).copy()\ndf_w_idx\n```\n",
      "@jreback I have the same issue and was playing around with a solution.\r\n\r\nThe `reset_index` function in `frame.py` function has the following code\r\n```\r\n            # if we have the labels, extract the values with a mask\r\n            if labels is not None:\r\n                mask = labels == -1\r\n                values = values.take(labels)\r\n```\r\nsee [here](https:\/\/github.com\/pandas-dev\/pandas\/blob\/5710947759e13f54a5bf896768fb7f9c214f245d\/pandas\/core\/frame.py#L2938).\r\n\r\nThe mask variable is all `True` only if all the labels are -1. Assuming this means all the values were `np.nan`, one solution I was playing with is as follows\r\n```\r\n            # if we have the labels, extract the values with a mask\r\n            if labels is not None:\r\n                mask = labels == -1\r\n                if mask.all():\r\n                    values = (np.nan * mask).values()\r\n                else:\r\n                    values = values.take(labels)\r\n```\r\n\r\nPlease advise. More than happy to create a pull request and create all appropriate test cases.\r\n\r\n",
      "@tzinckgraf you can just do the take if ``len(values)``; ``.take()`` is picky :>\r\n\r\nsure why don't you put up a PR with a test (you can use the above example)."
    ],
    "events":[
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced"
    ],
    "changed_files":3,
    "additions":39,
    "deletions":4
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":8711,
    "reporter":"jreback",
    "created_at":1414940580000,
    "closed_at":1487779593000,
    "resolver":"jgoppert",
    "resolved_in":"1400305899d55bee21253952de9f6f0cf245b089",
    "resolver_commit_num":0,
    "title":"ENH\/BUG: support TimedeltaIndex plotting",
    "body":"This raises\n\n\n\nThis will show the timedeltas with a formatted (albeit string index)\n\n\n\nwonder if we can just register a converter somehow? like #8614 \n",
    "labels":[
      "Bug",
      "Enhancement",
      "Visualization",
      "Timedelta"
    ],
    "comments":[
      "I don't think that matplotlib already has a converter for `datetime.timedelta`, so just registering our `Timedelta` type will not be enough. Eg `plt.plot(s.index.to_pytimedelta(), s)` also fails. \n\nBut writing a basic converter should not be that difficult I think (and if it also works for `datetime.timedelta` it could maybe also be pushed upstream to matplotlib)\n",
      "Timedelta is s. subclass of datetime.timedelta \n",
      "I just encountered a MemoryError when attempting to plot a TimedeltaIndex!\n\n```\npd.Series(range(15), pd.timedelta_range(0, freq='D', periods=15)).plot()\n```\n\n```\n---------------------------------------------------------------------------\nMemoryError                               Traceback (most recent call last)\n<ipython-input-113-e9a2d53dcace> in <module>()\n----> 1 pd.Series(range(15), pd.timedelta_range(0, freq='H', periods=15)).plot()\n\n\/Users\/shoyer\/dev\/pandas\/pandas\/tools\/plotting.pyc in plot_series(data, kind, ax, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, label, secondary_y, **kwds)\n   2516                  yerr=yerr, xerr=xerr,\n   2517                  label=label, secondary_y=secondary_y,\n-> 2518                  **kwds)\n   2519 \n   2520 \n\n\/Users\/shoyer\/dev\/pandas\/pandas\/tools\/plotting.pyc in _plot(data, x, y, subplots, ax, kind, **kwds)\n   2322         plot_obj = klass(data, subplots=subplots, ax=ax, kind=kind, **kwds)\n   2323 \n-> 2324     plot_obj.generate()\n   2325     plot_obj.draw()\n   2326     return plot_obj.result\n\n\/Users\/shoyer\/dev\/pandas\/pandas\/tools\/plotting.pyc in generate(self)\n    925         self._make_legend()\n    926         self._post_plot_logic()\n--> 927         self._adorn_subplots()\n    928 \n    929     def _args_adjust(self):\n\n\/Users\/shoyer\/dev\/pandas\/pandas\/tools\/plotting.pyc in _adorn_subplots(self)\n   1058                     ax.set_xticklabels(xticklabels)\n   1059                 self._apply_axis_properties(ax.xaxis, rot=self.rot,\n-> 1060                                             fontsize=self.fontsize)\n   1061                 self._apply_axis_properties(ax.yaxis, fontsize=self.fontsize)\n   1062             elif self.orientation == 'horizontal':\n\n\/Users\/shoyer\/dev\/pandas\/pandas\/tools\/plotting.pyc in _apply_axis_properties(self, axis, rot, fontsize)\n   1069 \n   1070     def _apply_axis_properties(self, axis, rot=None, fontsize=None):\n-> 1071         labels = axis.get_majorticklabels() + axis.get_minorticklabels()\n   1072         for label in labels:\n   1073             if rot is not None:\n\n\/Users\/shoyer\/miniconda\/envs\/rapid\/lib\/python2.7\/site-packages\/matplotlib\/axis.pyc in get_majorticklabels(self)\n   1166     def get_majorticklabels(self):\n   1167         'Return a list of Text instances for the major ticklabels'\n-> 1168         ticks = self.get_major_ticks()\n   1169         labels1 = [tick.label1 for tick in ticks if tick.label1On]\n   1170         labels2 = [tick.label2 for tick in ticks if tick.label2On]\n\n\/Users\/shoyer\/miniconda\/envs\/rapid\/lib\/python2.7\/site-packages\/matplotlib\/axis.pyc in get_major_ticks(self, numticks)\n   1295         'get the tick instances; grow as necessary'\n   1296         if numticks is None:\n-> 1297             numticks = len(self.get_major_locator()())\n   1298         if len(self.majorTicks) < numticks:\n   1299             # update the new tick label properties from the old\n\n\/Users\/shoyer\/dev\/pandas\/pandas\/tseries\/converter.pyc in __call__(self)\n    901             vmin, vmax = vmax, vmin\n    902         if self.isdynamic:\n--> 903             locs = self._get_default_locs(vmin, vmax)\n    904         else:  # pragma: no cover\n    905             base = self.base\n\n\/Users\/shoyer\/dev\/pandas\/pandas\/tseries\/converter.pyc in _get_default_locs(self, vmin, vmax)\n    882 \n    883         if self.plot_obj.date_axis_info is None:\n--> 884             self.plot_obj.date_axis_info = self.finder(vmin, vmax, self.freq)\n    885 \n    886         locator = self.plot_obj.date_axis_info\n\n\/Users\/shoyer\/dev\/pandas\/pandas\/tseries\/converter.pyc in _daily_finder(vmin, vmax, freq)\n    505                     Period(ordinal=int(vmax), freq=freq))\n    506     span = vmax.ordinal - vmin.ordinal + 1\n--> 507     dates_ = PeriodIndex(start=vmin, end=vmax, freq=freq)\n    508     # Initialize the output\n    509     info = np.zeros(span,\n\n\/Users\/shoyer\/dev\/pandas\/pandas\/tseries\/period.pyc in __new__(cls, data, ordinal, freq, start, end, periods, copy, name, tz, **kwargs)\n    637             else:\n    638                 data, freq = cls._generate_range(start, end, periods,\n--> 639                                                  freq, kwargs)\n    640         else:\n    641             ordinal, freq = cls._from_arraylike(data, freq, tz)\n\n\/Users\/shoyer\/dev\/pandas\/pandas\/tseries\/period.pyc in _generate_range(cls, start, end, periods, freq, fields)\n    651                 raise ValueError('Can either instantiate from fields '\n    652                                  'or endpoints, but not both')\n--> 653             subarr, freq = _get_ordinal_range(start, end, periods, freq)\n    654         elif field_count > 0:\n    655             subarr, freq = _range_from_fields(freq=freq, **fields)\n\n\/Users\/shoyer\/dev\/pandas\/pandas\/tseries\/period.pyc in _get_ordinal_range(start, end, periods, freq)\n   1317                              dtype=np.int64)\n   1318     else:\n-> 1319         data = np.arange(start.ordinal, end.ordinal + 1, dtype=np.int64)\n   1320 \n   1321     return data, freq\n\nMemoryError: \n\n> \/Users\/shoyer\/dev\/pandas\/pandas\/tseries\/period.py(1319)_get_ordinal_range()\n   1318     else:\n-> 1319         data = np.arange(start.ordinal, end.ordinal + 1, dtype=np.int64)\n   1320 \n```\n",
      "Working on this. Doesn't look too bad.\n",
      "As an update, it's a bit worse than I thought. I think it was @changhiskhan who put in a ton of heuristics for figuring out what to resolution to draw when plotting datetimes. I wasn't sure if we'd need that for timedeltas, and then I got busy with other thing. My branch is [here](https:\/\/github.com\/TomAugspurger\/pandas\/commit\/ddb52378c4c0f8b871b67dcf7b646d123aba032c)\n",
      "As a workaround, the following works with master:\n\n``` python\nplt.plot(s.index,s.values)\n```\n",
      "I don't think freq adjustment of different timedeltas is mandatory at initial version. If ok, I'll try.\n",
      "Coming here from #10650, and adding a little more info just in case it can help. In my case, the bug manifests in `_get_ordinal_range`'s `end` parameter having a huge `ordinal`. This means the [following line](https:\/\/github.com\/pydata\/pandas\/blob\/e244bdd\/pandas\/tseries\/period.py#L990):\n\n```\ndata = np.arange(start.ordinal, end.ordinal + 1, mult, dtype=np.int64)\n```\n\nallocates a gigantic array. To be specific, when doing:\n\n```\npd.Series(np.random.randn(4), index=pd.timedelta_range('0:00:00', periods=4, freq='min')).plot()\n```\n\nthe values of `start.ordinal` and `end.ordinal` are 0 and 180000000000, respectively.\n",
      "@lucas-eyer is the `mult` parameter on that line appropriate, or is it some very small number? That might be the source of the issue...\n",
      "I don't know what appropriate would be, but it's `1` (one).\n\nEdit: `pip freeze | grep pandas` gives `pandas==0.17.0`.\n",
      "I also just ran into this issue on 0.17.1. I'm not very familiar with the code, but it appears the issue is in `pandas.tseries.converter`.\n\nThe issue is that `vmin` and `vmax` as specified in the call to `_get_default_locs` in the `get_major_locator` function are in nanoseconds as returned from `XAxis.get_view_interval`:\n\n```\ndef __call__(self):\n    'Return the locations of the ticks.'\n    # axis calls Locator.set_axis inside set_m<xxxx>_formatter\n    vi = tuple(self.axis.get_view_interval())             # THIS IS IN NANOS\n    if vi != self.plot_obj.view_interval:\n        self.plot_obj.date_axis_info = None\n    self.plot_obj.view_interval = vi\n    vmin, vmax = vi\n    if vmax < vmin:\n        vmin, vmax = vmax, vmin\n    if self.isdynamic:\n        locs = self._get_default_locs(vmin, vmax)     # VMIN AND VMAX ARE IN NANOS\n    else:  # pragma: no cover\n        base = self.base\n        (d, m) = divmod(vmin, base)\n        vmin = (d + 1) * base\n        locs = lrange(vmin, vmax + 1, base)\n    return locs\n```\n\nBut downstream in _daily_finder the `freq` parameter is used, which means that the system is interpreting the deltas in terms of minutes\/hours\/etc. rather than nanos:\n\n```\ndef _daily_finder(vmin, vmax, freq):\n    periodsperday = -1\n\n    if freq >= FreqGroup.FR_HR:\n        if freq == FreqGroup.FR_NS:\n            periodsperday = 24 * 60 * 60 * 1000000000\n       # ETC MAPPING periodsperday\n       # .....\n    # save this for later usage\n    vmin_orig = vmin\n\n    (vmin, vmax) = (Period(ordinal=int(vmin), freq=freq),    # NOW THESE ARE INTERPRETED AS MINUTES (or whatever freq)\n                    Period(ordinal=int(vmax), freq=freq))\n```\n\nReplacing the final line above with \n\n```\n (vmin, vmax) = (Period(ordinal=int(vmin), freq='N'), Period(ordinal=int(vmax), freq='N'))\n```\n\nappears to fix the issue. \n",
      "@Liam3851 glad you have tracked this down! Any chance you're interested in making a pull request with the fix? :)\n",
      "Sure, I just have to figure out how to do it lol. Longtime pandas user but kinda new on this github thingy. I'll head over to the FAQ.\n",
      "Great! Give it a try and let us know if you have any questions :).\n\nOn Wed, Jan 13, 2016 at 11:48 AM, Liam3851 notifications@github.com wrote:\n\n> Sure, I just have to figure out how to do it lol. Longtime pandas user but\n> kinda new on this github thingy. I'll head over to the FAQ.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https:\/\/github.com\/pydata\/pandas\/issues\/8711#issuecomment-171412395.\n",
      "Lots of love from me too @Liam3851!\n",
      "Hmm, ok still slightly more complicated. Was testing the fix and the bounds are now right and the graphs themselves look correct but the axis labels don't always work properly (sometimes they disappear)-- probably something related to how the labels are interpreted. I'm busy these next few days but I'll try to get around to making the fix sound.\n",
      "Just guessing, but you could be hitting what [I ran into](https:\/\/github.com\/pydata\/pandas\/issues\/8711#issuecomment-110754442). I can't remember how much progress if any I made on that.\n",
      "@TomAugspurger Hmm.. I'll try your version to see what it does. From the diff it looks like we're taking slightly different paths. It looks like you were building a TimedeltaConverter that worked parallel to DatetimeConverter and TimeConverter; I've been trying to fix the codepath the timedeltas are currently taking (through DatetimeConverter). But it's entirely possible that getting it to look just right will require going down your path. \n",
      "I\u2019d say getting it somewhat functional is good enough for now. Hopefully you don\u2019t have to go down that rabbit hole.\n\n> On Jan 14, 2016, at 10:29 AM, Liam3851 notifications@github.com wrote:\n> \n> @TomAugspurger https:\/\/github.com\/TomAugspurger Hmm.. I'll try your version to see what it does. From the diff it looks like we're taking slightly different paths. It looks like you were building a TimedeltaConverter that worked parallel to DatetimeConverter and TimeConverter; I've been trying to fix the codepath the timedeltas are currently taking (through DatetimeConverter). But it's entirely possible that getting it to look just right will require going down your path.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub https:\/\/github.com\/pydata\/pandas\/issues\/8711#issuecomment-171691022.\n",
      "Hello. I am using pandas version 0.19.0 and matplotlib version 1.5.3 with python 3 and this issue is still there: If I try to plot a Dataframe where the index is a timedelta I get `Memory Error`. I am working around this by calling plt.plot(df.index, df.values) but it would be nice if there was a proper fix for this...\n",
      "@sam-cohan As you can see, the issue is still open, so it's indeed not yet solved. But any help is certainly welcome! \n",
      "Sorry I was looking at the wrong \"Closed\" :)\n",
      "Really wish this was fixed. I'm using datetime as a work around but stringing along 1970-01-01 to do time deltas is not fun.",
      "@TomAugspurger does your branch with a first attempt still exist? (the link above is not working anymore)",
      "So the issue here is that we are trying to use the Int64Index as a base class for TimedeltaIndex but we are trying to use the plotting routines for the PeriodIndex which relies on DatetimeIndex (matplotlib.date) underneath.  Matplotlib.date scales the view interval to the selected frequency. Int64Index does not, so this explains the issues above.\r\n\r\n### Options:\r\n\r\n1. Rebase timedelta index on DatetimeIndex\r\n2. Write a another routine to plot time deltas like this: http:\/\/stackoverflow.com\/questions\/15240003\/matplotlib-intelligent-axis-labels-for-timedelta. I think this is the easiest path forward, but I need help figuring out how to hook it in. With the time series mix-ins for plotting I'd have to override the plotting routines based on the type of index somewhere.",
      "@jgoppert you should take a look at `pandas\/tseries\/converter.py` and the `TimeConverter` and `DatetimeConverter` classes. A possible way forward is to make a new `TimedeltaConverter` similar to those. ",
      "@jorisvandenbossche I did consider that approach, but I think having a separate matplotlib plotting function  is cleaner and will require less maintenance. We also won't have to worry about ever seeing jan 1970 on the time delta plot like we do on the period index based plots now. It seems pretty robust and I have added nano-second level precision labels.",
      "> @TomAugspurger does your branch with a first attempt still exist? (the link above is not working anymore)\r\n\r\nSeems like I deleted that branch when I was cleaning up my fork. I didn't get far beyond the `TimedletaConverter`, which is pretty straightforward. IIRC the difficult part was getting the dynamic relabeling to work like datetimes do (which can be a separate fix from fixing the memory error).",
      "@TomAugspurger can you take a look at my PR. Totally different approach but seems to work for me."
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files":6,
    "additions":154,
    "deletions":23
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":15463,
    "reporter":"kernc",
    "created_at":1487633272000,
    "closed_at":1487856268000,
    "resolver":"csizsek",
    "resolved_in":"b94186d4c58ee055656a84f55618be537db0095a",
    "resolver_commit_num":0,
    "title":"Series.rolling(3).quantile(10) segmentation fault",
    "body":"#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nThe value 10 is over the fraction of 1 which .quantile() works with, but I guess it still shouldn't crash my interpreter.\r\n\r\n#### Expected Output\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\npandas 0.19.0+479.g4842bc7",
    "labels":[
      "Bug",
      "Difficulty Novice",
      "Numeric",
      "Reshaping",
      "Effort Low"
    ],
    "comments":[
      "yep, not very friendly...PR's appreciated!",
      "Hi @kernc and @jreback , I'm thinking about fixing this issue. Any idea where to start? (I have read the contribution documentation, set up the environment and was able to reproduce.)",
      "you can add a validation right here ``0 <= quantile <= 1``: https:\/\/github.com\/pandas-dev\/pandas\/blob\/master\/pandas\/window.pyx#L1288, and raise a ``ValueError`` if not in range. please add some tests as well.\r\n\r\nthanks!"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":19,
    "deletions":4
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":15420,
    "reporter":"dfd",
    "created_at":1487217680000,
    "closed_at":1487966252000,
    "resolver":"jeet63",
    "resolved_in":"3fe85afef47e9e079a0fa24f826bb6faaa2341d5",
    "resolver_commit_num":0,
    "title":"rank incorrectly orders ordered categories",
    "body":"#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nrank seems to be ignoring the order of ordered categories.\r\n\r\n#### Expected Output\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n\r\n<\/details>\r\n",
    "labels":[
      "Bug",
      "Categorical",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "@dfd Thanks for the report! That is indeed clearly a bug. \r\nFor example in `sort_values`, it takes the correct order into account, but `rank` was apparently missed.\r\n\r\n```\r\nIn [6]: a.A.sort_values()\r\nOut[6]: \r\n0     first\r\n1    second\r\n2     third\r\n3    fourth\r\n4     fifth\r\n5     sixth\r\nName: A, dtype: category\r\nCategories (6, object): [first < second < third < fourth < fifth < sixth]\r\n```\r\n\r\nI think this should be a rather easy fix (in the `pd.core.algorithms.rank`, we should need to check for categorical, and then pass the underlying integer codes). If you would be interested in trying to do a pull request with a fix, always welcome!"
    ],
    "events":[
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":4,
    "additions":105,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":15487,
    "reporter":"abast",
    "created_at":1487871355000,
    "closed_at":1487968906000,
    "resolver":"abast",
    "resolved_in":"d80275dfaa6a8ad50bc49dbaef9eacd5509008dc",
    "resolver_commit_num":0,
    "title":"support CategoricalIndex for read_msgpack",
    "body":"#### The following code fails:\r\n\r\n\r\n\r\n\r\n#### Problem description\r\n`read_msgpack` apparently does not seem to support a CategoricalIndex, however, it is possible to save a dataframe with a CategoricalIndex using `to_msgpack`. \r\n\r\nBackground: I am currently using the to_msgpack method to save a dask dataframe, where the index is (something like) a time stamp. It is not unique. I am overall very satisfied with the performance of `to_msgpack`, however when it comes to space efficency, having a categorical index would probably provide a significant improvement.\r\n\r\nOr maybe it works, but I am using it wrong?\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 2.6.16.60-0.42.5-smp\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 8.1.2\r\nsetuptools: 27.2.0\r\nCython: 0.24.1\r\nnumpy: 1.11.1\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.2.2\r\nsphinx: 1.4.6\r\npatsy: 0.4.1\r\ndateutil: 2.5.3\r\npytz: 2016.6.1\r\nblosc: 1.4.4\r\nbottleneck: 1.1.0\r\ntables: 3.2.3.1\r\nnumexpr: 2.6.1\r\nmatplotlib: 1.5.3\r\nopenpyxl: 2.3.2\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.3\r\nlxml: 3.6.4\r\nbs4: 4.5.1\r\nhtml5lib: None\r\n 0.9.2\r\napiclient: None\r\nsqlalchemy: 1.0.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.42.0\r\npandas_datareader: None\r\n\r\n\r\n<\/details>\r\n",
    "labels":[
      "Bug",
      "Categorical",
      "Difficulty Novice",
      "Effort Low",
      "Msgpack"
    ],
    "comments":[
      "this is almost trivial to add, just add the import (or maybe should just look things up as ``getattr(pd, ....)`` rather than ``globals()[...]``.\r\n\r\nof course some tests would be good!"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":11,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":11879,
    "reporter":"kynnjo",
    "created_at":1450721184000,
    "closed_at":1488224677000,
    "resolver":"AlexisMignon",
    "resolved_in":"25dcff597162a12dbe419da2ae23d9b0d6322bee",
    "resolver_commit_num":0,
    "title":"UnicodeEncodeError from DataFrame.to_records",
    "body":"The `DataFrame.to_records` method fails with a `UnicodeEncodeError` for some unicode column names.\n\n(This issue is related to   The example below extends the example given in that issue.)\n\n\n",
    "labels":[
      "Bug",
      "Reshaping",
      "Unicode",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "you are referring to a VERY old issue FYI. Pls show `pd.show_versions()`. This a bug in any event so pull-requests are welcome.\n\nthis should be: `lmap(compat.text_type, self.columns)` I think\n",
      "If you can't be bothered to verify the code I posted, then just delete the issue.  I don't give a damn.\n",
      "@kynnjo I did repro right after you posted that's why I marked it as a bug\nI asked nicely to have you post the diagnostic. I even put what I think the fix is.\n\nwe don't appreciate rude behavior. please use respectful language.\n",
      "just delete the issue and we're done\n",
      "I actually find this a valid issue. thank you for reporting. don't you wish to see pandas improved and others helped?\n",
      "This works on current HEAD:\n\n```\nIn [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame({u'c\/\\u03c3':[1,2,3]})\n\nIn [3]: df\nOut[3]: \n   c\/\u03c3\n0    1\n1    2\n2    3\n\nIn [4]: df.to_records()\nOut[4]: \nrec.array([(0, 1), (1, 2), (2, 3)], \n          dtype=[('index', '<i8'), ('c\/\u03c3', '<i8')])\n```\n\nPlease consider closing.\n",
      "This fails in py2.\n\n```\nIn [1]: df = pandas.DataFrame({u'c\/\\u03c3':[1,2,3]})\n\nIn [2]: df.to_records()\n---------------------------------------------------------------------------\nUnicodeEncodeError                        Traceback (most recent call last)\n<ipython-input-2-6d3142e97d2d> in <module>()\n----> 1 df.to_records()\n\n\/Users\/jreback\/pandas\/pandas\/core\/frame.pyc in to_records(self, index, convert_datetime64)\n   1063             elif index_names[0] is None:\n   1064                 index_names = ['index']\n-> 1065             names = lmap(str, index_names) + lmap(str, self.columns)\n   1066         else:\n   1067             arrays = [self[c].get_values() for c in self.columns]\n\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\u03c3' in position 2: ordinal not in range(128)\n```\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":25,
    "deletions":5
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13937,
    "reporter":"johngu",
    "created_at":1470671110000,
    "closed_at":1488462298000,
    "resolver":"amolkahat",
    "resolved_in":"f000a4eac361737c6524ca2273c158e8d3b04ab2",
    "resolver_commit_num":0,
    "title":"BUG: DataFrame.to_records() bug in converting datetime64 index with timezone",
    "body":"#### Fix\n\nin to_records(), use pandas.core.common.is_datetime64_any_dtype instead of pandas.core.common.is_datetime64_dtype to check to see if the index is in fact of the datetime64 type.\n#### Code Sample, a copy-pastable example if possible\n\n\n#### Expected Output\n\ndata\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 3.5.2.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 15.6.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 20.3\nCython: 0.23.4\nnumpy: 1.10.4\nscipy: 0.17.0\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.1.2\nsphinx: 1.3.5\npatsy: 0.4.0\ndateutil: 2.5.1\npytz: 2016.2\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.5\nmatplotlib: 1.5.1\nopenpyxl: 2.3.2\nxlrd: 0.9.4\nxlwt: 1.0.0\nxlsxwriter: 0.8.4\nlxml: 3.6.0\nbs4: 4.4.1\nhtml5lib: None\n None\napiclient: None\nsqlalchemy: 1.0.12\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: 2.39.0\npandas_datareader: None\n",
    "labels":[
      "Bug",
      "Reshaping",
      "Timezones",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "note that this should return an object array with tz-aware objects (as numpy has no clu about non-naive datetimes).\n\ne.g. similar to this (though this is tz-naive)\n\n```\nIn [35]: df\nOut[35]: \nEmpty DataFrame\nColumns: []\nIndex: [2016-08-08 15:50:08.058674, 2016-08-08 15:50:08.058699, 2016-08-08 15:50:08.058713, 2016-08-08 15:50:08.058716, 2016-08-08 15:50:08.058719, 2016-08-08 15:50:08.058722, 2016-08-08 15:50:08.058725, 2016-08-08 15:50:08.058728, 2016-08-08 15:50:08.058732, 2016-08-08 15:50:08.058735]\n\nIn [36]: df.to_records()\nOut[36]: \nrec.array([(datetime.datetime(2016, 8, 8, 15, 50, 8, 58674),),\n (datetime.datetime(2016, 8, 8, 15, 50, 8, 58699),),\n (datetime.datetime(2016, 8, 8, 15, 50, 8, 58713),),\n (datetime.datetime(2016, 8, 8, 15, 50, 8, 58716),),\n (datetime.datetime(2016, 8, 8, 15, 50, 8, 58719),),\n (datetime.datetime(2016, 8, 8, 15, 50, 8, 58722),),\n (datetime.datetime(2016, 8, 8, 15, 50, 8, 58725),),\n (datetime.datetime(2016, 8, 8, 15, 50, 8, 58728),),\n (datetime.datetime(2016, 8, 8, 15, 50, 8, 58732),),\n (datetime.datetime(2016, 8, 8, 15, 50, 8, 58735),)], \n          dtype=[('datetime', 'O')])\n```\n\npull-requests welcome. I don't think this is highly tested. Most people don't really use numpy arrays directly once they need something (e.g. tz) that they don't provide.\n",
      "note this code has been changed quite a in master. But the fix is essentially the same .\n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "renamed",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files":3,
    "additions":21,
    "deletions":8
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":11666,
    "reporter":"gfairchild",
    "created_at":1448041515000,
    "closed_at":1489073104000,
    "resolver":"goldenbull",
    "resolved_in":"0cfc95055ca78ae0ba5189dd84f9319d175586a8",
    "resolver_commit_num":0,
    "title":"ENH: add gzip\/bz2 compression to read_pickle() (and perhaps other read_*() methods)",
    "body":"Right now, `read_csv()` has a `compression` option, which allows the user to pass in a gzipped or bz2-compressed CSV file directly into Pandas to be read. It would be great if `read_pickle()` supported the same option. Pickles actually compress surprisingly well; I have a 567M Pandas pickle (resulting from `DataFrame.to_pickle()`) that packs down to 45M with `pigz --best`. An order of magnitude difference in size is pretty significant. This makes storing static pickles long-term as gzipped archives a very attractive option. Workflow would be made easier if Pandas could natively handle my `dataframe.pickle.gz` files in the same way it does compressed CSV files.\n\nMore generally, a `compression` option should probably be allowed for most `read_*` methods. Many of the `read_*` methods involve formats that compress very well.\n",
    "labels":[
      "Enhancement",
      "Data IO",
      "Difficulty Novice",
      "Compat",
      "Effort Low"
    ],
    "comments":[
      "yeh, this wouldn't be hard for gzip\/bz2\n",
      "xref #5924 \n",
      "Yes please. Especially for `read_json`!\n",
      "I like xz\/lzma2 format for pickle format :smile: \n",
      "@goldenbull pull-requests are welcome! (this is not very difficult, more of a bit of code reorg to share the compression code)\n"
    ],
    "events":[
      "renamed",
      "renamed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files":6,
    "additions":324,
    "deletions":19
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13247,
    "reporter":"jennolsen84",
    "created_at":1463808857000,
    "closed_at":1489498438000,
    "resolver":"jaehoonhwang",
    "resolved_in":"7d34d4d5c2d2c6c68b4124076571cfab9c3b4aee",
    "resolver_commit_num":0,
    "title":"BUG: upcasting on reshaping ops",
    "body":"#### Code Sample, a copy-pastable example if possible\n\n\n#### Current Output\n\n\n#### Expected Output\n\n\n#### output of `pd.show_versions()`\n\n\n",
    "labels":[
      "Bug",
      "Reshaping",
      "Dtypes",
      "Missing-data",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments":[
      "Thanks for the report. Though `Panel` is being replaced with `xarray` near future, sending a PR is appreciated.\n",
      "@sinhrks same behavior with `DataFrame`, is that going away too?\n",
      "```\nIn [7]: pn1 = pd.DataFrame(np.array([1.0], dtype=np.float32, ndmin=2)) # df of 1.0\n\nIn [8]: pn2 = pd.DataFrame(np.array([np.nan], dtype=np.float32, ndmin=2)) # df of nan\n\nIn [9]: print(pd.concat([pn2, pn2]).values.dtype)\nfloat64\n\nIn [10]: print(pd.concat([pn1, pn1]).values.dtype)\nfloat32\n```\n",
      "yeah, I suspect the null forces immediate upcast to `float64`. This should be cognizant that floats are available in their native itemsize even w\/NaN's (and so should take a size that can deal).\n\nI don't think this is very hard to fix in a general way.\n\ncare to take a stab?\n",
      "@jreback how's this for starters?  https:\/\/github.com\/jennolsen84\/pandas\/commit\/3b3797af12dcfa8608a904df8d65454e6904d0ce .  I can add the tests, whatsnew, etc. if it looks good to you.\n\nI did some tests and it seems to work for me for float types.  I did a quick check with ints, and that seemed to work just fine as well (I am guessing it just uses np rules there).\n\nI also checked concating int32 + float16, and in that case we use float64.  In that case, one might want a float64, so I left that alone.\n",
      "@jennolsen84 some comments. Ideally make this as general as possible, but the type stuff is a bit all over the place now #13147 should fix this a bit more (though that independent of this)\n"
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "renamed",
      "referenced",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files":6,
    "additions":35,
    "deletions":6
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":15662,
    "reporter":"zym1010",
    "created_at":1489332688000,
    "closed_at":1489584635000,
    "resolver":"zym1010",
    "resolved_in":"76e5185a5ad07672688b096acc94ad5a8a2ec18d",
    "resolver_commit_num":0,
    "title":"Pandas test fails with Scipy 0.19",
    "body":"#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nthere are some errors w.r.t. interpolation related functions. Seems that this is related to the change of behavior for some Scipy functions in 0.19. But which is correct?\r\n\r\n~~~\r\n======================================================================\r\nFAIL: test_interp_various (pandas.tests.frame.test_missing.TestDataFrameInterpolate)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"\/Users\/yimengzh\/miniconda2\/envs\/default35\/lib\/python3.5\/site-packages\/pandas\/tests\/frame\/test_missing.py\", line 513, in test_interp_various\r\n    assert_frame_equal(result, expected)\r\n  File \"\/Users\/yimengzh\/miniconda2\/envs\/default35\/lib\/python3.5\/site-packages\/pandas\/util\/testing.py\", line 1313, in assert_frame_equal\r\n    obj='DataFrame.iloc[:, {0}]'.format(i))\r\n  File \"\/Users\/yimengzh\/miniconda2\/envs\/default35\/lib\/python3.5\/site-packages\/pandas\/util\/testing.py\", line 1181, in assert_series_equal\r\n    obj='{0}'.format(obj))\r\n  File \"pandas\/src\/testing.pyx\", line 59, in pandas._testing.assert_almost_equal (pandas\/src\/testing.c:4156)\r\n  File \"pandas\/src\/testing.pyx\", line 173, in pandas._testing.assert_almost_equal (pandas\/src\/testing.c:3274)\r\n  File \"\/Users\/yimengzh\/miniconda2\/envs\/default35\/lib\/python3.5\/site-packages\/pandas\/util\/testing.py\", line 1018, in raise_assert_detail\r\n    raise AssertionError(msg)\r\nAssertionError: DataFrame.iloc[:, 0] are different\r\n\r\nDataFrame.iloc[:, 0] values are different (28.57143 %)\r\n[left]:  [1.0, 2.0, 2.81547781415, 4.0, 5.0, 5.52964175305, 7.0]\r\n[right]: [1.0, 2.0, 2.81621174, 4.0, 5.0, 5.64146581, 7.0]\r\n\r\n======================================================================\r\nFAIL: test_interp_scipy_basic (pandas.tests.series.test_missing.TestSeriesInterpolateData)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"\/Users\/yimengzh\/miniconda2\/envs\/default35\/lib\/python3.5\/site-packages\/pandas\/tests\/series\/test_missing.py\", line 711, in test_interp_scipy_basic\r\n    assert_series_equal(result, expected)\r\n  File \"\/Users\/yimengzh\/miniconda2\/envs\/default35\/lib\/python3.5\/site-packages\/pandas\/util\/testing.py\", line 1181, in assert_series_equal\r\n    obj='{0}'.format(obj))\r\n  File \"pandas\/src\/testing.pyx\", line 59, in pandas._testing.assert_almost_equal (pandas\/src\/testing.c:4156)\r\n  File \"pandas\/src\/testing.pyx\", line 173, in pandas._testing.assert_almost_equal (pandas\/src\/testing.c:3274)\r\n  File \"\/Users\/yimengzh\/miniconda2\/envs\/default35\/lib\/python3.5\/site-packages\/pandas\/util\/testing.py\", line 1018, in raise_assert_detail\r\n    raise AssertionError(msg)\r\nAssertionError: Series are different\r\n\r\nSeries values are different (33.33333 %)\r\n[left]:  [1.0, 3.0, 6.82352941176, 12.0, 18.0588235294, 25.0]\r\n[right]: [1.0, 3.0, 6.769231, 12.0, 18.230769, 25.0]\r\n\r\n======================================================================\r\nFAIL: test_cmov_window_regular (pandas.tests.test_window.TestMoments)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"\/Users\/yimengzh\/miniconda2\/envs\/default35\/lib\/python3.5\/site-packages\/pandas\/tests\/test_window.py\", line 878, in test_cmov_window_regular\r\n    tm.assert_series_equal(xp, rs)\r\n  File \"\/Users\/yimengzh\/miniconda2\/envs\/default35\/lib\/python3.5\/site-packages\/pandas\/util\/testing.py\", line 1181, in assert_series_equal\r\n    obj='{0}'.format(obj))\r\n  File \"pandas\/src\/testing.pyx\", line 59, in pandas._testing.assert_almost_equal (pandas\/src\/testing.c:4156)\r\n  File \"pandas\/src\/testing.pyx\", line 173, in pandas._testing.assert_almost_equal (pandas\/src\/testing.c:3274)\r\n  File \"\/Users\/yimengzh\/miniconda2\/envs\/default35\/lib\/python3.5\/site-packages\/pandas\/util\/testing.py\", line 1018, in raise_assert_detail\r\n    raise AssertionError(msg)\r\nAssertionError: Series are different\r\n\r\nSeries values are different (60.0 %)\r\n[left]:  [nan, nan, 9.28667, 10.34667, 12.00556, 13.33889, 13.38, 12.33667, nan, nan]\r\n[right]: [nan, nan, 9.60058823529, 10.8523529412, 12.86, 13.52, 12.7629411765, 12.2070588235, nan, nan]\r\n\r\n======================================================================\r\nFAIL: test_cmov_window_regular_linear_range (pandas.tests.test_window.TestMoments)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"\/Users\/yimengzh\/miniconda2\/envs\/default35\/lib\/python3.5\/site-packages\/pandas\/tests\/test_window.py\", line 895, in test_cmov_window_regular_linear_range\r\n    tm.assert_series_equal(xp, rs)\r\n  File \"\/Users\/yimengzh\/miniconda2\/envs\/default35\/lib\/python3.5\/site-packages\/pandas\/util\/testing.py\", line 1181, in assert_series_equal\r\n    obj='{0}'.format(obj))\r\n  File \"pandas\/src\/testing.pyx\", line 59, in pandas._testing.assert_almost_equal (pandas\/src\/testing.c:4156)\r\n  File \"pandas\/src\/testing.pyx\", line 173, in pandas._testing.assert_almost_equal (pandas\/src\/testing.c:3274)\r\n  File \"\/Users\/yimengzh\/miniconda2\/envs\/default35\/lib\/python3.5\/site-packages\/pandas\/util\/testing.py\", line 1018, in raise_assert_detail\r\n    raise AssertionError(msg)\r\nAssertionError: Series are different\r\n\r\nSeries values are different (60.0 %)\r\n[left]:  [nan, nan, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, nan, nan]\r\n[right]: [nan, nan, 2.35294117647, 3.35294117647, 4.35294117647, 5.35294117647, 6.35294117647, 7.35294117647, nan, nan]\r\n\r\n======================================================================\r\nFAIL: test_cmov_window_regular_missing_data (pandas.tests.test_window.TestMoments)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"\/Users\/yimengzh\/miniconda2\/envs\/default35\/lib\/python3.5\/site-packages\/pandas\/tests\/test_window.py\", line 928, in test_cmov_window_regular_missing_data\r\n    tm.assert_series_equal(xp, rs)\r\n  File \"\/Users\/yimengzh\/miniconda2\/envs\/default35\/lib\/python3.5\/site-packages\/pandas\/util\/testing.py\", line 1181, in assert_series_equal\r\n    obj='{0}'.format(obj))\r\n  File \"pandas\/src\/testing.pyx\", line 59, in pandas._testing.assert_almost_equal (pandas\/src\/testing.c:4156)\r\n  File \"pandas\/src\/testing.pyx\", line 173, in pandas._testing.assert_almost_equal (pandas\/src\/testing.c:3274)\r\n  File \"\/Users\/yimengzh\/miniconda2\/envs\/default35\/lib\/python3.5\/site-packages\/pandas\/util\/testing.py\", line 1018, in raise_assert_detail\r\n    raise AssertionError(msg)\r\nAssertionError: Series are different\r\n\r\nSeries values are different (80.0 %)\r\n[left]:  [nan, nan, 9.33167, 9.76125, 9.28667, 10.34667, 12.00556, 13.82125, 14.49429, 13.765]\r\n[right]: [nan, nan, 9.61230769231, 9.24125, 9.60058823529, 10.8523529412, 12.86, 14.3857142857, 14.1308333333, 13.3433333333]\r\n\r\n======================================================================\r\nFAIL: test_cmov_window_special (pandas.tests.test_window.TestMoments)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"\/Users\/yimengzh\/miniconda2\/envs\/default35\/lib\/python3.5\/site-packages\/pandas\/tests\/test_window.py\", line 955, in test_cmov_window_special\r\n    tm.assert_series_equal(xp, rs)\r\n  File \"\/Users\/yimengzh\/miniconda2\/envs\/default35\/lib\/python3.5\/site-packages\/pandas\/util\/testing.py\", line 1181, in assert_series_equal\r\n    obj='{0}'.format(obj))\r\n  File \"pandas\/src\/testing.pyx\", line 59, in pandas._testing.assert_almost_equal (pandas\/src\/testing.c:4156)\r\n  File \"pandas\/src\/testing.pyx\", line 173, in pandas._testing.assert_almost_equal (pandas\/src\/testing.c:3274)\r\n  File \"\/Users\/yimengzh\/miniconda2\/envs\/default35\/lib\/python3.5\/site-packages\/pandas\/util\/testing.py\", line 1018, in raise_assert_detail\r\n    raise AssertionError(msg)\r\nAssertionError: Series are different\r\n\r\nSeries values are different (60.0 %)\r\n[left]:  [nan, nan, 9.86851, 11.02969, 11.65161, 12.75129, 12.90702, 12.83757, nan, nan]\r\n[right]: [nan, nan, 9.95592026795, 11.1636020642, 11.8220886145, 12.6904154014, 12.797644924, 12.8491264878, nan, nan]\r\n\r\n======================================================================\r\nFAIL: test_cmov_window_special_linear_range (pandas.tests.test_window.TestMoments)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"\/Users\/yimengzh\/miniconda2\/envs\/default35\/lib\/python3.5\/site-packages\/pandas\/tests\/test_window.py\", line 973, in test_cmov_window_special_linear_range\r\n    tm.assert_series_equal(xp, rs)\r\n  File \"\/Users\/yimengzh\/miniconda2\/envs\/default35\/lib\/python3.5\/site-packages\/pandas\/util\/testing.py\", line 1181, in assert_series_equal\r\n    obj='{0}'.format(obj))\r\n  File \"pandas\/src\/testing.pyx\", line 59, in pandas._testing.assert_almost_equal (pandas\/src\/testing.c:4156)\r\n  File \"pandas\/src\/testing.pyx\", line 173, in pandas._testing.assert_almost_equal (pandas\/src\/testing.c:3274)\r\n  File \"\/Users\/yimengzh\/miniconda2\/envs\/default35\/lib\/python3.5\/site-packages\/pandas\/util\/testing.py\", line 1018, in raise_assert_detail\r\n    raise AssertionError(msg)\r\nAssertionError: Series are different\r\n\r\nSeries values are different (60.0 %)\r\n[left]:  [nan, nan, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, nan, nan]\r\n[right]: [nan, nan, 2.07210900201, 3.07210900201, 4.07210900201, 5.07210900201, 6.07210900201, 7.07210900201, nan, nan]\r\n\r\n----------------------------------------------------------------------\r\nRan 10252 tests in 287.945s\r\n\r\nFAILED (SKIP=627, failures=7)\r\n~~~\r\n\r\n#### Expected Output\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 15.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 8.1.2\r\nsetuptools: 26.1.1.post20160901\r\nCython: None\r\nnumpy: 1.11.3\r\nscipy: 0.19.0\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.3\r\npytz: 2016.6.1\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 1.5.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: None\r\npandas_datareader: None\r\n<\/details>\r\n",
    "labels":[
      "Numeric",
      "Compat",
      "Testing"
    ],
    "comments":[
      "well, since scipy 0.19.0 was just released and something I imagine changed I am not surprised. Would you have a look and see what changed and\/or provide a PR to fix? (keeping in mind that we are back-compat thru scipy 0.13)",
      "further scipy is not on the default conda channels as of yet, so this doesn't hit our tests yet.",
      "@jreback I will have a look at it later this week.",
      "@jreback after some digging, I think those `cmov_window` errors should be due to <https:\/\/github.com\/scipy\/scipy\/pull\/6483>, and those `interp` errors should be due to change of implementation for quadratic and cubic interpolation.\r\n\r\nTo fix the first one, it should do to pass an additional argument `False` to <https:\/\/github.com\/pandas-dev\/pandas\/blob\/648ae4f03622d8eafe1ca3b833bd6a99f56bece4\/pandas\/core\/window.py#L547>. For the second, no idea.",
      "@zym1010 for 1st one: so looks like they added an argument. Were we just ignoring this before? (so its now required). did they give *any* warning? is it back-compat to prior versions? (if so, let's just add the arg, no big deal).\r\n\r\nFor the 2nd just add in a conditional in the tests:\r\n\r\n```\r\nif scipy.__version__ >= LooseVersion('0.19.0'):\r\n    # this results\r\nelse:\r\n    # old results\r\n```\r\nWe do this in a couple of other places as well for scipy results.\r\n\r\ncan you do a PR? (note that we are not actually testing with 0.19.0 yet as we don't have a fully conda-forge build for a couple of reasons), but I think I can change one of ours to do that.\r\n\r\n",
      "@jreback for 1st one: the argument is always there, and it's nullified if the window length is odd in previous versions. Now it's not the case. So in some sense I would say this is a bug on pandas' side. But yeah, they gave no warning on this nullifying behavior, and I don't think this change is in their release notes.\r\n\r\nfor second one. Alright. I will take your approach. BTW, the change is due to their spline generator (previously they used `splmake`, now not). See <https:\/\/github.com\/scipy\/scipy\/issues\/6710>.\r\n\r\n> can you do a PR?\r\n\r\nNo problem. Should be done by midnight today when I'm more or less done with other work.",
      "@zym1010 awesome! I will put in place a PR to have testing on conda-forge (so we use 0.19.0 for scipy soon). It will fail master, but hopefully your PR should fix!.",
      "ok, merged #15668 \r\n\r\nchange the skipping on scipy 0.19.0 when you push.",
      "@jreback can you have a look at #15689? I'm not sure if I \"change the skipping on scipy 0.19.0\" correctly."
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed"
    ],
    "changed_files":5,
    "additions":46,
    "deletions":18
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":15605,
    "reporter":"memilanuk",
    "created_at":1488912359000,
    "closed_at":1489765346000,
    "resolver":"lorenzocestaro",
    "resolved_in":"0ad89761df376d52eaee90b52b9b15eb0f06af54",
    "resolver_commit_num":0,
    "title":"Broken link in cookbook for read_csv",
    "body":"The second link in the section -docs\/stable\/cookbook.html#csv\r\nis broken.\r\n\r\nI.e. the link [read_csv in action](?p=635) is supposed to go to Wes McKinney's blog, but returns a 404 error.  \r\n\r\nA cursory look at said blog didn't result in anything jumping out at me as far as what *should* be the correct entry to link to.\r\n",
    "labels":[
      "Docs",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "@wesm did this change? ``http:\/\/wesmckinney.com\/blog\/?p=635``",
      "Yeah, i migrated to pelican. think tha tlink may be http:\/\/wesmckinney.com\/blog\/update-on-upcoming-pandas-v0-10-new-file-parser-other-performance-wins\/",
      "Hi guys, this is my first attempt to contribute to an open source project. I have a branch ready with the fix: I changed the broken link with the one provided by @wesm.\r\n[This](https:\/\/github.com\/LorenzoCestaro\/pandas\/commit\/006eefa210fed693030a7fa50374441ef7ee90bf) is the commit with the change on my fork.\r\n\r\nI'd like to know if it is ok if I go ahead and submit a PR, also let me know if there is something else I should do before opening the PR (tests, builds, ...).\r\nThanks!",
      "nope, go ahead an make a PR"
    ],
    "events":[
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "commented",
      "labeled",
      "labeled",
      "referenced",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files":1,
    "additions":1,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":14372,
    "reporter":"rahulporuri",
    "created_at":1475823948000,
    "closed_at":1490017634000,
    "resolver":"pankajp",
    "resolved_in":"b1e29dba26ff86b826fe0f866182466ae42c0bc5",
    "resolver_commit_num":0,
    "title":"BUG : Check for and Use QApplication instance : clipboard",
    "body":"reading data from clipboard fails on Linux because pandas doesn't check if there's already a `QApplication` running - #L247\n\n\n#### Output of `pd.show_versions()`\n\n<details>\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.4.0-31-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_IN\n\npandas: 0.18.0\nnose: 1.3.7\npip: 8.1.2\nsetuptools: 23.1.0\nCython: 0.24\nnumpy: 1.10.4\nscipy: 0.17.1\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.1.2\nsphinx: 1.4.1\npatsy: 0.4.1\ndateutil: 2.5.2\npytz: 2016.3\nblosc: None\nbottleneck: None\ntables: 3.2.2\nnumexpr: 2.6.0\nmatplotlib: 1.5.1\nopenpyxl: 2.3.4\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: 3.6.0\nbs4: 4.4.1\nhtml5lib: 0.999\n None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: 2.8\nboto: None\n<\/details>\n",
    "labels":[
      "Data IO"
    ],
    "comments":[

    ],
    "events":[
      "cross-referenced",
      "labeled",
      "referenced",
      "cross-referenced",
      "milestoned"
    ],
    "changed_files":2,
    "additions":3,
    "deletions":2
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":15764,
    "reporter":"jreback",
    "created_at":1490100634000,
    "closed_at":1490183832000,
    "resolver":"StanczakDominik",
    "resolved_in":"2a3b05a3a7167c7b384375e9442c350f740e9629",
    "resolver_commit_num":0,
    "title":"CLN\/INT: rename *possibly* -> *maybe* functions",
    "body":"we have a mixed bag on naming this. Let's change ``possibly`` -> ``maybe``. Keep everything else the same. (if you do the same search for ``maybe`` you will fine it to be far more common). Note that the *usage* of these functions will have to be renamed as well :>\r\n\r\n",
    "labels":[
      "Clean",
      "Difficulty Novice",
      "Effort Medium",
      "Internals"
    ],
    "comments":[
      "cc @gfyoung ",
      "Seem reasonable.",
      "also (separately), we should rename all of the functions in ``pandas.types.cast`` to remove the leading ``_``. Just for some readability (they were like this because they were explicity private when I ported from ``pandas.core.common``). ``pandas.types`` is private so this is not big deal now.",
      "Figured I would take a stab at this one."
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "commented",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":22,
    "additions":228,
    "deletions":231
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13434,
    "reporter":"tdszyman",
    "created_at":1465824244000,
    "closed_at":1490830103000,
    "resolver":"brianhuey",
    "resolved_in":"0ab081345eb191937fd4152eba48b8c9692b02bf",
    "resolver_commit_num":0,
    "title":"read_html() doesn't handle tables with multiple header rows ",
    "body":"The `read_html()` function seems to treat every `<th>` in a table as a column, even if they occur in separate `<tr>`s. This means that it breaks even on simple tables generated by pandas' `to_html()` function.\n#### Code Sample, a copy-pastable example if possible\n\n\n\nThis is the value of `html`, generated by the `to_html()` function on the original data frame:\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>Age<\/th>\n      <th>Party<\/th>\n    <\/tr>\n    <tr>\n      <th>Name<\/th>\n      <th><\/th>\n      <th><\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>Hillary<\/th>\n      <td>68<\/td>\n      <td>D<\/td>\n    <\/tr>\n    <tr>\n      <th>Bernie<\/th>\n      <td>74<\/td>\n      <td>D<\/td>\n    <\/tr>\n    <tr>\n      <th>Donald<\/th>\n      <td>69<\/td>\n      <td>R<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>\n\n\n\n\nAnd this is the printed output of the newly-parsed dataframe `df2`:\n\n\n\nWhat happens is that the `to_html()` function produces an html table with two header rows, one for the column names and one with the index name. However the `read_html()` parser interprets each individual `th` cell as an expected column, resulting in twice the number of columns. Even worse, this produces a column with the same name as the original index but without any data. \n#### Expected Output\n\nThe `read_html` parser could either treat the multi-row header fully correctly:\n\n\n\nOr it could just ignore any rows after the first one:\n\n\n#### output of `pd.show_versions()`\n## INSTALLED VERSIONS\n\ncommit: None\npython: 2.7.11.final.0\npython-bits: 64\nOS: Darwin\nOS-release: 14.5.0\nmachine: x86_64\nprocessor: i386\nbyteorder: little\nLC_ALL: None\nLANG: en_IE.UTF-8\n\npandas: 0.18.1\nnose: 1.3.7\npip: 8.1.1\nsetuptools: 21.0.0\nCython: 0.23.4\nnumpy: 1.11.0\nscipy: 0.17.0\nstatsmodels: 0.6.1\nxarray: None\nIPython: 4.1.1\nsphinx: 1.3.5\npatsy: 0.4.1\ndateutil: 2.5.3\npytz: 2016.4\nblosc: None\nbottleneck: 1.0.0\ntables: 3.2.2\nnumexpr: 2.5\nmatplotlib: 1.5.1\nopenpyxl: 2.1.2\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: 3.3.5\nbs4: 4.4.1\nhtml5lib: 1.0b3\n 0.9.2\napiclient: None\nsqlalchemy: 1.0.11\npymysql: None\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\njinja2: 2.8\nboto: 2.40.0\npandas_datareader: None\n",
    "labels":[
      "IO HTML",
      "Difficulty Intermediate",
      "Effort Medium",
      "Enhancement"
    ],
    "comments":[
      "Correct me if I'm wrong here... would you be able to differentiate between HTML where the first row really is two blank strings, and a table with a header spanning multiple rows? My thoughts are `read_html` are that the user should expect to have a bit of cleanup work to do. But if the change to handle this case doesn't break anything and isn't too complicated, I'd say it'd be a good addition.\n",
      "@TomAugspurger the case I'm thinking of is where the first two rows are in the `<thead>` part of the `<table>`, and the other rows are in the `<tbody>` part. So yes they can clearly be distinguished from a row that is simply empty. Also, in the example I gave, every single `<tr>` element contains the same number of cells\/columns (whether they are `<th>` or `<td>`), so there is no reason to generate a data frame with a different number of columns.\n"
    ],
    "events":[
      "renamed",
      "commented",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":4,
    "additions":43,
    "deletions":20
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":15864,
    "reporter":"funnycrab",
    "created_at":1491109251000,
    "closed_at":1491142692000,
    "resolver":"funnycrab",
    "resolved_in":"7059d898511a62710d6bd6487c8b40d7f535c1a1",
    "resolver_commit_num":1,
    "title":"another to_json float precision bug",
    "body":"#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nThis issue is very similar to [#15716](-dev\/pandas\/issues\/15716). \r\n\r\nJust this one  reveals a more generalized pattern. \r\n\r\n#### Expected Output\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.19.0-42-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_HK.UTF-8\r\nLOCALE: en_HK.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: 1.3.7\r\npip: 8.1.2\r\nsetuptools: 27.2.0\r\nCython: 0.24.1\r\nnumpy: 1.11.3\r\nscipy: 0.18.1\r\nstatsmodels: 0.6.1\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: 1.4.6\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2016.6.1\r\nblosc: None\r\nbottleneck: 1.1.0\r\ntables: 3.2.3.1\r\nnumexpr: 2.6.2\r\nmatplotlib: 1.5.3\r\nopenpyxl: 2.3.2\r\nxlrd: 1.0.0\r\nxlwt: 1.1.2\r\nxlsxwriter: 0.9.3\r\nlxml: 3.6.4\r\nbs4: 4.5.1\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: 1.0.13\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.8\r\nboto: 2.42.0\r\npandas_datareader: None\r\n<\/details>\r\n",
    "labels":[

    ],
    "comments":[
      "After a quick check, I believe this issue may not be related to [here](https:\/\/github.com\/pandas-dev\/pandas\/blob\/61f6f6333fb7bb2dedf82736aee6c9878382a06f\/pandas\/_libs\/src\/ujson\/python\/objToJSON.c#L2368) as pointed out in the [#15716](https:\/\/github.com\/pandas-dev\/pandas\/issues\/15716).\r\n\r\nIt seems to have something to do with rollover handling implemented [here](https:\/\/github.com\/pandas-dev\/pandas\/blob\/master\/pandas\/_libs\/src\/ujson\/lib\/ultrajsonenc.c#L826).\r\n\r\nA [pull request](https:\/\/github.com\/pandas-dev\/pandas\/pull\/15865) has been submitted for fixing this issue. Please be so kind as to take a look.",
      "these seem almost the same, simply roll them together."
    ],
    "events":[
      "referenced",
      "cross-referenced",
      "commented"
    ],
    "changed_files":4,
    "additions":75,
    "deletions":5
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":14855,
    "reporter":"smartinsightsfromdata",
    "created_at":1481419823000,
    "closed_at":1491428854000,
    "resolver":"alexandercbooth",
    "resolved_in":"ba30e3a2e376035549b009079d44ba5ca7a4c48f",
    "resolver_commit_num":0,
    "title":"scatter_matrix `color` vs `c`",
    "body":"I reported the issue below to matplotlib [here](#issuecomment-266250371), but they have requested to report it here as it appears (to the matplotlib folks) related to pandas.\r\n\r\nThe following code gives error, but works replacing `color=` with `c=`\r\n\r\nI suspect is either a bug, or some piece of documentation missing?\r\n(matplotlib folks think it is a pandas \/ scatter_matrix bug - see below)\r\n\r\nI'm using:\r\nmatplotlib 1.5.3\r\npandas 0.19.1\r\npython 3.5\r\n\r\nThis is a sample code:\r\n\r\nThis is the error trace:\r\n\r\nAccording to @goyodiaz:\r\n\r\n> It looks like a pandas issue. pandas.scatter_matrix always pass c to scatter but the caller can optionally pass more keywords arguments, including color. If the caller pass only color then scatter_matrix will pass both c and color and matplotlib will raise with a message that is confusing to the user because they did not pass c. Probably scatter_matrix should either raise with a clearer message or drop the c keyword argument when the user pass color\r\n\r\n.\r\n",
    "labels":[
      "Bug",
      "Difficulty Novice",
      "Effort Low",
      "Visualization"
    ],
    "comments":[
      "Yes, this can indeed be solved on the pandas side as @goyodiaz indicated. We can just check for the user passing `color` as a kwarg and in that case let it replace the default `c` (and docs can maybe also use an update). \r\nThis should be an easy fix.\r\n"
    ],
    "events":[
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":10,
    "deletions":5
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":15912,
    "reporter":"mp-v2",
    "created_at":1491429288000,
    "closed_at":1491464145000,
    "resolver":"nrebena",
    "resolved_in":"3f02e6331fd4b17f2d12d3cd546e0d555ea40cd4",
    "resolver_commit_num":0,
    "title":"Pandas dataframe + groupby = failed zooming for x-axis ticks",
    "body":"This was posted onto stackoverflow but it seems more like a bug than anything:\r\n\r\n-dataframe-groupby-failed-zooming-for-x-axis-ticks\r\n\r\n\r\n",
    "labels":[

    ],
    "comments":[
      "Smaller reproducible example:\r\n\r\n```\r\nIn [43]: df = pd.DataFrame(np.random.randn(4, 2), columns=['A', 'B'], index=pd.MultiIndex.from_product([[2012, 2013], [1,2]]))\r\n\r\nIn [44]: df\r\nOut[44]: \r\n               A         B\r\n2012 1  0.144222  0.895911\r\n     2  0.885869  0.075482\r\n2013 1 -0.312316  0.705149\r\n     2 -0.974501  1.443551\r\n\r\nIn [45]: df.plot()\r\nOut[45]: <matplotlib.axes._subplots.AxesSubplot at 0x7f3e6a262ef0>\r\n```\r\n\r\nThe xtick label formatter cannot really handle a MultiIndex, as once you zoom in, the labels are put at the wrong place.\r\n\r\nBut the same is true when eg having a string index. So just a non-numerical \/ non-datetime index has this issue.",
      "This is a duplicate of https:\/\/github.com\/pandas-dev\/pandas\/issues\/7612"
    ],
    "events":[
      "commented"
    ],
    "changed_files":3,
    "additions":45,
    "deletions":3
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":15817,
    "reporter":"mchwalisz",
    "created_at":1490613055000,
    "closed_at":1491688445000,
    "resolver":"mchwalisz",
    "resolved_in":"860d555a9d27958b151412527034fddffb446b31",
    "resolver_commit_num":0,
    "title":"to_datetime() ns precision inconsistencies between s and ns",
    "body":"#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\nOutput\r\n\r\n\r\n\r\n#### Problem description\r\nI would expect that all of the following methods would give the same result. In the [documentation](-docs\/stable\/timeseries.html#epoch-timestamps) that the epoch times will be rounded to the nearest nanosecond. I don't understand why conversion from float using seconds `s` unit gives different rounding than nano seconds `ns`. Not to mention strange float parsing coming from csv.\r\n\r\nI expect the whole issue to be connected to #7307.\r\n\r\n#### Expected Output\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.12.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.10-040910-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 34.3.2\r\nCython: 0.25.2\r\nnumpy: 1.10.4\r\nscipy: 0.19.0\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.3.0\r\nsphinx: 1.4.8\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.3.0\r\nnumexpr: 2.6.2\r\nmatplotlib: 2.0.0\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: 4.5.3\r\nhtml5lib: 0.999999999\r\n 0.9.1\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.5\r\nboto: None\r\npandas_datareader: None\r\n<\/details>\r\n",
    "labels":[
      "Usage Question",
      "Docs",
      "Timeseries"
    ],
    "comments":[
      "This is not really related to #7307, but more with the (possible) precision of floats. \r\n\r\nThe float \"1490195805.433502912\" is too large \/ has too high precision to be represented faithfully as a floating point. Therefore, when converting this float to a datetime64 (which representation is based on integers) you get a conversion error.\r\n\r\n```\r\nIn [11]: 1490195805.433502912\r\nOut[11]: 1490195805.433503\r\n\r\nIn [12]: 1490195805433502912\r\nOut[12]: 1490195805433502912\r\n\r\nIn [14]: pd.to_datetime(1490195805.433502912, unit='s') \r\nOut[14]: Timestamp('2017-03-22 15:16:45.433503') \r\n\r\nIn [15]: pd.to_datetime(1490195805433502912, unit='ns')  \r\nOut[15]: Timestamp('2017-03-22 15:16:45.433502912')  \r\n```",
      "How do you explain the following then?\r\n\r\n```\r\ndf['from float^9 to ns'] = pd.to_datetime(df['full'] * 10**9, unit='ns')\r\n\r\nfrom float^9 to ns     2017-03-22 15:16:45.433502720\r\n```\r\n\r\n",
      "A string needs to be parsed to an actual floating point, and also here differences can occur based on which implementation you use:\r\n\r\n```\r\nIn [25]: 1490195805.433502912\r\nOut[25]: 1490195805.433503\r\n\r\nIn [26]: float('1490195805.433502912')\r\nOut[26]: 1490195805.433503\r\n\r\nIn [27]: pd.read_csv(StringIO(\"a\\n1490195805.433502912\")).iloc[0,0]\r\nOut[27]: 1490195805.4335027\r\n\r\nIn [28]: pd.read_csv(StringIO(\"a\\n1490195805.433502912\"), float_precision='high').iloc[0,0]\r\nOut[28]: 1490195805.4335029\r\n```\r\n\r\nAnd additionally, also floating point arithmetic will give small differences, a float * 10^9 does not necessarily result in the same number as combining both separate parts as integers",
      "@mchwalisz \r\n\r\nfurthermore, python floats are unbounded precision, while this is casted to a ``float64`` which has high (though limited precision), about 15 significant digits. rounding is unavoidable. The only way to have exact precision is to use a fixed-width exact type (e.g. an int64). This is why we store the datetimes as int64. You can round-trip them exactly even via text this way.\r\n\r\n```\r\nIn [6]: pd.to_datetime(1490195805.4335027 * 10**9, unit='ns').value\r\nOut[6]: 1490195805433502720\r\n\r\nIn [7]: pd.to_datetime(1490195805.4335029 * 10**9, unit='ns').value\r\nOut[7]: 1490195805433502976\r\n\r\nIn [8]: 1490195805.4335027 * 10**9\r\nOut[8]: 1.4901958054335027e+18\r\n\r\nIn [9]: 1490195805.4335029 * 10**9\r\nOut[9]: 1.490195805433503e+18\r\n```\r\n",
      "@mchwalisz happy to have a short section in ``timeseries.rst`` about this if you want \/ possibly in ``.to_datetime`` doc-string as well. not sure where else to put any docs about this. ",
      "A string needs to be parsed to an actual floating point, and also here differences can occur based on which implementation you use:\r\n\r\n```\r\nIn [25]: 1490195805.433502912\r\nOut[25]: 1490195805.433503\r\n\r\nIn [26]: float('1490195805.433502912')\r\nOut[26]: 1490195805.433503\r\n\r\nIn [27]: pd.read_csv(StringIO(\"a\\n1490195805.433502912\")).iloc[0,0]\r\nOut[27]: 1490195805.4335027\r\n\r\nIn [28]: pd.read_csv(StringIO(\"a\\n1490195805.433502912\"), float_precision='high').iloc[0,0]\r\nOut[28]: 1490195805.4335029\r\n```\r\n\r\nAnd additionally, also floating point arithmetic will give small differences, a float * 10^9 does not necessarily result in the same number as combining both separate parts as integers.\r\n\r\nThis is one of the reasons the timestamps are stored as integers.\r\n\r\nIf you want to read more information about this issue, you can see http:\/\/floating-point-gui.de\/"
    ],
    "events":[
      "commented",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "labeled",
      "closed",
      "reopened",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files":2,
    "additions":32,
    "deletions":6
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":13965,
    "reporter":"chrisaycock",
    "created_at":1470930649000,
    "closed_at":1492083554000,
    "resolver":"carlosdanielcsantos",
    "resolved_in":"73222392f389f918272a9d96c5f623f0b13966eb",
    "resolver_commit_num":0,
    "title":"Exact matches in time-based .rolling()",
    "body":"I would like to `allow_exact_matches` in time-based `.rolling()`. Using an example similar to the one found in the documentation:\n\n\n\nI can compute the three-second rolling sum:\n\n\n\nBut note that `09:00:03` does not include `09:00:00`, `09:00:05` does not include `09:00:02`, and `09:00:06` does not include `09:00:03`. If I were to include these timestamps, then I would have:\n\n\n\nA quick-and-dirty way of getting these values is with:\n\n\n\n`pd.merge_asof()` has the parameter `allow_exact_matches` to permit this. Would it be possible to add this to `.rolling()`?\n\nI can try this one myself.\n",
    "labels":[
      "Enhancement",
      "Resample",
      "Reshaping",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments":[
      "For similarity with resampling, would this make sense as a parameter `closed={'right', 'left', 'both'}`, where IIUC, `'right'` is the default and `'both'` is the option you're adding?\n",
      "Although `closed='left'` maybe doesn't make any sense.\n",
      "@chris-b1 The choice is rather binary: either we want all timestamps _strictly greater than_ the lowest bound, or we want all timestamps _greater than or equal to_ the lowest bound. `.rolling()` always defines the last row as the current one under consideration.\n",
      "Yep, fair point.  I might be off base, but I find `allow_exact_matches` hard to parse in this context, since here it's about including the earliest date in a window, where in `merge_asof` it's about including the latest date (right?).  \n\nThat said, regardless of how it's spelled, I do think it'd be a nice addition!\n",
      "@chris-b1 Fair point about first in `.rolling()` vs last in `pd.merge_asof()`. I do like reusing the parameter name since it's already around, but I'm open to anything that makes sense.\n",
      "Agreed with @chris-b1 that it would be a nice addition, but for me `allow_exact_matches` is not a very clear name as well (in the sense that I wouldn't directly think of that use case when I see the name). \n\nI think it is actually about an open vs closed interval? So maybe using `closed=True\/False` would also be an option, given that 'right'\/'left' is not the correct meaning here (as it is always right closed, and it's only the left that can be open or closed). Although that name is also not directly meaningful from its name alone.\n",
      "How about `left_closed`? #13968\n",
      "@chrisaycock re-reading this, is there a suggestion to change ``allow_exact_matches`` to a ``closed='left'|'both'`` parameter in ``merge_asof`` as well? (we can create a separate issue for that).",
      "@jreback Since there are only two possible values for that parameter, I'd rather that it be a boolean."
    ],
    "events":[
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "referenced",
      "cross-referenced",
      "commented",
      "referenced",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced"
    ],
    "changed_files":6,
    "additions":222,
    "deletions":51
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":15908,
    "reporter":"stangirala",
    "created_at":1491417933000,
    "closed_at":1492176624000,
    "resolver":"stangirala",
    "resolved_in":"3fde134617822773b23cf484310820298d9f88ac",
    "resolver_commit_num":0,
    "title":"Class label colours with parallel coordinates",
    "body":"#### Problem description\r\nThe parallel coordinates function seems to currently assume that the class labels don't have an inherent ordering. I'd like to provide a sort option or a class-to-colour mapping to capture this. My specific use case is situations where the label are quantized buckets and a smooth colour change between buckets make the final plot easy to visualize.\r\n\r\nThe current function zips the available classes and the colour_values without any sorting.\r\n\r\nI've identified where in the current '_parallel_coordinates' function a change should be made and would like to work on this.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.4.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 3.2.45-0.6.wd.865.49.315.metal1.x86_64\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 34.3.2\r\nCython: None\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.3.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 2.0.0\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999999999\r\n None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.5\r\nboto: None\r\npandas_datareader: None\r\n\r\n<\/details>",
    "labels":[
      "Difficulty Novice",
      "Effort Low",
      "Visualization"
    ],
    "comments":[

    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced",
      "cross-referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":31,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":4386,
    "reporter":"davidshinn",
    "created_at":1374987360000,
    "closed_at":1492555037000,
    "resolver":"yui-knk",
    "resolved_in":"e082eb2c95a22a16d67e533cbf581a304cf5e70e",
    "resolver_commit_num":0,
    "title":"DOC\/BUG: pivot_table returns Series in specific circumstance",
    "body":"The docstrings and other documentation say that the `pivot_table` function **returns a DataFrame**.   However, this likely leads to confusion like #4371, because under narrow circumstances, passing a certain set of argument dtypes results in the function **returning a Series** (see ipython examples at end):\n1. values is single string (not a list, not even a single valued list)\n2. cols=None\n3. aggfunc is single string\/function (not a list, not even a single valued list)\n\nUnfortunately, this is not clear from the docs or from normal use (except for condition 1).\n\nShould this:\n1. eventually be fixed to only return a DataFrame no matter the circumstances to be less confusing\n2. be documented correctly (seems a little difficult to convey in the docstring and other docs without a lot of bulk).\n\nMy thoughts are changing the function to return only a DataFrame in future versions (> 0.13) and providing some deprecation warning in the meantime is better than trying to explain this in the docs.\n\nI would be happy to provide the deprecation warning and document notes as a pull request.\n\nThanks.\n\n\n",
    "labels":[
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments":[
      "@davidshinn want to do a PR for this?\n",
      "Hi\ni want to ask about comment above:\n_passing a certain set of argument dtypes results in the function returning a Series_\n\ncalling `df.pivot_table` with empty index, also returns Series object instead of DataFrame\nis there reason for this behaviour?\nthanks\n",
      "if u would like to implement as indicated above that would be great\n",
      "@youlyst, feel free to do a PR to fix this.  Sorry I've been out of touch since I originally posted. If no one gets to this by February, I'll submit a PR so that pivot_table always returns a DataFrame for consistency.\n",
      "I do not feel for it yet, i am working with pandas very short time. but it could change after month...\nmy question was mainly to make sure that i am not doing something wrong.\n",
      "contributing docs are: http:\/\/pandas.pydata.org\/pandas-docs\/stable\/contributing.html\n"
    ],
    "events":[
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":72,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":1663,
    "reporter":"wesm",
    "created_at":1343085809000,
    "closed_at":1492650133000,
    "resolver":"jnothman",
    "resolved_in":"1b52b1218e973382ea958084c47c3f33eb0ed40c",
    "resolver_commit_num":0,
    "title":"Styling in DataFrame.to_excel",
    "body":"from mailing list\n\n\n",
    "labels":[
      "Good as first PR"
    ],
    "comments":[
      "openpyxl supports styles but afaik you can't use them with the optimized writer than pandas uses by default. So instead of using sheet.append_row(row) one has to access each cell independently.\nYou can set the column width by column but attributes like colour or number format have to be set by cell and this slows everything down dramatically.\nIt's probably not too hard to add faster \"by column\" styles to openpyxl but unfortunately the project is very inactive (I added cell comments in May and never got any feedback for my pull request).\n",
      "I have been looking into this as well recently, and found [this blog post](http:\/\/informedguess.wordpress.com\/2013\/01\/13\/pandas-to-excel-with-styles\/) and [related gist](https:\/\/gist.github.com\/dmvianna\/4602492) on the matter. \n\n@gerigk can you add a link to your PR?  I can't find it.\n\n@wesm interested in a PR on this?  What does it need to do to be accepted?\n",
      "@aflaxman see this as well: https:\/\/github.com\/pydata\/pandas\/pull\/7565\n\nalways interested in a PR!\n\npls write tests \/ code \/ update doc-strings\n\nsee here: https:\/\/github.com\/pydata\/pandas\/wiki\n",
      "Ha ha, I just looked at the day and month, not the year.  Does #7565 expose something analogous to the style parameter in https:\/\/gist.github.com\/dmvianna\/4602492#file-xlpandas2-py-L50 ?  I couldn't find it with a quick search.\n",
      "excel has multiple engines (xlsxwriter, xlwt, openpyxl), so is different for each. \n",
      "closed by using this PR: https:\/\/github.com\/pydata\/pandas\/pull\/7565\n",
      "@jreback #7565 didn't really cover this, still need to decide how we'd actually want to expose styling, I'm thinking per-column styles could get us 90% of the way there. @aflaxman - if you have ideas for how an API ought to work, please feel free to comment here or in a new issue.\n",
      "@jtratner sure\n",
      "For my purposes, it would get 90% of the way to have a few defaults for highlighting the first row and\/or the first column, and bonus points for zebra-striping a la excel.  But the case that inspired me to comment on this ticket was a series of tables where there were one or two cells that I need to highlight, and for that I think some sort of table-painter interface would be best.  Pandas already has plenty of functionality for this, so maybe just allowing an optional style dict in the `.to_excel` functions, e.g.\n\n```\ndf.to_excel(path, style=style_df)\n```\n\nwhere `style_df.shape = df.shape`, and non-empty entries of `style_df` are style dicts a la the examples here https:\/\/github.com\/pydata\/pandas\/pull\/7565#issuecomment-54767851\n\nI'll try it out and report back here.\n",
      "Here is [a notebook with a start in the direction I am thinking](http:\/\/nbviewer.ipython.org\/gist\/aflaxman\/962619c69f07a61152de).  It makes this:\n![image](https:\/\/cloud.githubusercontent.com\/assets\/51236\/4774674\/8810c952-5baf-11e4-9c1b-1780fa945df0.png)\n\nComments welcome!\n",
      "cc @neirbowj\n\nthis is using the new openpyxl2 stuff?\n",
      "Yup, it sure looks that way. :smile: \n",
      "yes, speaking of which, is the style dictionary documented any more than in the #7565 comments now?\n",
      "@aflaxman: No, it is not. However, the short, short version of the docs would go something like this: any keyword or literal value that the native openpyxl v2 style interface accepts, the pandas style dict should accept as well. Symbolic constants are the main exception that I can think of off hand (e.g. `{'color':'RED'}`).\n\nSo, for example, the default font, per [the openpyxl docs](https:\/\/openpyxl.readthedocs.org\/en\/latest\/styles.html), would look like this:\n\n``` python\n\"\"\"\"\nExcerpt --\n    font=Font(name='Calibri',\n        size=11,\n        bold=False,\n        italic=False,\n        vertAlign=None,\n        underline='none',\n        strike=False,\n        color='FF000000')\n\"\"\"\"\nfont = {\n    'font': {\n        'name': 'Calibri',\n        'size': 11,\n        'bold': False,\n        'italic': False,\n        'vertAlign': None,\n        'underline': 'none',\n        'strike': False,\n        'color': 'FF000000',\n    }\n}\n```\n"
    ],
    "events":[
      "subscribed",
      "commented",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "cross-referenced",
      "labeled",
      "commented",
      "closed",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "reopened",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "cross-referenced",
      "cross-referenced"
    ],
    "changed_files":13,
    "additions":1670,
    "deletions":380
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":15520,
    "reporter":"jreback",
    "created_at":1488212161000,
    "closed_at":1493213578000,
    "resolver":"analyticalmonk",
    "resolved_in":"61ca02274b898cc2dd15f305a0b885e2f6348dd8",
    "resolver_commit_num":0,
    "title":"BUG: invalid dtypes should raise",
    "body":"xref \r\n\r\nwhen an explict dtype is provided, we should raise if its not a valid ``numpy\/pandas`` dtype.\r\n\r\n",
    "labels":[
      "Difficulty Novice",
      "Dtypes",
      "Effort Low",
      "Error Reporting"
    ],
    "comments":[
      "Hi, I am looking into contributing to open source projects and this looks like a good first-time contibution to start with. I'd like to give it a try.",
      "great\r\n\r\ncontributing docs are http:\/\/pandas.pydata.org\/pandas-docs\/stable\/contributing.html",
      "Put in a pull request for this issue. This is my first contribution attempt, so please let me know if I'm way off base here. ",
      "Should be addressing this at the source of the problem (raise anytime a function passes a non-valid dtype), or should we raise only when this happens in a Series function call?",
      "Here is my suggestion for this issue: 607ae9c38e7d8f85d3a7f0b161545d5437afd285"
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "cross-referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "milestoned",
      "demilestoned",
      "referenced"
    ],
    "changed_files":8,
    "additions":57,
    "deletions":18
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":14488,
    "reporter":"ragesz",
    "created_at":1477389755000,
    "closed_at":1495103453000,
    "resolver":"jbschiratti",
    "resolved_in":"539de79692704c735a38975988ffe7293f6c2583",
    "resolver_commit_num":0,
    "title":"Adding optional pickle protocol version argument to pandas.to_pickle()",
    "body":"Sometimes pickle files saved in Python v3.x are needed to read in Python v2.x. It would be nice if one can easily set pickle protocol version in `to_pickle()`.\n\nIt can be done with the following little changes:\n\nIn file `\/pandas\/io\/pickle.py`:\n\n\n\nIn file `\/pandas\/core\/generic.py`:\n\n\n",
    "labels":[
      "API Design",
      "Compat",
      "Data IO",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "sure, you could add this. \n"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":4,
    "additions":72,
    "deletions":6
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":6608,
    "reporter":"rosnfeld",
    "created_at":1394579406000,
    "closed_at":1495627932000,
    "resolver":"huguesv",
    "resolved_in":"b0038ac72721058a3ae71f1dbcaa24d2f10f23c0",
    "resolver_commit_num":1,
    "title":"BUG: strange timeseries plot behavior",
    "body":"After some discussion below, here's a simple repro case:\n\n\n\ncauses\n\n\n\n-- ORIGINAL MESSAGE --\n\nHere's a small dataset:\n\n\n\nIf I read this in using\n\n\n\nand then try and plot it using\n\n\n\nI get more or less what I expect (perhaps the xlim doesn't go up to 2010-12-31, but otherwise fine).\n\nIf I delete out the BRA rows and try this again, I get:\n\n\n\nIf I delete out both BRA and VEN rows, then there is no exception raised but I only see one series plotted and the x-axis is not formatted as a date.\n\nOne could also approach this whole exercise via something like\n\n\n\nbut this works even worse, I just get a truncated VEN series and nothing else.\n\nThis is with current master pandas (but also happens in 0.13.1) and matplotlib 1.3.1.\n\nAre there known issues with plotting sparse-yet-overlapping timeseries?\n",
    "labels":[
      "Visualization",
      "Timeseries",
      "Difficulty Novice",
      "Effort Low",
      "Testing"
    ],
    "comments":[
      "I guess if I do\n\n``` python\ndata.pivot('date', 'region', 'value').interpolate().plot()\n```\n\nor\n\n``` python\npivoted = data.pivot('date', 'region', 'value')\npivoted.index = pd.to_datetime(pivoted.index)\npivoted.interpolate(method='time').plot()\n```\n\nI get something like what I wanted as it cleans up the missing values. Interpolate's a new feature I hadn't seen before. (cool!)\n\nMaybe this is all user-error but I had been doing groupby plotting like this for a while, and had been getting what looked like correct results. I feel there may actually be a bug here someplace, that groupby behavior seems so bizarre.\n",
      "Actually, interpolation is not really what I want, as it makes it seem as if there is more data than is actually present. I basically just want to see the various region series all plotted as they would be if they were plotted individually, except all together on the same axes.\n\n(and a loop like\n\n``` python\nfigure = plt.figure()\nax = figure.gca()\ndata = pd.read_csv('.\/data.csv', parse_dates='date')\nfor region in data.region.unique():\n    subset = data[data.region == region]\n    subset = subset.set_index('date')\n    subset.value.plot(ax=ax, label=region)\n```\n\nseems to just over-write the axes)\n",
      "I think the groupby\/plot issue seems certainly like a bug. I can't fully lay my hand on it, but I think it has something to do with combining regular\/irregular timeseries.\n\nThe issue with the xlim not respecting the data is because it is updated by the last group (while this is a smaller group), and this is a seperate issue I think (you can open another issue for that).\n\nThe reason you get almost no points on the plot with `pivot` you seem to already figured out, this is indeed because of all the NaN values. You can also deal with this by plotting points instead of lines.\n",
      "Thanks @jorisvandenbossche, it looks like the \"separate issue\" is already filed as #2960 . So this one is just the groupby\/plot weirdness.\n\nDid you mean to add to your comment? (it ends in what looks like the start to some code)\n",
      "@rosnfeld ah yes, I first wanted to add a code snippet how to do your last example easier, but this also had the same bug, but forgot to remove it. Removed it now \n",
      "@rosnfeld @jorisvandenbossche \n\nso this is the exception that's in the top section?\n\nwhat is causing this?\n",
      "Yeah, the exception is the most alarming thing, though changing the data slightly causes some other incorrect behavior (missing\/incorrect plotting, which is harder to spot\/diagnose).\n\nI don't know what's causing it without further investigation, but I can try to investigate and hopefully submit a fix. (it will be my first time digging into the plotting code, not sure how involved it is)\n",
      "The error is occurring in\n\n``` python\ndef _get_xlim(lines):\n    left, right = np.inf, -np.inf\n    for l in lines:\n        x = l.get_xdata()\n        left = min(x[0].ordinal, left)\n        right = max(x[-1].ordinal, right)\n    return left, right\n```\n\nThe line `left = min(to_ordinal(x[0]), left)` apparently expects a PeriodIndex.\nFor whatever reason, when you select `sub = data[data.region != 'BRA']` and plot that, you get an array of `datetime.datetime` objects at that point, instead of a PeriodIndex.\n\nI'm not too familiar with our Datetime code, but does anyone know why these aren't the same?\n\n``` python\nIn [40]: import datetime\nIn [41]: b = datetime.datetime(1997, 12, 31, 0, 0)\n\nIn [42]: y = pd.Period(year=1997, month=12, day=31, hour=0, minute=0, freq='S')\n\nIn [43]: b.toordinal()\nOut[43]: 729389\n\nIn [44]: y.ordinal\nOut[44]: 883526400\n```\n",
      "Some time ago I looked into this (for another issue), and then one of the fundamental problems was the design of pandas plotting for timeseries splitted in two ways: with datetimes when having irregular serieses, and with ordinals when having a regular timeseries (which was then converted to periodindex), and that those two types are incompatible with each other (so when you combine both types in a certain way it gives problems).\n\nBut I have to dig it up again to fully remember (I have some overview of the problem somewhere, but never finished it). I don't know i I find some time in the short term, but will try.\n",
      "And the `datetime.datetime.toordinal` is in days, the `pandas.Period.ordinal` int he frequency you specified (in this case seconds). \nPlus `datetime.datetime.toordinal` is since 01\/01\/0001, pandas base is 1970\n",
      "Yes, I looked at this a little last night, and agree with what you're saying - when you whittle the dataset down to just COL\/PAN\/VEN rows and then plot, COL and VEN get converted to have PeriodIndexes, but PAN stays with a DatetimeIndex for some reason, and then plotting them all on the same axes (via groupby) blows up somehow.\n",
      "Thanks.\n\n@rosnfeld you may want the `x_compat=True` keyword argument to plot. That seems to \"solve\" the problem\n",
      "Indeed, thanks! I actually hadn't seen that option before. It also fixes the \"missing series\" variant I mentioned in the original description. The bad xlim variant for the original dataset still remains, though.\n\nI'll try and dig into things a bit and see what can be done - I presume that not requiring x_compat is desirable.\n",
      "Yeah it would be desirable. May be tricky though. I'm guessing that argument was added for cases precisely like this one.\n",
      "For a bit more detail, I think this is what's going on: \n\npandas uses special timeseries plotting if it can infer a \"periodic\" frequency from a series. While it takes a bit of digging, this is part of the `_use_dynamic_x()` check in tools\/plotting.py:\n\n``` python\n    def _make_plot(self):\n        # this is slightly deceptive\n        if not self.x_compat and self.use_index and self._use_dynamic_x():\n            data = self._maybe_convert_index(self.data)\n            self._make_ts_plot(data)\n        else:\n...  # regular plotting\n```\n\nThis special tseries logic converts plotted series to use a PeriodIndex, and sets a \"base\" version of  the frequency on the axes object for later reference. (Note that x_compat disables all of this and uses regular, non-tseries plotting)\n\nThe first series to be plotted in my dataset (COL) gets a frequency of '5A-DEC', which can be converted to a period. In the timeseries plotting code the \"base\" version of this frequency ('A-DEC') gets assigned to the axes object.\n\nThe 3-item DatetimeIndex of 1997-12-31, 2003-12-31, and 2008-12-31 for the next series (PAN) has a surprising inferred frequency of 'WOM-5WED' since 1997, 2003, and 2008 all ended on a Wednesday (the 5th Wednesday of December). pandas can't convert frequencies like that to periods, so it uses regular plotting rather than the special timeseries plotting for that series, and its index is not converted to PeriodIndex. It doesn't try and use the axes frequency since it has already inferred a frequency for this series.\n\nThe next and final series (VEN) does not have an inferred frequency, so it inherits the axes frequency, and tries to use tseries plotting again. tseries plotting tries to re-calculate x_lim's to include all data, so it looks at the lines already plotted, but it assumes all existing lines will have PeriodIndex data. It blows up when it tries to call 'ordinal' on the DatetimeIndex entries from the earlier (PAN) series. \n\nI'm not sure what the right fix is here. Frequency inference clearly makes some interesting choices, that are relied on in other parts of the codebase. I'm not sure if either the frequency inference or the usage of it should be modified. Timeseries plotting should maybe tolerate non-PeriodIndex data when calculating x_lim, though I don't yet understand much of that code yet, e.g. why PeriodIndex is desirable.\n",
      "@rosnfeld so this occurs when you have **multiple** series overlaid on the same plot and 1 is converted to PeriodIndex for display while 1 is not.\n\ncan you edit the top of the post to make it easily copy-pastable for the failing case?\n",
      "I added an even simpler example at the start of the post. No groupby or anything, just plot a timeseries with an inferred frequency that can be converted to a period, then one that can't, then the first one again, all on the same axes, and you get the same stack trace.\n\nHope that's along the lines of what you were looking for.\n",
      "ok...I guess the soln is in the plotting routines to check if their is a plot on the axis already that has a conflicting axis\/index, then handle the current plotting better.\n\nI am not sure if this involves too much introspection or is even possible (e.g. you would have to get the index state from the axis and not sure if saved 'enough' to be able to figure out what is up)\n\n@rosnfeld give it a shot?\n",
      "moving to 0.15, but if you are able to figure out soon can move back\n",
      "Sure, I can take a shot at it. I'm optimistic something can be done.\n",
      "I think this one should be re-opened - my bad for having a comment in #7322 saying \"this does not fix #6608\".\n\nHowever, I think @sinhrks has some PRs that look to affect this behavior somewhat, changing this issue if not closing it.\n",
      "#7459 partially fixes this not to raise `AttributeError`.\n\nBut unable to set correct `xlim` and `formatter` yet. The result after #7459 is as below.\n![figure_1](https:\/\/cloud.githubusercontent.com\/assets\/1696302\/3485762\/7017b90e-03fe-11e4-93e1-957837347a2d.png)\n. \n",
      "Well, regular vs irregular series have pretty different ordinals, as in @TomAugspurger comment above, so I think the problem is unfortunately deeper than just xlim\/representation. A solution might be to rework `_use_dynamic_x()` (in tools\/plotting.py), to better catch cases that might mix these two together.\n",
      "@TomAugspurger push?\n",
      "@TomAugspurger status (pushing #7670) ok, so push this as well\n",
      "It looks like this issue is solved in the meantime. At least the simplified example at the top now works correctly for me.\n\n@rosnfeld Would you be able to test with your more complex example as well?\n",
      "Yes! I tested with the more complex example and everything works now. (as of 0.18.1)\n",
      "I see this is still open - should I close it?\r\n\r\nOr do people want some unit tests to explicitly try to protect against this happening again? Unfortunately given our (or at least my) incomplete understanding of why it was happening and how it has since been fixed, perhaps the best we could do would be writing a test that would have failed against code from a couple of years ago.\r\n\r\nNot sure what community practice is on things like this.",
      "Yes, I would first like to see a test added to confirm this (and keep it working!). A PR very welcome!"
    ],
    "events":[
      "commented",
      "commented",
      "labeled",
      "commented",
      "labeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "closed",
      "commented",
      "mentioned",
      "subscribed",
      "reopened",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "milestoned",
      "demilestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "milestoned",
      "demilestoned",
      "demilestoned",
      "milestoned",
      "milestoned",
      "demilestoned",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "unlabeled",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":1,
    "additions":16,
    "deletions":0
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":16493,
    "reporter":"CRP",
    "created_at":1495707148000,
    "closed_at":1496314269000,
    "resolver":"CRP",
    "resolved_in":"e3ee1869ce955df5d3daa59d59e08749471e5be5",
    "resolver_commit_num":0,
    "title":"to_html(index_names=False) still renders a row with index names",
    "body":"The index_names parameter is apparently ignored.\r\nI tried to debug the code, and it looks like line 1291 of pandas\/io\/formats\/format.py should contain \"and self.fmt.show_index_names\" in the if clause.\r\n\r\nhere is sample code:\r\n\r\n\r\nwhich produces the following output:\r\n<table border=\"1\" class=\"dataframe\">\r\n  <thead>\r\n    <tr style=\"text-align: right;\">\r\n      <th><\/th>\r\n      <th>a<\/th>\r\n      <th>b<\/th>\r\n      <th>c<\/th>\r\n      <th>d<\/th>\r\n    <\/tr>\r\n    <tr>\r\n      <th>pippo<\/th>\r\n      <th><\/th>\r\n      <th><\/th>\r\n      <th><\/th>\r\n      <th><\/th>\r\n    <\/tr>\r\n  <\/thead>\r\n  <tbody>\r\n    <tr>\r\n      <th>0<\/th>\r\n      <td>1.715511<\/td>\r\n      <td>-1.582624<\/td>\r\n      <td>0.027399<\/td>\r\n      <td>-0.276980<\/td>\r\n    <\/tr>\r\n    <tr>\r\n      <th>1<\/th>\r\n      <td>-0.003603<\/td>\r\n      <td>-1.227605<\/td>\r\n      <td>0.434763<\/td>\r\n      <td>0.039167<\/td>\r\n    <\/tr>\r\n    <tr>\r\n      <th>2<\/th>\r\n      <td>0.246343<\/td>\r\n      <td>-0.062897<\/td>\r\n      <td>-0.724532<\/td>\r\n      <td>-0.352672<\/td>\r\n    <\/tr>\r\n  <\/tbody>\r\n<\/table>\r\n\r\nthe second row of the thead should not be there...\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.20.1\r\npytest: None\r\npip: 9.0.1\r\nsetuptools: 35.0.2\r\nCython: None\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\nxarray: 0.9.5\r\nIPython: 6.0.0\r\nsphinx: None\r\npatsy: 0.4.1\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: 3.4.2\r\nnumexpr: 2.6.2\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: 2.4.7\r\nxlrd: 1.0.0\r\nxlwt: 1.2.0\r\nxlsxwriter: 0.9.6\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999999999\r\nsqlalchemy: 1.1.9\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n<\/details>\r\n",
    "labels":[
      "Bug",
      "IO HTML",
      "Output-Formatting",
      "Difficulty Novice",
      "Effort Low"
    ],
    "comments":[
      "May be as simple as also checking `self.fmt.index_names` in https:\/\/github.com\/pandas-dev\/pandas\/blob\/e41fe7f52a7ae6be962e683f40500624b2ba2cf6\/pandas\/io\/formats\/format.py#L1295 (and tests).\r\n\r\n@CRP interested in submitting a fix?",
      "just created pull request, not sure if I did everything right..."
    ],
    "events":[
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "labeled",
      "labeled",
      "cross-referenced",
      "cross-referenced",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":11,
    "deletions":1
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":16416,
    "reporter":"jorisvandenbossche",
    "created_at":1495473461000,
    "closed_at":1500162339000,
    "resolver":"aernlund",
    "resolved_in":"1212fe034b7302f40bf253aedd9e3989514eeb52",
    "resolver_commit_num":0,
    "title":"DOC: add examples to docstrings",
    "body":"This often used functions lacks some illustrating examples\r\n\r\n- [x] ``sort_values`` (#16437)\r\n- [x] ``reset_values`` (#16967)\r\n- [x] ``set_index`` (#16467)\r\n- [x] ``fillna`` (#16437)\r\n- [x] ``drop`` (#16437)\r\n- [x] ``pop`` (#16520)",
    "labels":[
      "Difficulty Novice",
      "Docs",
      "Effort Low"
    ],
    "comments":[
      "Other docstrings that are in the top most viewed and lacking examples: `reset_index`, `set_index`, `fillna`, `drop`",
      "@jorisvandenbossche I'd like to take this one. ",
      "@geoninja any chance you're at Pycon doing sprints right now?",
      "I can add some examples for `fillna` and `drop`.",
      "@jorisvandenbossche Both Vincent and I are at PyCon so we will split the work on this one",
      "I'm going to work on reset_values so we can close this issue.",
      "if anyone at sprints wants to add to ``.reset_index()`` we can close this one\r\n\r\n@TomAugspurger ",
      "I'm working on .reset_index() example."
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "renamed",
      "milestoned",
      "cross-referenced",
      "commented",
      "referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced"
    ],
    "changed_files":2,
    "additions":63,
    "deletions":0
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":16900,
    "reporter":"dwillmer",
    "created_at":1499903417000,
    "closed_at":1500634306000,
    "resolver":"dwillmer",
    "resolved_in":"8f309db542c893b46e7cc6cff72638f68f5a855b",
    "resolver_commit_num":1,
    "title":"BUG: pd.merge throws TypeError with categoricals",
    "body":"#### Code Sample\r\n\r\n\r\n#### Problem description\r\n\r\nIf you run the example above, you will get the following output:\r\n\r\n\r\n\r\nThis occurs when all of the following are true:\r\n\r\n- Both columns to merge on are categorical dates\r\n- The categoricals have the same dtype, but the values are different\r\n- The merge is 'outer'\r\n\r\nIf you change the merge to 'inner', or change the date values to be the same, then the code works as expected.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\nIn [3]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.1.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB.UTF-8\r\nLOCALE: en_GB.UTF-8\r\n\r\npandas: 0.20.2\r\npytest: 3.1.2\r\npip: 9.0.1\r\nsetuptools: 27.2.0\r\nCython: 0.25.2\r\nnumpy: 1.12.1\r\nscipy: 0.19.0\r\nxarray: None\r\nIPython: 6.0.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2017.2\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nfeather: None\r\nmatplotlib: 2.0.2\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: 0.999\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: 2.9.6\r\ns3fs: None\r\npandas_gbq: None\r\npandas_datareader: None\r\n\r\n<\/details>\r\n",
    "labels":[
      "Bug",
      "Categorical",
      "Difficulty Intermediate",
      "Effort Medium",
      "Reshaping",
      "Timeseries"
    ],
    "comments":[
      "looks like a bit of a bug in there. This is doing very odd things on the coercion. welcome to have you take a look.",
      "This is also incorrect with inner. Incorrect coercion on the datetimes.\r\n\r\n```\r\nIn [12]: df = pd.DataFrame(\r\n    ...:     [[date(2001, 1, 1), 1.1],\r\n    ...:      [date(2001, 1, 2), 1.3]],\r\n    ...:     columns=['date', 'num2']\r\n    ...: )\r\n    ...: df['date'] = df['date'].astype('category')\r\n    ...: \r\n    ...: df2 = pd.DataFrame(\r\n    ...:     [[date(2001, 1, 1), 1.3],\r\n    ...:      [date(2001, 1, 3), 1.4]],\r\n    ...:     columns=['date', 'num4']\r\n    ...: )\r\n    ...: df2['date'] = df2['date'].astype('category')\r\n    ...: \r\n    ...: result = pd.merge(\r\n    ...:     df, df2, how='inner', on=['date']\r\n    ...: )\r\n    ...: \r\n    ...: print(result)\r\n    ...: \r\n                 date  num2  num4\r\n0  978307200000000000   1.1   1.3\r\n```"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":3,
    "additions":55,
    "deletions":9
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":16798,
    "reporter":"jeffknupp",
    "created_at":1498753667000,
    "closed_at":1500827299000,
    "resolver":"jeffknupp",
    "resolved_in":"8d7d3fb545b4273cf9d1a61bf7ea3bfdde8a1199",
    "resolver_commit_num":0,
    "title":"Pandas core dumps when reading large CSV file using read_csv(..., low_memory=False)",
    "body":"#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\n\r\nFrom the stacktrace in the core file, pandas seems to be throwing an exception complaining \"out of memory\" (which it is not, the machine has 64 G of RAM and the interpreter was using maybe 5 G) but, during the cleanup of that exception, attempts to double free the `self->error_msg` pointer (according to gcc). Results in a `SIGSEGV`.\r\n\r\n#### Expected Output\r\n\r\nPandas successfully converts the CSV into a dataframe\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.5.2.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.4.0-81-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 36.0.1\r\nCython: 0.25.2\r\nnumpy: 1.13.0\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: None\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\n None\r\napiclient: None\r\nsqlalchemy: 1.1.4\r\npymysql: None\r\npsycopg2: 2.6.1 (dt dec pq3 ext lo64)\r\njinja2: 2.9.6\r\nboto: 2.47.0\r\npandas_datareader: None\r\n\r\n<\/details>\r\n\r\nI can provide the source CSV if necessary (though it happens reliably with \"large\" CSVs, for a definition of \"large\" I haven't nailed down but in the multi-GB range). Below is the stack trace:\r\n\r\n<details>\r\n#0  0x00007f278e970532 in __GI___libc_free (mem=0x7f2757218b0e) at malloc.c:2967\r\n#1  0x00007f275720fd2a in free_if_not_null (ptr=0x3cf5ee8) at pandas\/src\/parser\/tokenizer.c:94\r\n#2  parser_cleanup (self=self@entry=0x3cf5df0) at pandas\/src\/parser\/tokenizer.c:189\r\n#3  0x00007f275720ff09 in parser_free (self=0x3cf5df0) at pandas\/src\/parser\/tokenizer.c:285\r\n#4  0x00007f27571b0562 in __pyx_pf_6pandas_6parser_10TextReader_4__dealloc__ (__pyx_v_self=0x7f2757166bc8) at pandas\/parser.c:6330\r\n#5  __pyx_pw_6pandas_6parser_10TextReader_5__dealloc__ (__pyx_v_self=<pandas.parser.TextReader at remote 0x7f2757166bc8>) at pandas\/parser.c:6313\r\n#6  __pyx_tp_dealloc_6pandas_6parser_TextReader (o=<pandas.parser.TextReader at remote 0x7f2757166bc8>) at pandas\/parser.c:45130\r\n#7  0x000000000055dbea in dict_dealloc.lto_priv.164 (mp=0x7f2753a0bbc8) at ..\/Objects\/dictobject.c:1594\r\n#8  subtype_dealloc () at ..\/Objects\/typeobject.c:1193\r\n#9  0x000000000055dbea in dict_dealloc.lto_priv.164 (mp=0x7f2753c765c8) at ..\/Objects\/dictobject.c:1594\r\n#10 subtype_dealloc () at ..\/Objects\/typeobject.c:1193\r\n#11 0x00000000004e9137 in frame_dealloc.lto_priv () at ..\/Objects\/frameobject.c:431\r\n#12 0x0000000000541457 in tb_dealloc.lto_priv.286 (tb=0x7f27537b8048) at ..\/Python\/traceback.c:55\r\n#13 0x000000000054146d in tb_dealloc.lto_priv.286 (tb=0x7f27537b8088) at ..\/Python\/traceback.c:54\r\n#14 0x000000000054146d in tb_dealloc.lto_priv.286 (tb=0x7f27537b80c8) at ..\/Python\/traceback.c:54\r\n#15 0x000000000054146d in tb_dealloc.lto_priv.286 (tb=0x7f27537b8108) at ..\/Python\/traceback.c:54\r\n#16 0x0000000000526b99 in PyEval_EvalFrameEx () at ..\/Python\/ceval.c:2132\r\n#17 0x0000000000528814 in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7ffdc3b87590, func=<optimized out>) at ..\/Python\/ceval.c:4803\r\n#18 call_function (oparg=<optimized out>, pp_stack=0x7ffdc3b87590) at ..\/Python\/ceval.c:4730\r\n#19 PyEval_EvalFrameEx () at ..\/Python\/ceval.c:3236\r\n#20 0x0000000000528814 in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7ffdc3b876c0, func=<optimized out>) at ..\/Python\/ceval.c:4803\r\n#21 call_function (oparg=<optimized out>, pp_stack=0x7ffdc3b876c0) at ..\/Python\/ceval.c:4730\r\n#22 PyEval_EvalFrameEx () at ..\/Python\/ceval.c:3236\r\n#23 0x000000000052d2e3 in _PyEval_EvalCodeWithName () at ..\/Python\/ceval.c:4018\r\n#24 0x000000000052dfdf in PyEval_EvalCodeEx () at ..\/Python\/ceval.c:4039\r\n#25 PyEval_EvalCode (co=<optimized out>, globals=<optimized out>, locals=<optimized out>) at ..\/Python\/ceval.c:777\r\n#26 0x00000000005fd2c2 in run_mod () at ..\/Python\/pythonrun.c:976\r\n#27 0x00000000005ff76a in PyRun_FileExFlags () at ..\/Python\/pythonrun.c:929\r\n#28 0x00000000005ff95c in PyRun_SimpleFileExFlags () at ..\/Python\/pythonrun.c:396\r\n#29 0x000000000063e7d6 in run_file (p_cf=0x7ffdc3b87930, filename=0x1e96260 L\"\/home\/jknupp\/lion\/components\/api\/scripts\/parquet_export.py\", fp=0x1fa1310) at ..\/Modules\/main.c:318\r\n#30 Py_Main () at ..\/Modules\/main.c:768\r\n#31 0x00000000004cfe41 in main () at ..\/Programs\/python.c:65\r\n#32 0x00007f278e90c830 in __libc_start_main (main=0x4cfd60 <main>, argc=2, argv=0x7ffdc3b87b48, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7ffdc3b87b38)\r\n    at ..\/csu\/libc-start.c:291\r\n#33 0x00000000005d5f29 in _start ()\r\n<\/details>",
    "labels":[
      "Bug",
      "Difficulty Intermediate",
      "Effort Medium",
      "IO CSV"
    ],
    "comments":[
      "this is fixed in 20.2 give a try ",
      "Great! @jreback Could you point me to the issue that fixes it (just for my records)?",
      "well i would suggest u try it",
      "This doesn't fix the issue. Can you point me to the issue you thought fixed it? It might point me in the right direction.",
      "just search in the whatsnew there were a few\r\n\r\nw\/o a repro this is impossible to debug",
      "I can post a link to the CSV in question. It's quite large, but cores almost immediately.",
      "then progressively make it smaller until it doesn't core (and show the 1 before)\r\nthese types of errors are generally repo on small sets",
      "Smallest amount I could get it to core reliably on is 500,000 lines (1 GB uncompressed). Here is a link to a `bzip2` compressed version: https:\/\/www.dropbox.com\/s\/hguhamz0gdv2uzv\/test_data.csv.bz2?dl=0\r\n\r\nMD5 of uncompressed file is listed below:\r\n\r\n> MD5 (test_data.csv) = 9a66139195677008b4fcb56468e19234\r\n",
      "If it's helpful, I can create a Dockerfile that reliably reproduces this crash.",
      "#14696 would fix this - give a try\r\nif yes then completing the PR would be great",
      "So that patch fixes the segfault, but the original issue still remains: pandas thinks it is OOM (or is trying to allocate a ridiculously large buffer, I can't tell which yet) when the process is using a reasonable (< 3GB) amount of memory and the machine has 64 GB available. So the issue still remains, it just no longer segfaults while cleaning up the issue (which is all that #14696 fixed).\r\n\r\nI can provide you the logs when run in debug mode and compiled with -DVERBOSE if you'd like. I can also provide a Dockerfile to create an env which reproduces this reliably.",
      "you can post the traceback. someone would need to debug this and figure out what is causing the problem, then make a smaller repro",
      "The traceback won't be useful, since without the patch it's just showing the segfault the patch addresses, while with the patch there is no crash (an Exception is raised in the tokenizer).\r\n\r\nI'm not sure how\/why you would expect someone to debug this and figure out the problem and _then_ make a \"smaller repro\" (assuming you're talking about test data size). Once they've figured out the problem (ostensibly using the existing bz2 archive, which is only ~130 MB), couldn't they then just fix it? Why would they need to also reduce the size of the data to reproduce it? \r\n\r\nRegardless, I'm willing to work with anyone who's willing to help solve this issue. I'll try to resolve it myself if I have time, but I was hoping it might be something easy.",
      "> I'm not sure how\/why you would expect someone to debug this and figure out the problem and then make a \"smaller repro\" (assuming you're talking about test data size). Once they've figured out the problem (ostensibly using the existing bz2 archive, which is only ~130 MB), \r\n\r\nwell in order to have it part of the test suite we need a repro example that is smalllish, doesn't rely open a remote link, and has minimal deps (iow doesn't use docker)\r\n\r\nvirtually any non trivial change has a test - sure on occasion we don't explicitly add one for something hard to repro - but we actually go pretty far on testing",
      "Ah, OK, you were referring to testing. As this is somewhat related to an issue in Apache Arrow,\r\n [ARROW-1167](https:\/\/issues.apache.org\/jira\/browse\/ARROW-1167) (this CSV fails in Pandas as described, but I can get around that by specifying `dtype=<column-types>` and removing `low_memory=False`), which @wesm has been looking at, I'm guessing it's going to turn out to be a similar underlying issue. It may just requiring trying to load a \"CSV\" (really a `StringIO` buffer or a generator that yields random values of a given type and implements the `file` interface or something) that is sufficiently large enough to trigger this issue.\r\n\r\nWes saw that one of the 40+ columns accounted for 2+GB of the 3GB file. I'm guessing there is buffer (re)allocation going on somewhere that eventually tries to allocate a ridiculous sized buffer, or something to that effect.\r\n\r\nI'll take some time now to look at it and see if I can't nail down where the issue is.",
      "If this is a problem that only occurs with large datasets then coming up with a minimal reproduction could be tricky. If you need some help debugging this let me know and I can try to help",
      "There are two bugs involved. The first is rather trivial: the `error_msg` pointer in `tokenizer.c` begins uninitialized and must be `malloc()`d any time you want to fill in an error message. It looks like someone caught this problem a while ago but requiring individual allocations at the point of use rather than just pre-allocating a fixed size buffer seems brittle. Of course, if someone forgets to pre-allocate, you'll get an illegal free when the tokenizer is being cleaned up, which is exactly what happened.\r\n\r\nTwo simple fixes for this: either just add the missing allocations, or pre-allocate a fixed size buffer that will hold an `error_msg`. The latter of course leaves open the possibility that the error message being set is larger than the buffer, but in practice these are all set via `snprintf` so it shouldn't actually be an issue.\r\n\r\nI'll describe the \"real\" issue in the next comment but would like to know how you'd like me to solve this first problem.",
      "The issue that triggers the bug above occurs when reading in a large CSV with `from_csv(..., low_memory=False)`. The underlying buffer used during tokenization is resized based on the size of the stream, but if the stream is large enough this will cause an overflow in the integer keeping track of the capacity, causing it to go negative and trigger its out-of-memory error (the issue above manifests itself because one of the `error_msg`s missing an allocation was \"out of memory\").\r\n\r\nDoes it make sense to change this to a 64-bit integer? I think this will only occur more and more frequently given how cheap RAM is, and ideally, the maximum size of the buffer matches the underlying machine's architecture, where I'm guessing it's far more common to be running x86_64 rather than i386. The other possibliity is to guard against such an overflow and just error out if it _would_ happen, but that limits the amount of RAM one can use to parse very large CSVs\r\n\r\n@wesm @jreback Any input here would be appreciated. I'm happy to fix the issues and write the tests. Just want to know which approach (or another one altogether) you'd like to take.",
      "(And, just to clarify, I propose switching everything to `size_t` rather than `int` if that wasn't clear, so not strictly a 64-bit integer necessarily)",
      "I recommend we use `int64_t` since `size_t` is a platform-specific type",
      "this is also noted in https:\/\/github.com\/pandas-dev\/pandas\/issues\/14131 where we came to the same conclusion. so @jeffknupp I would\r\n\r\n- fix the error message issue by pre-allocation\r\n- ``int`` -> ``int64_t``\r\n\r\ncan close both issue by this.",
      "I have a patch for this that I'll be sending a PR for in momentarily. @wesm: Re `int64_t` vs `size_t`, isn't the platform-specificity a positive property? If I'm on a 32-bit machine, I wouldn't want pandas assuming it can create byte arrays with more elements than I can address (in that case the \"out of memory\" message would be triggered as expected and, after this patch, just not core dump)? ",
      "> Re int64_t vs size_t, isn't the platform-specificity a positive property?\r\n\r\nIn my experience it's better to stick with platform independent types exclusively if you can -- see Google's C++ guidelines for some discussion of this https:\/\/google.github.io\/styleguide\/cppguide.html#Integer_Types \r\n\r\n> For integers we know can be \"big\" [Wes: on some platform], use int64_t\r\n\r\nI find it reduces the amount of platform specific code that leaks into your application. If you need to convert from `int64_t` to `int32_t`, then you should static cast, but it may be unclear when you need to insert an explicit cast when you convert from a platform specific type to a platform agnostic type or vice versa.",
      "@wesm Yes, but from the Google style guide (see last clause): \r\n\r\n> <stdint.h> defines types like int16_t, uint32_t, int64_t, etc. You should always use those in preference to short, unsigned long long and the like, when you need a guarantee on the size of an integer. Of the C integer types, only int should be used. When appropriate, you are welcome to use standard types like size_t and ptrdiff_t.\r\n\r\nI would say this is one case where the use of a standard type whose sole purpose is to accurately reflect the size of a container seems reasonable but will defer to you if you disagree.",
      "Created PR #17040. I stuck with `size_t` based on the comment above and didn't want to delay pushing the PR before getting a final verdict (can be easily changed if necessary)."
    ],
    "events":[
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "commented",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":7,
    "additions":187,
    "deletions":139
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":17276,
    "reporter":"jreback",
    "created_at":1503016687000,
    "closed_at":1503359631000,
    "resolver":"mgasvoda",
    "resolved_in":"910207ffe518413e84cfa95d772cb66d57a0d08e",
    "resolver_commit_num":0,
    "title":"BUG: clip should handle null values",
    "body":"We need treat a ``np.nan`` as a clip arg as ``None`` (which means don't clip on that side).\r\nThis makes tihngs more user friendly with no loss of generality.\r\n\r\n\r\n\r\nthis is easily fixed by inside ``.clip()`` by:\r\n\r\n",
    "labels":[
      "Bug",
      "Difficulty Novice",
      "Effort Low",
      "Missing-data"
    ],
    "comments":[
      "cc @cpcloud ",
      "I would like to help out with this as a first-time contributor if that's alright",
      "@mgasvoda Alright and welcomed!  You can read the documentation here:\r\n\r\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/contributing.html\r\n\r\nregarding contributing to `pandas`.  Let us know if you have any questions!",
      "Great, thanks for the help!"
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "milestoned",
      "demilestoned"
    ],
    "changed_files":4,
    "additions":30,
    "deletions":21
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":16842,
    "reporter":"mcakes",
    "created_at":1499369231000,
    "closed_at":1503579555000,
    "resolver":"step4me",
    "resolved_in":"96f92eb1c696723b6465fdc273dc8406201c606a",
    "resolver_commit_num":0,
    "title":"Cannot use tz-aware origin in to_datetime()",
    "body":"#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\nAttempting to use a timezone aware origin argument in `pandas.to_datetime()` causes an error due to the line\r\n`offset = tslib.Timestamp(origin) - tslib.Timestamp(0)`\r\n\r\nShould this be allowed? ",
    "labels":[
      "Difficulty Novice",
      "Effort Low",
      "Error Reporting",
      "Timezones"
    ],
    "comments":[
      "``to_datetime`` doesn't explicity handle timezones, rather it gives you back naive datetimes (ex the ``utc=`` kwarg).\r\n\r\n Origin is *only* useful when specifying ``unit=``, so you can't pass anything but ints\/floats. I would just localize after.\r\n\r\nHowever a more explicit error message would be good. IOW we will require that the origin (if its a Timestamp is naive).",
      "a PR would be welcome!",
      "I would like to take this up if no one else is working on it!"
    ],
    "events":[
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files":3,
    "additions":15,
    "deletions":3
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":18113,
    "reporter":"jreback",
    "created_at":1509825127000,
    "closed_at":1512324265000,
    "resolver":"gkonefal-reef",
    "resolved_in":"a9e47312697a933e6af495a8682ce73717cf0ff8",
    "resolver_commit_num":0,
    "title":"BLD: since we already use setuptools, let's remove the optional logic in setup.py",
    "body":"no need to have the branching in setup.py we always have setuptools available.",
    "labels":[
      "Build",
      "Difficulty Intermediate",
      "Effort Medium"
    ],
    "comments":[
      "Besides cleaning up `setup.py` does something else should be done? Are there any caveats regarding `pandas` build process?",
      "no i think this all that is needed; we do sufficient testing on building in the CI",
      "@jreback Do you think we could have `Build` subsection in `whatsnew` docs for `BLD` related entries? Or do they fall under one of existing subsections?",
      "the try\/except of `import setuptools` also includes `import pkg_resources`.  Is it also always the case that we always have pkg_resources?"
    ],
    "events":[
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "commented",
      "commented",
      "cross-referenced",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "referenced"
    ],
    "changed_files":2,
    "additions":15,
    "deletions":42
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":17610,
    "reporter":"gloryfromca",
    "created_at":1505969960000,
    "closed_at":1513870131000,
    "resolver":"gloryfromca",
    "resolved_in":"7a1b0eead6c0314875aa5d3a9f6d1976165a635a",
    "resolver_commit_num":0,
    "title":"BUG: duplicate indexing with embedded non-orderables",
    "body":"#### code below is scene that issue happened. \r\n\r\n#### Problem description\r\n#### when I wanted to get a set from a Series , it happended.\r\n#### it has worked well for a long time,but suddenly it broke out. raise Traceback like this: \r\n\r\nTraceback (most recent call last):\r\n  File \"\/usr\/local\/lib\/python3.5\/threading.py\", line 914, in _bootstrap_inner\r\n    self.run()\r\n  File \"\/home\/datascience\/zh\/suijiesuihuan\/datascience\/my_threads.py\", line 87, in run\r\n    ret_data, result = make_decision(int(self.layerId), df_input)\r\n  File \"\/home\/datascience\/zh\/suijiesuihuan\/datascience\/layered_decision.py\", line 52, in make_decision\r\n    data_df, result_df = load_features_group_loader(features_group_loader_id, mocking)(data_df)\r\n  File \"\/home\/datascience\/zh\/suijiesuihuan\/datascience\/dynamic_content_loader.py\", line 166, in inner\r\n    return load_and_make_decision(data_df, config_new)\r\n  File \"\/home\/datascience\/zh\/suijiesuihuan\/datascience\/dynamic_content_loader.py\", line 111, in load_and_make_decision\r\n    new_feature_group_df, error_df = _load_data(pending_df, features_loader_id, features_loader, mocking)\r\n  File \"\/home\/datascience\/zh\/suijiesuihuan\/datascience\/dynamic_content_loader.py\", line 76, in _load_data\r\n    data_df, result_df = features_loader(data_df)\r\n  File \"\/home\/datascience\/zh\/suijiesuihuan\/dynamic_contents\/feature_group_loader\/feature_group_loader_0003\/feature_group_loader.py\", line 55, in load\r\n    call_number_list = row.get(Names.call.call_number_list)\r\n  File \"\/home\/datascience\/zh\/venv\/lib\/python3.5\/site-packages\/pandas\/core\/generic.py\", line 1633, in get\r\n    return self[key]\r\n  File \"\/home\/datascience\/zh\/venv\/lib\/python3.5\/site-packages\/pandas\/core\/series.py\", line 611, in __getitem__\r\n    dtype=self.dtype).__finalize__(self)\r\n  File \"\/home\/datascience\/zh\/venv\/lib\/python3.5\/site-packages\/pandas\/core\/series.py\", line 227, in __init__\r\n    \"\".format(data.__class__.__name__))\r\nTypeError: 'set' type is unordered\r\n \r\n#### someone knows what happened?\r\n",
    "labels":[
      "Bug",
      "Difficulty Intermediate",
      "Effort Medium",
      "Indexing"
    ],
    "comments":[
      "First, it is not clear at all that this is a bug.  A bug report typically contains a short, runnable example of the problem, and a description of why the observed behavior is wrong.\r\n\r\nI tried to reproduce this with pandas 0.19 but could not.\r\n```\r\nIn [1]: import pandas\r\n\r\nIn [2]: s = pandas.Series({'a': 0, 'b': 1})\r\n\r\nIn [3]: s.get(['b', 'a'])\r\nOut[3]:\r\nb    1\r\na    0\r\ndtype: int64\r\n\r\nIn [4]: s.get({'b', 'a'})  # note set\r\nOut[4]:\r\na    0\r\nb    1\r\ndtype: int64\r\n```\r\n\r\nCould you try calling `get` with a list argument, as in this?\r\n```\r\n        call_number_list = row.get(list(Names.call.call_number_list))\r\n```\r\n",
      "@gloryfromca : Also, could you provide version information (`pandas.show_versions`)?",
      "you can try codes below to reproduce the problem:\n\n>>> from pandas import Series\n>>> Series_contain_duplicates = Series({'1':333,'s':set([1,2,3])})\n>>> Series_contain_duplicates =\nSeries_contain_duplicates.append(Series({'1':2}))\n>>> Series_contain_duplicates['s']\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File\n\"\/home\/zhanghui\/py35\/lib\/python3.5\/site-packages\/pandas\/core\/series.py\",\nline 611, in __getitem__\n    dtype=self.dtype).__finalize__(self)\n  File\n\"\/home\/zhanghui\/py35\/lib\/python3.5\/site-packages\/pandas\/core\/series.py\",\nline 227, in __init__\n    \"\".format(data.__class__.__name__))\nTypeError: 'set' type is unordered\n\nIt's not normal behaviour, right? I'm very confused at this TypeError at\nfirst sight.\n\nmy colleague may find how the problems produce:\ncode in row608:if not self.index.is_unique:\n\nMaybe index.is_index is a wrong determination conditions ?\n\nThank you!\n\n\n\n\n2017-09-21 22:13 GMT+08:00 Nicholas Musolino <notifications@github.com>:\n\n> First, it is not clear at all that this is a bug. A bug report typically\n> contains a short, runnable example of the problem, and a description of why\n> the observed behavior is wrong.\n>\n> I tried to reproduce this with pandas 0.19 but could not.\n>\n> In [1]: import pandas\n>\n> In [2]: s = pandas.Series({'a': 0, 'b': 1})\n>\n> In [3]: s.get(['b', 'a'])\n> Out[3]:\n> b    1\n> a    0\n> dtype: int64\n>\n> In [4]: s.get({'b', 'a'})  # note set\n> Out[4]:\n> a    0\n> b    1\n> dtype: int64\n>\n> Could you try calling get with a list argument, as in this?\n>\n>         call_number_list = row.get(list(Names.call.call_number_list))\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/pandas-dev\/pandas\/issues\/17610#issuecomment-331168932>,\n> or mute the thread\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AWW152DsbehNPwYLz-khP8MPuCd3QXQlks5skm7rgaJpZM4PezI0>\n> .\n>\n",
      "Both 0.19 and 0.20 has this problems.\n\n2017-09-22 2:23 GMT+08:00 gfyoung <notifications@github.com>:\n\n> @gloryfromca <https:\/\/github.com\/gloryfromca> : Also, could you provide\n> version information?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/pandas-dev\/pandas\/issues\/17610#issuecomment-331240875>,\n> or mute the thread\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AWW151q9mN8LjbWT8kHSnP4-w8NA1xqEks5skqmvgaJpZM4PezI0>\n> .\n>\n",
      "I guess this is a bug, though I am not sure why you would ever do this. embedding non-scalars (e.g. a ``set``) is non-idiomatic. Using duplicate indices requires care as well.\r\n\r\nI'll mark it, but it would require a community pull request to fix.\r\n\r\n```\r\nIn [2]: s = Series({'1':333,'s':set([1,2,3])})\r\n\r\nIn [3]: s\r\nOut[3]: \r\n1          333\r\ns    {1, 2, 3}\r\ndtype: object\r\n\r\nIn [13]: s2 = s.append(Series({'1':2}))\r\n\r\nIn [14]: s2\r\nOut[14]: \r\n1          333\r\ns    {1, 2, 3}\r\n1            2\r\ndtype: object\r\n\r\nIn [15]: s2[1]\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-15-c2e7db717c6c> in <module>()\r\n----> 1 s2[1]\r\n\r\n~\/pandas\/pandas\/core\/series.py in __getitem__(self, key)\r\n    629                         result = self._constructor(\r\n    630                             result, index=[key] * len(result),\r\n--> 631                             dtype=self.dtype).__finalize__(self)\r\n    632 \r\n    633             return result\r\n\r\n~\/pandas\/pandas\/core\/series.py in __init__(self, data, index, dtype, name, copy, fastpath)\r\n    239             elif isinstance(data, (set, frozenset)):\r\n    240                 raise TypeError(\"{0!r} type is unordered\"\r\n--> 241                                 \"\".format(data.__class__.__name__))\r\n    242             else:\r\n    243 \r\n\r\nTypeError: 'set' type is unordered\r\n```",
      "This problem is mixture of coincidence and bad practice. I am beginner of\nGitHub, so What does a community pull request means? Will you fix it or\nShould I create a pull request for it ?\n\n2017-10-10 7:17 GMT+08:00 Jeff Reback <notifications@github.com>:\n\n> I guess this is a bug, though I am not sure why you would ever do this.\n> embedding non-scalars (e.g. a set) is non-idiomatic. Using duplicate\n> indices requires care as well.\n>\n> I'll mark it, but it would require a community pull request to fix.\n>\n> In [2]: s = Series({'1':333,'s':set([1,2,3])})\n>\n> In [3]: s\n> Out[3]:\n> 1          333\n> s    {1, 2, 3}\n> dtype: object\n>\n> In [13]: s2 = s.append(Series({'1':2}))\n>\n> In [14]: s2\n> Out[14]:\n> 1          333\n> s    {1, 2, 3}\n> 1            2\n> dtype: object\n>\n> In [15]: s2[1]\n> ---------------------------------------------------------------------------\n> TypeError                                 Traceback (most recent call last)\n> <ipython-input-15-c2e7db717c6c> in <module>()\n> ----> 1 s2[1]\n>\n> ~\/pandas\/pandas\/core\/series.py in __getitem__(self, key)\n>     629                         result = self._constructor(\n>     630                             result, index=[key] * len(result),\n> --> 631                             dtype=self.dtype).__finalize__(self)\n>     632\n>     633             return result\n>\n> ~\/pandas\/pandas\/core\/series.py in __init__(self, data, index, dtype, name, copy, fastpath)\n>     239             elif isinstance(data, (set, frozenset)):\n>     240                 raise TypeError(\"{0!r} type is unordered\"\n> --> 241                                 \"\".format(data.__class__.__name__))\n>     242             else:\n>     243\n>\n> TypeError: 'set' type is unordered\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/pandas-dev\/pandas\/issues\/17610#issuecomment-335314967>,\n> or mute the thread\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AWW154tj1jfoiGHbNe-dK2mGMRCoIF46ks5sqqlvgaJpZM4PezI0>\n> .\n>\n",
      "> Should I create a pull request for it ?\r\n\r\nsure, docs are: http:\/\/pandas.pydata.org\/pandas-docs\/stable\/contributing.html",
      "I fixed the bug, and ran asv benchmarks.Then It raised information as below:\nSOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.\nIs it OK? or the way I fixed the bug is unproper?\nPS:attachment is photo of benchmarks detail information.\n\n2017-10-10 11:19 GMT+08:00 Jeff Reback <notifications@github.com>:\n\n> Should I create a pull request for it ?\n>\n> sure, docs are: http:\/\/pandas.pydata.org\/pandas-docs\/stable\/\n> contributing.html\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/pandas-dev\/pandas\/issues\/17610#issuecomment-335348548>,\n> or mute the thread\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AWW1562eKtoronGip1xvvx3hfa-HXacfks5squIzgaJpZM4PezI0>\n> .\n>\n"
    ],
    "events":[
      "commented",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "unlabeled",
      "milestoned",
      "renamed",
      "commented",
      "commented",
      "commented",
      "cross-referenced",
      "cross-referenced",
      "referenced",
      "cross-referenced",
      "referenced",
      "referenced",
      "demilestoned",
      "milestoned",
      "referenced"
    ],
    "changed_files":3,
    "additions":30,
    "deletions":6
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":19526,
    "reporter":"jbandlow",
    "created_at":1517681605000,
    "closed_at":1517960975000,
    "resolver":"jbandlow",
    "resolved_in":"983d71fa8477439aaa227367d7f2f14952e4e235",
    "resolver_commit_num":0,
    "title":"Rounding errors with Timestamps and `.last()`",
    "body":"#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n#### Problem description\r\nThe value of the Timestamp in the output is not equal to the value in the input. \r\n\r\n#### Expected Output\r\n\r\nThe issue is a bit subtle. Here are three related examples, all of which **do** work as expected:\r\n\r\nI haven't been able to repro with `.nth(-1)` in place of `last()`:\r\n\r\n\r\n\r\nI haven't been able to repro if `NaT` is not present:\r\n\r\n\r\nI haven't been able to repro for times which are integer seconds:\r\n\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n\r\n<\/details>\r\n",
    "labels":[
      "Groupby",
      "Dtypes",
      "Difficulty Intermediate",
      "Effort Low"
    ],
    "comments":[
      "Can you reproduce without the groupby step?",
      "I can't repro without groupby.  I did a little more experimentation, and it appears that the set of aggregation functions that have the issue are exactly [these](https:\/\/github.com\/pandas-dev\/pandas\/blob\/v0.22.0\/pandas\/core\/groupby.py#L1290-L1295) (`max`, `min`, `first`, `last`).",
      "this should not be going thru a float round trip which is the symptom you are seeing\r\nif you want to have a more detailed look and see where the issue is",
      "Thanks for the hint @jreback . The offending block seems to be [here](https:\/\/github.com\/pandas-dev\/pandas\/blob\/v0.22.0\/pandas\/core\/groupby.py#L2279-L2283) where the integer result is getting cast as float.  The coercion back to a datetime happens [in here](https:\/\/github.com\/pandas-dev\/pandas\/blob\/v0.22.0\/pandas\/core\/groupby.py#L3674), and from what I can tell, that \"does the right thing\" with `iNaT`.\r\n\r\nThis is my first look at pandas internals, but if the fix is likely to be just ripping out those lines and adding a test, I can probably manage to put a PR together.",
      "it\u2019s not likely to be \u2018ripping it out\u2019 but pls have a look"
    ],
    "events":[
      "commented",
      "commented",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "referenced",
      "referenced",
      "cross-referenced",
      "labeled",
      "labeled",
      "labeled",
      "labeled",
      "milestoned",
      "referenced"
    ],
    "changed_files":3,
    "additions":20,
    "deletions":2
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":25487,
    "reporter":"WillAyd",
    "created_at":1551392510000,
    "closed_at":1561927986000,
    "resolver":"yehia67",
    "resolved_in":"bd49d2f2af5c44d4f96031757743ba83e6b29408",
    "resolver_commit_num":0,
    "title":"DOC: Force Removal of pandas From Dependencies",
    "body":"From a discussion on Gitter with @TomAugspurger when following the pandas contributing guide the step:\r\n\r\n\r\n\r\nCauses pandas to be installed in the environment from conda as a result of dependency resolution with third party libraries (ex: statsmodels, pyarrow, seaborn). \r\n\r\nTo prevent undesired conflicts between the local development version and the installed version we should update the documentation to have an explicit removal of the bundled version as such:\r\n\r\n",
    "labels":[
      "Docs",
      "good first issue"
    ],
    "comments":[
      "So you want to Update this section \r\n```\r\n# Create and activate the build environment\r\nconda env create -f environment.yml\r\nconda activate pandas-dev\r\n```\r\n\r\nto be like that?\r\n\r\n```\r\n# Create and activate the build environment\r\nconda uninstall --force pandas\r\nconda env create -f environment.yml\r\nconda activate pandas-dev\r\n```\r\n",
      "Swap the first and second lines but yes\n\nSent from my iPhone\n\n> On Feb 28, 2019, at 2:45 PM, yehia67 <notifications@github.com> wrote:\n> \n> So you want to Update this section\n> \n> # Create and activate the build environment\n> conda env create -f environment.yml\n> conda activate pandas-dev\n> to be like that?\n> \n> # Create and activate the build environment\n> conda uninstall --force pandas\n> conda env create -f environment.yml\n> conda activate pandas-dev\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n",
      "okay",
      "I am not sure this is actually needed to recommend. It's true that conda does not work very nice with `-e` installed dev packages, but by force removing it, you might also run into troubles when updating \/ installing other packages that depend on pandas (as they don't find pandas, and conda then might install pandas again shadowing the dev version. Not sure this is what happens though).\r\n\r\nThe latest conda release with better pip integration enabled also solves this, by actually recognizing the dev installed package. But I think this is not yet enabled by default I think.",
      "I think the common case is \"setup an env from scratch\", in which case the\nold way would guarantee a double-install of pandas, right?\n\nOn Mon, Mar 4, 2019 at 7:54 AM Joris Van den Bossche <\nnotifications@github.com> wrote:\n\n> I am not sure this is actually needed to recommend. It's true that conda\n> does not work very nice with -e installed dev packages, but by force\n> removing it, you might also run into troubles when updating \/ installing\n> other packages that depend on pandas (as they don't find pandas, and conda\n> then might install pandas again shadowing the dev version. Not sure this is\n> what happens though).\n>\n> The latest conda release with better pip integration enabled also solves\n> this, by actually recognizing the dev installed package. But I think this\n> is not yet enabled by default I think.\n>\n> \u2014\n> You are receiving this because you modified the open\/close state.\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/pandas-dev\/pandas\/issues\/25487#issuecomment-469260527>,\n> or mute the thread\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/ABQHIpt4iINt-v63E_T7valTRTxXFKQQks5vTSWLgaJpZM4bX3FA>\n> .\n>\n",
      "Yes, but you typically don't keep this new env as is for the rest of your contributing lifetime no? (or at least, I update my dev environment from time to time in place, not each time re-creating it from scratch). What happens when I eg update seaborn that depends on pandas, if pandas was force-removed like this?",
      "Yes I think so. Will conda overwrite a locally installed pandas? If so,\nthen yes, we should maybe revert this.\n\nOn Mon, Mar 4, 2019 at 8:04 AM Joris Van den Bossche <\nnotifications@github.com> wrote:\n\n> Yes, but you typically don't keep this new env as is for the rest of your\n> contributing lifetime no? (or at least, I update my dev environment from\n> time to time in place, not each time re-creating it from scratch). What\n> happens when I eg update seaborn that depends on pandas, if pandas was\n> force-removed like this?\n>\n> \u2014\n> You are receiving this because you modified the open\/close state.\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/pandas-dev\/pandas\/issues\/25487#issuecomment-469263657>,\n> or mute the thread\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/ABQHIm1DfO4hkkmt93nyHT0ir0NuRdmmks5vTSfRgaJpZM4bX3FA>\n> .\n>\n",
      "It seems so. Eg doing `conda install seaborn` in the example below would have installed pandas 0.24.1 again. \r\nDoing a `conda config --set pip_interop_enabled True` seems to prevent that however (but with that option, you would probably also not have to force remove pandas in the first place). Maybe we should recommend doing that instead.\r\n\r\n<details>\r\n\r\n```\r\n(base) joris@joris-XPS-13-9350:~\/scipy$ conda create -n test-pandas python=3.7 pandas cython\r\nCollecting package metadata: done\r\nSolving environment: done\r\n\r\n## Package Plan ##\r\n\r\n  environment location: \/home\/joris\/miniconda3\/envs\/test-pandas\r\n\r\n  added \/ updated specs:\r\n    - cython\r\n    - pandas\r\n    - python=3.7\r\n\r\n\r\nThe following packages will be downloaded:\r\n\r\n    package                    |            build\r\n    ---------------------------|-----------------\r\n    cython-0.29.6              |   py37hf484d3e_0         2.2 MB  conda-forge\r\n    numpy-1.16.2               |py37_blas_openblash1522bff_0         4.3 MB  conda-forge\r\n    openssl-1.1.1b             |       h14c3975_0         4.0 MB  conda-forge\r\n    python-3.7.1               |    h381d211_1002        36.4 MB  conda-forge\r\n    ------------------------------------------------------------\r\n                                           Total:        46.8 MB\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n  blas               conda-forge\/linux-64::blas-1.1-openblas\r\n  bzip2              conda-forge\/linux-64::bzip2-1.0.6-h14c3975_1002\r\n  ca-certificates    conda-forge\/linux-64::ca-certificates-2018.11.29-ha4d7672_0\r\n  certifi            conda-forge\/linux-64::certifi-2018.11.29-py37_1000\r\n  cython             conda-forge\/linux-64::cython-0.29.6-py37hf484d3e_0\r\n  libffi             conda-forge\/linux-64::libffi-3.2.1-hf484d3e_1005\r\n  libgcc-ng          conda-forge\/linux-64::libgcc-ng-7.3.0-hdf63c60_0\r\n  libgfortran-ng     conda-forge\/linux-64::libgfortran-ng-7.2.0-hdf63c60_3\r\n  libstdcxx-ng       conda-forge\/linux-64::libstdcxx-ng-7.3.0-hdf63c60_0\r\n  ncurses            conda-forge\/linux-64::ncurses-6.1-hf484d3e_1002\r\n  numpy              conda-forge\/linux-64::numpy-1.16.2-py37_blas_openblash1522bff_0\r\n  openblas           conda-forge\/linux-64::openblas-0.3.3-h9ac9557_1001\r\n  openssl            conda-forge\/linux-64::openssl-1.1.1b-h14c3975_0\r\n  pandas             conda-forge\/linux-64::pandas-0.24.1-py37hf484d3e_0\r\n  pip                conda-forge\/linux-64::pip-19.0.3-py37_0\r\n  python             conda-forge\/linux-64::python-3.7.1-h381d211_1002\r\n  python-dateutil    conda-forge\/noarch::python-dateutil-2.8.0-py_0\r\n  pytz               conda-forge\/noarch::pytz-2018.9-py_0\r\n  readline           conda-forge\/linux-64::readline-7.0-hf8c457e_1001\r\n  setuptools         conda-forge\/linux-64::setuptools-40.8.0-py37_0\r\n  six                conda-forge\/linux-64::six-1.12.0-py37_1000\r\n  sqlite             conda-forge\/linux-64::sqlite-3.26.0-h67949de_1000\r\n  tk                 conda-forge\/linux-64::tk-8.6.9-h84994c4_1000\r\n  wheel              conda-forge\/linux-64::wheel-0.33.1-py37_0\r\n  xz                 conda-forge\/linux-64::xz-5.2.4-h14c3975_1001\r\n  zlib               conda-forge\/linux-64::zlib-1.2.11-h14c3975_1004\r\n\r\n\r\nProceed ([y]\/n)? \r\n\r\n\r\nDownloading and Extracting Packages\r\npython-3.7.1         | 36.4 MB   | ##################################################################################################################################### | 100% \r\nnumpy-1.16.2         | 4.3 MB    | ##################################################################################################################################### | 100% \r\nopenssl-1.1.1b       | 4.0 MB    | ##################################################################################################################################### | 100% \r\ncython-0.29.6        | 2.2 MB    | ##################################################################################################################################### | 100% \r\nPreparing transaction: done\r\nVerifying transaction: done\r\nExecuting transaction: done\r\n#\r\n# To activate this environment, use\r\n#\r\n#     $ conda activate test-pandas\r\n#\r\n# To deactivate an active environment, use\r\n#\r\n#     $ conda deactivate\r\n\r\n(base) joris@joris-XPS-13-9350:~\/scipy$ act test-pandas\r\n(test-pandas) joris@joris-XPS-13-9350:~\/scipy$ conda uninstall --force pandas\r\n\r\n## Package Plan ##\r\n\r\n  environment location: \/home\/joris\/miniconda3\/envs\/test-pandas\r\n\r\n  removed specs:\r\n    - pandas\r\n\r\n\r\nThe following packages will be REMOVED:\r\n\r\n  pandas-0.24.1-py37hf484d3e_0\r\n\r\n\r\nProceed ([y]\/n)? \r\n\r\nPreparing transaction: done\r\nVerifying transaction: done\r\nExecuting transaction: done\r\n\r\n(test-pandas) joris@joris-XPS-13-9350:~\/scipy\/pandas$ pip install -e .\r\nObtaining file:\/\/\/home\/joris\/scipy\/pandas\r\nRequirement already satisfied: python-dateutil>=2.5.0 in \/home\/joris\/miniconda3\/envs\/test-pandas\/lib\/python3.7\/site-packages (from pandas==0.25.0.dev0+192.gbd49d2f2a) (2.8.0)\r\nRequirement already satisfied: pytz>=2011k in \/home\/joris\/miniconda3\/envs\/test-pandas\/lib\/python3.7\/site-packages (from pandas==0.25.0.dev0+192.gbd49d2f2a) (2018.9)\r\nRequirement already satisfied: numpy>=1.12.0 in \/home\/joris\/miniconda3\/envs\/test-pandas\/lib\/python3.7\/site-packages (from pandas==0.25.0.dev0+192.gbd49d2f2a) (1.16.2)\r\nRequirement already satisfied: six>=1.5 in \/home\/joris\/miniconda3\/envs\/test-pandas\/lib\/python3.7\/site-packages (from python-dateutil>=2.5.0->pandas==0.25.0.dev0+192.gbd49d2f2a) (1.12.0)\r\nInstalling collected packages: pandas\r\n  Running setup.py develop for pandas\r\nSuccessfully installed pandas\r\n(test-pandas) joris@joris-XPS-13-9350:~\/scipy\/pandas$ conda list\r\n# packages in environment at \/home\/joris\/miniconda3\/envs\/test-pandas:\r\n#\r\n# Name                    Version                   Build  Channel\r\nblas                      1.1                    openblas    conda-forge\r\nbzip2                     1.0.6             h14c3975_1002    conda-forge\r\nca-certificates           2018.11.29           ha4d7672_0    conda-forge\r\ncertifi                   2018.11.29            py37_1000    conda-forge\r\ncython                    0.29.6           py37hf484d3e_0    conda-forge\r\nlibffi                    3.2.1             hf484d3e_1005    conda-forge\r\nlibgcc-ng                 7.3.0                hdf63c60_0    conda-forge\r\nlibgfortran-ng            7.2.0                hdf63c60_3    conda-forge\r\nlibstdcxx-ng              7.3.0                hdf63c60_0    conda-forge\r\nncurses                   6.1               hf484d3e_1002    conda-forge\r\nnumpy                     1.16.2          py37_blas_openblash1522bff_0  [blas_openblas]  conda-forge\r\nopenblas                  0.3.3             h9ac9557_1001    conda-forge\r\nopenssl                   1.1.1b               h14c3975_0    conda-forge\r\npandas                    0.25.0.dev0+192.gbd49d2f2a           dev_0    <develop>\r\npip                       19.0.3                   py37_0    conda-forge\r\npython                    3.7.1             h381d211_1002    conda-forge\r\npython-dateutil           2.8.0                      py_0    conda-forge\r\npytz                      2018.9                     py_0    conda-forge\r\nreadline                  7.0               hf8c457e_1001    conda-forge\r\nsetuptools                40.8.0                   py37_0    conda-forge\r\nsix                       1.12.0                py37_1000    conda-forge\r\nsqlite                    3.26.0            h67949de_1000    conda-forge\r\ntk                        8.6.9             h84994c4_1000    conda-forge\r\nwheel                     0.33.1                   py37_0    conda-forge\r\nxz                        5.2.4             h14c3975_1001    conda-forge\r\nzlib                      1.2.11            h14c3975_1004    conda-forge\r\n(test-pandas) joris@joris-XPS-13-9350:~\/scipy\/pandas$ conda install seaborn\r\nCollecting package metadata: done\r\nSolving environment: done\r\n\r\n## Package Plan ##\r\n\r\n  environment location: \/home\/joris\/miniconda3\/envs\/test-pandas\r\n\r\n  added \/ updated specs:\r\n    - seaborn\r\n\r\n\r\nThe following packages will be downloaded:\r\n\r\n    package                    |            build\r\n    ---------------------------|-----------------\r\n    matplotlib-3.0.3           |           py37_0           6 KB  conda-forge\r\n    matplotlib-base-3.0.3      |   py37h167e16e_0         6.6 MB  conda-forge\r\n    tornado-6.0.1              |   py37h14c3975_0         635 KB  conda-forge\r\n    ------------------------------------------------------------\r\n                                           Total:         7.2 MB\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n  atk                conda-forge\/linux-64::atk-2.25.90-hb9dd440_1002\r\n  cairo              conda-forge\/linux-64::cairo-1.16.0-ha4e643d_1000\r\n  cycler             conda-forge\/noarch::cycler-0.10.0-py_1\r\n  dbus               conda-forge\/linux-64::dbus-1.13.0-h4e0c4b3_1000\r\n  expat              conda-forge\/linux-64::expat-2.2.5-hf484d3e_1002\r\n  fontconfig         conda-forge\/linux-64::fontconfig-2.13.1-h2176d3f_1000\r\n  freetype           conda-forge\/linux-64::freetype-2.9.1-h94bbf69_1005\r\n  gdk-pixbuf         conda-forge\/linux-64::gdk-pixbuf-2.36.12-h49783d7_1002\r\n  gettext            conda-forge\/linux-64::gettext-0.19.8.1-h9745a5d_1001\r\n  glib               conda-forge\/linux-64::glib-2.58.2-hf63aee3_1001\r\n  gobject-introspec~ conda-forge\/linux-64::gobject-introspection-1.58.2-py37h2da5eee_1000\r\n  graphite2          conda-forge\/linux-64::graphite2-1.3.13-hf484d3e_1000\r\n  gstreamer          conda-forge\/linux-64::gstreamer-1.14.4-h66beb1c_1001\r\n  gtk2               conda-forge\/linux-64::gtk2-2.24.31-hb68c50a_1001\r\n  harfbuzz           conda-forge\/linux-64::harfbuzz-2.3.1-h6824563_0\r\n  icu                conda-forge\/linux-64::icu-58.2-hf484d3e_1000\r\n  jpeg               conda-forge\/linux-64::jpeg-9c-h14c3975_1001\r\n  kiwisolver         conda-forge\/linux-64::kiwisolver-1.0.1-py37h6bb024c_1002\r\n  libiconv           conda-forge\/linux-64::libiconv-1.15-h14c3975_1004\r\n  libpng             conda-forge\/linux-64::libpng-1.6.36-h84994c4_1000\r\n  libtiff            conda-forge\/linux-64::libtiff-4.0.10-h9022e91_1002\r\n  libuuid            conda-forge\/linux-64::libuuid-2.32.1-h14c3975_1000\r\n  libxcb             conda-forge\/linux-64::libxcb-1.13-h14c3975_1002\r\n  libxml2            conda-forge\/linux-64::libxml2-2.9.8-h143f9aa_1005\r\n  matplotlib         conda-forge\/linux-64::matplotlib-3.0.3-py37_0\r\n  matplotlib-base    conda-forge\/linux-64::matplotlib-base-3.0.3-py37h167e16e_0\r\n  pandas             conda-forge\/linux-64::pandas-0.24.1-py37hf484d3e_0\r\n  pango              conda-forge\/linux-64::pango-1.40.14-h4ea9474_1004\r\n  patsy              conda-forge\/noarch::patsy-0.5.1-py_0\r\n  pcre               conda-forge\/linux-64::pcre-8.41-hf484d3e_1003\r\n  pixman             conda-forge\/linux-64::pixman-0.34.0-h14c3975_1003\r\n  pthread-stubs      conda-forge\/linux-64::pthread-stubs-0.4-h14c3975_1001\r\n  pyparsing          conda-forge\/noarch::pyparsing-2.3.1-py_0\r\n  pyqt               conda-forge\/linux-64::pyqt-5.6.0-py37h13b7fb3_1008\r\n  qt                 conda-forge\/linux-64::qt-5.6.2-hce4f676_1013\r\n  scipy              conda-forge\/linux-64::scipy-1.2.1-py37_blas_openblash1522bff_0\r\n  seaborn            conda-forge\/noarch::seaborn-0.9.0-py_0\r\n  sip                conda-forge\/linux-64::sip-4.18.1-py37hf484d3e_1000\r\n  statsmodels        conda-forge\/linux-64::statsmodels-0.9.0-py37h3010b51_1000\r\n  tornado            conda-forge\/linux-64::tornado-6.0.1-py37h14c3975_0\r\n  xorg-kbproto       conda-forge\/linux-64::xorg-kbproto-1.0.7-h14c3975_1002\r\n  xorg-libice        conda-forge\/linux-64::xorg-libice-1.0.9-h14c3975_1004\r\n  xorg-libsm         conda-forge\/linux-64::xorg-libsm-1.2.3-h4937e3b_1000\r\n  xorg-libx11        conda-forge\/linux-64::xorg-libx11-1.6.7-h14c3975_1000\r\n  xorg-libxau        conda-forge\/linux-64::xorg-libxau-1.0.9-h14c3975_0\r\n  xorg-libxdmcp      conda-forge\/linux-64::xorg-libxdmcp-1.1.2-h14c3975_1007\r\n  xorg-libxext       conda-forge\/linux-64::xorg-libxext-1.3.3-h14c3975_1004\r\n  xorg-libxrender    conda-forge\/linux-64::xorg-libxrender-0.9.10-h14c3975_1002\r\n  xorg-libxt         conda-forge\/linux-64::xorg-libxt-1.1.5-h14c3975_1002\r\n  xorg-renderproto   conda-forge\/linux-64::xorg-renderproto-0.11.1-h14c3975_1002\r\n  xorg-xextproto     conda-forge\/linux-64::xorg-xextproto-7.3.0-h14c3975_1002\r\n  xorg-xproto        conda-forge\/linux-64::xorg-xproto-7.0.31-h14c3975_1007\r\n  zstd               conda-forge\/linux-64::zstd-1.3.3-1\r\n\r\nThe following packages will be DOWNGRADED:\r\n\r\n  openssl                                 1.1.1b-h14c3975_0 --> 1.0.2r-h14c3975_0\r\n  python                                3.7.1-h381d211_1002 --> 3.7.1-hd21baee_1001\r\n\r\n\r\nProceed ([y]\/n)? n\r\n\r\n\r\nExiting.\r\n\r\n```\r\n\r\n\r\n<\/details>",
      "Thanks for checking. Recommending that conda config seems OK to me. Happy\nto volunteer our contributors a guinea pigs for conda :)\n\nOn Mon, Mar 4, 2019 at 8:59 AM Joris Van den Bossche <\nnotifications@github.com> wrote:\n\n> It seems so. Eg doing conda install seaborn in the example below would\n> have installed pandas 0.24.1 again.\n> Doing a conda config --set pip_interop_enabled True seems to prevent that\n> however (but with that option, you would probably also not have to force\n> remove pandas in the first place). Maybe we should recommend doing that\n> instead.\n>\n> (base) joris@joris-XPS-13-9350:~\/scipy$ conda create -n test-pandas python=3.7 pandas cython\n> Collecting package metadata: done\n> Solving environment: done\n>\n> ## Package Plan ##\n>\n>   environment location: \/home\/joris\/miniconda3\/envs\/test-pandas\n>\n>   added \/ updated specs:\n>     - cython\n>     - pandas\n>     - python=3.7\n>\n>\n> The following packages will be downloaded:\n>\n>     package                    |            build\n>     ---------------------------|-----------------\n>     cython-0.29.6              |   py37hf484d3e_0         2.2 MB  conda-forge\n>     numpy-1.16.2               |py37_blas_openblash1522bff_0         4.3 MB  conda-forge\n>     openssl-1.1.1b             |       h14c3975_0         4.0 MB  conda-forge\n>     python-3.7.1               |    h381d211_1002        36.4 MB  conda-forge\n>     ------------------------------------------------------------\n>                                            Total:        46.8 MB\n>\n> The following NEW packages will be INSTALLED:\n>\n>   blas               conda-forge\/linux-64::blas-1.1-openblas\n>   bzip2              conda-forge\/linux-64::bzip2-1.0.6-h14c3975_1002\n>   ca-certificates    conda-forge\/linux-64::ca-certificates-2018.11.29-ha4d7672_0\n>   certifi            conda-forge\/linux-64::certifi-2018.11.29-py37_1000\n>   cython             conda-forge\/linux-64::cython-0.29.6-py37hf484d3e_0\n>   libffi             conda-forge\/linux-64::libffi-3.2.1-hf484d3e_1005\n>   libgcc-ng          conda-forge\/linux-64::libgcc-ng-7.3.0-hdf63c60_0\n>   libgfortran-ng     conda-forge\/linux-64::libgfortran-ng-7.2.0-hdf63c60_3\n>   libstdcxx-ng       conda-forge\/linux-64::libstdcxx-ng-7.3.0-hdf63c60_0\n>   ncurses            conda-forge\/linux-64::ncurses-6.1-hf484d3e_1002\n>   numpy              conda-forge\/linux-64::numpy-1.16.2-py37_blas_openblash1522bff_0\n>   openblas           conda-forge\/linux-64::openblas-0.3.3-h9ac9557_1001\n>   openssl            conda-forge\/linux-64::openssl-1.1.1b-h14c3975_0\n>   pandas             conda-forge\/linux-64::pandas-0.24.1-py37hf484d3e_0\n>   pip                conda-forge\/linux-64::pip-19.0.3-py37_0\n>   python             conda-forge\/linux-64::python-3.7.1-h381d211_1002\n>   python-dateutil    conda-forge\/noarch::python-dateutil-2.8.0-py_0\n>   pytz               conda-forge\/noarch::pytz-2018.9-py_0\n>   readline           conda-forge\/linux-64::readline-7.0-hf8c457e_1001\n>   setuptools         conda-forge\/linux-64::setuptools-40.8.0-py37_0\n>   six                conda-forge\/linux-64::six-1.12.0-py37_1000\n>   sqlite             conda-forge\/linux-64::sqlite-3.26.0-h67949de_1000\n>   tk                 conda-forge\/linux-64::tk-8.6.9-h84994c4_1000\n>   wheel              conda-forge\/linux-64::wheel-0.33.1-py37_0\n>   xz                 conda-forge\/linux-64::xz-5.2.4-h14c3975_1001\n>   zlib               conda-forge\/linux-64::zlib-1.2.11-h14c3975_1004\n>\n>\n> Proceed ([y]\/n)?\n>\n>\n> Downloading and Extracting Packages\n> python-3.7.1         | 36.4 MB   | ##################################################################################################################################### | 100%\n> numpy-1.16.2         | 4.3 MB    | ##################################################################################################################################### | 100%\n> openssl-1.1.1b       | 4.0 MB    | ##################################################################################################################################### | 100%\n> cython-0.29.6        | 2.2 MB    | ##################################################################################################################################### | 100%\n> Preparing transaction: done\n> Verifying transaction: done\n> Executing transaction: done\n> #\n> # To activate this environment, use\n> #\n> #     $ conda activate test-pandas\n> #\n> # To deactivate an active environment, use\n> #\n> #     $ conda deactivate\n>\n> (base) joris@joris-XPS-13-9350:~\/scipy$ act test-pandas\n> (test-pandas) joris@joris-XPS-13-9350:~\/scipy$ conda uninstall --force pandas\n>\n> ## Package Plan ##\n>\n>   environment location: \/home\/joris\/miniconda3\/envs\/test-pandas\n>\n>   removed specs:\n>     - pandas\n>\n>\n> The following packages will be REMOVED:\n>\n>   pandas-0.24.1-py37hf484d3e_0\n>\n>\n> Proceed ([y]\/n)?\n>\n> Preparing transaction: done\n> Verifying transaction: done\n> Executing transaction: done\n>\n> (test-pandas) joris@joris-XPS-13-9350:~\/scipy\/pandas$ pip install -e .\n> Obtaining file:\/\/\/home\/joris\/scipy\/pandas\n> Requirement already satisfied: python-dateutil>=2.5.0 in \/home\/joris\/miniconda3\/envs\/test-pandas\/lib\/python3.7\/site-packages (from pandas==0.25.0.dev0+192.gbd49d2f2a) (2.8.0)\n> Requirement already satisfied: pytz>=2011k in \/home\/joris\/miniconda3\/envs\/test-pandas\/lib\/python3.7\/site-packages (from pandas==0.25.0.dev0+192.gbd49d2f2a) (2018.9)\n> Requirement already satisfied: numpy>=1.12.0 in \/home\/joris\/miniconda3\/envs\/test-pandas\/lib\/python3.7\/site-packages (from pandas==0.25.0.dev0+192.gbd49d2f2a) (1.16.2)\n> Requirement already satisfied: six>=1.5 in \/home\/joris\/miniconda3\/envs\/test-pandas\/lib\/python3.7\/site-packages (from python-dateutil>=2.5.0->pandas==0.25.0.dev0+192.gbd49d2f2a) (1.12.0)\n> Installing collected packages: pandas\n>   Running setup.py develop for pandas\n> Successfully installed pandas\n> (test-pandas) joris@joris-XPS-13-9350:~\/scipy\/pandas$ conda list\n> # packages in environment at \/home\/joris\/miniconda3\/envs\/test-pandas:\n> #\n> # Name                    Version                   Build  Channel\n> blas                      1.1                    openblas    conda-forge\n> bzip2                     1.0.6             h14c3975_1002    conda-forge\n> ca-certificates           2018.11.29           ha4d7672_0    conda-forge\n> certifi                   2018.11.29            py37_1000    conda-forge\n> cython                    0.29.6           py37hf484d3e_0    conda-forge\n> libffi                    3.2.1             hf484d3e_1005    conda-forge\n> libgcc-ng                 7.3.0                hdf63c60_0    conda-forge\n> libgfortran-ng            7.2.0                hdf63c60_3    conda-forge\n> libstdcxx-ng              7.3.0                hdf63c60_0    conda-forge\n> ncurses                   6.1               hf484d3e_1002    conda-forge\n> numpy                     1.16.2          py37_blas_openblash1522bff_0  [blas_openblas]  conda-forge\n> openblas                  0.3.3             h9ac9557_1001    conda-forge\n> openssl                   1.1.1b               h14c3975_0    conda-forge\n> pandas                    0.25.0.dev0+192.gbd49d2f2a           dev_0    <develop>\n> pip                       19.0.3                   py37_0    conda-forge\n> python                    3.7.1             h381d211_1002    conda-forge\n> python-dateutil           2.8.0                      py_0    conda-forge\n> pytz                      2018.9                     py_0    conda-forge\n> readline                  7.0               hf8c457e_1001    conda-forge\n> setuptools                40.8.0                   py37_0    conda-forge\n> six                       1.12.0                py37_1000    conda-forge\n> sqlite                    3.26.0            h67949de_1000    conda-forge\n> tk                        8.6.9             h84994c4_1000    conda-forge\n> wheel                     0.33.1                   py37_0    conda-forge\n> xz                        5.2.4             h14c3975_1001    conda-forge\n> zlib                      1.2.11            h14c3975_1004    conda-forge\n> (test-pandas) joris@joris-XPS-13-9350:~\/scipy\/pandas$ conda install seaborn\n> Collecting package metadata: done\n> Solving environment: done\n>\n> ## Package Plan ##\n>\n>   environment location: \/home\/joris\/miniconda3\/envs\/test-pandas\n>\n>   added \/ updated specs:\n>     - seaborn\n>\n>\n> The following packages will be downloaded:\n>\n>     package                    |            build\n>     ---------------------------|-----------------\n>     matplotlib-3.0.3           |           py37_0           6 KB  conda-forge\n>     matplotlib-base-3.0.3      |   py37h167e16e_0         6.6 MB  conda-forge\n>     tornado-6.0.1              |   py37h14c3975_0         635 KB  conda-forge\n>     ------------------------------------------------------------\n>                                            Total:         7.2 MB\n>\n> The following NEW packages will be INSTALLED:\n>\n>   atk                conda-forge\/linux-64::atk-2.25.90-hb9dd440_1002\n>   cairo              conda-forge\/linux-64::cairo-1.16.0-ha4e643d_1000\n>   cycler             conda-forge\/noarch::cycler-0.10.0-py_1\n>   dbus               conda-forge\/linux-64::dbus-1.13.0-h4e0c4b3_1000\n>   expat              conda-forge\/linux-64::expat-2.2.5-hf484d3e_1002\n>   fontconfig         conda-forge\/linux-64::fontconfig-2.13.1-h2176d3f_1000\n>   freetype           conda-forge\/linux-64::freetype-2.9.1-h94bbf69_1005\n>   gdk-pixbuf         conda-forge\/linux-64::gdk-pixbuf-2.36.12-h49783d7_1002\n>   gettext            conda-forge\/linux-64::gettext-0.19.8.1-h9745a5d_1001\n>   glib               conda-forge\/linux-64::glib-2.58.2-hf63aee3_1001\n>   gobject-introspec~ conda-forge\/linux-64::gobject-introspection-1.58.2-py37h2da5eee_1000\n>   graphite2          conda-forge\/linux-64::graphite2-1.3.13-hf484d3e_1000\n>   gstreamer          conda-forge\/linux-64::gstreamer-1.14.4-h66beb1c_1001\n>   gtk2               conda-forge\/linux-64::gtk2-2.24.31-hb68c50a_1001\n>   harfbuzz           conda-forge\/linux-64::harfbuzz-2.3.1-h6824563_0\n>   icu                conda-forge\/linux-64::icu-58.2-hf484d3e_1000\n>   jpeg               conda-forge\/linux-64::jpeg-9c-h14c3975_1001\n>   kiwisolver         conda-forge\/linux-64::kiwisolver-1.0.1-py37h6bb024c_1002\n>   libiconv           conda-forge\/linux-64::libiconv-1.15-h14c3975_1004\n>   libpng             conda-forge\/linux-64::libpng-1.6.36-h84994c4_1000\n>   libtiff            conda-forge\/linux-64::libtiff-4.0.10-h9022e91_1002\n>   libuuid            conda-forge\/linux-64::libuuid-2.32.1-h14c3975_1000\n>   libxcb             conda-forge\/linux-64::libxcb-1.13-h14c3975_1002\n>   libxml2            conda-forge\/linux-64::libxml2-2.9.8-h143f9aa_1005\n>   matplotlib         conda-forge\/linux-64::matplotlib-3.0.3-py37_0\n>   matplotlib-base    conda-forge\/linux-64::matplotlib-base-3.0.3-py37h167e16e_0\n>   pandas             conda-forge\/linux-64::pandas-0.24.1-py37hf484d3e_0\n>   pango              conda-forge\/linux-64::pango-1.40.14-h4ea9474_1004\n>   patsy              conda-forge\/noarch::patsy-0.5.1-py_0\n>   pcre               conda-forge\/linux-64::pcre-8.41-hf484d3e_1003\n>   pixman             conda-forge\/linux-64::pixman-0.34.0-h14c3975_1003\n>   pthread-stubs      conda-forge\/linux-64::pthread-stubs-0.4-h14c3975_1001\n>   pyparsing          conda-forge\/noarch::pyparsing-2.3.1-py_0\n>   pyqt               conda-forge\/linux-64::pyqt-5.6.0-py37h13b7fb3_1008\n>   qt                 conda-forge\/linux-64::qt-5.6.2-hce4f676_1013\n>   scipy              conda-forge\/linux-64::scipy-1.2.1-py37_blas_openblash1522bff_0\n>   seaborn            conda-forge\/noarch::seaborn-0.9.0-py_0\n>   sip                conda-forge\/linux-64::sip-4.18.1-py37hf484d3e_1000\n>   statsmodels        conda-forge\/linux-64::statsmodels-0.9.0-py37h3010b51_1000\n>   tornado            conda-forge\/linux-64::tornado-6.0.1-py37h14c3975_0\n>   xorg-kbproto       conda-forge\/linux-64::xorg-kbproto-1.0.7-h14c3975_1002\n>   xorg-libice        conda-forge\/linux-64::xorg-libice-1.0.9-h14c3975_1004\n>   xorg-libsm         conda-forge\/linux-64::xorg-libsm-1.2.3-h4937e3b_1000\n>   xorg-libx11        conda-forge\/linux-64::xorg-libx11-1.6.7-h14c3975_1000\n>   xorg-libxau        conda-forge\/linux-64::xorg-libxau-1.0.9-h14c3975_0\n>   xorg-libxdmcp      conda-forge\/linux-64::xorg-libxdmcp-1.1.2-h14c3975_1007\n>   xorg-libxext       conda-forge\/linux-64::xorg-libxext-1.3.3-h14c3975_1004\n>   xorg-libxrender    conda-forge\/linux-64::xorg-libxrender-0.9.10-h14c3975_1002\n>   xorg-libxt         conda-forge\/linux-64::xorg-libxt-1.1.5-h14c3975_1002\n>   xorg-renderproto   conda-forge\/linux-64::xorg-renderproto-0.11.1-h14c3975_1002\n>   xorg-xextproto     conda-forge\/linux-64::xorg-xextproto-7.3.0-h14c3975_1002\n>   xorg-xproto        conda-forge\/linux-64::xorg-xproto-7.0.31-h14c3975_1007\n>   zstd               conda-forge\/linux-64::zstd-1.3.3-1\n>\n> The following packages will be DOWNGRADED:\n>\n>   openssl                                 1.1.1b-h14c3975_0 --> 1.0.2r-h14c3975_0\n>   python                                3.7.1-h381d211_1002 --> 3.7.1-hd21baee_1001\n>\n>\n> Proceed ([y]\/n)? n\n>\n>\n> Exiting.\n>\n>\n> \u2014\n> You are receiving this because you modified the open\/close state.\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/pandas-dev\/pandas\/issues\/25487#issuecomment-469283114>,\n> or mute the thread\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/ABQHInqI3nkO9vYfytt2ClfPVfPN128Vks5vTTTmgaJpZM4bX3FA>\n> .\n>\n",
      "Is this issue still open?",
      "@rajat315315 It says it's going to be closed when the commit hksonngan did on March, 12th is merged to master (check the information icon to the right of the commit entry)",
      "It was reopened afterwards, because we are not fully sure it was actually the good solution."
    ],
    "events":[
      "labeled",
      "labeled",
      "mentioned",
      "subscribed",
      "commented",
      "commented",
      "commented",
      "referenced",
      "cross-referenced",
      "milestoned",
      "closed",
      "referenced",
      "commented",
      "commented",
      "commented",
      "commented",
      "commented",
      "reopened",
      "commented",
      "referenced",
      "commented",
      "commented",
      "mentioned",
      "subscribed",
      "commented",
      "demilestoned",
      "milestoned",
      "demilestoned",
      "milestoned",
      "cross-referenced"
    ],
    "changed_files":1,
    "additions":1,
    "deletions":0
  },
  {
    "owner":"pandas-dev",
    "name":"pandas",
    "number":37705,
    "reporter":"simon-spier0",
    "created_at":1604868454000,
    "closed_at":1605061816000,
    "resolver":"inspurwusixuan",
    "resolved_in":"ee1b75c5af60ab4cd7b8c757ce1ca9ef3aef7505",
    "resolver_commit_num":0,
    "title":"BUG: read_html - file path cannot be pathlib.Path type",
    "body":"\r\n\r\nWhy do `read_excel()`, `read_csv()`, `to_excel()`, `to_csv()`, ... support `pathlib.Path` while `read_html()` doesn't?",
    "labels":[
      "Bug",
      "IO HTML",
      "good first issue"
    ],
    "comments":[
      "thanks @simon-spier0 for the report!\r\n\r\nAn easy fix would be to call `stringify_path` (from pandas\/io\/common.py) in `read_html`.",
      "> thanks @simon-spier0 for the report!\r\n> \r\n> An easy fix would be to call `stringify_path` (from pandas\/io\/common.py) in `read_html`.\r\n\r\nHi, I'm new here and want to start my first contribution. Can I take this issue? :)  @twoertwein \r\n\r\nThanks!",
      "@inspurwusixuan  of course!"
    ],
    "events":[
      "labeled",
      "labeled",
      "unlabeled",
      "labeled",
      "commented",
      "mentioned",
      "subscribed",
      "labeled",
      "milestoned",
      "commented",
      "mentioned",
      "subscribed",
      "mentioned",
      "subscribed",
      "commented",
      "mentioned",
      "subscribed",
      "cross-referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced",
      "referenced"
    ],
    "changed_files":3,
    "additions":14,
    "deletions":1
  }
]